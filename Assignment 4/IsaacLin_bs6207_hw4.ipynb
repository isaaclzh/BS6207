{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89391681",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6609f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import image as mp_image\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5670391",
   "metadata": {},
   "source": [
    "## Import pytorch libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a37b0010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646db35b",
   "metadata": {},
   "source": [
    "## Training folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ba7f618",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = '/Users/isaaclin/OneDrive/BS6207/4/train'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b50007",
   "metadata": {},
   "source": [
    "## Testing folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48ec0c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folder = '/Users/isaaclin/OneDrive/BS6207/4/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28ff17b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'artifacts', 'cancer_regions', 'normal_regions', 'other']\n"
     ]
    }
   ],
   "source": [
    "classes = sorted(os.listdir(train_folder))\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6a25ff",
   "metadata": {},
   "source": [
    "## Prepare and augment train and validation datasets (70%-30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3815e8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(data_path):\n",
    "\n",
    "    # Load all the images\n",
    "    # Transform the images\n",
    "    transformation = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(0.5),\n",
    "        transforms.RandomVerticalFlip(0.3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "    # Load all of the images, transforming them\n",
    "    full_dataset = torchvision.datasets.ImageFolder(\n",
    "        root=data_path,\n",
    "        transform=transformation\n",
    "    )\n",
    "    \n",
    "    # Split into training 70% and validation 30% datasets\n",
    "    train_size = int(0.7 * len(full_dataset))\n",
    "    validation_size = len(full_dataset) - train_size\n",
    "    \n",
    "    # use torch.utils.data.random_split for training/validation split\n",
    "    train_dataset, validation_dataset = torch.utils.data.random_split(full_dataset, [train_size, validation_size])\n",
    "    \n",
    "    # define a loader for the training data we can iterate through in 40-image batches\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=40,\n",
    "        num_workers=0,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # define a loader for the validation data we can iterate through in 40-image batches\n",
    "    validation_loader = torch.utils.data.DataLoader(\n",
    "        validation_dataset,\n",
    "        batch_size=40,\n",
    "        num_workers=0,\n",
    "        shuffle=False\n",
    "    )\n",
    "        \n",
    "    return train_loader, validation_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dc28754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaders ready to read /Users/isaaclin/OneDrive/BS6207/4/train\n"
     ]
    }
   ],
   "source": [
    "train_loader, validation_loader = load_dataset(train_folder)\n",
    "batch_size = train_loader.batch_size\n",
    "print(\"Data loaders ready to read\", train_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c00f86d",
   "metadata": {},
   "source": [
    "## Define CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b794310",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes=3):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Our images are RGB, so we have input channels = 3. \n",
    "        # We will apply 12 filters in the first convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # A second convolutional layer takes 12 input channels, and generates 24 outputs\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # We in the end apply max pooling with a kernel size of 2\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # A drop layer deletes 20% of the features to help prevent overfitting\n",
    "        self.drop = nn.Dropout2d(p=0.2)\n",
    "        \n",
    "        # Our 128x128 image tensors will be pooled twice with a kernel size of 2. 128/2/2 is 32.\n",
    "        # This means that our feature tensors are now 32 x 32, and we've generated 24 of them\n",
    "        \n",
    "        # We need to flatten these in order to feed them to a fully-connected layer\n",
    "        self.fc = nn.Linear(in_features=32 * 32 * 24, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # In the forward function, pass the data through the layers we defined in the init function\n",
    "        # Use a ReLU activation function after layer 1 (convolution 1 and pool)\n",
    "        x = F.relu(self.pool(self.conv1(x))) \n",
    "        \n",
    "        # Use a ReLU activation function after layer 2\n",
    "        x = F.relu(self.pool(self.conv2(x)))  \n",
    "        \n",
    "        # Select some features to drop to prevent overfitting (only drop during training)\n",
    "        x = F.dropout(self.drop(x), training=self.training)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(-1, 32 * 32 * 24)\n",
    "        # Feed to fully-connected layer to predict class\n",
    "        x = self.fc(x)\n",
    "        # Return class probabilities via a log_softmax function \n",
    "        return torch.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e786a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (drop): Dropout2d(p=0.2, inplace=False)\n",
      "  (fc): Linear(in_features=24576, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the model class and allocate it to the device\n",
    "model = Net(num_classes=len(classes))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16da219f",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f96c8190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, epoch, batch_no):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    print(\"Epoch:\", epoch)\n",
    "    # Process the images in batches\n",
    "    torch.manual_seed(0)\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # set_batch\n",
    "        batch = batch_idx + 1\n",
    "        # Reset the optimizer\n",
    "        optimizer.zero_grad()     \n",
    "        # Push the data forward through the model layers\n",
    "        output = model(data)       \n",
    "        # Get the loss\n",
    "        loss = loss_criteria(output, target)\n",
    "        # Keep a running total\n",
    "        train_loss += loss.item()       \n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        optimizer.step()     \n",
    "        # Print metrics \n",
    "        print('\\tTraining batch {} Loss: {:.6f}'.format(batch_idx + 1, loss.item()))\n",
    "        if batch == batch_no:\n",
    "            break\n",
    "            \n",
    "    # return average loss for the epoch\n",
    "    avg_loss = train_loss / (batch_idx+1)\n",
    "    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13424695",
   "metadata": {},
   "source": [
    "## Validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f75bf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, validation_loader):\n",
    "    # Switch the model to evaluation mode \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        batch_count = 0\n",
    "        for data, target in validation_loader:\n",
    "            batch_count += 1          \n",
    "            # Get the predicted classes for this batch\n",
    "            output = model(data)            \n",
    "            # Calculate the loss for this batch\n",
    "            test_loss += loss_criteria(output, target).item()            \n",
    "            # Calculate the accuracy for this batch\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            correct += torch.sum(target==predicted).item()\n",
    "\n",
    "    validation_predict = 100. * correct / len(validation_loader.dataset)        \n",
    "    # Calculate the average loss and total accuracy for this epoch\n",
    "    avg_loss = test_loss / batch_count\n",
    "    print('Validation set: Average loss: {:.6f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        avg_loss, correct, len(validation_loader.dataset),\n",
    "        validation_predict))\n",
    "    \n",
    "    # return average loss for the epoch\n",
    "    return avg_loss, validation_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79053e1",
   "metadata": {},
   "source": [
    "## Training the model (Test run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c25e7731",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 1.610274\n",
      "Training set: Average loss: 1.610274\n",
      "Validation set: Average loss: 7.206017, Accuracy: 672/1959 (34.30%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 3.206706\n",
      "Training set: Average loss: 3.206706\n",
      "Validation set: Average loss: 10.556764, Accuracy: 849/1959 (43.34%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 5.296914\n",
      "Training set: Average loss: 5.296914\n",
      "Validation set: Average loss: 2.361781, Accuracy: 1392/1959 (71.06%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.093465\n",
      "Training set: Average loss: 0.093465\n",
      "Validation set: Average loss: 2.766865, Accuracy: 954/1959 (48.70%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.237943\n",
      "Training set: Average loss: 0.237943\n",
      "Validation set: Average loss: 3.495619, Accuracy: 866/1959 (44.21%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.329319\n",
      "Training set: Average loss: 0.329319\n",
      "Validation set: Average loss: 3.606985, Accuracy: 844/1959 (43.08%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.158345\n",
      "Training set: Average loss: 0.158345\n",
      "Validation set: Average loss: 3.103109, Accuracy: 1143/1959 (58.35%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.121666\n",
      "Training set: Average loss: 0.121666\n",
      "Validation set: Average loss: 3.484703, Accuracy: 1238/1959 (63.20%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.027147\n",
      "Training set: Average loss: 0.027147\n",
      "Validation set: Average loss: 4.013347, Accuracy: 1316/1959 (67.18%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.016579\n",
      "Training set: Average loss: 0.016579\n",
      "Validation set: Average loss: 4.482293, Accuracy: 1356/1959 (69.22%)\n",
      "\n",
      "Epoch: 11\n",
      "\tTraining batch 1 Loss: 0.010901\n",
      "Training set: Average loss: 0.010901\n",
      "Validation set: Average loss: 4.730822, Accuracy: 1364/1959 (69.63%)\n",
      "\n",
      "Epoch: 12\n",
      "\tTraining batch 1 Loss: 0.001055\n",
      "Training set: Average loss: 0.001055\n",
      "Validation set: Average loss: 4.862437, Accuracy: 1370/1959 (69.93%)\n",
      "\n",
      "Epoch: 13\n",
      "\tTraining batch 1 Loss: 0.000219\n",
      "Training set: Average loss: 0.000219\n",
      "Validation set: Average loss: 4.916044, Accuracy: 1376/1959 (70.24%)\n",
      "\n",
      "Epoch: 14\n",
      "\tTraining batch 1 Loss: 0.000172\n",
      "Training set: Average loss: 0.000172\n",
      "Validation set: Average loss: 4.926700, Accuracy: 1378/1959 (70.34%)\n",
      "\n",
      "Epoch: 15\n",
      "\tTraining batch 1 Loss: 0.000207\n",
      "Training set: Average loss: 0.000207\n",
      "Validation set: Average loss: 4.972760, Accuracy: 1377/1959 (70.29%)\n",
      "\n",
      "Epoch: 16\n",
      "\tTraining batch 1 Loss: 0.000294\n",
      "Training set: Average loss: 0.000294\n",
      "Validation set: Average loss: 5.173086, Accuracy: 1335/1959 (68.15%)\n",
      "\n",
      "Epoch: 17\n",
      "\tTraining batch 1 Loss: 0.000454\n",
      "Training set: Average loss: 0.000454\n",
      "Validation set: Average loss: 5.524893, Accuracy: 1290/1959 (65.85%)\n",
      "\n",
      "Epoch: 18\n",
      "\tTraining batch 1 Loss: 0.000602\n",
      "Training set: Average loss: 0.000602\n",
      "Validation set: Average loss: 5.896811, Accuracy: 1259/1959 (64.27%)\n",
      "\n",
      "Epoch: 19\n",
      "\tTraining batch 1 Loss: 0.000364\n",
      "Training set: Average loss: 0.000364\n",
      "Validation set: Average loss: 6.230392, Accuracy: 1255/1959 (64.06%)\n",
      "\n",
      "Epoch: 20\n",
      "\tTraining batch 1 Loss: 0.000158\n",
      "Training set: Average loss: 0.000158\n",
      "Validation set: Average loss: 6.541452, Accuracy: 1259/1959 (64.27%)\n",
      "\n",
      "Epoch: 21\n",
      "\tTraining batch 1 Loss: 0.000078\n",
      "Training set: Average loss: 0.000078\n",
      "Validation set: Average loss: 6.841750, Accuracy: 1269/1959 (64.78%)\n",
      "\n",
      "Epoch: 22\n",
      "\tTraining batch 1 Loss: 0.000043\n",
      "Training set: Average loss: 0.000043\n",
      "Validation set: Average loss: 7.135330, Accuracy: 1284/1959 (65.54%)\n",
      "\n",
      "Epoch: 23\n",
      "\tTraining batch 1 Loss: 0.000024\n",
      "Training set: Average loss: 0.000024\n",
      "Validation set: Average loss: 7.422975, Accuracy: 1290/1959 (65.85%)\n",
      "\n",
      "Epoch: 24\n",
      "\tTraining batch 1 Loss: 0.000014\n",
      "Training set: Average loss: 0.000014\n",
      "Validation set: Average loss: 7.703891, Accuracy: 1293/1959 (66.00%)\n",
      "\n",
      "Epoch: 25\n",
      "\tTraining batch 1 Loss: 0.000008\n",
      "Training set: Average loss: 0.000008\n",
      "Validation set: Average loss: 7.976723, Accuracy: 1304/1959 (66.56%)\n",
      "\n",
      "Epoch: 26\n",
      "\tTraining batch 1 Loss: 0.000005\n",
      "Training set: Average loss: 0.000005\n",
      "Validation set: Average loss: 8.240111, Accuracy: 1309/1959 (66.82%)\n",
      "\n",
      "Epoch: 27\n",
      "\tTraining batch 1 Loss: 0.000003\n",
      "Training set: Average loss: 0.000003\n",
      "Validation set: Average loss: 8.492881, Accuracy: 1308/1959 (66.77%)\n",
      "\n",
      "Epoch: 28\n",
      "\tTraining batch 1 Loss: 0.000002\n",
      "Training set: Average loss: 0.000002\n",
      "Validation set: Average loss: 8.734001, Accuracy: 1311/1959 (66.92%)\n",
      "\n",
      "Epoch: 29\n",
      "\tTraining batch 1 Loss: 0.000001\n",
      "Training set: Average loss: 0.000001\n",
      "Validation set: Average loss: 8.962773, Accuracy: 1313/1959 (67.02%)\n",
      "\n",
      "Epoch: 30\n",
      "\tTraining batch 1 Loss: 0.000001\n",
      "Training set: Average loss: 0.000001\n",
      "Validation set: Average loss: 9.178814, Accuracy: 1312/1959 (66.97%)\n",
      "\n",
      "Epoch: 31\n",
      "\tTraining batch 1 Loss: 0.000001\n",
      "Training set: Average loss: 0.000001\n",
      "Validation set: Average loss: 9.382002, Accuracy: 1311/1959 (66.92%)\n",
      "\n",
      "Epoch: 32\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "Training set: Average loss: 0.000000\n",
      "Validation set: Average loss: 9.572405, Accuracy: 1313/1959 (67.02%)\n",
      "\n",
      "Epoch: 33\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "Training set: Average loss: 0.000000\n",
      "Validation set: Average loss: 9.750240, Accuracy: 1315/1959 (67.13%)\n",
      "\n",
      "Epoch: 34\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "Training set: Average loss: 0.000000\n",
      "Validation set: Average loss: 9.915861, Accuracy: 1317/1959 (67.23%)\n",
      "\n",
      "Epoch: 35\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "Training set: Average loss: 0.000000\n",
      "Validation set: Average loss: 10.069758, Accuracy: 1315/1959 (67.13%)\n",
      "\n",
      "Epoch: 36\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "Training set: Average loss: 0.000000\n",
      "Validation set: Average loss: 10.212419, Accuracy: 1314/1959 (67.08%)\n",
      "\n",
      "Epoch: 37\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "Training set: Average loss: 0.000000\n",
      "Validation set: Average loss: 10.344405, Accuracy: 1316/1959 (67.18%)\n",
      "\n",
      "Epoch: 38\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "Training set: Average loss: 0.000000\n",
      "Validation set: Average loss: 10.466300, Accuracy: 1316/1959 (67.18%)\n",
      "\n",
      "Epoch: 39\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "Training set: Average loss: 0.000000\n",
      "Validation set: Average loss: 10.578701, Accuracy: 1313/1959 (67.02%)\n",
      "\n",
      "Epoch: 40\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "Training set: Average loss: 0.000000\n",
      "Validation set: Average loss: 10.682195, Accuracy: 1313/1959 (67.02%)\n",
      "\n",
      "Epoch: 41\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "Training set: Average loss: 0.000000\n",
      "Validation set: Average loss: 10.777361, Accuracy: 1314/1959 (67.08%)\n",
      "\n",
      "Epoch: 42\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "Training set: Average loss: 0.000000\n",
      "Validation set: Average loss: 10.864772, Accuracy: 1313/1959 (67.02%)\n",
      "\n",
      "Epoch: 43\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "Training set: Average loss: 0.000000\n",
      "Validation set: Average loss: 10.944973, Accuracy: 1311/1959 (66.92%)\n",
      "\n",
      "Epoch: 44\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "Training set: Average loss: 0.000000\n",
      "Validation set: Average loss: 11.018480, Accuracy: 1312/1959 (66.97%)\n",
      "\n",
      "Epoch: 45\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "Training set: Average loss: 0.000000\n",
      "Validation set: Average loss: 11.085794, Accuracy: 1313/1959 (67.02%)\n",
      "\n",
      "Epoch: 46\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "Training set: Average loss: 0.000000\n",
      "Validation set: Average loss: 11.147387, Accuracy: 1310/1959 (66.87%)\n",
      "\n",
      "Epoch: 47\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "Training set: Average loss: 0.000000\n",
      "Validation set: Average loss: 11.203703, Accuracy: 1310/1959 (66.87%)\n",
      "\n",
      "Epoch: 48\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "Training set: Average loss: 0.000000\n",
      "Validation set: Average loss: 11.255163, Accuracy: 1309/1959 (66.82%)\n",
      "\n",
      "Epoch: 49\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "Training set: Average loss: 0.000000\n",
      "Validation set: Average loss: 11.302154, Accuracy: 1310/1959 (66.87%)\n",
      "\n",
      "Epoch: 50\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "Training set: Average loss: 0.000000\n",
      "Validation set: Average loss: 11.345042, Accuracy: 1310/1959 (66.87%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use an \"Adam\" optimizer to adjust weights\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "# Specify the loss criteria\n",
    "loss_criteria = nn.CrossEntropyLoss()\n",
    "\n",
    "# Track metrics in these arrays\n",
    "epoch_nums = []\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "# Train over 50 epochs\n",
    "epochs = 50\n",
    "batch_no = 1\n",
    "early_loss = 0.0\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train(model, train_loader, optimizer, epoch, batch_no)\n",
    "    test_loss, validate_predict = validation(model, validation_loader)\n",
    "    epoch_nums.append(epoch)\n",
    "    training_loss.append(train_loss)\n",
    "    validation_loss.append(test_loss)\n",
    "    #print('Early loss', early_loss)\n",
    "    #print('Test loss', test_loss)\n",
    "    #if round(early_loss,2) == round(test_loss,2):\n",
    "    #    print(\"Early stopping\")\n",
    "    #    break\n",
    "    #early_loss = test_loss  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd20e4c6",
   "metadata": {},
   "source": [
    "## View Loss History (Test run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f234afae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(14,10))\n",
    "plt.plot(epoch_nums, training_loss)\n",
    "plt.plot(epoch_nums, validation_loss)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['training', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbd016b",
   "metadata": {},
   "source": [
    "## Prepare and augment test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40408407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_testset(data_path):\n",
    "    \n",
    "    # Load all the images\n",
    "    # Transform the images\n",
    "    transformation = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(0.5),\n",
    "        transforms.RandomVerticalFlip(0.3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "    # Load all of the images, transforming them\n",
    "    test_dataset = torchvision.datasets.ImageFolder(\n",
    "        root=data_path,\n",
    "        transform=transformation\n",
    "    )\n",
    "    \n",
    "    # define a loader for the test data we can iterate through in 40-image batches\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        num_workers=0,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1205b9",
   "metadata": {},
   "source": [
    "## Labels and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b6b7087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaders ready to read /Users/isaaclin/OneDrive/BS6207/4/test\n"
     ]
    }
   ],
   "source": [
    "test_loader = load_testset(test_folder)\n",
    "print(\"Data loaders ready to read\", test_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f08c645",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "Training set: Average loss: 0.000000\n",
      "Validation set: Average loss: 59.063335, Accuracy: 1383/1959 (70.60%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "Training set: Average loss: 0.000000\n",
      "Validation set: Average loss: 59.934030, Accuracy: 1384/1959 (70.65%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "Training set: Average loss: 0.000000\n",
      "Validation set: Average loss: 60.480829, Accuracy: 1384/1959 (70.65%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 46.5%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 53.180378\n",
      "Training set: Average loss: 26.590189\n",
      "Validation set: Average loss: 31.328044, Accuracy: 1434/1959 (73.20%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 29.693628\n",
      "Training set: Average loss: 14.846814\n",
      "Validation set: Average loss: 46.200915, Accuracy: 924/1959 (47.17%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 17.340406\n",
      "Training set: Average loss: 8.670203\n",
      "Validation set: Average loss: 46.923559, Accuracy: 1070/1959 (54.62%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 3.324649\n",
      "Training set: Average loss: 1.662324\n",
      "Validation set: Average loss: 37.837949, Accuracy: 1194/1959 (60.95%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 1.056494\n",
      "Training set: Average loss: 0.528247\n",
      "Validation set: Average loss: 30.582485, Accuracy: 1289/1959 (65.80%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.796168\n",
      "Training set: Average loss: 0.398084\n",
      "Validation set: Average loss: 26.013430, Accuracy: 1335/1959 (68.15%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.259062\n",
      "Training set: Average loss: 0.129531\n",
      "Validation set: Average loss: 22.312874, Accuracy: 1366/1959 (69.73%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000115\n",
      "Training set: Average loss: 0.000058\n",
      "Validation set: Average loss: 19.570641, Accuracy: 1383/1959 (70.60%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "Training set: Average loss: 0.000000\n",
      "Validation set: Average loss: 17.658000, Accuracy: 1399/1959 (71.41%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.128994\n",
      "Training set: Average loss: 0.064497\n",
      "Validation set: Average loss: 16.399089, Accuracy: 1398/1959 (71.36%)\n",
      "\n",
      "Epoch: 11\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000002\n",
      "Training set: Average loss: 0.000001\n",
      "Validation set: Average loss: 15.625186, Accuracy: 1387/1959 (70.80%)\n",
      "\n",
      "Epoch: 12\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000001\n",
      "Training set: Average loss: 0.000000\n",
      "Validation set: Average loss: 15.104999, Accuracy: 1369/1959 (69.88%)\n",
      "\n",
      "Epoch: 13\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "Training set: Average loss: 0.000000\n",
      "Validation set: Average loss: 14.753830, Accuracy: 1340/1959 (68.40%)\n",
      "\n",
      "Epoch: 14\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "Training set: Average loss: 0.000000\n",
      "Validation set: Average loss: 14.512093, Accuracy: 1318/1959 (67.28%)\n",
      "\n",
      "Epoch: 15\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "Training set: Average loss: 0.000000\n",
      "Validation set: Average loss: 14.324511, Accuracy: 1304/1959 (66.56%)\n",
      "\n",
      "Epoch: 16\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "Training set: Average loss: 0.000000\n",
      "Validation set: Average loss: 14.168016, Accuracy: 1298/1959 (66.26%)\n",
      "\n",
      "Epoch: 17\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "Training set: Average loss: 0.000000\n",
      "Validation set: Average loss: 14.037470, Accuracy: 1294/1959 (66.05%)\n",
      "\n",
      "Epoch: 18\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "Training set: Average loss: 0.000000\n",
      "Validation set: Average loss: 13.930303, Accuracy: 1287/1959 (65.70%)\n",
      "\n",
      "Epoch: 19\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "Training set: Average loss: 0.000000\n",
      "Validation set: Average loss: 13.842967, Accuracy: 1285/1959 (65.59%)\n",
      "\n",
      "Epoch: 20\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "Training set: Average loss: 0.000000\n",
      "Validation set: Average loss: 13.771893, Accuracy: 1275/1959 (65.08%)\n",
      "\n",
      "Epoch: 21\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "Training set: Average loss: 0.000000\n",
      "Validation set: Average loss: 13.714019, Accuracy: 1272/1959 (64.93%)\n",
      "\n",
      "Epoch: 22\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "Training set: Average loss: 0.000000\n",
      "Validation set: Average loss: 13.666860, Accuracy: 1265/1959 (64.57%)\n",
      "\n",
      "Epoch: 23\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "Training set: Average loss: 0.000000\n",
      "Validation set: Average loss: 13.628390, Accuracy: 1262/1959 (64.42%)\n",
      "\n",
      "Epoch: 24\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "Training set: Average loss: 0.000000\n",
      "Validation set: Average loss: 13.596979, Accuracy: 1260/1959 (64.32%)\n",
      "\n",
      "Epoch: 25\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "Training set: Average loss: 0.000000\n",
      "Validation set: Average loss: 13.571304, Accuracy: 1259/1959 (64.27%)\n",
      "\n",
      "Epoch: 26\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "Training set: Average loss: 0.000000\n",
      "Validation set: Average loss: 13.550300, Accuracy: 1259/1959 (64.27%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 45.25%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 23.745953\n",
      "Training set: Average loss: 7.915318\n",
      "Validation set: Average loss: 10.802475, Accuracy: 1395/1959 (71.21%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 8.896870\n",
      "Training set: Average loss: 2.965623\n",
      "Validation set: Average loss: 8.340190, Accuracy: 1419/1959 (72.43%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000007\n",
      "\tTraining batch 3 Loss: 3.260225\n",
      "Training set: Average loss: 1.086744\n",
      "Validation set: Average loss: 7.480730, Accuracy: 1291/1959 (65.90%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.021372\n",
      "\tTraining batch 3 Loss: 0.612198\n",
      "Training set: Average loss: 0.211190\n",
      "Validation set: Average loss: 7.139526, Accuracy: 1371/1959 (69.98%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000001\n",
      "\tTraining batch 3 Loss: 0.122840\n",
      "Training set: Average loss: 0.040947\n",
      "Validation set: Average loss: 7.683544, Accuracy: 1433/1959 (73.15%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000003\n",
      "\tTraining batch 3 Loss: 0.036999\n",
      "Training set: Average loss: 0.012334\n",
      "Validation set: Average loss: 7.895056, Accuracy: 1446/1959 (73.81%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000004\n",
      "\tTraining batch 3 Loss: 0.035387\n",
      "Training set: Average loss: 0.011797\n",
      "Validation set: Average loss: 7.942093, Accuracy: 1445/1959 (73.76%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000007\n",
      "\tTraining batch 3 Loss: 0.034446\n",
      "Training set: Average loss: 0.011484\n",
      "Validation set: Average loss: 7.870462, Accuracy: 1449/1959 (73.97%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000010\n",
      "\tTraining batch 3 Loss: 0.033737\n",
      "Training set: Average loss: 0.011249\n",
      "Validation set: Average loss: 7.745620, Accuracy: 1448/1959 (73.92%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000013\n",
      "\tTraining batch 3 Loss: 0.033126\n",
      "Training set: Average loss: 0.011047\n",
      "Validation set: Average loss: 7.565535, Accuracy: 1448/1959 (73.92%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 48.25%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000017\n",
      "\tTraining batch 3 Loss: 0.032526\n",
      "\tTraining batch 4 Loss: 8.072836\n",
      "Training set: Average loss: 2.026345\n",
      "Validation set: Average loss: 5.217385, Accuracy: 1423/1959 (72.64%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000002\n",
      "\tTraining batch 2 Loss: 0.005642\n",
      "\tTraining batch 3 Loss: 0.226828\n",
      "\tTraining batch 4 Loss: 2.991128\n",
      "Training set: Average loss: 0.805900\n",
      "Validation set: Average loss: 3.128513, Accuracy: 1144/1959 (58.40%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.001158\n",
      "\tTraining batch 2 Loss: 0.030688\n",
      "\tTraining batch 3 Loss: 0.098595\n",
      "\tTraining batch 4 Loss: 1.480466\n",
      "Training set: Average loss: 0.402727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set: Average loss: 1.899030, Accuracy: 1276/1959 (65.14%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000340\n",
      "\tTraining batch 2 Loss: 0.143683\n",
      "\tTraining batch 3 Loss: 0.056802\n",
      "\tTraining batch 4 Loss: 0.992354\n",
      "Training set: Average loss: 0.298295\n",
      "Validation set: Average loss: 1.375687, Accuracy: 1330/1959 (67.89%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.001412\n",
      "\tTraining batch 2 Loss: 0.151069\n",
      "\tTraining batch 3 Loss: 0.166361\n",
      "\tTraining batch 4 Loss: 0.452170\n",
      "Training set: Average loss: 0.192753\n",
      "Validation set: Average loss: 1.332851, Accuracy: 1071/1959 (54.67%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.013197\n",
      "\tTraining batch 2 Loss: 0.034916\n",
      "\tTraining batch 3 Loss: 0.043482\n",
      "\tTraining batch 4 Loss: 0.629066\n",
      "Training set: Average loss: 0.180165\n",
      "Validation set: Average loss: 1.284515, Accuracy: 1408/1959 (71.87%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.007537\n",
      "\tTraining batch 2 Loss: 0.008781\n",
      "\tTraining batch 3 Loss: 0.041412\n",
      "\tTraining batch 4 Loss: 0.038069\n",
      "Training set: Average loss: 0.023950\n",
      "Validation set: Average loss: 1.245126, Accuracy: 1397/1959 (71.31%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.003434\n",
      "\tTraining batch 2 Loss: 0.014736\n",
      "\tTraining batch 3 Loss: 0.037770\n",
      "\tTraining batch 4 Loss: 0.021789\n",
      "Training set: Average loss: 0.019432\n",
      "Validation set: Average loss: 1.457680, Accuracy: 1410/1959 (71.98%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.001521\n",
      "\tTraining batch 2 Loss: 0.009360\n",
      "\tTraining batch 3 Loss: 0.031289\n",
      "\tTraining batch 4 Loss: 0.013862\n",
      "Training set: Average loss: 0.014008\n",
      "Validation set: Average loss: 1.733771, Accuracy: 1429/1959 (72.95%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000716\n",
      "\tTraining batch 2 Loss: 0.005419\n",
      "\tTraining batch 3 Loss: 0.027988\n",
      "\tTraining batch 4 Loss: 0.009811\n",
      "Training set: Average loss: 0.010983\n",
      "Validation set: Average loss: 1.995575, Accuracy: 1431/1959 (73.05%)\n",
      "\n",
      "Epoch: 11\n",
      "\tTraining batch 1 Loss: 0.000379\n",
      "\tTraining batch 2 Loss: 0.003307\n",
      "\tTraining batch 3 Loss: 0.025923\n",
      "\tTraining batch 4 Loss: 0.007516\n",
      "Training set: Average loss: 0.009281\n",
      "Validation set: Average loss: 2.210328, Accuracy: 1431/1959 (73.05%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 49.25%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000237\n",
      "\tTraining batch 2 Loss: 0.002268\n",
      "\tTraining batch 3 Loss: 0.024140\n",
      "\tTraining batch 4 Loss: 0.005961\n",
      "\tTraining batch 5 Loss: 3.203984\n",
      "Training set: Average loss: 0.647318\n",
      "Validation set: Average loss: 2.037113, Accuracy: 1445/1959 (73.76%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000505\n",
      "\tTraining batch 2 Loss: 0.012396\n",
      "\tTraining batch 3 Loss: 0.026460\n",
      "\tTraining batch 4 Loss: 0.015396\n",
      "\tTraining batch 5 Loss: 1.021289\n",
      "Training set: Average loss: 0.215209\n",
      "Validation set: Average loss: 1.382781, Accuracy: 1391/1959 (71.01%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.009361\n",
      "\tTraining batch 2 Loss: 0.068547\n",
      "\tTraining batch 3 Loss: 0.044829\n",
      "\tTraining batch 4 Loss: 0.044578\n",
      "\tTraining batch 5 Loss: 0.383089\n",
      "Training set: Average loss: 0.110081\n",
      "Validation set: Average loss: 1.157233, Accuracy: 1403/1959 (71.62%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.019130\n",
      "\tTraining batch 2 Loss: 0.072903\n",
      "\tTraining batch 3 Loss: 0.040550\n",
      "\tTraining batch 4 Loss: 0.035327\n",
      "\tTraining batch 5 Loss: 0.174185\n",
      "Training set: Average loss: 0.068419\n",
      "Validation set: Average loss: 1.305442, Accuracy: 1420/1959 (72.49%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.021380\n",
      "\tTraining batch 2 Loss: 0.048306\n",
      "\tTraining batch 3 Loss: 0.024050\n",
      "\tTraining batch 4 Loss: 0.020137\n",
      "\tTraining batch 5 Loss: 0.086863\n",
      "Training set: Average loss: 0.040147\n",
      "Validation set: Average loss: 1.417886, Accuracy: 1439/1959 (73.46%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.008940\n",
      "\tTraining batch 2 Loss: 0.023216\n",
      "\tTraining batch 3 Loss: 0.014524\n",
      "\tTraining batch 4 Loss: 0.013288\n",
      "\tTraining batch 5 Loss: 0.053432\n",
      "Training set: Average loss: 0.022680\n",
      "Validation set: Average loss: 1.624347, Accuracy: 1428/1959 (72.89%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.003537\n",
      "\tTraining batch 2 Loss: 0.011027\n",
      "\tTraining batch 3 Loss: 0.009343\n",
      "\tTraining batch 4 Loss: 0.009210\n",
      "\tTraining batch 5 Loss: 0.036266\n",
      "Training set: Average loss: 0.013877\n",
      "Validation set: Average loss: 1.813111, Accuracy: 1420/1959 (72.49%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.001561\n",
      "\tTraining batch 2 Loss: 0.005731\n",
      "\tTraining batch 3 Loss: 0.006335\n",
      "\tTraining batch 4 Loss: 0.006670\n",
      "\tTraining batch 5 Loss: 0.026996\n",
      "Training set: Average loss: 0.009459\n",
      "Validation set: Average loss: 1.964571, Accuracy: 1416/1959 (72.28%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000844\n",
      "\tTraining batch 2 Loss: 0.003413\n",
      "\tTraining batch 3 Loss: 0.004430\n",
      "\tTraining batch 4 Loss: 0.005070\n",
      "\tTraining batch 5 Loss: 0.021605\n",
      "Training set: Average loss: 0.007073\n",
      "Validation set: Average loss: 2.080974, Accuracy: 1415/1959 (72.23%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000544\n",
      "\tTraining batch 2 Loss: 0.002303\n",
      "\tTraining batch 3 Loss: 0.003148\n",
      "\tTraining batch 4 Loss: 0.003989\n",
      "\tTraining batch 5 Loss: 0.018192\n",
      "Training set: Average loss: 0.005635\n",
      "Validation set: Average loss: 2.172098, Accuracy: 1415/1959 (72.23%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 49.0%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000393\n",
      "\tTraining batch 2 Loss: 0.001719\n",
      "\tTraining batch 3 Loss: 0.002297\n",
      "\tTraining batch 4 Loss: 0.003249\n",
      "\tTraining batch 5 Loss: 0.015847\n",
      "\tTraining batch 6 Loss: 2.829633\n",
      "Training set: Average loss: 0.475523\n",
      "Validation set: Average loss: 1.785092, Accuracy: 1418/1959 (72.38%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000464\n",
      "\tTraining batch 2 Loss: 0.003728\n",
      "\tTraining batch 3 Loss: 0.010554\n",
      "\tTraining batch 4 Loss: 0.028010\n",
      "\tTraining batch 5 Loss: 0.754973\n",
      "\tTraining batch 6 Loss: 0.898745\n",
      "Training set: Average loss: 0.282746\n",
      "Validation set: Average loss: 1.091054, Accuracy: 1371/1959 (69.98%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.003070\n",
      "\tTraining batch 2 Loss: 0.012680\n",
      "\tTraining batch 3 Loss: 0.012816\n",
      "\tTraining batch 4 Loss: 0.019032\n",
      "\tTraining batch 5 Loss: 0.076850\n",
      "\tTraining batch 6 Loss: 0.396275\n",
      "Training set: Average loss: 0.086787\n",
      "Validation set: Average loss: 1.867358, Accuracy: 1256/1959 (64.11%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.016855\n",
      "\tTraining batch 2 Loss: 0.032378\n",
      "\tTraining batch 3 Loss: 0.028303\n",
      "\tTraining batch 4 Loss: 0.040794\n",
      "\tTraining batch 5 Loss: 0.038230\n",
      "\tTraining batch 6 Loss: 0.107780\n",
      "Training set: Average loss: 0.044057\n",
      "Validation set: Average loss: 1.580710, Accuracy: 1376/1959 (70.24%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.007987\n",
      "\tTraining batch 2 Loss: 0.010203\n",
      "\tTraining batch 3 Loss: 0.002678\n",
      "\tTraining batch 4 Loss: 0.010364\n",
      "\tTraining batch 5 Loss: 0.019617\n",
      "\tTraining batch 6 Loss: 0.067507\n",
      "Training set: Average loss: 0.019726\n",
      "Validation set: Average loss: 1.733529, Accuracy: 1381/1959 (70.50%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.003758\n",
      "\tTraining batch 2 Loss: 0.006074\n",
      "\tTraining batch 3 Loss: 0.004190\n",
      "\tTraining batch 4 Loss: 0.006297\n",
      "\tTraining batch 5 Loss: 0.015423\n",
      "\tTraining batch 6 Loss: 0.045487\n",
      "Training set: Average loss: 0.013538\n",
      "Validation set: Average loss: 1.884071, Accuracy: 1367/1959 (69.78%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.001075\n",
      "\tTraining batch 2 Loss: 0.003441\n",
      "\tTraining batch 3 Loss: 0.001299\n",
      "\tTraining batch 4 Loss: 0.004109\n",
      "\tTraining batch 5 Loss: 0.013248\n",
      "\tTraining batch 6 Loss: 0.032683\n",
      "Training set: Average loss: 0.009309\n",
      "Validation set: Average loss: 1.938723, Accuracy: 1362/1959 (69.53%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000421\n",
      "\tTraining batch 2 Loss: 0.002329\n",
      "\tTraining batch 3 Loss: 0.001023\n",
      "\tTraining batch 4 Loss: 0.002906\n",
      "\tTraining batch 5 Loss: 0.011776\n",
      "\tTraining batch 6 Loss: 0.025335\n",
      "Training set: Average loss: 0.007298\n",
      "Validation set: Average loss: 1.964389, Accuracy: 1363/1959 (69.58%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000229\n",
      "\tTraining batch 2 Loss: 0.001784\n",
      "\tTraining batch 3 Loss: 0.000818\n",
      "\tTraining batch 4 Loss: 0.002200\n",
      "\tTraining batch 5 Loss: 0.010724\n",
      "\tTraining batch 6 Loss: 0.020339\n",
      "Training set: Average loss: 0.006016\n",
      "Validation set: Average loss: 1.984218, Accuracy: 1366/1959 (69.73%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000156\n",
      "\tTraining batch 2 Loss: 0.001471\n",
      "\tTraining batch 3 Loss: 0.000665\n",
      "\tTraining batch 4 Loss: 0.001759\n",
      "\tTraining batch 5 Loss: 0.009909\n",
      "\tTraining batch 6 Loss: 0.016506\n",
      "Training set: Average loss: 0.005078\n",
      "Validation set: Average loss: 2.004534, Accuracy: 1366/1959 (69.73%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 44.75%\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 1 Loss: 0.000121\n",
      "\tTraining batch 2 Loss: 0.001259\n",
      "\tTraining batch 3 Loss: 0.000557\n",
      "\tTraining batch 4 Loss: 0.001464\n",
      "\tTraining batch 5 Loss: 0.009217\n",
      "\tTraining batch 6 Loss: 0.013514\n",
      "\tTraining batch 7 Loss: 1.852692\n",
      "Training set: Average loss: 0.268403\n",
      "Validation set: Average loss: 1.789890, Accuracy: 1396/1959 (71.26%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000147\n",
      "\tTraining batch 2 Loss: 0.001637\n",
      "\tTraining batch 3 Loss: 0.010365\n",
      "\tTraining batch 4 Loss: 0.063352\n",
      "\tTraining batch 5 Loss: 0.286354\n",
      "\tTraining batch 6 Loss: 0.089617\n",
      "\tTraining batch 7 Loss: 0.929386\n",
      "Training set: Average loss: 0.197266\n",
      "Validation set: Average loss: 1.652267, Accuracy: 1337/1959 (68.25%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000707\n",
      "\tTraining batch 2 Loss: 0.004961\n",
      "\tTraining batch 3 Loss: 0.032547\n",
      "\tTraining batch 4 Loss: 0.027095\n",
      "\tTraining batch 5 Loss: 0.281792\n",
      "\tTraining batch 6 Loss: 0.182895\n",
      "\tTraining batch 7 Loss: 0.260440\n",
      "Training set: Average loss: 0.112920\n",
      "Validation set: Average loss: 1.275927, Accuracy: 1394/1959 (71.16%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000715\n",
      "\tTraining batch 2 Loss: 0.003998\n",
      "\tTraining batch 3 Loss: 0.025482\n",
      "\tTraining batch 4 Loss: 0.009574\n",
      "\tTraining batch 5 Loss: 0.007679\n",
      "\tTraining batch 6 Loss: 0.024111\n",
      "\tTraining batch 7 Loss: 0.043426\n",
      "Training set: Average loss: 0.016426\n",
      "Validation set: Average loss: 1.830391, Accuracy: 1405/1959 (71.72%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000464\n",
      "\tTraining batch 2 Loss: 0.001303\n",
      "\tTraining batch 3 Loss: 0.019002\n",
      "\tTraining batch 4 Loss: 0.003224\n",
      "\tTraining batch 5 Loss: 0.004560\n",
      "\tTraining batch 6 Loss: 0.012951\n",
      "\tTraining batch 7 Loss: 0.021539\n",
      "Training set: Average loss: 0.009006\n",
      "Validation set: Average loss: 2.783651, Accuracy: 1355/1959 (69.17%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000327\n",
      "\tTraining batch 2 Loss: 0.002053\n",
      "\tTraining batch 3 Loss: 0.029018\n",
      "\tTraining batch 4 Loss: 0.001898\n",
      "\tTraining batch 5 Loss: 0.003899\n",
      "\tTraining batch 6 Loss: 0.008938\n",
      "\tTraining batch 7 Loss: 0.007728\n",
      "Training set: Average loss: 0.007694\n",
      "Validation set: Average loss: 2.676494, Accuracy: 1378/1959 (70.34%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000183\n",
      "\tTraining batch 2 Loss: 0.000514\n",
      "\tTraining batch 3 Loss: 0.008839\n",
      "\tTraining batch 4 Loss: 0.001456\n",
      "\tTraining batch 5 Loss: 0.003190\n",
      "\tTraining batch 6 Loss: 0.006523\n",
      "\tTraining batch 7 Loss: 0.004167\n",
      "Training set: Average loss: 0.003553\n",
      "Validation set: Average loss: 2.582330, Accuracy: 1392/1959 (71.06%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000129\n",
      "\tTraining batch 2 Loss: 0.000419\n",
      "\tTraining batch 3 Loss: 0.006490\n",
      "\tTraining batch 4 Loss: 0.001224\n",
      "\tTraining batch 5 Loss: 0.002833\n",
      "\tTraining batch 6 Loss: 0.004833\n",
      "\tTraining batch 7 Loss: 0.002769\n",
      "Training set: Average loss: 0.002671\n",
      "Validation set: Average loss: 2.589328, Accuracy: 1396/1959 (71.26%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000098\n",
      "\tTraining batch 2 Loss: 0.000363\n",
      "\tTraining batch 3 Loss: 0.004740\n",
      "\tTraining batch 4 Loss: 0.001085\n",
      "\tTraining batch 5 Loss: 0.002595\n",
      "\tTraining batch 6 Loss: 0.003696\n",
      "\tTraining batch 7 Loss: 0.002057\n",
      "Training set: Average loss: 0.002090\n",
      "Validation set: Average loss: 2.621135, Accuracy: 1402/1959 (71.57%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000079\n",
      "\tTraining batch 2 Loss: 0.000322\n",
      "\tTraining batch 3 Loss: 0.003545\n",
      "\tTraining batch 4 Loss: 0.000985\n",
      "\tTraining batch 5 Loss: 0.002415\n",
      "\tTraining batch 6 Loss: 0.002933\n",
      "\tTraining batch 7 Loss: 0.001640\n",
      "Training set: Average loss: 0.001703\n",
      "Validation set: Average loss: 2.656960, Accuracy: 1401/1959 (71.52%)\n",
      "\n",
      "Epoch: 11\n",
      "\tTraining batch 1 Loss: 0.000066\n",
      "\tTraining batch 2 Loss: 0.000290\n",
      "\tTraining batch 3 Loss: 0.002608\n",
      "\tTraining batch 4 Loss: 0.000907\n",
      "\tTraining batch 5 Loss: 0.002268\n",
      "\tTraining batch 6 Loss: 0.002411\n",
      "\tTraining batch 7 Loss: 0.001375\n",
      "Training set: Average loss: 0.001418\n",
      "Validation set: Average loss: 2.690923, Accuracy: 1400/1959 (71.47%)\n",
      "\n",
      "Epoch: 12\n",
      "\tTraining batch 1 Loss: 0.000058\n",
      "\tTraining batch 2 Loss: 0.000266\n",
      "\tTraining batch 3 Loss: 0.001989\n",
      "\tTraining batch 4 Loss: 0.000840\n",
      "\tTraining batch 5 Loss: 0.002142\n",
      "\tTraining batch 6 Loss: 0.002042\n",
      "\tTraining batch 7 Loss: 0.001192\n",
      "Training set: Average loss: 0.001218\n",
      "Validation set: Average loss: 2.721675, Accuracy: 1402/1959 (71.57%)\n",
      "\n",
      "Epoch: 13\n",
      "\tTraining batch 1 Loss: 0.000051\n",
      "\tTraining batch 2 Loss: 0.000245\n",
      "\tTraining batch 3 Loss: 0.001582\n",
      "\tTraining batch 4 Loss: 0.000781\n",
      "\tTraining batch 5 Loss: 0.002030\n",
      "\tTraining batch 6 Loss: 0.001772\n",
      "\tTraining batch 7 Loss: 0.001056\n",
      "Training set: Average loss: 0.001074\n",
      "Validation set: Average loss: 2.749470, Accuracy: 1403/1959 (71.62%)\n",
      "\n",
      "Epoch: 14\n",
      "\tTraining batch 1 Loss: 0.000047\n",
      "\tTraining batch 2 Loss: 0.000228\n",
      "\tTraining batch 3 Loss: 0.001301\n",
      "\tTraining batch 4 Loss: 0.000728\n",
      "\tTraining batch 5 Loss: 0.001927\n",
      "\tTraining batch 6 Loss: 0.001568\n",
      "\tTraining batch 7 Loss: 0.000950\n",
      "Training set: Average loss: 0.000964\n",
      "Validation set: Average loss: 2.774791, Accuracy: 1405/1959 (71.72%)\n",
      "\n",
      "Epoch: 15\n",
      "\tTraining batch 1 Loss: 0.000043\n",
      "\tTraining batch 2 Loss: 0.000214\n",
      "\tTraining batch 3 Loss: 0.001100\n",
      "\tTraining batch 4 Loss: 0.000681\n",
      "\tTraining batch 5 Loss: 0.001832\n",
      "\tTraining batch 6 Loss: 0.001408\n",
      "\tTraining batch 7 Loss: 0.000864\n",
      "Training set: Average loss: 0.000877\n",
      "Validation set: Average loss: 2.798157, Accuracy: 1405/1959 (71.72%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 47.75%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000039\n",
      "\tTraining batch 2 Loss: 0.000201\n",
      "\tTraining batch 3 Loss: 0.000948\n",
      "\tTraining batch 4 Loss: 0.000638\n",
      "\tTraining batch 5 Loss: 0.001744\n",
      "\tTraining batch 6 Loss: 0.001279\n",
      "\tTraining batch 7 Loss: 0.000793\n",
      "\tTraining batch 8 Loss: 3.649327\n",
      "Training set: Average loss: 0.456871\n",
      "Validation set: Average loss: 2.108817, Accuracy: 1404/1959 (71.67%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000260\n",
      "\tTraining batch 2 Loss: 0.001599\n",
      "\tTraining batch 3 Loss: 0.016575\n",
      "\tTraining batch 4 Loss: 0.030290\n",
      "\tTraining batch 5 Loss: 0.022857\n",
      "\tTraining batch 6 Loss: 0.119368\n",
      "\tTraining batch 7 Loss: 0.088489\n",
      "\tTraining batch 8 Loss: 0.893203\n",
      "Training set: Average loss: 0.146580\n",
      "Validation set: Average loss: 1.206309, Accuracy: 1288/1959 (65.75%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.013334\n",
      "\tTraining batch 2 Loss: 0.021207\n",
      "\tTraining batch 3 Loss: 0.015117\n",
      "\tTraining batch 4 Loss: 0.026626\n",
      "\tTraining batch 5 Loss: 0.026362\n",
      "\tTraining batch 6 Loss: 0.057935\n",
      "\tTraining batch 7 Loss: 0.043632\n",
      "\tTraining batch 8 Loss: 0.204641\n",
      "Training set: Average loss: 0.051107\n",
      "Validation set: Average loss: 1.244992, Accuracy: 1446/1959 (73.81%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.001213\n",
      "\tTraining batch 2 Loss: 0.005614\n",
      "\tTraining batch 3 Loss: 0.008206\n",
      "\tTraining batch 4 Loss: 0.023956\n",
      "\tTraining batch 5 Loss: 0.015104\n",
      "\tTraining batch 6 Loss: 0.030136\n",
      "\tTraining batch 7 Loss: 0.023644\n",
      "\tTraining batch 8 Loss: 0.066770\n",
      "Training set: Average loss: 0.021830\n",
      "Validation set: Average loss: 1.759945, Accuracy: 1383/1959 (70.60%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000621\n",
      "\tTraining batch 2 Loss: 0.002085\n",
      "\tTraining batch 3 Loss: 0.008381\n",
      "\tTraining batch 4 Loss: 0.007794\n",
      "\tTraining batch 5 Loss: 0.007724\n",
      "\tTraining batch 6 Loss: 0.014358\n",
      "\tTraining batch 7 Loss: 0.013355\n",
      "\tTraining batch 8 Loss: 0.022515\n",
      "Training set: Average loss: 0.009604\n",
      "Validation set: Average loss: 2.305857, Accuracy: 1325/1959 (67.64%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000480\n",
      "\tTraining batch 2 Loss: 0.001760\n",
      "\tTraining batch 3 Loss: 0.002820\n",
      "\tTraining batch 4 Loss: 0.003153\n",
      "\tTraining batch 5 Loss: 0.004487\n",
      "\tTraining batch 6 Loss: 0.007293\n",
      "\tTraining batch 7 Loss: 0.008116\n",
      "\tTraining batch 8 Loss: 0.010618\n",
      "Training set: Average loss: 0.004841\n",
      "Validation set: Average loss: 2.422677, Accuracy: 1332/1959 (67.99%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000240\n",
      "\tTraining batch 2 Loss: 0.000721\n",
      "\tTraining batch 3 Loss: 0.000704\n",
      "\tTraining batch 4 Loss: 0.001620\n",
      "\tTraining batch 5 Loss: 0.002344\n",
      "\tTraining batch 6 Loss: 0.004804\n",
      "\tTraining batch 7 Loss: 0.005186\n",
      "\tTraining batch 8 Loss: 0.006109\n",
      "Training set: Average loss: 0.002716\n",
      "Validation set: Average loss: 2.444107, Accuracy: 1351/1959 (68.96%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000126\n",
      "\tTraining batch 2 Loss: 0.000385\n",
      "\tTraining batch 3 Loss: 0.000380\n",
      "\tTraining batch 4 Loss: 0.001058\n",
      "\tTraining batch 5 Loss: 0.001720\n",
      "\tTraining batch 6 Loss: 0.003564\n",
      "\tTraining batch 7 Loss: 0.003627\n",
      "\tTraining batch 8 Loss: 0.004170\n",
      "Training set: Average loss: 0.001879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set: Average loss: 2.474049, Accuracy: 1365/1959 (69.68%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000079\n",
      "\tTraining batch 2 Loss: 0.000264\n",
      "\tTraining batch 3 Loss: 0.000273\n",
      "\tTraining batch 4 Loss: 0.000795\n",
      "\tTraining batch 5 Loss: 0.001418\n",
      "\tTraining batch 6 Loss: 0.002826\n",
      "\tTraining batch 7 Loss: 0.002744\n",
      "\tTraining batch 8 Loss: 0.003161\n",
      "Training set: Average loss: 0.001445\n",
      "Validation set: Average loss: 2.508181, Accuracy: 1370/1959 (69.93%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000058\n",
      "\tTraining batch 2 Loss: 0.000205\n",
      "\tTraining batch 3 Loss: 0.000220\n",
      "\tTraining batch 4 Loss: 0.000650\n",
      "\tTraining batch 5 Loss: 0.001238\n",
      "\tTraining batch 6 Loss: 0.002337\n",
      "\tTraining batch 7 Loss: 0.002201\n",
      "\tTraining batch 8 Loss: 0.002560\n",
      "Training set: Average loss: 0.001183\n",
      "Validation set: Average loss: 2.541106, Accuracy: 1375/1959 (70.19%)\n",
      "\n",
      "Epoch: 11\n",
      "\tTraining batch 1 Loss: 0.000046\n",
      "\tTraining batch 2 Loss: 0.000169\n",
      "\tTraining batch 3 Loss: 0.000186\n",
      "\tTraining batch 4 Loss: 0.000558\n",
      "\tTraining batch 5 Loss: 0.001113\n",
      "\tTraining batch 6 Loss: 0.001988\n",
      "\tTraining batch 7 Loss: 0.001837\n",
      "\tTraining batch 8 Loss: 0.002159\n",
      "Training set: Average loss: 0.001007\n",
      "Validation set: Average loss: 2.571840, Accuracy: 1377/1959 (70.29%)\n",
      "\n",
      "Epoch: 12\n",
      "\tTraining batch 1 Loss: 0.000038\n",
      "\tTraining batch 2 Loss: 0.000146\n",
      "\tTraining batch 3 Loss: 0.000163\n",
      "\tTraining batch 4 Loss: 0.000492\n",
      "\tTraining batch 5 Loss: 0.001020\n",
      "\tTraining batch 6 Loss: 0.001728\n",
      "\tTraining batch 7 Loss: 0.001578\n",
      "\tTraining batch 8 Loss: 0.001873\n",
      "Training set: Average loss: 0.000880\n",
      "Validation set: Average loss: 2.599900, Accuracy: 1378/1959 (70.34%)\n",
      "\n",
      "Epoch: 13\n",
      "\tTraining batch 1 Loss: 0.000033\n",
      "\tTraining batch 2 Loss: 0.000128\n",
      "\tTraining batch 3 Loss: 0.000146\n",
      "\tTraining batch 4 Loss: 0.000442\n",
      "\tTraining batch 5 Loss: 0.000946\n",
      "\tTraining batch 6 Loss: 0.001525\n",
      "\tTraining batch 7 Loss: 0.001382\n",
      "\tTraining batch 8 Loss: 0.001656\n",
      "Training set: Average loss: 0.000782\n",
      "Validation set: Average loss: 2.625209, Accuracy: 1380/1959 (70.44%)\n",
      "\n",
      "Epoch: 14\n",
      "\tTraining batch 1 Loss: 0.000029\n",
      "\tTraining batch 2 Loss: 0.000115\n",
      "\tTraining batch 3 Loss: 0.000132\n",
      "\tTraining batch 4 Loss: 0.000403\n",
      "\tTraining batch 5 Loss: 0.000882\n",
      "\tTraining batch 6 Loss: 0.001364\n",
      "\tTraining batch 7 Loss: 0.001227\n",
      "\tTraining batch 8 Loss: 0.001484\n",
      "Training set: Average loss: 0.000704\n",
      "Validation set: Average loss: 2.647823, Accuracy: 1380/1959 (70.44%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 46.25%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000025\n",
      "\tTraining batch 2 Loss: 0.000104\n",
      "\tTraining batch 3 Loss: 0.000121\n",
      "\tTraining batch 4 Loss: 0.000370\n",
      "\tTraining batch 5 Loss: 0.000827\n",
      "\tTraining batch 6 Loss: 0.001232\n",
      "\tTraining batch 7 Loss: 0.001102\n",
      "\tTraining batch 8 Loss: 0.001343\n",
      "\tTraining batch 9 Loss: 5.251607\n",
      "Training set: Average loss: 0.584081\n",
      "Validation set: Average loss: 1.794932, Accuracy: 1431/1959 (73.05%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000031\n",
      "\tTraining batch 2 Loss: 0.012088\n",
      "\tTraining batch 3 Loss: 0.204196\n",
      "\tTraining batch 4 Loss: 0.995886\n",
      "\tTraining batch 5 Loss: 0.831438\n",
      "\tTraining batch 6 Loss: 0.352230\n",
      "\tTraining batch 7 Loss: 0.071513\n",
      "\tTraining batch 8 Loss: 0.038554\n",
      "\tTraining batch 9 Loss: 0.324336\n",
      "Training set: Average loss: 0.314475\n",
      "Validation set: Average loss: 1.783356, Accuracy: 1201/1959 (61.31%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.065860\n",
      "\tTraining batch 2 Loss: 0.156192\n",
      "\tTraining batch 3 Loss: 0.260752\n",
      "\tTraining batch 4 Loss: 0.037997\n",
      "\tTraining batch 5 Loss: 0.046103\n",
      "\tTraining batch 6 Loss: 0.017906\n",
      "\tTraining batch 7 Loss: 0.031026\n",
      "\tTraining batch 8 Loss: 0.046925\n",
      "\tTraining batch 9 Loss: 0.199532\n",
      "Training set: Average loss: 0.095810\n",
      "Validation set: Average loss: 2.246388, Accuracy: 1353/1959 (69.07%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.016634\n",
      "\tTraining batch 2 Loss: 0.006062\n",
      "\tTraining batch 3 Loss: 0.004765\n",
      "\tTraining batch 4 Loss: 0.005280\n",
      "\tTraining batch 5 Loss: 0.020541\n",
      "\tTraining batch 6 Loss: 0.025173\n",
      "\tTraining batch 7 Loss: 0.019267\n",
      "\tTraining batch 8 Loss: 0.175345\n",
      "\tTraining batch 9 Loss: 0.009563\n",
      "Training set: Average loss: 0.031403\n",
      "Validation set: Average loss: 2.217184, Accuracy: 1357/1959 (69.27%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000880\n",
      "\tTraining batch 2 Loss: 0.004537\n",
      "\tTraining batch 3 Loss: 0.002835\n",
      "\tTraining batch 4 Loss: 0.006047\n",
      "\tTraining batch 5 Loss: 0.013019\n",
      "\tTraining batch 6 Loss: 0.009577\n",
      "\tTraining batch 7 Loss: 0.006231\n",
      "\tTraining batch 8 Loss: 0.004760\n",
      "\tTraining batch 9 Loss: 0.000340\n",
      "Training set: Average loss: 0.005358\n",
      "Validation set: Average loss: 2.280680, Accuracy: 1406/1959 (71.77%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000149\n",
      "\tTraining batch 2 Loss: 0.002564\n",
      "\tTraining batch 3 Loss: 0.000704\n",
      "\tTraining batch 4 Loss: 0.003050\n",
      "\tTraining batch 5 Loss: 0.007202\n",
      "\tTraining batch 6 Loss: 0.003185\n",
      "\tTraining batch 7 Loss: 0.002493\n",
      "\tTraining batch 8 Loss: 0.001882\n",
      "\tTraining batch 9 Loss: 0.000156\n",
      "Training set: Average loss: 0.002376\n",
      "Validation set: Average loss: 2.581569, Accuracy: 1394/1959 (71.16%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000063\n",
      "\tTraining batch 2 Loss: 0.001598\n",
      "\tTraining batch 3 Loss: 0.000420\n",
      "\tTraining batch 4 Loss: 0.001388\n",
      "\tTraining batch 5 Loss: 0.004816\n",
      "\tTraining batch 6 Loss: 0.001582\n",
      "\tTraining batch 7 Loss: 0.001413\n",
      "\tTraining batch 8 Loss: 0.001125\n",
      "\tTraining batch 9 Loss: 0.000256\n",
      "Training set: Average loss: 0.001407\n",
      "Validation set: Average loss: 2.764711, Accuracy: 1385/1959 (70.70%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000048\n",
      "\tTraining batch 2 Loss: 0.001144\n",
      "\tTraining batch 3 Loss: 0.000343\n",
      "\tTraining batch 4 Loss: 0.000802\n",
      "\tTraining batch 5 Loss: 0.003665\n",
      "\tTraining batch 6 Loss: 0.001030\n",
      "\tTraining batch 7 Loss: 0.000993\n",
      "\tTraining batch 8 Loss: 0.000838\n",
      "\tTraining batch 9 Loss: 0.000259\n",
      "Training set: Average loss: 0.001014\n",
      "Validation set: Average loss: 2.858042, Accuracy: 1378/1959 (70.34%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000043\n",
      "\tTraining batch 2 Loss: 0.000898\n",
      "\tTraining batch 3 Loss: 0.000296\n",
      "\tTraining batch 4 Loss: 0.000574\n",
      "\tTraining batch 5 Loss: 0.003024\n",
      "\tTraining batch 6 Loss: 0.000780\n",
      "\tTraining batch 7 Loss: 0.000781\n",
      "\tTraining batch 8 Loss: 0.000686\n",
      "\tTraining batch 9 Loss: 0.000181\n",
      "Training set: Average loss: 0.000807\n",
      "Validation set: Average loss: 2.906338, Accuracy: 1379/1959 (70.39%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000040\n",
      "\tTraining batch 2 Loss: 0.000740\n",
      "\tTraining batch 3 Loss: 0.000259\n",
      "\tTraining batch 4 Loss: 0.000463\n",
      "\tTraining batch 5 Loss: 0.002614\n",
      "\tTraining batch 6 Loss: 0.000643\n",
      "\tTraining batch 7 Loss: 0.000653\n",
      "\tTraining batch 8 Loss: 0.000589\n",
      "\tTraining batch 9 Loss: 0.000124\n",
      "Training set: Average loss: 0.000680\n",
      "Validation set: Average loss: 2.935607, Accuracy: 1379/1959 (70.39%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 45.75%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000038\n",
      "\tTraining batch 2 Loss: 0.000630\n",
      "\tTraining batch 3 Loss: 0.000229\n",
      "\tTraining batch 4 Loss: 0.000398\n",
      "\tTraining batch 5 Loss: 0.002322\n",
      "\tTraining batch 6 Loss: 0.000555\n",
      "\tTraining batch 7 Loss: 0.000565\n",
      "\tTraining batch 8 Loss: 0.000519\n",
      "\tTraining batch 9 Loss: 0.000092\n",
      "\tTraining batch 10 Loss: 3.523598\n",
      "Training set: Average loss: 0.352895\n",
      "Validation set: Average loss: 2.094300, Accuracy: 1383/1959 (70.60%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000225\n",
      "\tTraining batch 2 Loss: 0.002997\n",
      "\tTraining batch 3 Loss: 0.010754\n",
      "\tTraining batch 4 Loss: 0.032930\n",
      "\tTraining batch 5 Loss: 0.045873\n",
      "\tTraining batch 6 Loss: 0.104361\n",
      "\tTraining batch 7 Loss: 0.193232\n",
      "\tTraining batch 8 Loss: 0.214077\n",
      "\tTraining batch 9 Loss: 0.339932\n",
      "\tTraining batch 10 Loss: 0.912219\n",
      "Training set: Average loss: 0.185660\n",
      "Validation set: Average loss: 1.167612, Accuracy: 1327/1959 (67.74%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.045825\n",
      "\tTraining batch 2 Loss: 0.034868\n",
      "\tTraining batch 3 Loss: 0.064831\n",
      "\tTraining batch 4 Loss: 0.136238\n",
      "\tTraining batch 5 Loss: 0.179603\n",
      "\tTraining batch 6 Loss: 0.166207\n",
      "\tTraining batch 7 Loss: 0.127147\n",
      "\tTraining batch 8 Loss: 0.219680\n",
      "\tTraining batch 9 Loss: 0.061506\n",
      "\tTraining batch 10 Loss: 0.294142\n",
      "Training set: Average loss: 0.133005\n",
      "Validation set: Average loss: 1.583100, Accuracy: 1390/1959 (70.95%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.005376\n",
      "\tTraining batch 2 Loss: 0.007675\n",
      "\tTraining batch 3 Loss: 0.017014\n",
      "\tTraining batch 4 Loss: 0.003616\n",
      "\tTraining batch 5 Loss: 0.004926\n",
      "\tTraining batch 6 Loss: 0.030061\n",
      "\tTraining batch 7 Loss: 0.027180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 8 Loss: 0.072666\n",
      "\tTraining batch 9 Loss: 0.007040\n",
      "\tTraining batch 10 Loss: 0.232119\n",
      "Training set: Average loss: 0.040767\n",
      "Validation set: Average loss: 3.578167, Accuracy: 1297/1959 (66.21%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.003890\n",
      "\tTraining batch 2 Loss: 0.000987\n",
      "\tTraining batch 3 Loss: 0.021068\n",
      "\tTraining batch 4 Loss: 0.000149\n",
      "\tTraining batch 5 Loss: 0.001578\n",
      "\tTraining batch 6 Loss: 0.003174\n",
      "\tTraining batch 7 Loss: 0.207532\n",
      "\tTraining batch 8 Loss: 0.004377\n",
      "\tTraining batch 9 Loss: 0.000049\n",
      "\tTraining batch 10 Loss: 0.010661\n",
      "Training set: Average loss: 0.025346\n",
      "Validation set: Average loss: 3.013971, Accuracy: 1320/1959 (67.38%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000133\n",
      "\tTraining batch 2 Loss: 0.000203\n",
      "\tTraining batch 3 Loss: 0.000721\n",
      "\tTraining batch 4 Loss: 0.003632\n",
      "\tTraining batch 5 Loss: 0.001285\n",
      "\tTraining batch 6 Loss: 0.004908\n",
      "\tTraining batch 7 Loss: 0.005162\n",
      "\tTraining batch 8 Loss: 0.003685\n",
      "\tTraining batch 9 Loss: 0.012888\n",
      "\tTraining batch 10 Loss: 0.740307\n",
      "Training set: Average loss: 0.077292\n",
      "Validation set: Average loss: 3.358097, Accuracy: 1230/1959 (62.79%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000076\n",
      "\tTraining batch 2 Loss: 0.000109\n",
      "\tTraining batch 3 Loss: 0.000693\n",
      "\tTraining batch 4 Loss: 0.000258\n",
      "\tTraining batch 5 Loss: 0.001019\n",
      "\tTraining batch 6 Loss: 0.000634\n",
      "\tTraining batch 7 Loss: 0.000953\n",
      "\tTraining batch 8 Loss: 0.001254\n",
      "\tTraining batch 9 Loss: 0.083761\n",
      "\tTraining batch 10 Loss: 0.022392\n",
      "Training set: Average loss: 0.011115\n",
      "Validation set: Average loss: 2.528263, Accuracy: 1308/1959 (66.77%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000069\n",
      "\tTraining batch 2 Loss: 0.119334\n",
      "\tTraining batch 3 Loss: 0.000577\n",
      "\tTraining batch 4 Loss: 0.000196\n",
      "\tTraining batch 5 Loss: 0.000768\n",
      "\tTraining batch 6 Loss: 0.000827\n",
      "\tTraining batch 7 Loss: 0.000735\n",
      "\tTraining batch 8 Loss: 0.000558\n",
      "\tTraining batch 9 Loss: 0.000037\n",
      "\tTraining batch 10 Loss: 0.003236\n",
      "Training set: Average loss: 0.012634\n",
      "Validation set: Average loss: 2.540622, Accuracy: 1403/1959 (71.62%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000027\n",
      "\tTraining batch 2 Loss: 0.000048\n",
      "\tTraining batch 3 Loss: 0.000672\n",
      "\tTraining batch 4 Loss: 0.000157\n",
      "\tTraining batch 5 Loss: 0.000368\n",
      "\tTraining batch 6 Loss: 0.000620\n",
      "\tTraining batch 7 Loss: 0.000705\n",
      "\tTraining batch 8 Loss: 0.000445\n",
      "\tTraining batch 9 Loss: 0.000025\n",
      "\tTraining batch 10 Loss: 0.002092\n",
      "Training set: Average loss: 0.000516\n",
      "Validation set: Average loss: 3.247517, Accuracy: 1357/1959 (69.27%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000018\n",
      "\tTraining batch 2 Loss: 0.000033\n",
      "\tTraining batch 3 Loss: 0.000976\n",
      "\tTraining batch 4 Loss: 0.000146\n",
      "\tTraining batch 5 Loss: 0.001409\n",
      "\tTraining batch 6 Loss: 0.000599\n",
      "\tTraining batch 7 Loss: 0.000762\n",
      "\tTraining batch 8 Loss: 0.000558\n",
      "\tTraining batch 9 Loss: 0.000027\n",
      "\tTraining batch 10 Loss: 0.001924\n",
      "Training set: Average loss: 0.000645\n",
      "Validation set: Average loss: 3.568879, Accuracy: 1339/1959 (68.35%)\n",
      "\n",
      "Epoch: 11\n",
      "\tTraining batch 1 Loss: 0.000016\n",
      "\tTraining batch 2 Loss: 0.000029\n",
      "\tTraining batch 3 Loss: 0.000992\n",
      "\tTraining batch 4 Loss: 0.000139\n",
      "\tTraining batch 5 Loss: 0.000807\n",
      "\tTraining batch 6 Loss: 0.000564\n",
      "\tTraining batch 7 Loss: 0.000751\n",
      "\tTraining batch 8 Loss: 0.000490\n",
      "\tTraining batch 9 Loss: 0.000027\n",
      "\tTraining batch 10 Loss: 0.001720\n",
      "Training set: Average loss: 0.000553\n",
      "Validation set: Average loss: 3.657559, Accuracy: 1329/1959 (67.84%)\n",
      "\n",
      "Epoch: 12\n",
      "\tTraining batch 1 Loss: 0.000015\n",
      "\tTraining batch 2 Loss: 0.000027\n",
      "\tTraining batch 3 Loss: 0.000862\n",
      "\tTraining batch 4 Loss: 0.000130\n",
      "\tTraining batch 5 Loss: 0.000334\n",
      "\tTraining batch 6 Loss: 0.000507\n",
      "\tTraining batch 7 Loss: 0.000703\n",
      "\tTraining batch 8 Loss: 0.000386\n",
      "\tTraining batch 9 Loss: 0.000024\n",
      "\tTraining batch 10 Loss: 0.001491\n",
      "Training set: Average loss: 0.000448\n",
      "Validation set: Average loss: 3.678632, Accuracy: 1330/1959 (67.89%)\n",
      "\n",
      "Epoch: 13\n",
      "\tTraining batch 1 Loss: 0.000014\n",
      "\tTraining batch 2 Loss: 0.000025\n",
      "\tTraining batch 3 Loss: 0.000720\n",
      "\tTraining batch 4 Loss: 0.000121\n",
      "\tTraining batch 5 Loss: 0.000240\n",
      "\tTraining batch 6 Loss: 0.000450\n",
      "\tTraining batch 7 Loss: 0.000648\n",
      "\tTraining batch 8 Loss: 0.000330\n",
      "\tTraining batch 9 Loss: 0.000022\n",
      "\tTraining batch 10 Loss: 0.001296\n",
      "Training set: Average loss: 0.000387\n",
      "Validation set: Average loss: 3.687310, Accuracy: 1332/1959 (67.99%)\n",
      "\n",
      "Epoch: 14\n",
      "\tTraining batch 1 Loss: 0.000013\n",
      "\tTraining batch 2 Loss: 0.000024\n",
      "\tTraining batch 3 Loss: 0.000605\n",
      "\tTraining batch 4 Loss: 0.000112\n",
      "\tTraining batch 5 Loss: 0.000209\n",
      "\tTraining batch 6 Loss: 0.000400\n",
      "\tTraining batch 7 Loss: 0.000596\n",
      "\tTraining batch 8 Loss: 0.000297\n",
      "\tTraining batch 9 Loss: 0.000020\n",
      "\tTraining batch 10 Loss: 0.001141\n",
      "Training set: Average loss: 0.000342\n",
      "Validation set: Average loss: 3.694547, Accuracy: 1334/1959 (68.10%)\n",
      "\n",
      "Epoch: 15\n",
      "\tTraining batch 1 Loss: 0.000013\n",
      "\tTraining batch 2 Loss: 0.000022\n",
      "\tTraining batch 3 Loss: 0.000518\n",
      "\tTraining batch 4 Loss: 0.000104\n",
      "\tTraining batch 5 Loss: 0.000193\n",
      "\tTraining batch 6 Loss: 0.000360\n",
      "\tTraining batch 7 Loss: 0.000551\n",
      "\tTraining batch 8 Loss: 0.000273\n",
      "\tTraining batch 9 Loss: 0.000018\n",
      "\tTraining batch 10 Loss: 0.001021\n",
      "Training set: Average loss: 0.000307\n",
      "Validation set: Average loss: 3.702019, Accuracy: 1333/1959 (68.04%)\n",
      "\n",
      "Epoch: 16\n",
      "\tTraining batch 1 Loss: 0.000013\n",
      "\tTraining batch 2 Loss: 0.000021\n",
      "\tTraining batch 3 Loss: 0.000451\n",
      "\tTraining batch 4 Loss: 0.000097\n",
      "\tTraining batch 5 Loss: 0.000183\n",
      "\tTraining batch 6 Loss: 0.000326\n",
      "\tTraining batch 7 Loss: 0.000510\n",
      "\tTraining batch 8 Loss: 0.000255\n",
      "\tTraining batch 9 Loss: 0.000017\n",
      "\tTraining batch 10 Loss: 0.000924\n",
      "Training set: Average loss: 0.000280\n",
      "Validation set: Average loss: 3.709868, Accuracy: 1334/1959 (68.10%)\n",
      "\n",
      "Epoch: 17\n",
      "\tTraining batch 1 Loss: 0.000012\n",
      "\tTraining batch 2 Loss: 0.000020\n",
      "\tTraining batch 3 Loss: 0.000400\n",
      "\tTraining batch 4 Loss: 0.000091\n",
      "\tTraining batch 5 Loss: 0.000175\n",
      "\tTraining batch 6 Loss: 0.000298\n",
      "\tTraining batch 7 Loss: 0.000475\n",
      "\tTraining batch 8 Loss: 0.000239\n",
      "\tTraining batch 9 Loss: 0.000016\n",
      "\tTraining batch 10 Loss: 0.000843\n",
      "Training set: Average loss: 0.000257\n",
      "Validation set: Average loss: 3.718040, Accuracy: 1334/1959 (68.10%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 44.25%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000012\n",
      "\tTraining batch 2 Loss: 0.000019\n",
      "\tTraining batch 3 Loss: 0.000358\n",
      "\tTraining batch 4 Loss: 0.000086\n",
      "\tTraining batch 5 Loss: 0.000168\n",
      "\tTraining batch 6 Loss: 0.000275\n",
      "\tTraining batch 7 Loss: 0.000443\n",
      "\tTraining batch 8 Loss: 0.000226\n",
      "\tTraining batch 9 Loss: 0.000015\n",
      "\tTraining batch 10 Loss: 0.000775\n",
      "\tTraining batch 11 Loss: 4.046303\n",
      "Training set: Average loss: 0.368062\n",
      "Validation set: Average loss: 2.671945, Accuracy: 1414/1959 (72.18%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000030\n",
      "\tTraining batch 2 Loss: 0.000084\n",
      "\tTraining batch 3 Loss: 0.000526\n",
      "\tTraining batch 4 Loss: 0.003136\n",
      "\tTraining batch 5 Loss: 0.002174\n",
      "\tTraining batch 6 Loss: 0.040061\n",
      "\tTraining batch 7 Loss: 0.386424\n",
      "\tTraining batch 8 Loss: 0.051723\n",
      "\tTraining batch 9 Loss: 0.045987\n",
      "\tTraining batch 10 Loss: 0.024175\n",
      "\tTraining batch 11 Loss: 0.413130\n",
      "Training set: Average loss: 0.087950\n",
      "Validation set: Average loss: 1.619244, Accuracy: 1394/1959 (71.16%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.001815\n",
      "\tTraining batch 2 Loss: 0.002282\n",
      "\tTraining batch 3 Loss: 0.004694\n",
      "\tTraining batch 4 Loss: 0.115774\n",
      "\tTraining batch 5 Loss: 0.012761\n",
      "\tTraining batch 6 Loss: 0.072597\n",
      "\tTraining batch 7 Loss: 0.013692\n",
      "\tTraining batch 8 Loss: 0.031270\n",
      "\tTraining batch 9 Loss: 0.035635\n",
      "\tTraining batch 10 Loss: 0.025273\n",
      "\tTraining batch 11 Loss: 0.142530\n",
      "Training set: Average loss: 0.041666\n",
      "Validation set: Average loss: 2.417537, Accuracy: 1344/1959 (68.61%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.001257\n",
      "\tTraining batch 2 Loss: 0.001120\n",
      "\tTraining batch 3 Loss: 0.001122\n",
      "\tTraining batch 4 Loss: 0.001234\n",
      "\tTraining batch 5 Loss: 0.001352\n",
      "\tTraining batch 6 Loss: 0.002093\n",
      "\tTraining batch 7 Loss: 0.003192\n",
      "\tTraining batch 8 Loss: 0.002495\n",
      "\tTraining batch 9 Loss: 0.000422\n",
      "\tTraining batch 10 Loss: 0.014251\n",
      "\tTraining batch 11 Loss: 0.125599\n",
      "Training set: Average loss: 0.014012\n",
      "Validation set: Average loss: 2.141475, Accuracy: 1367/1959 (69.78%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000135\n",
      "\tTraining batch 2 Loss: 0.000521\n",
      "\tTraining batch 3 Loss: 0.000759\n",
      "\tTraining batch 4 Loss: 0.004136\n",
      "\tTraining batch 5 Loss: 0.007476\n",
      "\tTraining batch 6 Loss: 0.010966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 7 Loss: 0.010190\n",
      "\tTraining batch 8 Loss: 0.018094\n",
      "\tTraining batch 9 Loss: 0.001650\n",
      "\tTraining batch 10 Loss: 0.018831\n",
      "\tTraining batch 11 Loss: 0.041187\n",
      "Training set: Average loss: 0.010359\n",
      "Validation set: Average loss: 2.043687, Accuracy: 1401/1959 (71.52%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000186\n",
      "\tTraining batch 2 Loss: 0.000239\n",
      "\tTraining batch 3 Loss: 0.000722\n",
      "\tTraining batch 4 Loss: 0.001208\n",
      "\tTraining batch 5 Loss: 0.002814\n",
      "\tTraining batch 6 Loss: 0.004019\n",
      "\tTraining batch 7 Loss: 0.002504\n",
      "\tTraining batch 8 Loss: 0.001538\n",
      "\tTraining batch 9 Loss: 0.000350\n",
      "\tTraining batch 10 Loss: 0.002837\n",
      "\tTraining batch 11 Loss: 0.027354\n",
      "Training set: Average loss: 0.003979\n",
      "Validation set: Average loss: 2.160728, Accuracy: 1405/1959 (71.72%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000048\n",
      "\tTraining batch 2 Loss: 0.000118\n",
      "\tTraining batch 3 Loss: 0.000493\n",
      "\tTraining batch 4 Loss: 0.000597\n",
      "\tTraining batch 5 Loss: 0.000951\n",
      "\tTraining batch 6 Loss: 0.001623\n",
      "\tTraining batch 7 Loss: 0.001106\n",
      "\tTraining batch 8 Loss: 0.000726\n",
      "\tTraining batch 9 Loss: 0.000110\n",
      "\tTraining batch 10 Loss: 0.001376\n",
      "\tTraining batch 11 Loss: 0.023972\n",
      "Training set: Average loss: 0.002829\n",
      "Validation set: Average loss: 2.286047, Accuracy: 1405/1959 (71.72%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 46.75%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000032\n",
      "\tTraining batch 2 Loss: 0.000095\n",
      "\tTraining batch 3 Loss: 0.000378\n",
      "\tTraining batch 4 Loss: 0.000395\n",
      "\tTraining batch 5 Loss: 0.000636\n",
      "\tTraining batch 6 Loss: 0.000940\n",
      "\tTraining batch 7 Loss: 0.000729\n",
      "\tTraining batch 8 Loss: 0.000501\n",
      "\tTraining batch 9 Loss: 0.000065\n",
      "\tTraining batch 10 Loss: 0.000971\n",
      "\tTraining batch 11 Loss: 0.020427\n",
      "\tTraining batch 12 Loss: 3.107883\n",
      "Training set: Average loss: 0.261088\n",
      "Validation set: Average loss: 1.936572, Accuracy: 1381/1959 (70.50%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000097\n",
      "\tTraining batch 2 Loss: 0.000346\n",
      "\tTraining batch 3 Loss: 0.005738\n",
      "\tTraining batch 4 Loss: 0.063324\n",
      "\tTraining batch 5 Loss: 0.015359\n",
      "\tTraining batch 6 Loss: 0.115455\n",
      "\tTraining batch 7 Loss: 0.106184\n",
      "\tTraining batch 8 Loss: 0.237132\n",
      "\tTraining batch 9 Loss: 0.043098\n",
      "\tTraining batch 10 Loss: 0.171857\n",
      "\tTraining batch 11 Loss: 0.066278\n",
      "\tTraining batch 12 Loss: 0.581197\n",
      "Training set: Average loss: 0.117172\n",
      "Validation set: Average loss: 1.346058, Accuracy: 1397/1959 (71.31%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.003265\n",
      "\tTraining batch 2 Loss: 0.006108\n",
      "\tTraining batch 3 Loss: 0.008906\n",
      "\tTraining batch 4 Loss: 0.020308\n",
      "\tTraining batch 5 Loss: 0.008482\n",
      "\tTraining batch 6 Loss: 0.029696\n",
      "\tTraining batch 7 Loss: 0.011609\n",
      "\tTraining batch 8 Loss: 0.014484\n",
      "\tTraining batch 9 Loss: 0.097558\n",
      "\tTraining batch 10 Loss: 0.126723\n",
      "\tTraining batch 11 Loss: 0.058264\n",
      "\tTraining batch 12 Loss: 0.201797\n",
      "Training set: Average loss: 0.048933\n",
      "Validation set: Average loss: 4.060396, Accuracy: 1409/1959 (71.92%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000003\n",
      "\tTraining batch 2 Loss: 0.320794\n",
      "\tTraining batch 3 Loss: 0.007861\n",
      "\tTraining batch 4 Loss: 0.000159\n",
      "\tTraining batch 5 Loss: 0.061216\n",
      "\tTraining batch 6 Loss: 0.000144\n",
      "\tTraining batch 7 Loss: 0.000134\n",
      "\tTraining batch 8 Loss: 0.000616\n",
      "\tTraining batch 9 Loss: 0.000002\n",
      "\tTraining batch 10 Loss: 0.010439\n",
      "\tTraining batch 11 Loss: 0.031660\n",
      "\tTraining batch 12 Loss: 0.076669\n",
      "Training set: Average loss: 0.042475\n",
      "Validation set: Average loss: 4.042914, Accuracy: 1325/1959 (67.64%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.002166\n",
      "\tTraining batch 2 Loss: 0.000494\n",
      "\tTraining batch 3 Loss: 0.003353\n",
      "\tTraining batch 4 Loss: 0.000021\n",
      "\tTraining batch 5 Loss: 0.001214\n",
      "\tTraining batch 6 Loss: 0.000406\n",
      "\tTraining batch 7 Loss: 0.000139\n",
      "\tTraining batch 8 Loss: 0.006188\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.006079\n",
      "\tTraining batch 11 Loss: 0.027397\n",
      "\tTraining batch 12 Loss: 0.005740\n",
      "Training set: Average loss: 0.004433\n",
      "Validation set: Average loss: 3.984509, Accuracy: 1359/1959 (69.37%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000229\n",
      "\tTraining batch 2 Loss: 0.000125\n",
      "\tTraining batch 3 Loss: 0.000024\n",
      "\tTraining batch 4 Loss: 0.000009\n",
      "\tTraining batch 5 Loss: 0.001171\n",
      "\tTraining batch 6 Loss: 0.000292\n",
      "\tTraining batch 7 Loss: 0.000107\n",
      "\tTraining batch 8 Loss: 0.000517\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.004000\n",
      "\tTraining batch 11 Loss: 0.018945\n",
      "\tTraining batch 12 Loss: 0.004380\n",
      "Training set: Average loss: 0.002483\n",
      "Validation set: Average loss: 4.035292, Accuracy: 1373/1959 (70.09%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000100\n",
      "\tTraining batch 2 Loss: 0.000084\n",
      "\tTraining batch 3 Loss: 0.000012\n",
      "\tTraining batch 4 Loss: 0.000007\n",
      "\tTraining batch 5 Loss: 0.000905\n",
      "\tTraining batch 6 Loss: 0.000254\n",
      "\tTraining batch 7 Loss: 0.000084\n",
      "\tTraining batch 8 Loss: 0.000194\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.002948\n",
      "\tTraining batch 11 Loss: 0.014789\n",
      "\tTraining batch 12 Loss: 0.003223\n",
      "Training set: Average loss: 0.001883\n",
      "Validation set: Average loss: 4.085781, Accuracy: 1375/1959 (70.19%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000073\n",
      "\tTraining batch 2 Loss: 0.000067\n",
      "\tTraining batch 3 Loss: 0.000011\n",
      "\tTraining batch 4 Loss: 0.000006\n",
      "\tTraining batch 5 Loss: 0.000709\n",
      "\tTraining batch 6 Loss: 0.000231\n",
      "\tTraining batch 7 Loss: 0.000069\n",
      "\tTraining batch 8 Loss: 0.000136\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.002312\n",
      "\tTraining batch 11 Loss: 0.012465\n",
      "\tTraining batch 12 Loss: 0.002491\n",
      "Training set: Average loss: 0.001547\n",
      "Validation set: Average loss: 4.129410, Accuracy: 1375/1959 (70.19%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 46.0%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000060\n",
      "\tTraining batch 2 Loss: 0.000056\n",
      "\tTraining batch 3 Loss: 0.000010\n",
      "\tTraining batch 4 Loss: 0.000005\n",
      "\tTraining batch 5 Loss: 0.000582\n",
      "\tTraining batch 6 Loss: 0.000213\n",
      "\tTraining batch 7 Loss: 0.000058\n",
      "\tTraining batch 8 Loss: 0.000112\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.001878\n",
      "\tTraining batch 11 Loss: 0.010803\n",
      "\tTraining batch 12 Loss: 0.002026\n",
      "\tTraining batch 13 Loss: 5.981638\n",
      "Training set: Average loss: 0.461342\n",
      "Validation set: Average loss: 3.008481, Accuracy: 1378/1959 (70.34%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000072\n",
      "\tTraining batch 2 Loss: 0.000230\n",
      "\tTraining batch 3 Loss: 0.001100\n",
      "\tTraining batch 4 Loss: 0.001982\n",
      "\tTraining batch 5 Loss: 0.001942\n",
      "\tTraining batch 6 Loss: 0.031358\n",
      "\tTraining batch 7 Loss: 0.034443\n",
      "\tTraining batch 8 Loss: 0.115709\n",
      "\tTraining batch 9 Loss: 0.014132\n",
      "\tTraining batch 10 Loss: 0.030604\n",
      "\tTraining batch 11 Loss: 0.135877\n",
      "\tTraining batch 12 Loss: 0.068014\n",
      "\tTraining batch 13 Loss: 0.319080\n",
      "Training set: Average loss: 0.058042\n",
      "Validation set: Average loss: 1.142849, Accuracy: 1385/1959 (70.70%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.028549\n",
      "\tTraining batch 2 Loss: 0.023653\n",
      "\tTraining batch 3 Loss: 0.005954\n",
      "\tTraining batch 4 Loss: 0.017855\n",
      "\tTraining batch 5 Loss: 0.010018\n",
      "\tTraining batch 6 Loss: 0.048882\n",
      "\tTraining batch 7 Loss: 0.011109\n",
      "\tTraining batch 8 Loss: 0.010798\n",
      "\tTraining batch 9 Loss: 0.001009\n",
      "\tTraining batch 10 Loss: 0.009159\n",
      "\tTraining batch 11 Loss: 0.040868\n",
      "\tTraining batch 12 Loss: 0.006131\n",
      "\tTraining batch 13 Loss: 0.257230\n",
      "Training set: Average loss: 0.036247\n",
      "Validation set: Average loss: 2.257123, Accuracy: 1354/1959 (69.12%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.001257\n",
      "\tTraining batch 2 Loss: 0.017205\n",
      "\tTraining batch 3 Loss: 0.103079\n",
      "\tTraining batch 4 Loss: 0.003694\n",
      "\tTraining batch 5 Loss: 0.014796\n",
      "\tTraining batch 6 Loss: 0.004335\n",
      "\tTraining batch 7 Loss: 0.005808\n",
      "\tTraining batch 8 Loss: 0.061972\n",
      "\tTraining batch 9 Loss: 0.000261\n",
      "\tTraining batch 10 Loss: 0.061107\n",
      "\tTraining batch 11 Loss: 0.015184\n",
      "\tTraining batch 12 Loss: 0.004859\n",
      "\tTraining batch 13 Loss: 0.155163\n",
      "Training set: Average loss: 0.034517\n",
      "Validation set: Average loss: 2.491155, Accuracy: 1398/1959 (71.36%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000024\n",
      "\tTraining batch 2 Loss: 0.000033\n",
      "\tTraining batch 3 Loss: 0.001090\n",
      "\tTraining batch 4 Loss: 0.011003\n",
      "\tTraining batch 5 Loss: 0.000318\n",
      "\tTraining batch 6 Loss: 0.000392\n",
      "\tTraining batch 7 Loss: 0.000955\n",
      "\tTraining batch 8 Loss: 0.000485\n",
      "\tTraining batch 9 Loss: 0.000021\n",
      "\tTraining batch 10 Loss: 0.000741\n",
      "\tTraining batch 11 Loss: 0.024657\n",
      "\tTraining batch 12 Loss: 0.002396\n",
      "\tTraining batch 13 Loss: 0.054943\n",
      "Training set: Average loss: 0.007466\n",
      "Validation set: Average loss: 3.054839, Accuracy: 1298/1959 (66.26%)\n",
      "\n",
      "Epoch: 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 1 Loss: 0.000025\n",
      "\tTraining batch 2 Loss: 0.000075\n",
      "\tTraining batch 3 Loss: 0.003481\n",
      "\tTraining batch 4 Loss: 0.000076\n",
      "\tTraining batch 5 Loss: 0.000096\n",
      "\tTraining batch 6 Loss: 0.015451\n",
      "\tTraining batch 7 Loss: 0.000709\n",
      "\tTraining batch 8 Loss: 0.000337\n",
      "\tTraining batch 9 Loss: 0.000005\n",
      "\tTraining batch 10 Loss: 0.000301\n",
      "\tTraining batch 11 Loss: 0.022677\n",
      "\tTraining batch 12 Loss: 0.000492\n",
      "\tTraining batch 13 Loss: 0.047081\n",
      "Training set: Average loss: 0.006985\n",
      "Validation set: Average loss: 2.573052, Accuracy: 1362/1959 (69.53%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000031\n",
      "\tTraining batch 2 Loss: 0.000204\n",
      "\tTraining batch 3 Loss: 0.000140\n",
      "\tTraining batch 4 Loss: 0.000061\n",
      "\tTraining batch 5 Loss: 0.000094\n",
      "\tTraining batch 6 Loss: 0.000013\n",
      "\tTraining batch 7 Loss: 0.000949\n",
      "\tTraining batch 8 Loss: 0.000250\n",
      "\tTraining batch 9 Loss: 0.000003\n",
      "\tTraining batch 10 Loss: 0.000214\n",
      "\tTraining batch 11 Loss: 0.019308\n",
      "\tTraining batch 12 Loss: 0.000305\n",
      "\tTraining batch 13 Loss: 0.041054\n",
      "Training set: Average loss: 0.004817\n",
      "Validation set: Average loss: 2.602768, Accuracy: 1377/1959 (70.29%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000025\n",
      "\tTraining batch 2 Loss: 0.000146\n",
      "\tTraining batch 3 Loss: 0.000044\n",
      "\tTraining batch 4 Loss: 0.000040\n",
      "\tTraining batch 5 Loss: 0.000068\n",
      "\tTraining batch 6 Loss: 0.000009\n",
      "\tTraining batch 7 Loss: 0.000662\n",
      "\tTraining batch 8 Loss: 0.000162\n",
      "\tTraining batch 9 Loss: 0.000002\n",
      "\tTraining batch 10 Loss: 0.000172\n",
      "\tTraining batch 11 Loss: 0.016989\n",
      "\tTraining batch 12 Loss: 0.000233\n",
      "\tTraining batch 13 Loss: 0.034778\n",
      "Training set: Average loss: 0.004102\n",
      "Validation set: Average loss: 2.652272, Accuracy: 1381/1959 (70.50%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000020\n",
      "\tTraining batch 2 Loss: 0.000106\n",
      "\tTraining batch 3 Loss: 0.000027\n",
      "\tTraining batch 4 Loss: 0.000029\n",
      "\tTraining batch 5 Loss: 0.000057\n",
      "\tTraining batch 6 Loss: 0.000007\n",
      "\tTraining batch 7 Loss: 0.000489\n",
      "\tTraining batch 8 Loss: 0.000112\n",
      "\tTraining batch 9 Loss: 0.000002\n",
      "\tTraining batch 10 Loss: 0.000140\n",
      "\tTraining batch 11 Loss: 0.014867\n",
      "\tTraining batch 12 Loss: 0.000197\n",
      "\tTraining batch 13 Loss: 0.028043\n",
      "Training set: Average loss: 0.003392\n",
      "Validation set: Average loss: 2.701696, Accuracy: 1379/1959 (70.39%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000016\n",
      "\tTraining batch 2 Loss: 0.000076\n",
      "\tTraining batch 3 Loss: 0.000019\n",
      "\tTraining batch 4 Loss: 0.000022\n",
      "\tTraining batch 5 Loss: 0.000051\n",
      "\tTraining batch 6 Loss: 0.000006\n",
      "\tTraining batch 7 Loss: 0.000383\n",
      "\tTraining batch 8 Loss: 0.000082\n",
      "\tTraining batch 9 Loss: 0.000001\n",
      "\tTraining batch 10 Loss: 0.000115\n",
      "\tTraining batch 11 Loss: 0.012884\n",
      "\tTraining batch 12 Loss: 0.000178\n",
      "\tTraining batch 13 Loss: 0.021421\n",
      "Training set: Average loss: 0.002712\n",
      "Validation set: Average loss: 2.751424, Accuracy: 1383/1959 (70.60%)\n",
      "\n",
      "Epoch: 11\n",
      "\tTraining batch 1 Loss: 0.000013\n",
      "\tTraining batch 2 Loss: 0.000054\n",
      "\tTraining batch 3 Loss: 0.000014\n",
      "\tTraining batch 4 Loss: 0.000017\n",
      "\tTraining batch 5 Loss: 0.000048\n",
      "\tTraining batch 6 Loss: 0.000005\n",
      "\tTraining batch 7 Loss: 0.000308\n",
      "\tTraining batch 8 Loss: 0.000062\n",
      "\tTraining batch 9 Loss: 0.000001\n",
      "\tTraining batch 10 Loss: 0.000095\n",
      "\tTraining batch 11 Loss: 0.011009\n",
      "\tTraining batch 12 Loss: 0.000165\n",
      "\tTraining batch 13 Loss: 0.015229\n",
      "Training set: Average loss: 0.002078\n",
      "Validation set: Average loss: 2.804782, Accuracy: 1380/1959 (70.44%)\n",
      "\n",
      "Epoch: 12\n",
      "\tTraining batch 1 Loss: 0.000010\n",
      "\tTraining batch 2 Loss: 0.000040\n",
      "\tTraining batch 3 Loss: 0.000011\n",
      "\tTraining batch 4 Loss: 0.000013\n",
      "\tTraining batch 5 Loss: 0.000045\n",
      "\tTraining batch 6 Loss: 0.000004\n",
      "\tTraining batch 7 Loss: 0.000251\n",
      "\tTraining batch 8 Loss: 0.000048\n",
      "\tTraining batch 9 Loss: 0.000001\n",
      "\tTraining batch 10 Loss: 0.000080\n",
      "\tTraining batch 11 Loss: 0.009338\n",
      "\tTraining batch 12 Loss: 0.000158\n",
      "\tTraining batch 13 Loss: 0.010282\n",
      "Training set: Average loss: 0.001560\n",
      "Validation set: Average loss: 2.860412, Accuracy: 1382/1959 (70.55%)\n",
      "\n",
      "Epoch: 13\n",
      "\tTraining batch 1 Loss: 0.000008\n",
      "\tTraining batch 2 Loss: 0.000031\n",
      "\tTraining batch 3 Loss: 0.000008\n",
      "\tTraining batch 4 Loss: 0.000011\n",
      "\tTraining batch 5 Loss: 0.000043\n",
      "\tTraining batch 6 Loss: 0.000004\n",
      "\tTraining batch 7 Loss: 0.000212\n",
      "\tTraining batch 8 Loss: 0.000039\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000069\n",
      "\tTraining batch 11 Loss: 0.008000\n",
      "\tTraining batch 12 Loss: 0.000150\n",
      "\tTraining batch 13 Loss: 0.007113\n",
      "Training set: Average loss: 0.001207\n",
      "Validation set: Average loss: 2.911854, Accuracy: 1380/1959 (70.44%)\n",
      "\n",
      "Epoch: 14\n",
      "\tTraining batch 1 Loss: 0.000006\n",
      "\tTraining batch 2 Loss: 0.000025\n",
      "\tTraining batch 3 Loss: 0.000007\n",
      "\tTraining batch 4 Loss: 0.000010\n",
      "\tTraining batch 5 Loss: 0.000042\n",
      "\tTraining batch 6 Loss: 0.000003\n",
      "\tTraining batch 7 Loss: 0.000183\n",
      "\tTraining batch 8 Loss: 0.000033\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000060\n",
      "\tTraining batch 11 Loss: 0.006918\n",
      "\tTraining batch 12 Loss: 0.000142\n",
      "\tTraining batch 13 Loss: 0.005004\n",
      "Training set: Average loss: 0.000956\n",
      "Validation set: Average loss: 2.959396, Accuracy: 1381/1959 (70.50%)\n",
      "\n",
      "Epoch: 15\n",
      "\tTraining batch 1 Loss: 0.000005\n",
      "\tTraining batch 2 Loss: 0.000021\n",
      "\tTraining batch 3 Loss: 0.000005\n",
      "\tTraining batch 4 Loss: 0.000009\n",
      "\tTraining batch 5 Loss: 0.000041\n",
      "\tTraining batch 6 Loss: 0.000003\n",
      "\tTraining batch 7 Loss: 0.000161\n",
      "\tTraining batch 8 Loss: 0.000028\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000053\n",
      "\tTraining batch 11 Loss: 0.006030\n",
      "\tTraining batch 12 Loss: 0.000135\n",
      "\tTraining batch 13 Loss: 0.003692\n",
      "Training set: Average loss: 0.000783\n",
      "Validation set: Average loss: 3.002634, Accuracy: 1378/1959 (70.34%)\n",
      "\n",
      "Epoch: 16\n",
      "\tTraining batch 1 Loss: 0.000004\n",
      "\tTraining batch 2 Loss: 0.000018\n",
      "\tTraining batch 3 Loss: 0.000005\n",
      "\tTraining batch 4 Loss: 0.000008\n",
      "\tTraining batch 5 Loss: 0.000040\n",
      "\tTraining batch 6 Loss: 0.000003\n",
      "\tTraining batch 7 Loss: 0.000143\n",
      "\tTraining batch 8 Loss: 0.000025\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000048\n",
      "\tTraining batch 11 Loss: 0.005274\n",
      "\tTraining batch 12 Loss: 0.000128\n",
      "\tTraining batch 13 Loss: 0.002855\n",
      "Training set: Average loss: 0.000658\n",
      "Validation set: Average loss: 3.042087, Accuracy: 1380/1959 (70.44%)\n",
      "\n",
      "Epoch: 17\n",
      "\tTraining batch 1 Loss: 0.000004\n",
      "\tTraining batch 2 Loss: 0.000016\n",
      "\tTraining batch 3 Loss: 0.000004\n",
      "\tTraining batch 4 Loss: 0.000008\n",
      "\tTraining batch 5 Loss: 0.000040\n",
      "\tTraining batch 6 Loss: 0.000002\n",
      "\tTraining batch 7 Loss: 0.000128\n",
      "\tTraining batch 8 Loss: 0.000022\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000044\n",
      "\tTraining batch 11 Loss: 0.004655\n",
      "\tTraining batch 12 Loss: 0.000122\n",
      "\tTraining batch 13 Loss: 0.002294\n",
      "Training set: Average loss: 0.000564\n",
      "Validation set: Average loss: 3.077823, Accuracy: 1378/1959 (70.34%)\n",
      "\n",
      "Epoch: 18\n",
      "\tTraining batch 1 Loss: 0.000003\n",
      "\tTraining batch 2 Loss: 0.000014\n",
      "\tTraining batch 3 Loss: 0.000004\n",
      "\tTraining batch 4 Loss: 0.000007\n",
      "\tTraining batch 5 Loss: 0.000039\n",
      "\tTraining batch 6 Loss: 0.000002\n",
      "\tTraining batch 7 Loss: 0.000116\n",
      "\tTraining batch 8 Loss: 0.000021\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000040\n",
      "\tTraining batch 11 Loss: 0.004139\n",
      "\tTraining batch 12 Loss: 0.000117\n",
      "\tTraining batch 13 Loss: 0.001892\n",
      "Training set: Average loss: 0.000492\n",
      "Validation set: Average loss: 3.110572, Accuracy: 1375/1959 (70.19%)\n",
      "\n",
      "Epoch: 19\n",
      "\tTraining batch 1 Loss: 0.000003\n",
      "\tTraining batch 2 Loss: 0.000012\n",
      "\tTraining batch 3 Loss: 0.000003\n",
      "\tTraining batch 4 Loss: 0.000007\n",
      "\tTraining batch 5 Loss: 0.000039\n",
      "\tTraining batch 6 Loss: 0.000002\n",
      "\tTraining batch 7 Loss: 0.000106\n",
      "\tTraining batch 8 Loss: 0.000019\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000037\n",
      "\tTraining batch 11 Loss: 0.003704\n",
      "\tTraining batch 12 Loss: 0.000112\n",
      "\tTraining batch 13 Loss: 0.001598\n",
      "Training set: Average loss: 0.000434\n",
      "Validation set: Average loss: 3.140739, Accuracy: 1374/1959 (70.14%)\n",
      "\n",
      "Epoch: 20\n",
      "\tTraining batch 1 Loss: 0.000002\n",
      "\tTraining batch 2 Loss: 0.000011\n",
      "\tTraining batch 3 Loss: 0.000003\n",
      "\tTraining batch 4 Loss: 0.000006\n",
      "\tTraining batch 5 Loss: 0.000039\n",
      "\tTraining batch 6 Loss: 0.000002\n",
      "\tTraining batch 7 Loss: 0.000097\n",
      "\tTraining batch 8 Loss: 0.000018\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000034\n",
      "\tTraining batch 11 Loss: 0.003336\n",
      "\tTraining batch 12 Loss: 0.000107\n",
      "\tTraining batch 13 Loss: 0.001375\n",
      "Training set: Average loss: 0.000387\n",
      "Validation set: Average loss: 3.168721, Accuracy: 1375/1959 (70.19%)\n",
      "\n",
      "Epoch: 21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 1 Loss: 0.000002\n",
      "\tTraining batch 2 Loss: 0.000010\n",
      "\tTraining batch 3 Loss: 0.000003\n",
      "\tTraining batch 4 Loss: 0.000006\n",
      "\tTraining batch 5 Loss: 0.000038\n",
      "\tTraining batch 6 Loss: 0.000002\n",
      "\tTraining batch 7 Loss: 0.000090\n",
      "\tTraining batch 8 Loss: 0.000017\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000032\n",
      "\tTraining batch 11 Loss: 0.003021\n",
      "\tTraining batch 12 Loss: 0.000103\n",
      "\tTraining batch 13 Loss: 0.001200\n",
      "Training set: Average loss: 0.000348\n",
      "Validation set: Average loss: 3.194849, Accuracy: 1374/1959 (70.14%)\n",
      "\n",
      "Epoch: 22\n",
      "\tTraining batch 1 Loss: 0.000002\n",
      "\tTraining batch 2 Loss: 0.000009\n",
      "\tTraining batch 3 Loss: 0.000002\n",
      "\tTraining batch 4 Loss: 0.000006\n",
      "\tTraining batch 5 Loss: 0.000038\n",
      "\tTraining batch 6 Loss: 0.000002\n",
      "\tTraining batch 7 Loss: 0.000083\n",
      "\tTraining batch 8 Loss: 0.000016\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000030\n",
      "\tTraining batch 11 Loss: 0.002750\n",
      "\tTraining batch 12 Loss: 0.000100\n",
      "\tTraining batch 13 Loss: 0.001059\n",
      "Training set: Average loss: 0.000315\n",
      "Validation set: Average loss: 3.219489, Accuracy: 1374/1959 (70.14%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 47.5%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000002\n",
      "\tTraining batch 2 Loss: 0.000009\n",
      "\tTraining batch 3 Loss: 0.000002\n",
      "\tTraining batch 4 Loss: 0.000005\n",
      "\tTraining batch 5 Loss: 0.000038\n",
      "\tTraining batch 6 Loss: 0.000002\n",
      "\tTraining batch 7 Loss: 0.000078\n",
      "\tTraining batch 8 Loss: 0.000015\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000028\n",
      "\tTraining batch 11 Loss: 0.002514\n",
      "\tTraining batch 12 Loss: 0.000096\n",
      "\tTraining batch 13 Loss: 0.000943\n",
      "\tTraining batch 14 Loss: 3.201457\n",
      "Training set: Average loss: 0.228942\n",
      "Validation set: Average loss: 2.761666, Accuracy: 1370/1959 (69.93%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000007\n",
      "\tTraining batch 2 Loss: 0.000138\n",
      "\tTraining batch 3 Loss: 0.001996\n",
      "\tTraining batch 4 Loss: 0.051532\n",
      "\tTraining batch 5 Loss: 0.023516\n",
      "\tTraining batch 6 Loss: 0.179296\n",
      "\tTraining batch 7 Loss: 0.128953\n",
      "\tTraining batch 8 Loss: 0.197407\n",
      "\tTraining batch 9 Loss: 0.003767\n",
      "\tTraining batch 10 Loss: 0.002798\n",
      "\tTraining batch 11 Loss: 0.025789\n",
      "\tTraining batch 12 Loss: 0.004584\n",
      "\tTraining batch 13 Loss: 0.145335\n",
      "\tTraining batch 14 Loss: 0.036826\n",
      "Training set: Average loss: 0.057282\n",
      "Validation set: Average loss: 2.294894, Accuracy: 1371/1959 (69.98%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000066\n",
      "\tTraining batch 2 Loss: 0.000118\n",
      "\tTraining batch 3 Loss: 0.000358\n",
      "\tTraining batch 4 Loss: 0.000196\n",
      "\tTraining batch 5 Loss: 0.000893\n",
      "\tTraining batch 6 Loss: 0.000684\n",
      "\tTraining batch 7 Loss: 0.000407\n",
      "\tTraining batch 8 Loss: 0.000318\n",
      "\tTraining batch 9 Loss: 0.000004\n",
      "\tTraining batch 10 Loss: 0.013583\n",
      "\tTraining batch 11 Loss: 0.025632\n",
      "\tTraining batch 12 Loss: 0.148858\n",
      "\tTraining batch 13 Loss: 0.143899\n",
      "\tTraining batch 14 Loss: 0.014629\n",
      "Training set: Average loss: 0.024975\n",
      "Validation set: Average loss: 3.355557, Accuracy: 1330/1959 (67.89%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000164\n",
      "\tTraining batch 2 Loss: 0.000047\n",
      "\tTraining batch 3 Loss: 0.000132\n",
      "\tTraining batch 4 Loss: 0.000432\n",
      "\tTraining batch 5 Loss: 0.000114\n",
      "\tTraining batch 6 Loss: 0.002804\n",
      "\tTraining batch 7 Loss: 0.000082\n",
      "\tTraining batch 8 Loss: 0.002603\n",
      "\tTraining batch 9 Loss: 0.000018\n",
      "\tTraining batch 10 Loss: 0.010705\n",
      "\tTraining batch 11 Loss: 0.017830\n",
      "\tTraining batch 12 Loss: 0.175176\n",
      "\tTraining batch 13 Loss: 0.010705\n",
      "\tTraining batch 14 Loss: 0.010074\n",
      "Training set: Average loss: 0.016492\n",
      "Validation set: Average loss: 3.415664, Accuracy: 1278/1959 (65.24%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000079\n",
      "\tTraining batch 2 Loss: 0.143175\n",
      "\tTraining batch 3 Loss: 0.000028\n",
      "\tTraining batch 4 Loss: 0.000063\n",
      "\tTraining batch 5 Loss: 0.000159\n",
      "\tTraining batch 6 Loss: 0.001099\n",
      "\tTraining batch 7 Loss: 0.000074\n",
      "\tTraining batch 8 Loss: 0.000501\n",
      "\tTraining batch 9 Loss: 0.002422\n",
      "\tTraining batch 10 Loss: 0.002922\n",
      "\tTraining batch 11 Loss: 0.012892\n",
      "\tTraining batch 12 Loss: 0.000332\n",
      "\tTraining batch 13 Loss: 0.010711\n",
      "\tTraining batch 14 Loss: 0.297948\n",
      "Training set: Average loss: 0.033743\n",
      "Validation set: Average loss: 4.260786, Accuracy: 1356/1959 (69.22%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000009\n",
      "\tTraining batch 2 Loss: 0.000920\n",
      "\tTraining batch 3 Loss: 0.000688\n",
      "\tTraining batch 4 Loss: 0.015615\n",
      "\tTraining batch 5 Loss: 0.016828\n",
      "\tTraining batch 6 Loss: 0.021249\n",
      "\tTraining batch 7 Loss: 0.000208\n",
      "\tTraining batch 8 Loss: 0.097125\n",
      "\tTraining batch 9 Loss: 0.000014\n",
      "\tTraining batch 10 Loss: 0.000373\n",
      "\tTraining batch 11 Loss: 0.004080\n",
      "\tTraining batch 12 Loss: 0.002413\n",
      "\tTraining batch 13 Loss: 0.004339\n",
      "\tTraining batch 14 Loss: 0.001779\n",
      "Training set: Average loss: 0.011832\n",
      "Validation set: Average loss: 2.752091, Accuracy: 1372/1959 (70.04%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000178\n",
      "\tTraining batch 2 Loss: 0.000086\n",
      "\tTraining batch 3 Loss: 0.000455\n",
      "\tTraining batch 4 Loss: 0.000962\n",
      "\tTraining batch 5 Loss: 0.000241\n",
      "\tTraining batch 6 Loss: 0.000742\n",
      "\tTraining batch 7 Loss: 0.000554\n",
      "\tTraining batch 8 Loss: 0.000341\n",
      "\tTraining batch 9 Loss: 0.000028\n",
      "\tTraining batch 10 Loss: 0.000476\n",
      "\tTraining batch 11 Loss: 0.008648\n",
      "\tTraining batch 12 Loss: 0.002192\n",
      "\tTraining batch 13 Loss: 0.006391\n",
      "\tTraining batch 14 Loss: 0.003379\n",
      "Training set: Average loss: 0.001762\n",
      "Validation set: Average loss: 2.770040, Accuracy: 1347/1959 (68.76%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000221\n",
      "\tTraining batch 2 Loss: 0.000101\n",
      "\tTraining batch 3 Loss: 0.000339\n",
      "\tTraining batch 4 Loss: 0.000937\n",
      "\tTraining batch 5 Loss: 0.000173\n",
      "\tTraining batch 6 Loss: 0.000453\n",
      "\tTraining batch 7 Loss: 0.000272\n",
      "\tTraining batch 8 Loss: 0.000184\n",
      "\tTraining batch 9 Loss: 0.000015\n",
      "\tTraining batch 10 Loss: 0.000385\n",
      "\tTraining batch 11 Loss: 0.007891\n",
      "\tTraining batch 12 Loss: 0.000525\n",
      "\tTraining batch 13 Loss: 0.004264\n",
      "\tTraining batch 14 Loss: 0.000831\n",
      "Training set: Average loss: 0.001185\n",
      "Validation set: Average loss: 2.954207, Accuracy: 1344/1959 (68.61%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000118\n",
      "\tTraining batch 2 Loss: 0.000046\n",
      "\tTraining batch 3 Loss: 0.000185\n",
      "\tTraining batch 4 Loss: 0.000321\n",
      "\tTraining batch 5 Loss: 0.000101\n",
      "\tTraining batch 6 Loss: 0.000215\n",
      "\tTraining batch 7 Loss: 0.000163\n",
      "\tTraining batch 8 Loss: 0.000100\n",
      "\tTraining batch 9 Loss: 0.000008\n",
      "\tTraining batch 10 Loss: 0.000270\n",
      "\tTraining batch 11 Loss: 0.006569\n",
      "\tTraining batch 12 Loss: 0.000258\n",
      "\tTraining batch 13 Loss: 0.002814\n",
      "\tTraining batch 14 Loss: 0.000406\n",
      "Training set: Average loss: 0.000827\n",
      "Validation set: Average loss: 3.060352, Accuracy: 1349/1959 (68.86%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000080\n",
      "\tTraining batch 2 Loss: 0.000027\n",
      "\tTraining batch 3 Loss: 0.000132\n",
      "\tTraining batch 4 Loss: 0.000177\n",
      "\tTraining batch 5 Loss: 0.000072\n",
      "\tTraining batch 6 Loss: 0.000139\n",
      "\tTraining batch 7 Loss: 0.000123\n",
      "\tTraining batch 8 Loss: 0.000073\n",
      "\tTraining batch 9 Loss: 0.000005\n",
      "\tTraining batch 10 Loss: 0.000195\n",
      "\tTraining batch 11 Loss: 0.005536\n",
      "\tTraining batch 12 Loss: 0.000180\n",
      "\tTraining batch 13 Loss: 0.002042\n",
      "\tTraining batch 14 Loss: 0.000293\n",
      "Training set: Average loss: 0.000648\n",
      "Validation set: Average loss: 3.125297, Accuracy: 1346/1959 (68.71%)\n",
      "\n",
      "Epoch: 11\n",
      "\tTraining batch 1 Loss: 0.000064\n",
      "\tTraining batch 2 Loss: 0.000020\n",
      "\tTraining batch 3 Loss: 0.000111\n",
      "\tTraining batch 4 Loss: 0.000125\n",
      "\tTraining batch 5 Loss: 0.000059\n",
      "\tTraining batch 6 Loss: 0.000107\n",
      "\tTraining batch 7 Loss: 0.000100\n",
      "\tTraining batch 8 Loss: 0.000061\n",
      "\tTraining batch 9 Loss: 0.000004\n",
      "\tTraining batch 10 Loss: 0.000151\n",
      "\tTraining batch 11 Loss: 0.004776\n",
      "\tTraining batch 12 Loss: 0.000144\n",
      "\tTraining batch 13 Loss: 0.001566\n",
      "\tTraining batch 14 Loss: 0.000238\n",
      "Training set: Average loss: 0.000538\n",
      "Validation set: Average loss: 3.172750, Accuracy: 1353/1959 (69.07%)\n",
      "\n",
      "Epoch: 12\n",
      "\tTraining batch 1 Loss: 0.000055\n",
      "\tTraining batch 2 Loss: 0.000016\n",
      "\tTraining batch 3 Loss: 0.000100\n",
      "\tTraining batch 4 Loss: 0.000099\n",
      "\tTraining batch 5 Loss: 0.000051\n",
      "\tTraining batch 6 Loss: 0.000089\n",
      "\tTraining batch 7 Loss: 0.000084\n",
      "\tTraining batch 8 Loss: 0.000053\n",
      "\tTraining batch 9 Loss: 0.000003\n",
      "\tTraining batch 10 Loss: 0.000122\n",
      "\tTraining batch 11 Loss: 0.004182\n",
      "\tTraining batch 12 Loss: 0.000123\n",
      "\tTraining batch 13 Loss: 0.001245\n",
      "\tTraining batch 14 Loss: 0.000199\n",
      "Training set: Average loss: 0.000459\n",
      "Validation set: Average loss: 3.212099, Accuracy: 1354/1959 (69.12%)\n",
      "\n",
      "Epoch: 13\n",
      "\tTraining batch 1 Loss: 0.000049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 2 Loss: 0.000013\n",
      "\tTraining batch 3 Loss: 0.000092\n",
      "\tTraining batch 4 Loss: 0.000083\n",
      "\tTraining batch 5 Loss: 0.000046\n",
      "\tTraining batch 6 Loss: 0.000076\n",
      "\tTraining batch 7 Loss: 0.000073\n",
      "\tTraining batch 8 Loss: 0.000048\n",
      "\tTraining batch 9 Loss: 0.000003\n",
      "\tTraining batch 10 Loss: 0.000101\n",
      "\tTraining batch 11 Loss: 0.003705\n",
      "\tTraining batch 12 Loss: 0.000109\n",
      "\tTraining batch 13 Loss: 0.001016\n",
      "\tTraining batch 14 Loss: 0.000170\n",
      "Training set: Average loss: 0.000399\n",
      "Validation set: Average loss: 3.246818, Accuracy: 1357/1959 (69.27%)\n",
      "\n",
      "Epoch: 14\n",
      "\tTraining batch 1 Loss: 0.000044\n",
      "\tTraining batch 2 Loss: 0.000011\n",
      "\tTraining batch 3 Loss: 0.000086\n",
      "\tTraining batch 4 Loss: 0.000072\n",
      "\tTraining batch 5 Loss: 0.000042\n",
      "\tTraining batch 6 Loss: 0.000068\n",
      "\tTraining batch 7 Loss: 0.000064\n",
      "\tTraining batch 8 Loss: 0.000044\n",
      "\tTraining batch 9 Loss: 0.000002\n",
      "\tTraining batch 10 Loss: 0.000086\n",
      "\tTraining batch 11 Loss: 0.003294\n",
      "\tTraining batch 12 Loss: 0.000098\n",
      "\tTraining batch 13 Loss: 0.000848\n",
      "\tTraining batch 14 Loss: 0.000148\n",
      "Training set: Average loss: 0.000350\n",
      "Validation set: Average loss: 3.278532, Accuracy: 1358/1959 (69.32%)\n",
      "\n",
      "Epoch: 15\n",
      "\tTraining batch 1 Loss: 0.000040\n",
      "\tTraining batch 2 Loss: 0.000010\n",
      "\tTraining batch 3 Loss: 0.000080\n",
      "\tTraining batch 4 Loss: 0.000063\n",
      "\tTraining batch 5 Loss: 0.000039\n",
      "\tTraining batch 6 Loss: 0.000061\n",
      "\tTraining batch 7 Loss: 0.000057\n",
      "\tTraining batch 8 Loss: 0.000041\n",
      "\tTraining batch 9 Loss: 0.000002\n",
      "\tTraining batch 10 Loss: 0.000074\n",
      "\tTraining batch 11 Loss: 0.002932\n",
      "\tTraining batch 12 Loss: 0.000088\n",
      "\tTraining batch 13 Loss: 0.000716\n",
      "\tTraining batch 14 Loss: 0.000131\n",
      "Training set: Average loss: 0.000309\n",
      "Validation set: Average loss: 3.308290, Accuracy: 1358/1959 (69.32%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 45.75%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000036\n",
      "\tTraining batch 2 Loss: 0.000008\n",
      "\tTraining batch 3 Loss: 0.000076\n",
      "\tTraining batch 4 Loss: 0.000057\n",
      "\tTraining batch 5 Loss: 0.000036\n",
      "\tTraining batch 6 Loss: 0.000055\n",
      "\tTraining batch 7 Loss: 0.000051\n",
      "\tTraining batch 8 Loss: 0.000038\n",
      "\tTraining batch 9 Loss: 0.000002\n",
      "\tTraining batch 10 Loss: 0.000065\n",
      "\tTraining batch 11 Loss: 0.002626\n",
      "\tTraining batch 12 Loss: 0.000080\n",
      "\tTraining batch 13 Loss: 0.000614\n",
      "\tTraining batch 14 Loss: 0.000116\n",
      "\tTraining batch 15 Loss: 3.986241\n",
      "Training set: Average loss: 0.266007\n",
      "Validation set: Average loss: 3.240292, Accuracy: 1316/1959 (67.18%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000050\n",
      "\tTraining batch 2 Loss: 0.000097\n",
      "\tTraining batch 3 Loss: 0.000909\n",
      "\tTraining batch 4 Loss: 0.006451\n",
      "\tTraining batch 5 Loss: 0.130382\n",
      "\tTraining batch 6 Loss: 0.406357\n",
      "\tTraining batch 7 Loss: 0.416344\n",
      "\tTraining batch 8 Loss: 0.001981\n",
      "\tTraining batch 9 Loss: 0.000812\n",
      "\tTraining batch 10 Loss: 0.004296\n",
      "\tTraining batch 11 Loss: 0.023106\n",
      "\tTraining batch 12 Loss: 0.012581\n",
      "\tTraining batch 13 Loss: 0.026290\n",
      "\tTraining batch 14 Loss: 0.012658\n",
      "\tTraining batch 15 Loss: 1.028319\n",
      "Training set: Average loss: 0.138042\n",
      "Validation set: Average loss: 2.511563, Accuracy: 1277/1959 (65.19%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.002806\n",
      "\tTraining batch 2 Loss: 0.001037\n",
      "\tTraining batch 3 Loss: 0.001663\n",
      "\tTraining batch 4 Loss: 0.005409\n",
      "\tTraining batch 5 Loss: 0.002587\n",
      "\tTraining batch 6 Loss: 0.013496\n",
      "\tTraining batch 7 Loss: 0.006181\n",
      "\tTraining batch 8 Loss: 0.004682\n",
      "\tTraining batch 9 Loss: 0.002726\n",
      "\tTraining batch 10 Loss: 0.026867\n",
      "\tTraining batch 11 Loss: 0.015221\n",
      "\tTraining batch 12 Loss: 0.001474\n",
      "\tTraining batch 13 Loss: 0.039023\n",
      "\tTraining batch 14 Loss: 0.003339\n",
      "\tTraining batch 15 Loss: 0.002919\n",
      "Training set: Average loss: 0.008629\n",
      "Validation set: Average loss: 1.991161, Accuracy: 1419/1959 (72.43%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000443\n",
      "\tTraining batch 2 Loss: 0.004178\n",
      "\tTraining batch 3 Loss: 0.001730\n",
      "\tTraining batch 4 Loss: 0.025461\n",
      "\tTraining batch 5 Loss: 0.006582\n",
      "\tTraining batch 6 Loss: 0.015147\n",
      "\tTraining batch 7 Loss: 0.010552\n",
      "\tTraining batch 8 Loss: 0.001547\n",
      "\tTraining batch 9 Loss: 0.000642\n",
      "\tTraining batch 10 Loss: 0.002782\n",
      "\tTraining batch 11 Loss: 0.009327\n",
      "\tTraining batch 12 Loss: 0.001246\n",
      "\tTraining batch 13 Loss: 0.013828\n",
      "\tTraining batch 14 Loss: 0.002493\n",
      "\tTraining batch 15 Loss: 0.001551\n",
      "Training set: Average loss: 0.006501\n",
      "Validation set: Average loss: 2.105306, Accuracy: 1390/1959 (70.95%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000226\n",
      "\tTraining batch 2 Loss: 0.000557\n",
      "\tTraining batch 3 Loss: 0.000639\n",
      "\tTraining batch 4 Loss: 0.000655\n",
      "\tTraining batch 5 Loss: 0.001331\n",
      "\tTraining batch 6 Loss: 0.000719\n",
      "\tTraining batch 7 Loss: 0.001779\n",
      "\tTraining batch 8 Loss: 0.000540\n",
      "\tTraining batch 9 Loss: 0.000113\n",
      "\tTraining batch 10 Loss: 0.000781\n",
      "\tTraining batch 11 Loss: 0.006217\n",
      "\tTraining batch 12 Loss: 0.001266\n",
      "\tTraining batch 13 Loss: 0.007396\n",
      "\tTraining batch 14 Loss: 0.003327\n",
      "\tTraining batch 15 Loss: 0.001270\n",
      "Training set: Average loss: 0.001788\n",
      "Validation set: Average loss: 2.279166, Accuracy: 1373/1959 (70.09%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000211\n",
      "\tTraining batch 2 Loss: 0.000365\n",
      "\tTraining batch 3 Loss: 0.000494\n",
      "\tTraining batch 4 Loss: 0.000458\n",
      "\tTraining batch 5 Loss: 0.000875\n",
      "\tTraining batch 6 Loss: 0.000463\n",
      "\tTraining batch 7 Loss: 0.000945\n",
      "\tTraining batch 8 Loss: 0.000341\n",
      "\tTraining batch 9 Loss: 0.000058\n",
      "\tTraining batch 10 Loss: 0.000414\n",
      "\tTraining batch 11 Loss: 0.004649\n",
      "\tTraining batch 12 Loss: 0.000708\n",
      "\tTraining batch 13 Loss: 0.005072\n",
      "\tTraining batch 14 Loss: 0.001293\n",
      "\tTraining batch 15 Loss: 0.000793\n",
      "Training set: Average loss: 0.001143\n",
      "Validation set: Average loss: 2.407672, Accuracy: 1378/1959 (70.34%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000121\n",
      "\tTraining batch 2 Loss: 0.000247\n",
      "\tTraining batch 3 Loss: 0.000376\n",
      "\tTraining batch 4 Loss: 0.000263\n",
      "\tTraining batch 5 Loss: 0.000630\n",
      "\tTraining batch 6 Loss: 0.000356\n",
      "\tTraining batch 7 Loss: 0.000634\n",
      "\tTraining batch 8 Loss: 0.000228\n",
      "\tTraining batch 9 Loss: 0.000036\n",
      "\tTraining batch 10 Loss: 0.000277\n",
      "\tTraining batch 11 Loss: 0.003745\n",
      "\tTraining batch 12 Loss: 0.000447\n",
      "\tTraining batch 13 Loss: 0.003813\n",
      "\tTraining batch 14 Loss: 0.000756\n",
      "\tTraining batch 15 Loss: 0.000555\n",
      "Training set: Average loss: 0.000832\n",
      "Validation set: Average loss: 2.508970, Accuracy: 1380/1959 (70.44%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000077\n",
      "\tTraining batch 2 Loss: 0.000188\n",
      "\tTraining batch 3 Loss: 0.000309\n",
      "\tTraining batch 4 Loss: 0.000178\n",
      "\tTraining batch 5 Loss: 0.000504\n",
      "\tTraining batch 6 Loss: 0.000295\n",
      "\tTraining batch 7 Loss: 0.000484\n",
      "\tTraining batch 8 Loss: 0.000175\n",
      "\tTraining batch 9 Loss: 0.000025\n",
      "\tTraining batch 10 Loss: 0.000217\n",
      "\tTraining batch 11 Loss: 0.003147\n",
      "\tTraining batch 12 Loss: 0.000328\n",
      "\tTraining batch 13 Loss: 0.003005\n",
      "\tTraining batch 14 Loss: 0.000564\n",
      "\tTraining batch 15 Loss: 0.000434\n",
      "Training set: Average loss: 0.000662\n",
      "Validation set: Average loss: 2.589565, Accuracy: 1384/1959 (70.65%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000056\n",
      "\tTraining batch 2 Loss: 0.000155\n",
      "\tTraining batch 3 Loss: 0.000268\n",
      "\tTraining batch 4 Loss: 0.000136\n",
      "\tTraining batch 5 Loss: 0.000431\n",
      "\tTraining batch 6 Loss: 0.000253\n",
      "\tTraining batch 7 Loss: 0.000397\n",
      "\tTraining batch 8 Loss: 0.000144\n",
      "\tTraining batch 9 Loss: 0.000019\n",
      "\tTraining batch 10 Loss: 0.000180\n",
      "\tTraining batch 11 Loss: 0.002703\n",
      "\tTraining batch 12 Loss: 0.000263\n",
      "\tTraining batch 13 Loss: 0.002426\n",
      "\tTraining batch 14 Loss: 0.000465\n",
      "\tTraining batch 15 Loss: 0.000358\n",
      "Training set: Average loss: 0.000550\n",
      "Validation set: Average loss: 2.657540, Accuracy: 1384/1959 (70.65%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 49.0%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000045\n",
      "\tTraining batch 2 Loss: 0.000133\n",
      "\tTraining batch 3 Loss: 0.000237\n",
      "\tTraining batch 4 Loss: 0.000110\n",
      "\tTraining batch 5 Loss: 0.000381\n",
      "\tTraining batch 6 Loss: 0.000223\n",
      "\tTraining batch 7 Loss: 0.000337\n",
      "\tTraining batch 8 Loss: 0.000123\n",
      "\tTraining batch 9 Loss: 0.000016\n",
      "\tTraining batch 10 Loss: 0.000153\n",
      "\tTraining batch 11 Loss: 0.002357\n",
      "\tTraining batch 12 Loss: 0.000221\n",
      "\tTraining batch 13 Loss: 0.001997\n",
      "\tTraining batch 14 Loss: 0.000407\n",
      "\tTraining batch 15 Loss: 0.000307\n",
      "\tTraining batch 16 Loss: 1.490388\n",
      "Training set: Average loss: 0.093590\n",
      "Validation set: Average loss: 2.334887, Accuracy: 1385/1959 (70.70%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000126\n",
      "\tTraining batch 2 Loss: 0.000445\n",
      "\tTraining batch 3 Loss: 0.001406\n",
      "\tTraining batch 4 Loss: 0.008674\n",
      "\tTraining batch 5 Loss: 0.007766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 6 Loss: 0.006338\n",
      "\tTraining batch 7 Loss: 0.022793\n",
      "\tTraining batch 8 Loss: 0.038498\n",
      "\tTraining batch 9 Loss: 0.012567\n",
      "\tTraining batch 10 Loss: 0.010777\n",
      "\tTraining batch 11 Loss: 0.063173\n",
      "\tTraining batch 12 Loss: 0.094415\n",
      "\tTraining batch 13 Loss: 0.045677\n",
      "\tTraining batch 14 Loss: 0.079099\n",
      "\tTraining batch 15 Loss: 0.010406\n",
      "\tTraining batch 16 Loss: 0.151118\n",
      "Training set: Average loss: 0.034580\n",
      "Validation set: Average loss: 1.636311, Accuracy: 1412/1959 (72.08%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000658\n",
      "\tTraining batch 2 Loss: 0.001539\n",
      "\tTraining batch 3 Loss: 0.000505\n",
      "\tTraining batch 4 Loss: 0.007607\n",
      "\tTraining batch 5 Loss: 0.005514\n",
      "\tTraining batch 6 Loss: 0.007814\n",
      "\tTraining batch 7 Loss: 0.014170\n",
      "\tTraining batch 8 Loss: 0.005286\n",
      "\tTraining batch 9 Loss: 0.028290\n",
      "\tTraining batch 10 Loss: 0.040898\n",
      "\tTraining batch 11 Loss: 0.073948\n",
      "\tTraining batch 12 Loss: 0.000740\n",
      "\tTraining batch 13 Loss: 0.023931\n",
      "\tTraining batch 14 Loss: 0.014114\n",
      "\tTraining batch 15 Loss: 0.001454\n",
      "\tTraining batch 16 Loss: 0.036435\n",
      "Training set: Average loss: 0.016431\n",
      "Validation set: Average loss: 3.012736, Accuracy: 1375/1959 (70.19%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.040940\n",
      "\tTraining batch 2 Loss: 0.012757\n",
      "\tTraining batch 3 Loss: 0.001659\n",
      "\tTraining batch 4 Loss: 0.000916\n",
      "\tTraining batch 5 Loss: 0.001043\n",
      "\tTraining batch 6 Loss: 0.000441\n",
      "\tTraining batch 7 Loss: 0.000462\n",
      "\tTraining batch 8 Loss: 0.000287\n",
      "\tTraining batch 9 Loss: 0.000022\n",
      "\tTraining batch 10 Loss: 0.000140\n",
      "\tTraining batch 11 Loss: 0.003349\n",
      "\tTraining batch 12 Loss: 0.001022\n",
      "\tTraining batch 13 Loss: 0.005584\n",
      "\tTraining batch 14 Loss: 0.000622\n",
      "\tTraining batch 15 Loss: 0.008161\n",
      "\tTraining batch 16 Loss: 0.021011\n",
      "Training set: Average loss: 0.006151\n",
      "Validation set: Average loss: 3.195624, Accuracy: 1336/1959 (68.20%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000158\n",
      "\tTraining batch 2 Loss: 0.000857\n",
      "\tTraining batch 3 Loss: 0.001032\n",
      "\tTraining batch 4 Loss: 0.000296\n",
      "\tTraining batch 5 Loss: 0.000799\n",
      "\tTraining batch 6 Loss: 0.000173\n",
      "\tTraining batch 7 Loss: 0.000532\n",
      "\tTraining batch 8 Loss: 0.000353\n",
      "\tTraining batch 9 Loss: 0.000016\n",
      "\tTraining batch 10 Loss: 0.000038\n",
      "\tTraining batch 11 Loss: 0.001929\n",
      "\tTraining batch 12 Loss: 0.000533\n",
      "\tTraining batch 13 Loss: 0.001758\n",
      "\tTraining batch 14 Loss: 0.000160\n",
      "\tTraining batch 15 Loss: 0.001465\n",
      "\tTraining batch 16 Loss: 0.001642\n",
      "Training set: Average loss: 0.000734\n",
      "Validation set: Average loss: 3.139622, Accuracy: 1371/1959 (69.98%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000033\n",
      "\tTraining batch 2 Loss: 0.000215\n",
      "\tTraining batch 3 Loss: 0.000576\n",
      "\tTraining batch 4 Loss: 0.000233\n",
      "\tTraining batch 5 Loss: 0.000633\n",
      "\tTraining batch 6 Loss: 0.000194\n",
      "\tTraining batch 7 Loss: 0.000320\n",
      "\tTraining batch 8 Loss: 0.000215\n",
      "\tTraining batch 9 Loss: 0.000009\n",
      "\tTraining batch 10 Loss: 0.000022\n",
      "\tTraining batch 11 Loss: 0.001581\n",
      "\tTraining batch 12 Loss: 0.000274\n",
      "\tTraining batch 13 Loss: 0.001221\n",
      "\tTraining batch 14 Loss: 0.000092\n",
      "\tTraining batch 15 Loss: 0.000677\n",
      "\tTraining batch 16 Loss: 0.000770\n",
      "Training set: Average loss: 0.000442\n",
      "Validation set: Average loss: 3.197390, Accuracy: 1379/1959 (70.39%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000017\n",
      "\tTraining batch 2 Loss: 0.000127\n",
      "\tTraining batch 3 Loss: 0.000378\n",
      "\tTraining batch 4 Loss: 0.000200\n",
      "\tTraining batch 5 Loss: 0.000413\n",
      "\tTraining batch 6 Loss: 0.000172\n",
      "\tTraining batch 7 Loss: 0.000244\n",
      "\tTraining batch 8 Loss: 0.000161\n",
      "\tTraining batch 9 Loss: 0.000006\n",
      "\tTraining batch 10 Loss: 0.000016\n",
      "\tTraining batch 11 Loss: 0.001410\n",
      "\tTraining batch 12 Loss: 0.000194\n",
      "\tTraining batch 13 Loss: 0.001015\n",
      "\tTraining batch 14 Loss: 0.000074\n",
      "\tTraining batch 15 Loss: 0.000479\n",
      "\tTraining batch 16 Loss: 0.000554\n",
      "Training set: Average loss: 0.000341\n",
      "Validation set: Average loss: 3.256271, Accuracy: 1382/1959 (70.55%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000013\n",
      "\tTraining batch 2 Loss: 0.000100\n",
      "\tTraining batch 3 Loss: 0.000298\n",
      "\tTraining batch 4 Loss: 0.000179\n",
      "\tTraining batch 5 Loss: 0.000285\n",
      "\tTraining batch 6 Loss: 0.000147\n",
      "\tTraining batch 7 Loss: 0.000204\n",
      "\tTraining batch 8 Loss: 0.000134\n",
      "\tTraining batch 9 Loss: 0.000005\n",
      "\tTraining batch 10 Loss: 0.000014\n",
      "\tTraining batch 11 Loss: 0.001281\n",
      "\tTraining batch 12 Loss: 0.000154\n",
      "\tTraining batch 13 Loss: 0.000886\n",
      "\tTraining batch 14 Loss: 0.000065\n",
      "\tTraining batch 15 Loss: 0.000390\n",
      "\tTraining batch 16 Loss: 0.000451\n",
      "Training set: Average loss: 0.000288\n",
      "Validation set: Average loss: 3.307403, Accuracy: 1384/1959 (70.65%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000011\n",
      "\tTraining batch 2 Loss: 0.000086\n",
      "\tTraining batch 3 Loss: 0.000253\n",
      "\tTraining batch 4 Loss: 0.000164\n",
      "\tTraining batch 5 Loss: 0.000221\n",
      "\tTraining batch 6 Loss: 0.000128\n",
      "\tTraining batch 7 Loss: 0.000177\n",
      "\tTraining batch 8 Loss: 0.000117\n",
      "\tTraining batch 9 Loss: 0.000004\n",
      "\tTraining batch 10 Loss: 0.000012\n",
      "\tTraining batch 11 Loss: 0.001176\n",
      "\tTraining batch 12 Loss: 0.000129\n",
      "\tTraining batch 13 Loss: 0.000788\n",
      "\tTraining batch 14 Loss: 0.000060\n",
      "\tTraining batch 15 Loss: 0.000334\n",
      "\tTraining batch 16 Loss: 0.000385\n",
      "Training set: Average loss: 0.000253\n",
      "Validation set: Average loss: 3.352309, Accuracy: 1385/1959 (70.70%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000009\n",
      "\tTraining batch 2 Loss: 0.000076\n",
      "\tTraining batch 3 Loss: 0.000221\n",
      "\tTraining batch 4 Loss: 0.000152\n",
      "\tTraining batch 5 Loss: 0.000183\n",
      "\tTraining batch 6 Loss: 0.000114\n",
      "\tTraining batch 7 Loss: 0.000156\n",
      "\tTraining batch 8 Loss: 0.000105\n",
      "\tTraining batch 9 Loss: 0.000003\n",
      "\tTraining batch 10 Loss: 0.000011\n",
      "\tTraining batch 11 Loss: 0.001086\n",
      "\tTraining batch 12 Loss: 0.000112\n",
      "\tTraining batch 13 Loss: 0.000709\n",
      "\tTraining batch 14 Loss: 0.000056\n",
      "\tTraining batch 15 Loss: 0.000294\n",
      "\tTraining batch 16 Loss: 0.000338\n",
      "Training set: Average loss: 0.000227\n",
      "Validation set: Average loss: 3.392755, Accuracy: 1386/1959 (70.75%)\n",
      "\n",
      "Epoch: 11\n",
      "\tTraining batch 1 Loss: 0.000008\n",
      "\tTraining batch 2 Loss: 0.000068\n",
      "\tTraining batch 3 Loss: 0.000197\n",
      "\tTraining batch 4 Loss: 0.000142\n",
      "\tTraining batch 5 Loss: 0.000158\n",
      "\tTraining batch 6 Loss: 0.000103\n",
      "\tTraining batch 7 Loss: 0.000140\n",
      "\tTraining batch 8 Loss: 0.000095\n",
      "\tTraining batch 9 Loss: 0.000003\n",
      "\tTraining batch 10 Loss: 0.000010\n",
      "\tTraining batch 11 Loss: 0.001008\n",
      "\tTraining batch 12 Loss: 0.000099\n",
      "\tTraining batch 13 Loss: 0.000643\n",
      "\tTraining batch 14 Loss: 0.000053\n",
      "\tTraining batch 15 Loss: 0.000262\n",
      "\tTraining batch 16 Loss: 0.000302\n",
      "Training set: Average loss: 0.000206\n",
      "Validation set: Average loss: 3.430071, Accuracy: 1387/1959 (70.80%)\n",
      "\n",
      "Epoch: 12\n",
      "\tTraining batch 1 Loss: 0.000007\n",
      "\tTraining batch 2 Loss: 0.000061\n",
      "\tTraining batch 3 Loss: 0.000177\n",
      "\tTraining batch 4 Loss: 0.000134\n",
      "\tTraining batch 5 Loss: 0.000139\n",
      "\tTraining batch 6 Loss: 0.000093\n",
      "\tTraining batch 7 Loss: 0.000126\n",
      "\tTraining batch 8 Loss: 0.000087\n",
      "\tTraining batch 9 Loss: 0.000002\n",
      "\tTraining batch 10 Loss: 0.000009\n",
      "\tTraining batch 11 Loss: 0.000938\n",
      "\tTraining batch 12 Loss: 0.000088\n",
      "\tTraining batch 13 Loss: 0.000587\n",
      "\tTraining batch 14 Loss: 0.000050\n",
      "\tTraining batch 15 Loss: 0.000237\n",
      "\tTraining batch 16 Loss: 0.000273\n",
      "Training set: Average loss: 0.000188\n",
      "Validation set: Average loss: 3.464478, Accuracy: 1388/1959 (70.85%)\n",
      "\n",
      "Epoch: 13\n",
      "\tTraining batch 1 Loss: 0.000007\n",
      "\tTraining batch 2 Loss: 0.000056\n",
      "\tTraining batch 3 Loss: 0.000161\n",
      "\tTraining batch 4 Loss: 0.000125\n",
      "\tTraining batch 5 Loss: 0.000124\n",
      "\tTraining batch 6 Loss: 0.000085\n",
      "\tTraining batch 7 Loss: 0.000115\n",
      "\tTraining batch 8 Loss: 0.000080\n",
      "\tTraining batch 9 Loss: 0.000002\n",
      "\tTraining batch 10 Loss: 0.000008\n",
      "\tTraining batch 11 Loss: 0.000877\n",
      "\tTraining batch 12 Loss: 0.000080\n",
      "\tTraining batch 13 Loss: 0.000539\n",
      "\tTraining batch 14 Loss: 0.000048\n",
      "\tTraining batch 15 Loss: 0.000217\n",
      "\tTraining batch 16 Loss: 0.000249\n",
      "Training set: Average loss: 0.000173\n",
      "Validation set: Average loss: 3.496487, Accuracy: 1386/1959 (70.75%)\n",
      "\n",
      "Epoch: 14\n",
      "\tTraining batch 1 Loss: 0.000006\n",
      "\tTraining batch 2 Loss: 0.000052\n",
      "\tTraining batch 3 Loss: 0.000148\n",
      "\tTraining batch 4 Loss: 0.000118\n",
      "\tTraining batch 5 Loss: 0.000113\n",
      "\tTraining batch 6 Loss: 0.000079\n",
      "\tTraining batch 7 Loss: 0.000106\n",
      "\tTraining batch 8 Loss: 0.000074\n",
      "\tTraining batch 9 Loss: 0.000002\n",
      "\tTraining batch 10 Loss: 0.000008\n",
      "\tTraining batch 11 Loss: 0.000822\n",
      "\tTraining batch 12 Loss: 0.000073\n",
      "\tTraining batch 13 Loss: 0.000497\n",
      "\tTraining batch 14 Loss: 0.000046\n",
      "\tTraining batch 15 Loss: 0.000199\n",
      "\tTraining batch 16 Loss: 0.000229\n",
      "Training set: Average loss: 0.000161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set: Average loss: 3.526456, Accuracy: 1385/1959 (70.70%)\n",
      "\n",
      "Epoch: 15\n",
      "\tTraining batch 1 Loss: 0.000006\n",
      "\tTraining batch 2 Loss: 0.000048\n",
      "\tTraining batch 3 Loss: 0.000137\n",
      "\tTraining batch 4 Loss: 0.000111\n",
      "\tTraining batch 5 Loss: 0.000103\n",
      "\tTraining batch 6 Loss: 0.000073\n",
      "\tTraining batch 7 Loss: 0.000097\n",
      "\tTraining batch 8 Loss: 0.000069\n",
      "\tTraining batch 9 Loss: 0.000002\n",
      "\tTraining batch 10 Loss: 0.000007\n",
      "\tTraining batch 11 Loss: 0.000773\n",
      "\tTraining batch 12 Loss: 0.000067\n",
      "\tTraining batch 13 Loss: 0.000460\n",
      "\tTraining batch 14 Loss: 0.000044\n",
      "\tTraining batch 15 Loss: 0.000185\n",
      "\tTraining batch 16 Loss: 0.000212\n",
      "Training set: Average loss: 0.000150\n",
      "Validation set: Average loss: 3.554444, Accuracy: 1386/1959 (70.75%)\n",
      "\n",
      "Epoch: 16\n",
      "\tTraining batch 1 Loss: 0.000005\n",
      "\tTraining batch 2 Loss: 0.000044\n",
      "\tTraining batch 3 Loss: 0.000127\n",
      "\tTraining batch 4 Loss: 0.000105\n",
      "\tTraining batch 5 Loss: 0.000095\n",
      "\tTraining batch 6 Loss: 0.000068\n",
      "\tTraining batch 7 Loss: 0.000090\n",
      "\tTraining batch 8 Loss: 0.000064\n",
      "\tTraining batch 9 Loss: 0.000002\n",
      "\tTraining batch 10 Loss: 0.000007\n",
      "\tTraining batch 11 Loss: 0.000729\n",
      "\tTraining batch 12 Loss: 0.000062\n",
      "\tTraining batch 13 Loss: 0.000428\n",
      "\tTraining batch 14 Loss: 0.000043\n",
      "\tTraining batch 15 Loss: 0.000172\n",
      "\tTraining batch 16 Loss: 0.000197\n",
      "Training set: Average loss: 0.000140\n",
      "Validation set: Average loss: 3.580784, Accuracy: 1385/1959 (70.70%)\n",
      "\n",
      "Epoch: 17\n",
      "\tTraining batch 1 Loss: 0.000005\n",
      "\tTraining batch 2 Loss: 0.000041\n",
      "\tTraining batch 3 Loss: 0.000119\n",
      "\tTraining batch 4 Loss: 0.000100\n",
      "\tTraining batch 5 Loss: 0.000089\n",
      "\tTraining batch 6 Loss: 0.000064\n",
      "\tTraining batch 7 Loss: 0.000084\n",
      "\tTraining batch 8 Loss: 0.000061\n",
      "\tTraining batch 9 Loss: 0.000001\n",
      "\tTraining batch 10 Loss: 0.000006\n",
      "\tTraining batch 11 Loss: 0.000689\n",
      "\tTraining batch 12 Loss: 0.000057\n",
      "\tTraining batch 13 Loss: 0.000399\n",
      "\tTraining batch 14 Loss: 0.000041\n",
      "\tTraining batch 15 Loss: 0.000160\n",
      "\tTraining batch 16 Loss: 0.000184\n",
      "Training set: Average loss: 0.000131\n",
      "Validation set: Average loss: 3.605700, Accuracy: 1386/1959 (70.75%)\n",
      "\n",
      "Epoch: 18\n",
      "\tTraining batch 1 Loss: 0.000004\n",
      "\tTraining batch 2 Loss: 0.000039\n",
      "\tTraining batch 3 Loss: 0.000112\n",
      "\tTraining batch 4 Loss: 0.000094\n",
      "\tTraining batch 5 Loss: 0.000083\n",
      "\tTraining batch 6 Loss: 0.000060\n",
      "\tTraining batch 7 Loss: 0.000078\n",
      "\tTraining batch 8 Loss: 0.000057\n",
      "\tTraining batch 9 Loss: 0.000001\n",
      "\tTraining batch 10 Loss: 0.000006\n",
      "\tTraining batch 11 Loss: 0.000653\n",
      "\tTraining batch 12 Loss: 0.000053\n",
      "\tTraining batch 13 Loss: 0.000374\n",
      "\tTraining batch 14 Loss: 0.000040\n",
      "\tTraining batch 15 Loss: 0.000150\n",
      "\tTraining batch 16 Loss: 0.000172\n",
      "Training set: Average loss: 0.000124\n",
      "Validation set: Average loss: 3.629293, Accuracy: 1386/1959 (70.75%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 46.0%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000004\n",
      "\tTraining batch 2 Loss: 0.000036\n",
      "\tTraining batch 3 Loss: 0.000105\n",
      "\tTraining batch 4 Loss: 0.000089\n",
      "\tTraining batch 5 Loss: 0.000078\n",
      "\tTraining batch 6 Loss: 0.000057\n",
      "\tTraining batch 7 Loss: 0.000073\n",
      "\tTraining batch 8 Loss: 0.000054\n",
      "\tTraining batch 9 Loss: 0.000001\n",
      "\tTraining batch 10 Loss: 0.000006\n",
      "\tTraining batch 11 Loss: 0.000620\n",
      "\tTraining batch 12 Loss: 0.000050\n",
      "\tTraining batch 13 Loss: 0.000351\n",
      "\tTraining batch 14 Loss: 0.000039\n",
      "\tTraining batch 15 Loss: 0.000141\n",
      "\tTraining batch 16 Loss: 0.000162\n",
      "\tTraining batch 17 Loss: 5.817237\n",
      "Training set: Average loss: 0.342300\n",
      "Validation set: Average loss: 2.374188, Accuracy: 1420/1959 (72.49%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000014\n",
      "\tTraining batch 2 Loss: 0.000934\n",
      "\tTraining batch 3 Loss: 0.065894\n",
      "\tTraining batch 4 Loss: 0.068509\n",
      "\tTraining batch 5 Loss: 0.371785\n",
      "\tTraining batch 6 Loss: 0.261061\n",
      "\tTraining batch 7 Loss: 0.127477\n",
      "\tTraining batch 8 Loss: 0.164125\n",
      "\tTraining batch 9 Loss: 0.209711\n",
      "\tTraining batch 10 Loss: 0.089822\n",
      "\tTraining batch 11 Loss: 0.134441\n",
      "\tTraining batch 12 Loss: 0.051844\n",
      "\tTraining batch 13 Loss: 0.145660\n",
      "\tTraining batch 14 Loss: 0.024856\n",
      "\tTraining batch 15 Loss: 0.060062\n",
      "\tTraining batch 16 Loss: 0.178669\n",
      "\tTraining batch 17 Loss: 0.191212\n",
      "Training set: Average loss: 0.126240\n",
      "Validation set: Average loss: 2.738863, Accuracy: 1293/1959 (66.00%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.008527\n",
      "\tTraining batch 2 Loss: 0.006516\n",
      "\tTraining batch 3 Loss: 0.026983\n",
      "\tTraining batch 4 Loss: 0.004678\n",
      "\tTraining batch 5 Loss: 0.001047\n",
      "\tTraining batch 6 Loss: 0.003763\n",
      "\tTraining batch 7 Loss: 0.002496\n",
      "\tTraining batch 8 Loss: 0.011296\n",
      "\tTraining batch 9 Loss: 0.005841\n",
      "\tTraining batch 10 Loss: 0.003138\n",
      "\tTraining batch 11 Loss: 0.001057\n",
      "\tTraining batch 12 Loss: 0.001708\n",
      "\tTraining batch 13 Loss: 0.000044\n",
      "\tTraining batch 14 Loss: 0.125245\n",
      "\tTraining batch 15 Loss: 0.432355\n",
      "\tTraining batch 16 Loss: 0.461257\n",
      "\tTraining batch 17 Loss: 0.007437\n",
      "Training set: Average loss: 0.064905\n",
      "Validation set: Average loss: 3.646039, Accuracy: 1360/1959 (69.42%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000002\n",
      "\tTraining batch 2 Loss: 0.000275\n",
      "\tTraining batch 3 Loss: 0.005903\n",
      "\tTraining batch 4 Loss: 0.003638\n",
      "\tTraining batch 5 Loss: 0.001400\n",
      "\tTraining batch 6 Loss: 0.097204\n",
      "\tTraining batch 7 Loss: 0.000168\n",
      "\tTraining batch 8 Loss: 0.000087\n",
      "\tTraining batch 9 Loss: 0.000022\n",
      "\tTraining batch 10 Loss: 0.000496\n",
      "\tTraining batch 11 Loss: 0.002460\n",
      "\tTraining batch 12 Loss: 0.000787\n",
      "\tTraining batch 13 Loss: 0.012915\n",
      "\tTraining batch 14 Loss: 0.000082\n",
      "\tTraining batch 15 Loss: 0.000570\n",
      "\tTraining batch 16 Loss: 0.001316\n",
      "\tTraining batch 17 Loss: 0.108686\n",
      "Training set: Average loss: 0.013883\n",
      "Validation set: Average loss: 2.419545, Accuracy: 1395/1959 (71.21%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000694\n",
      "\tTraining batch 2 Loss: 0.000146\n",
      "\tTraining batch 3 Loss: 0.000283\n",
      "\tTraining batch 4 Loss: 0.005056\n",
      "\tTraining batch 5 Loss: 0.000603\n",
      "\tTraining batch 6 Loss: 0.026804\n",
      "\tTraining batch 7 Loss: 0.001497\n",
      "\tTraining batch 8 Loss: 0.000566\n",
      "\tTraining batch 9 Loss: 0.000067\n",
      "\tTraining batch 10 Loss: 0.000471\n",
      "\tTraining batch 11 Loss: 0.006337\n",
      "\tTraining batch 12 Loss: 0.001458\n",
      "\tTraining batch 13 Loss: 0.013045\n",
      "\tTraining batch 14 Loss: 0.000773\n",
      "\tTraining batch 15 Loss: 0.000699\n",
      "\tTraining batch 16 Loss: 0.000437\n",
      "\tTraining batch 17 Loss: 0.000651\n",
      "Training set: Average loss: 0.003505\n",
      "Validation set: Average loss: 2.842082, Accuracy: 1416/1959 (72.28%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000008\n",
      "\tTraining batch 2 Loss: 0.000083\n",
      "\tTraining batch 3 Loss: 0.000177\n",
      "\tTraining batch 4 Loss: 0.000642\n",
      "\tTraining batch 5 Loss: 0.000189\n",
      "\tTraining batch 6 Loss: 0.000115\n",
      "\tTraining batch 7 Loss: 0.000613\n",
      "\tTraining batch 8 Loss: 0.000575\n",
      "\tTraining batch 9 Loss: 0.000031\n",
      "\tTraining batch 10 Loss: 0.000195\n",
      "\tTraining batch 11 Loss: 0.005771\n",
      "\tTraining batch 12 Loss: 0.000421\n",
      "\tTraining batch 13 Loss: 0.004545\n",
      "\tTraining batch 14 Loss: 0.000159\n",
      "\tTraining batch 15 Loss: 0.000119\n",
      "\tTraining batch 16 Loss: 0.000191\n",
      "\tTraining batch 17 Loss: 0.000293\n",
      "Training set: Average loss: 0.000831\n",
      "Validation set: Average loss: 2.924132, Accuracy: 1419/1959 (72.43%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000002\n",
      "\tTraining batch 2 Loss: 0.000060\n",
      "\tTraining batch 3 Loss: 0.000105\n",
      "\tTraining batch 4 Loss: 0.000153\n",
      "\tTraining batch 5 Loss: 0.000049\n",
      "\tTraining batch 6 Loss: 0.000064\n",
      "\tTraining batch 7 Loss: 0.000350\n",
      "\tTraining batch 8 Loss: 0.000194\n",
      "\tTraining batch 9 Loss: 0.000019\n",
      "\tTraining batch 10 Loss: 0.000120\n",
      "\tTraining batch 11 Loss: 0.002364\n",
      "\tTraining batch 12 Loss: 0.000290\n",
      "\tTraining batch 13 Loss: 0.002668\n",
      "\tTraining batch 14 Loss: 0.000089\n",
      "\tTraining batch 15 Loss: 0.000068\n",
      "\tTraining batch 16 Loss: 0.000112\n",
      "\tTraining batch 17 Loss: 0.000216\n",
      "Training set: Average loss: 0.000407\n",
      "Validation set: Average loss: 3.023885, Accuracy: 1416/1959 (72.28%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000001\n",
      "\tTraining batch 2 Loss: 0.000034\n",
      "\tTraining batch 3 Loss: 0.000057\n",
      "\tTraining batch 4 Loss: 0.000100\n",
      "\tTraining batch 5 Loss: 0.000033\n",
      "\tTraining batch 6 Loss: 0.000044\n",
      "\tTraining batch 7 Loss: 0.000246\n",
      "\tTraining batch 8 Loss: 0.000109\n",
      "\tTraining batch 9 Loss: 0.000018\n",
      "\tTraining batch 10 Loss: 0.000073\n",
      "\tTraining batch 11 Loss: 0.000580\n",
      "\tTraining batch 12 Loss: 0.000218\n",
      "\tTraining batch 13 Loss: 0.001607\n",
      "\tTraining batch 14 Loss: 0.000064\n",
      "\tTraining batch 15 Loss: 0.000050\n",
      "\tTraining batch 16 Loss: 0.000072\n",
      "\tTraining batch 17 Loss: 0.000172\n",
      "Training set: Average loss: 0.000205\n",
      "Validation set: Average loss: 3.100439, Accuracy: 1414/1959 (72.18%)\n",
      "\n",
      "Epoch: 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 1 Loss: 0.000001\n",
      "\tTraining batch 2 Loss: 0.000019\n",
      "\tTraining batch 3 Loss: 0.000040\n",
      "\tTraining batch 4 Loss: 0.000083\n",
      "\tTraining batch 5 Loss: 0.000028\n",
      "\tTraining batch 6 Loss: 0.000036\n",
      "\tTraining batch 7 Loss: 0.000189\n",
      "\tTraining batch 8 Loss: 0.000087\n",
      "\tTraining batch 9 Loss: 0.000017\n",
      "\tTraining batch 10 Loss: 0.000047\n",
      "\tTraining batch 11 Loss: 0.000305\n",
      "\tTraining batch 12 Loss: 0.000172\n",
      "\tTraining batch 13 Loss: 0.001034\n",
      "\tTraining batch 14 Loss: 0.000052\n",
      "\tTraining batch 15 Loss: 0.000043\n",
      "\tTraining batch 16 Loss: 0.000051\n",
      "\tTraining batch 17 Loss: 0.000144\n",
      "Training set: Average loss: 0.000138\n",
      "Validation set: Average loss: 3.149723, Accuracy: 1414/1959 (72.18%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 46.5%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000001\n",
      "\tTraining batch 2 Loss: 0.000012\n",
      "\tTraining batch 3 Loss: 0.000032\n",
      "\tTraining batch 4 Loss: 0.000073\n",
      "\tTraining batch 5 Loss: 0.000025\n",
      "\tTraining batch 6 Loss: 0.000032\n",
      "\tTraining batch 7 Loss: 0.000151\n",
      "\tTraining batch 8 Loss: 0.000076\n",
      "\tTraining batch 9 Loss: 0.000016\n",
      "\tTraining batch 10 Loss: 0.000035\n",
      "\tTraining batch 11 Loss: 0.000202\n",
      "\tTraining batch 12 Loss: 0.000142\n",
      "\tTraining batch 13 Loss: 0.000691\n",
      "\tTraining batch 14 Loss: 0.000045\n",
      "\tTraining batch 15 Loss: 0.000038\n",
      "\tTraining batch 16 Loss: 0.000040\n",
      "\tTraining batch 17 Loss: 0.000123\n",
      "\tTraining batch 18 Loss: 9.339005\n",
      "Training set: Average loss: 0.518930\n",
      "Validation set: Average loss: 1.721158, Accuracy: 1372/1959 (70.04%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.145002\n",
      "\tTraining batch 2 Loss: 0.054681\n",
      "\tTraining batch 3 Loss: 0.798619\n",
      "\tTraining batch 4 Loss: 0.493490\n",
      "\tTraining batch 5 Loss: 0.057121\n",
      "\tTraining batch 6 Loss: 0.124120\n",
      "\tTraining batch 7 Loss: 0.222953\n",
      "\tTraining batch 8 Loss: 0.232084\n",
      "\tTraining batch 9 Loss: 0.543622\n",
      "\tTraining batch 10 Loss: 0.311340\n",
      "\tTraining batch 11 Loss: 0.098029\n",
      "\tTraining batch 12 Loss: 0.296869\n",
      "\tTraining batch 13 Loss: 0.333916\n",
      "\tTraining batch 14 Loss: 0.138173\n",
      "\tTraining batch 15 Loss: 0.593552\n",
      "\tTraining batch 16 Loss: 0.341221\n",
      "\tTraining batch 17 Loss: 0.342504\n",
      "\tTraining batch 18 Loss: 0.587333\n",
      "Training set: Average loss: 0.317480\n",
      "Validation set: Average loss: 1.612644, Accuracy: 1176/1959 (60.03%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.650643\n",
      "\tTraining batch 2 Loss: 0.702624\n",
      "\tTraining batch 3 Loss: 0.231318\n",
      "\tTraining batch 4 Loss: 0.378554\n",
      "\tTraining batch 5 Loss: 0.419126\n",
      "\tTraining batch 6 Loss: 0.330601\n",
      "\tTraining batch 7 Loss: 0.072556\n",
      "\tTraining batch 8 Loss: 0.195015\n",
      "\tTraining batch 9 Loss: 0.101181\n",
      "\tTraining batch 10 Loss: 0.224924\n",
      "\tTraining batch 11 Loss: 0.146528\n",
      "\tTraining batch 12 Loss: 0.126361\n",
      "\tTraining batch 13 Loss: 0.189463\n",
      "\tTraining batch 14 Loss: 0.104144\n",
      "\tTraining batch 15 Loss: 0.043886\n",
      "\tTraining batch 16 Loss: 0.011278\n",
      "\tTraining batch 17 Loss: 0.326829\n",
      "\tTraining batch 18 Loss: 0.225350\n",
      "Training set: Average loss: 0.248910\n",
      "Validation set: Average loss: 2.245723, Accuracy: 1348/1959 (68.81%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.034077\n",
      "\tTraining batch 2 Loss: 0.129769\n",
      "\tTraining batch 3 Loss: 0.067057\n",
      "\tTraining batch 4 Loss: 0.251052\n",
      "\tTraining batch 5 Loss: 0.036015\n",
      "\tTraining batch 6 Loss: 0.181984\n",
      "\tTraining batch 7 Loss: 0.038876\n",
      "\tTraining batch 8 Loss: 0.022126\n",
      "\tTraining batch 9 Loss: 0.062245\n",
      "\tTraining batch 10 Loss: 0.171231\n",
      "\tTraining batch 11 Loss: 0.095470\n",
      "\tTraining batch 12 Loss: 0.141839\n",
      "\tTraining batch 13 Loss: 0.129709\n",
      "\tTraining batch 14 Loss: 0.010833\n",
      "\tTraining batch 15 Loss: 0.069384\n",
      "\tTraining batch 16 Loss: 0.006612\n",
      "\tTraining batch 17 Loss: 0.065960\n",
      "\tTraining batch 18 Loss: 0.018291\n",
      "Training set: Average loss: 0.085141\n",
      "Validation set: Average loss: 2.345890, Accuracy: 1358/1959 (69.32%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.017657\n",
      "\tTraining batch 2 Loss: 0.013888\n",
      "\tTraining batch 3 Loss: 0.040735\n",
      "\tTraining batch 4 Loss: 0.002606\n",
      "\tTraining batch 5 Loss: 0.048103\n",
      "\tTraining batch 6 Loss: 0.011596\n",
      "\tTraining batch 7 Loss: 0.004637\n",
      "\tTraining batch 8 Loss: 0.001068\n",
      "\tTraining batch 9 Loss: 0.002181\n",
      "\tTraining batch 10 Loss: 0.202649\n",
      "\tTraining batch 11 Loss: 0.030354\n",
      "\tTraining batch 12 Loss: 0.054992\n",
      "\tTraining batch 13 Loss: 0.062089\n",
      "\tTraining batch 14 Loss: 0.001289\n",
      "\tTraining batch 15 Loss: 0.029933\n",
      "\tTraining batch 16 Loss: 0.076698\n",
      "\tTraining batch 17 Loss: 0.022996\n",
      "\tTraining batch 18 Loss: 0.000142\n",
      "Training set: Average loss: 0.034645\n",
      "Validation set: Average loss: 2.993590, Accuracy: 1338/1959 (68.30%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.003387\n",
      "\tTraining batch 2 Loss: 0.002161\n",
      "\tTraining batch 3 Loss: 0.004664\n",
      "\tTraining batch 4 Loss: 0.003992\n",
      "\tTraining batch 5 Loss: 0.003790\n",
      "\tTraining batch 6 Loss: 0.002394\n",
      "\tTraining batch 7 Loss: 0.006883\n",
      "\tTraining batch 8 Loss: 0.000585\n",
      "\tTraining batch 9 Loss: 0.001712\n",
      "\tTraining batch 10 Loss: 0.043463\n",
      "\tTraining batch 11 Loss: 0.032057\n",
      "\tTraining batch 12 Loss: 0.147832\n",
      "\tTraining batch 13 Loss: 0.092153\n",
      "\tTraining batch 14 Loss: 0.006034\n",
      "\tTraining batch 15 Loss: 0.094119\n",
      "\tTraining batch 16 Loss: 0.003732\n",
      "\tTraining batch 17 Loss: 0.005477\n",
      "\tTraining batch 18 Loss: 0.000036\n",
      "Training set: Average loss: 0.025248\n",
      "Validation set: Average loss: 3.054233, Accuracy: 1344/1959 (68.61%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.001263\n",
      "\tTraining batch 2 Loss: 0.002922\n",
      "\tTraining batch 3 Loss: 0.001899\n",
      "\tTraining batch 4 Loss: 0.007305\n",
      "\tTraining batch 5 Loss: 0.002225\n",
      "\tTraining batch 6 Loss: 0.001601\n",
      "\tTraining batch 7 Loss: 0.007251\n",
      "\tTraining batch 8 Loss: 0.000037\n",
      "\tTraining batch 9 Loss: 0.000183\n",
      "\tTraining batch 10 Loss: 0.029796\n",
      "\tTraining batch 11 Loss: 0.017584\n",
      "\tTraining batch 12 Loss: 0.015534\n",
      "\tTraining batch 13 Loss: 0.058688\n",
      "\tTraining batch 14 Loss: 0.003889\n",
      "\tTraining batch 15 Loss: 0.029309\n",
      "\tTraining batch 16 Loss: 0.000724\n",
      "\tTraining batch 17 Loss: 0.005151\n",
      "\tTraining batch 18 Loss: 0.001775\n",
      "Training set: Average loss: 0.010396\n",
      "Validation set: Average loss: 3.583785, Accuracy: 1351/1959 (68.96%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.004325\n",
      "\tTraining batch 2 Loss: 0.003201\n",
      "\tTraining batch 3 Loss: 0.000744\n",
      "\tTraining batch 4 Loss: 0.004717\n",
      "\tTraining batch 5 Loss: 0.001478\n",
      "\tTraining batch 6 Loss: 0.001023\n",
      "\tTraining batch 7 Loss: 0.002078\n",
      "\tTraining batch 8 Loss: 0.000035\n",
      "\tTraining batch 9 Loss: 0.000161\n",
      "\tTraining batch 10 Loss: 0.009502\n",
      "\tTraining batch 11 Loss: 0.014121\n",
      "\tTraining batch 12 Loss: 0.007442\n",
      "\tTraining batch 13 Loss: 0.055957\n",
      "\tTraining batch 14 Loss: 0.001373\n",
      "\tTraining batch 15 Loss: 0.025738\n",
      "\tTraining batch 16 Loss: 0.000416\n",
      "\tTraining batch 17 Loss: 0.003044\n",
      "\tTraining batch 18 Loss: 0.000135\n",
      "Training set: Average loss: 0.007527\n",
      "Validation set: Average loss: 3.446325, Accuracy: 1346/1959 (68.71%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.001105\n",
      "\tTraining batch 2 Loss: 0.001069\n",
      "\tTraining batch 3 Loss: 0.000275\n",
      "\tTraining batch 4 Loss: 0.001269\n",
      "\tTraining batch 5 Loss: 0.000953\n",
      "\tTraining batch 6 Loss: 0.000492\n",
      "\tTraining batch 7 Loss: 0.000782\n",
      "\tTraining batch 8 Loss: 0.000016\n",
      "\tTraining batch 9 Loss: 0.000171\n",
      "\tTraining batch 10 Loss: 0.003075\n",
      "\tTraining batch 11 Loss: 0.009818\n",
      "\tTraining batch 12 Loss: 0.003329\n",
      "\tTraining batch 13 Loss: 0.053570\n",
      "\tTraining batch 14 Loss: 0.000682\n",
      "\tTraining batch 15 Loss: 0.022849\n",
      "\tTraining batch 16 Loss: 0.000203\n",
      "\tTraining batch 17 Loss: 0.002002\n",
      "\tTraining batch 18 Loss: 0.000146\n",
      "Training set: Average loss: 0.005656\n",
      "Validation set: Average loss: 3.543769, Accuracy: 1346/1959 (68.71%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 44.0%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000801\n",
      "\tTraining batch 2 Loss: 0.000548\n",
      "\tTraining batch 3 Loss: 0.000135\n",
      "\tTraining batch 4 Loss: 0.000594\n",
      "\tTraining batch 5 Loss: 0.000575\n",
      "\tTraining batch 6 Loss: 0.000290\n",
      "\tTraining batch 7 Loss: 0.000383\n",
      "\tTraining batch 8 Loss: 0.000011\n",
      "\tTraining batch 9 Loss: 0.000157\n",
      "\tTraining batch 10 Loss: 0.001485\n",
      "\tTraining batch 11 Loss: 0.006211\n",
      "\tTraining batch 12 Loss: 0.001748\n",
      "\tTraining batch 13 Loss: 0.052173\n",
      "\tTraining batch 14 Loss: 0.000490\n",
      "\tTraining batch 15 Loss: 0.019622\n",
      "\tTraining batch 16 Loss: 0.000118\n",
      "\tTraining batch 17 Loss: 0.001532\n",
      "\tTraining batch 18 Loss: 0.000187\n",
      "\tTraining batch 19 Loss: 6.110581\n",
      "Training set: Average loss: 0.326192\n",
      "Validation set: Average loss: 2.375531, Accuracy: 1336/1959 (68.20%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.001106\n",
      "\tTraining batch 2 Loss: 0.005685\n",
      "\tTraining batch 3 Loss: 0.017770\n",
      "\tTraining batch 4 Loss: 0.176753\n",
      "\tTraining batch 5 Loss: 0.168277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 6 Loss: 0.225743\n",
      "\tTraining batch 7 Loss: 0.255878\n",
      "\tTraining batch 8 Loss: 0.251060\n",
      "\tTraining batch 9 Loss: 0.365291\n",
      "\tTraining batch 10 Loss: 0.372648\n",
      "\tTraining batch 11 Loss: 0.099286\n",
      "\tTraining batch 12 Loss: 0.131550\n",
      "\tTraining batch 13 Loss: 0.207750\n",
      "\tTraining batch 14 Loss: 0.108914\n",
      "\tTraining batch 15 Loss: 0.211365\n",
      "\tTraining batch 16 Loss: 0.173575\n",
      "\tTraining batch 17 Loss: 0.114900\n",
      "\tTraining batch 18 Loss: 0.119424\n",
      "\tTraining batch 19 Loss: 0.408457\n",
      "Training set: Average loss: 0.179760\n",
      "Validation set: Average loss: 1.753134, Accuracy: 1291/1959 (65.90%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.014625\n",
      "\tTraining batch 2 Loss: 0.013852\n",
      "\tTraining batch 3 Loss: 0.025259\n",
      "\tTraining batch 4 Loss: 0.033701\n",
      "\tTraining batch 5 Loss: 0.003237\n",
      "\tTraining batch 6 Loss: 0.013525\n",
      "\tTraining batch 7 Loss: 0.014321\n",
      "\tTraining batch 8 Loss: 0.001419\n",
      "\tTraining batch 9 Loss: 0.000777\n",
      "\tTraining batch 10 Loss: 0.004310\n",
      "\tTraining batch 11 Loss: 0.023930\n",
      "\tTraining batch 12 Loss: 0.067321\n",
      "\tTraining batch 13 Loss: 0.077534\n",
      "\tTraining batch 14 Loss: 0.107141\n",
      "\tTraining batch 15 Loss: 0.092751\n",
      "\tTraining batch 16 Loss: 0.001886\n",
      "\tTraining batch 17 Loss: 0.049738\n",
      "\tTraining batch 18 Loss: 0.004606\n",
      "\tTraining batch 19 Loss: 0.092754\n",
      "Training set: Average loss: 0.033826\n",
      "Validation set: Average loss: 2.937381, Accuracy: 1285/1959 (65.59%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.048735\n",
      "\tTraining batch 2 Loss: 0.059656\n",
      "\tTraining batch 3 Loss: 0.001950\n",
      "\tTraining batch 4 Loss: 0.026754\n",
      "\tTraining batch 5 Loss: 0.006443\n",
      "\tTraining batch 6 Loss: 0.001006\n",
      "\tTraining batch 7 Loss: 0.000935\n",
      "\tTraining batch 8 Loss: 0.001514\n",
      "\tTraining batch 9 Loss: 0.002060\n",
      "\tTraining batch 10 Loss: 0.002132\n",
      "\tTraining batch 11 Loss: 0.037577\n",
      "\tTraining batch 12 Loss: 0.045449\n",
      "\tTraining batch 13 Loss: 0.058408\n",
      "\tTraining batch 14 Loss: 0.003470\n",
      "\tTraining batch 15 Loss: 0.026309\n",
      "\tTraining batch 16 Loss: 0.001572\n",
      "\tTraining batch 17 Loss: 0.006608\n",
      "\tTraining batch 18 Loss: 0.041590\n",
      "\tTraining batch 19 Loss: 0.018201\n",
      "Training set: Average loss: 0.020546\n",
      "Validation set: Average loss: 2.788448, Accuracy: 1345/1959 (68.66%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.001779\n",
      "\tTraining batch 2 Loss: 0.001408\n",
      "\tTraining batch 3 Loss: 0.000019\n",
      "\tTraining batch 4 Loss: 0.001582\n",
      "\tTraining batch 5 Loss: 0.000157\n",
      "\tTraining batch 6 Loss: 0.000588\n",
      "\tTraining batch 7 Loss: 0.001161\n",
      "\tTraining batch 8 Loss: 0.000858\n",
      "\tTraining batch 9 Loss: 0.000321\n",
      "\tTraining batch 10 Loss: 0.000560\n",
      "\tTraining batch 11 Loss: 0.006401\n",
      "\tTraining batch 12 Loss: 0.000629\n",
      "\tTraining batch 13 Loss: 0.055362\n",
      "\tTraining batch 14 Loss: 0.000309\n",
      "\tTraining batch 15 Loss: 0.023893\n",
      "\tTraining batch 16 Loss: 0.000436\n",
      "\tTraining batch 17 Loss: 0.001460\n",
      "\tTraining batch 18 Loss: 0.000192\n",
      "\tTraining batch 19 Loss: 0.020686\n",
      "Training set: Average loss: 0.006200\n",
      "Validation set: Average loss: 3.046069, Accuracy: 1342/1959 (68.50%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000598\n",
      "\tTraining batch 2 Loss: 0.002509\n",
      "\tTraining batch 3 Loss: 0.000121\n",
      "\tTraining batch 4 Loss: 0.000382\n",
      "\tTraining batch 5 Loss: 0.000136\n",
      "\tTraining batch 6 Loss: 0.000310\n",
      "\tTraining batch 7 Loss: 0.001433\n",
      "\tTraining batch 8 Loss: 0.000341\n",
      "\tTraining batch 9 Loss: 0.000354\n",
      "\tTraining batch 10 Loss: 0.000480\n",
      "\tTraining batch 11 Loss: 0.005468\n",
      "\tTraining batch 12 Loss: 0.000256\n",
      "\tTraining batch 13 Loss: 0.054543\n",
      "\tTraining batch 14 Loss: 0.000203\n",
      "\tTraining batch 15 Loss: 0.023664\n",
      "\tTraining batch 16 Loss: 0.000352\n",
      "\tTraining batch 17 Loss: 0.000755\n",
      "\tTraining batch 18 Loss: 0.000167\n",
      "\tTraining batch 19 Loss: 0.008316\n",
      "Training set: Average loss: 0.005284\n",
      "Validation set: Average loss: 3.208392, Accuracy: 1341/1959 (68.45%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000227\n",
      "\tTraining batch 2 Loss: 0.000831\n",
      "\tTraining batch 3 Loss: 0.000061\n",
      "\tTraining batch 4 Loss: 0.000167\n",
      "\tTraining batch 5 Loss: 0.000115\n",
      "\tTraining batch 6 Loss: 0.000222\n",
      "\tTraining batch 7 Loss: 0.001086\n",
      "\tTraining batch 8 Loss: 0.000243\n",
      "\tTraining batch 9 Loss: 0.000293\n",
      "\tTraining batch 10 Loss: 0.000375\n",
      "\tTraining batch 11 Loss: 0.003743\n",
      "\tTraining batch 12 Loss: 0.000116\n",
      "\tTraining batch 13 Loss: 0.053645\n",
      "\tTraining batch 14 Loss: 0.000145\n",
      "\tTraining batch 15 Loss: 0.023452\n",
      "\tTraining batch 16 Loss: 0.000227\n",
      "\tTraining batch 17 Loss: 0.000534\n",
      "\tTraining batch 18 Loss: 0.000148\n",
      "\tTraining batch 19 Loss: 0.005456\n",
      "Training set: Average loss: 0.004794\n",
      "Validation set: Average loss: 3.301807, Accuracy: 1341/1959 (68.45%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 44.25%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000120\n",
      "\tTraining batch 2 Loss: 0.000535\n",
      "\tTraining batch 3 Loss: 0.000035\n",
      "\tTraining batch 4 Loss: 0.000109\n",
      "\tTraining batch 5 Loss: 0.000104\n",
      "\tTraining batch 6 Loss: 0.000172\n",
      "\tTraining batch 7 Loss: 0.000867\n",
      "\tTraining batch 8 Loss: 0.000199\n",
      "\tTraining batch 9 Loss: 0.000235\n",
      "\tTraining batch 10 Loss: 0.000313\n",
      "\tTraining batch 11 Loss: 0.002709\n",
      "\tTraining batch 12 Loss: 0.000074\n",
      "\tTraining batch 13 Loss: 0.052693\n",
      "\tTraining batch 14 Loss: 0.000118\n",
      "\tTraining batch 15 Loss: 0.023202\n",
      "\tTraining batch 16 Loss: 0.000156\n",
      "\tTraining batch 17 Loss: 0.000437\n",
      "\tTraining batch 18 Loss: 0.000126\n",
      "\tTraining batch 19 Loss: 0.004085\n",
      "\tTraining batch 20 Loss: 4.474651\n",
      "Training set: Average loss: 0.228047\n",
      "Validation set: Average loss: 2.567392, Accuracy: 1315/1959 (67.13%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000709\n",
      "\tTraining batch 2 Loss: 0.024948\n",
      "\tTraining batch 3 Loss: 0.087504\n",
      "\tTraining batch 4 Loss: 0.155226\n",
      "\tTraining batch 5 Loss: 0.092987\n",
      "\tTraining batch 6 Loss: 0.101457\n",
      "\tTraining batch 7 Loss: 0.187927\n",
      "\tTraining batch 8 Loss: 0.130316\n",
      "\tTraining batch 9 Loss: 0.235897\n",
      "\tTraining batch 10 Loss: 0.165318\n",
      "\tTraining batch 11 Loss: 0.127951\n",
      "\tTraining batch 12 Loss: 0.062070\n",
      "\tTraining batch 13 Loss: 0.119507\n",
      "\tTraining batch 14 Loss: 0.201207\n",
      "\tTraining batch 15 Loss: 0.116316\n",
      "\tTraining batch 16 Loss: 0.059281\n",
      "\tTraining batch 17 Loss: 0.112442\n",
      "\tTraining batch 18 Loss: 0.129897\n",
      "\tTraining batch 19 Loss: 0.089007\n",
      "\tTraining batch 20 Loss: 0.205870\n",
      "Training set: Average loss: 0.120292\n",
      "Validation set: Average loss: 1.637609, Accuracy: 1364/1959 (69.63%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.043119\n",
      "\tTraining batch 2 Loss: 0.035399\n",
      "\tTraining batch 3 Loss: 0.002887\n",
      "\tTraining batch 4 Loss: 0.007370\n",
      "\tTraining batch 5 Loss: 0.030234\n",
      "\tTraining batch 6 Loss: 0.004183\n",
      "\tTraining batch 7 Loss: 0.008637\n",
      "\tTraining batch 8 Loss: 0.004042\n",
      "\tTraining batch 9 Loss: 0.002193\n",
      "\tTraining batch 10 Loss: 0.007208\n",
      "\tTraining batch 11 Loss: 0.017227\n",
      "\tTraining batch 12 Loss: 0.018385\n",
      "\tTraining batch 13 Loss: 0.087423\n",
      "\tTraining batch 14 Loss: 0.082124\n",
      "\tTraining batch 15 Loss: 0.025287\n",
      "\tTraining batch 16 Loss: 0.005467\n",
      "\tTraining batch 17 Loss: 0.027140\n",
      "\tTraining batch 18 Loss: 0.004679\n",
      "\tTraining batch 19 Loss: 0.008238\n",
      "\tTraining batch 20 Loss: 0.001601\n",
      "Training set: Average loss: 0.021142\n",
      "Validation set: Average loss: 2.944101, Accuracy: 1397/1959 (71.31%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000915\n",
      "\tTraining batch 2 Loss: 0.000307\n",
      "\tTraining batch 3 Loss: 0.000009\n",
      "\tTraining batch 4 Loss: 0.000111\n",
      "\tTraining batch 5 Loss: 0.002555\n",
      "\tTraining batch 6 Loss: 0.001357\n",
      "\tTraining batch 7 Loss: 0.000180\n",
      "\tTraining batch 8 Loss: 0.027200\n",
      "\tTraining batch 9 Loss: 0.000346\n",
      "\tTraining batch 10 Loss: 0.000700\n",
      "\tTraining batch 11 Loss: 0.000425\n",
      "\tTraining batch 12 Loss: 0.000070\n",
      "\tTraining batch 13 Loss: 0.051080\n",
      "\tTraining batch 14 Loss: 0.000096\n",
      "\tTraining batch 15 Loss: 0.010501\n",
      "\tTraining batch 16 Loss: 0.000075\n",
      "\tTraining batch 17 Loss: 0.001994\n",
      "\tTraining batch 18 Loss: 0.000100\n",
      "\tTraining batch 19 Loss: 0.004300\n",
      "\tTraining batch 20 Loss: 0.001187\n",
      "Training set: Average loss: 0.005175\n",
      "Validation set: Average loss: 3.301416, Accuracy: 1385/1959 (70.70%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000683\n",
      "\tTraining batch 2 Loss: 0.000103\n",
      "\tTraining batch 3 Loss: 0.000085\n",
      "\tTraining batch 4 Loss: 0.000046\n",
      "\tTraining batch 5 Loss: 0.001938\n",
      "\tTraining batch 6 Loss: 0.000158\n",
      "\tTraining batch 7 Loss: 0.000142\n",
      "\tTraining batch 8 Loss: 0.000258\n",
      "\tTraining batch 9 Loss: 0.000009\n",
      "\tTraining batch 10 Loss: 0.000694\n",
      "\tTraining batch 11 Loss: 0.000297\n",
      "\tTraining batch 12 Loss: 0.000245\n",
      "\tTraining batch 13 Loss: 0.040642\n",
      "\tTraining batch 14 Loss: 0.000126\n",
      "\tTraining batch 15 Loss: 0.008828\n",
      "\tTraining batch 16 Loss: 0.000091\n",
      "\tTraining batch 17 Loss: 0.001538\n",
      "\tTraining batch 18 Loss: 0.000155\n",
      "\tTraining batch 19 Loss: 0.002898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 20 Loss: 0.000898\n",
      "Training set: Average loss: 0.002992\n",
      "Validation set: Average loss: 3.317397, Accuracy: 1384/1959 (70.65%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000345\n",
      "\tTraining batch 2 Loss: 0.000077\n",
      "\tTraining batch 3 Loss: 0.000024\n",
      "\tTraining batch 4 Loss: 0.000026\n",
      "\tTraining batch 5 Loss: 0.000886\n",
      "\tTraining batch 6 Loss: 0.000086\n",
      "\tTraining batch 7 Loss: 0.000086\n",
      "\tTraining batch 8 Loss: 0.000058\n",
      "\tTraining batch 9 Loss: 0.000007\n",
      "\tTraining batch 10 Loss: 0.000363\n",
      "\tTraining batch 11 Loss: 0.000203\n",
      "\tTraining batch 12 Loss: 0.000043\n",
      "\tTraining batch 13 Loss: 0.011176\n",
      "\tTraining batch 14 Loss: 0.000099\n",
      "\tTraining batch 15 Loss: 0.004753\n",
      "\tTraining batch 16 Loss: 0.000067\n",
      "\tTraining batch 17 Loss: 0.000779\n",
      "\tTraining batch 18 Loss: 0.000099\n",
      "\tTraining batch 19 Loss: 0.003025\n",
      "\tTraining batch 20 Loss: 0.000518\n",
      "Training set: Average loss: 0.001136\n",
      "Validation set: Average loss: 3.405329, Accuracy: 1373/1959 (70.09%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000238\n",
      "\tTraining batch 2 Loss: 0.000086\n",
      "\tTraining batch 3 Loss: 0.000010\n",
      "\tTraining batch 4 Loss: 0.000016\n",
      "\tTraining batch 5 Loss: 0.000591\n",
      "\tTraining batch 6 Loss: 0.000062\n",
      "\tTraining batch 7 Loss: 0.000056\n",
      "\tTraining batch 8 Loss: 0.000027\n",
      "\tTraining batch 9 Loss: 0.000007\n",
      "\tTraining batch 10 Loss: 0.000336\n",
      "\tTraining batch 11 Loss: 0.000163\n",
      "\tTraining batch 12 Loss: 0.000038\n",
      "\tTraining batch 13 Loss: 0.000892\n",
      "\tTraining batch 14 Loss: 0.000091\n",
      "\tTraining batch 15 Loss: 0.002306\n",
      "\tTraining batch 16 Loss: 0.113157\n",
      "\tTraining batch 17 Loss: 0.000713\n",
      "\tTraining batch 18 Loss: 0.000218\n",
      "\tTraining batch 19 Loss: 0.001041\n",
      "\tTraining batch 20 Loss: 0.002248\n",
      "Training set: Average loss: 0.006115\n",
      "Validation set: Average loss: 3.744974, Accuracy: 1370/1959 (69.93%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.001412\n",
      "\tTraining batch 2 Loss: 0.000353\n",
      "\tTraining batch 3 Loss: 0.000182\n",
      "\tTraining batch 4 Loss: 0.000038\n",
      "\tTraining batch 5 Loss: 0.100563\n",
      "\tTraining batch 6 Loss: 0.000570\n",
      "\tTraining batch 7 Loss: 0.000115\n",
      "\tTraining batch 8 Loss: 0.002581\n",
      "\tTraining batch 9 Loss: 0.000004\n",
      "\tTraining batch 10 Loss: 0.000785\n",
      "\tTraining batch 11 Loss: 0.000711\n",
      "\tTraining batch 12 Loss: 0.000214\n",
      "\tTraining batch 13 Loss: 0.043476\n",
      "\tTraining batch 14 Loss: 0.000119\n",
      "\tTraining batch 15 Loss: 0.010436\n",
      "\tTraining batch 16 Loss: 0.000061\n",
      "\tTraining batch 17 Loss: 0.000783\n",
      "\tTraining batch 18 Loss: 0.000585\n",
      "\tTraining batch 19 Loss: 0.001437\n",
      "\tTraining batch 20 Loss: 0.000328\n",
      "Training set: Average loss: 0.008238\n",
      "Validation set: Average loss: 4.565839, Accuracy: 1366/1959 (69.73%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.216384\n",
      "\tTraining batch 2 Loss: 0.000047\n",
      "\tTraining batch 3 Loss: 0.000015\n",
      "\tTraining batch 4 Loss: 0.000017\n",
      "\tTraining batch 5 Loss: 0.001382\n",
      "\tTraining batch 6 Loss: 0.000628\n",
      "\tTraining batch 7 Loss: 0.000767\n",
      "\tTraining batch 8 Loss: 0.011154\n",
      "\tTraining batch 9 Loss: 0.001492\n",
      "\tTraining batch 10 Loss: 0.005568\n",
      "\tTraining batch 11 Loss: 0.000758\n",
      "\tTraining batch 12 Loss: 0.000157\n",
      "\tTraining batch 13 Loss: 0.032466\n",
      "\tTraining batch 14 Loss: 0.015879\n",
      "\tTraining batch 15 Loss: 0.072101\n",
      "\tTraining batch 16 Loss: 0.000110\n",
      "\tTraining batch 17 Loss: 0.000228\n",
      "\tTraining batch 18 Loss: 0.001310\n",
      "\tTraining batch 19 Loss: 0.105893\n",
      "\tTraining batch 20 Loss: 0.050627\n",
      "Training set: Average loss: 0.025849\n",
      "Validation set: Average loss: 2.985540, Accuracy: 1400/1959 (71.47%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000043\n",
      "\tTraining batch 2 Loss: 0.000031\n",
      "\tTraining batch 3 Loss: 0.000053\n",
      "\tTraining batch 4 Loss: 0.000164\n",
      "\tTraining batch 5 Loss: 0.000067\n",
      "\tTraining batch 6 Loss: 0.000677\n",
      "\tTraining batch 7 Loss: 0.001213\n",
      "\tTraining batch 8 Loss: 0.000989\n",
      "\tTraining batch 9 Loss: 0.002000\n",
      "\tTraining batch 10 Loss: 0.000616\n",
      "\tTraining batch 11 Loss: 0.007697\n",
      "\tTraining batch 12 Loss: 0.000209\n",
      "\tTraining batch 13 Loss: 0.011754\n",
      "\tTraining batch 14 Loss: 0.001676\n",
      "\tTraining batch 15 Loss: 0.012132\n",
      "\tTraining batch 16 Loss: 0.002853\n",
      "\tTraining batch 17 Loss: 0.010980\n",
      "\tTraining batch 18 Loss: 0.000485\n",
      "\tTraining batch 19 Loss: 0.019279\n",
      "\tTraining batch 20 Loss: 0.073243\n",
      "Training set: Average loss: 0.007308\n",
      "Validation set: Average loss: 2.884340, Accuracy: 1334/1959 (68.10%)\n",
      "\n",
      "Epoch: 11\n",
      "\tTraining batch 1 Loss: 0.005797\n",
      "\tTraining batch 2 Loss: 0.001894\n",
      "\tTraining batch 3 Loss: 0.084135\n",
      "\tTraining batch 4 Loss: 0.003114\n",
      "\tTraining batch 5 Loss: 0.000609\n",
      "\tTraining batch 6 Loss: 0.002129\n",
      "\tTraining batch 7 Loss: 0.001614\n",
      "\tTraining batch 8 Loss: 0.000798\n",
      "\tTraining batch 9 Loss: 0.002069\n",
      "\tTraining batch 10 Loss: 0.008131\n",
      "\tTraining batch 11 Loss: 0.007692\n",
      "\tTraining batch 12 Loss: 0.000046\n",
      "\tTraining batch 13 Loss: 0.001374\n",
      "\tTraining batch 14 Loss: 0.000231\n",
      "\tTraining batch 15 Loss: 0.005451\n",
      "\tTraining batch 16 Loss: 0.000924\n",
      "\tTraining batch 17 Loss: 0.001018\n",
      "\tTraining batch 18 Loss: 0.000021\n",
      "\tTraining batch 19 Loss: 0.002858\n",
      "\tTraining batch 20 Loss: 0.000627\n",
      "Training set: Average loss: 0.006527\n",
      "Validation set: Average loss: 3.149184, Accuracy: 1345/1959 (68.66%)\n",
      "\n",
      "Epoch: 12\n",
      "\tTraining batch 1 Loss: 0.000198\n",
      "\tTraining batch 2 Loss: 0.000044\n",
      "\tTraining batch 3 Loss: 0.000108\n",
      "\tTraining batch 4 Loss: 0.000046\n",
      "\tTraining batch 5 Loss: 0.000020\n",
      "\tTraining batch 6 Loss: 0.000138\n",
      "\tTraining batch 7 Loss: 0.000093\n",
      "\tTraining batch 8 Loss: 0.000143\n",
      "\tTraining batch 9 Loss: 0.000018\n",
      "\tTraining batch 10 Loss: 0.000067\n",
      "\tTraining batch 11 Loss: 0.000583\n",
      "\tTraining batch 12 Loss: 0.000011\n",
      "\tTraining batch 13 Loss: 0.000336\n",
      "\tTraining batch 14 Loss: 0.000071\n",
      "\tTraining batch 15 Loss: 0.002443\n",
      "\tTraining batch 16 Loss: 0.000461\n",
      "\tTraining batch 17 Loss: 0.000622\n",
      "\tTraining batch 18 Loss: 0.000008\n",
      "\tTraining batch 19 Loss: 0.001667\n",
      "\tTraining batch 20 Loss: 0.000356\n",
      "Training set: Average loss: 0.000372\n",
      "Validation set: Average loss: 3.215432, Accuracy: 1361/1959 (69.47%)\n",
      "\n",
      "Epoch: 13\n",
      "\tTraining batch 1 Loss: 0.000121\n",
      "\tTraining batch 2 Loss: 0.000022\n",
      "\tTraining batch 3 Loss: 0.000042\n",
      "\tTraining batch 4 Loss: 0.000042\n",
      "\tTraining batch 5 Loss: 0.000010\n",
      "\tTraining batch 6 Loss: 0.000079\n",
      "\tTraining batch 7 Loss: 0.000050\n",
      "\tTraining batch 8 Loss: 0.000069\n",
      "\tTraining batch 9 Loss: 0.000009\n",
      "\tTraining batch 10 Loss: 0.000055\n",
      "\tTraining batch 11 Loss: 0.000348\n",
      "\tTraining batch 12 Loss: 0.000009\n",
      "\tTraining batch 13 Loss: 0.000204\n",
      "\tTraining batch 14 Loss: 0.000055\n",
      "\tTraining batch 15 Loss: 0.001550\n",
      "\tTraining batch 16 Loss: 0.000282\n",
      "\tTraining batch 17 Loss: 0.000449\n",
      "\tTraining batch 18 Loss: 0.000006\n",
      "\tTraining batch 19 Loss: 0.001345\n",
      "\tTraining batch 20 Loss: 0.000271\n",
      "Training set: Average loss: 0.000251\n",
      "Validation set: Average loss: 3.268806, Accuracy: 1363/1959 (69.58%)\n",
      "\n",
      "Epoch: 14\n",
      "\tTraining batch 1 Loss: 0.000099\n",
      "\tTraining batch 2 Loss: 0.000019\n",
      "\tTraining batch 3 Loss: 0.000029\n",
      "\tTraining batch 4 Loss: 0.000039\n",
      "\tTraining batch 5 Loss: 0.000008\n",
      "\tTraining batch 6 Loss: 0.000065\n",
      "\tTraining batch 7 Loss: 0.000038\n",
      "\tTraining batch 8 Loss: 0.000050\n",
      "\tTraining batch 9 Loss: 0.000007\n",
      "\tTraining batch 10 Loss: 0.000047\n",
      "\tTraining batch 11 Loss: 0.000268\n",
      "\tTraining batch 12 Loss: 0.000007\n",
      "\tTraining batch 13 Loss: 0.000155\n",
      "\tTraining batch 14 Loss: 0.000049\n",
      "\tTraining batch 15 Loss: 0.001134\n",
      "\tTraining batch 16 Loss: 0.000207\n",
      "\tTraining batch 17 Loss: 0.000338\n",
      "\tTraining batch 18 Loss: 0.000005\n",
      "\tTraining batch 19 Loss: 0.001156\n",
      "\tTraining batch 20 Loss: 0.000221\n",
      "Training set: Average loss: 0.000197\n",
      "Validation set: Average loss: 3.308879, Accuracy: 1361/1959 (69.47%)\n",
      "\n",
      "Epoch: 15\n",
      "\tTraining batch 1 Loss: 0.000081\n",
      "\tTraining batch 2 Loss: 0.000019\n",
      "\tTraining batch 3 Loss: 0.000023\n",
      "\tTraining batch 4 Loss: 0.000037\n",
      "\tTraining batch 5 Loss: 0.000006\n",
      "\tTraining batch 6 Loss: 0.000057\n",
      "\tTraining batch 7 Loss: 0.000031\n",
      "\tTraining batch 8 Loss: 0.000040\n",
      "\tTraining batch 9 Loss: 0.000006\n",
      "\tTraining batch 10 Loss: 0.000042\n",
      "\tTraining batch 11 Loss: 0.000221\n",
      "\tTraining batch 12 Loss: 0.000006\n",
      "\tTraining batch 13 Loss: 0.000127\n",
      "\tTraining batch 14 Loss: 0.000045\n",
      "\tTraining batch 15 Loss: 0.000873\n",
      "\tTraining batch 16 Loss: 0.000165\n",
      "\tTraining batch 17 Loss: 0.000269\n",
      "\tTraining batch 18 Loss: 0.000005\n",
      "\tTraining batch 19 Loss: 0.001014\n",
      "\tTraining batch 20 Loss: 0.000186\n",
      "Training set: Average loss: 0.000163\n",
      "Validation set: Average loss: 3.342424, Accuracy: 1363/1959 (69.58%)\n",
      "\n",
      "Epoch: 16\n",
      "\tTraining batch 1 Loss: 0.000067\n",
      "\tTraining batch 2 Loss: 0.000019\n",
      "\tTraining batch 3 Loss: 0.000019\n",
      "\tTraining batch 4 Loss: 0.000034\n",
      "\tTraining batch 5 Loss: 0.000005\n",
      "\tTraining batch 6 Loss: 0.000051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 7 Loss: 0.000027\n",
      "\tTraining batch 8 Loss: 0.000033\n",
      "\tTraining batch 9 Loss: 0.000005\n",
      "\tTraining batch 10 Loss: 0.000038\n",
      "\tTraining batch 11 Loss: 0.000186\n",
      "\tTraining batch 12 Loss: 0.000006\n",
      "\tTraining batch 13 Loss: 0.000107\n",
      "\tTraining batch 14 Loss: 0.000041\n",
      "\tTraining batch 15 Loss: 0.000699\n",
      "\tTraining batch 16 Loss: 0.000136\n",
      "\tTraining batch 17 Loss: 0.000222\n",
      "\tTraining batch 18 Loss: 0.000004\n",
      "\tTraining batch 19 Loss: 0.000902\n",
      "\tTraining batch 20 Loss: 0.000161\n",
      "Training set: Average loss: 0.000138\n",
      "Validation set: Average loss: 3.371467, Accuracy: 1362/1959 (69.53%)\n",
      "\n",
      "Epoch: 17\n",
      "\tTraining batch 1 Loss: 0.000058\n",
      "\tTraining batch 2 Loss: 0.000019\n",
      "\tTraining batch 3 Loss: 0.000016\n",
      "\tTraining batch 4 Loss: 0.000032\n",
      "\tTraining batch 5 Loss: 0.000005\n",
      "\tTraining batch 6 Loss: 0.000046\n",
      "\tTraining batch 7 Loss: 0.000023\n",
      "\tTraining batch 8 Loss: 0.000028\n",
      "\tTraining batch 9 Loss: 0.000004\n",
      "\tTraining batch 10 Loss: 0.000035\n",
      "\tTraining batch 11 Loss: 0.000160\n",
      "\tTraining batch 12 Loss: 0.000005\n",
      "\tTraining batch 13 Loss: 0.000092\n",
      "\tTraining batch 14 Loss: 0.000039\n",
      "\tTraining batch 15 Loss: 0.000577\n",
      "\tTraining batch 16 Loss: 0.000115\n",
      "\tTraining batch 17 Loss: 0.000188\n",
      "\tTraining batch 18 Loss: 0.000004\n",
      "\tTraining batch 19 Loss: 0.000813\n",
      "\tTraining batch 20 Loss: 0.000142\n",
      "Training set: Average loss: 0.000120\n",
      "Validation set: Average loss: 3.397027, Accuracy: 1363/1959 (69.58%)\n",
      "\n",
      "Epoch: 18\n",
      "\tTraining batch 1 Loss: 0.000051\n",
      "\tTraining batch 2 Loss: 0.000019\n",
      "\tTraining batch 3 Loss: 0.000013\n",
      "\tTraining batch 4 Loss: 0.000030\n",
      "\tTraining batch 5 Loss: 0.000004\n",
      "\tTraining batch 6 Loss: 0.000041\n",
      "\tTraining batch 7 Loss: 0.000021\n",
      "\tTraining batch 8 Loss: 0.000025\n",
      "\tTraining batch 9 Loss: 0.000004\n",
      "\tTraining batch 10 Loss: 0.000032\n",
      "\tTraining batch 11 Loss: 0.000139\n",
      "\tTraining batch 12 Loss: 0.000005\n",
      "\tTraining batch 13 Loss: 0.000081\n",
      "\tTraining batch 14 Loss: 0.000036\n",
      "\tTraining batch 15 Loss: 0.000488\n",
      "\tTraining batch 16 Loss: 0.000098\n",
      "\tTraining batch 17 Loss: 0.000163\n",
      "\tTraining batch 18 Loss: 0.000004\n",
      "\tTraining batch 19 Loss: 0.000738\n",
      "\tTraining batch 20 Loss: 0.000127\n",
      "Training set: Average loss: 0.000106\n",
      "Validation set: Average loss: 3.420243, Accuracy: 1364/1959 (69.63%)\n",
      "\n",
      "Epoch: 19\n",
      "\tTraining batch 1 Loss: 0.000046\n",
      "\tTraining batch 2 Loss: 0.000020\n",
      "\tTraining batch 3 Loss: 0.000012\n",
      "\tTraining batch 4 Loss: 0.000028\n",
      "\tTraining batch 5 Loss: 0.000004\n",
      "\tTraining batch 6 Loss: 0.000038\n",
      "\tTraining batch 7 Loss: 0.000019\n",
      "\tTraining batch 8 Loss: 0.000022\n",
      "\tTraining batch 9 Loss: 0.000003\n",
      "\tTraining batch 10 Loss: 0.000030\n",
      "\tTraining batch 11 Loss: 0.000123\n",
      "\tTraining batch 12 Loss: 0.000005\n",
      "\tTraining batch 13 Loss: 0.000073\n",
      "\tTraining batch 14 Loss: 0.000034\n",
      "\tTraining batch 15 Loss: 0.000421\n",
      "\tTraining batch 16 Loss: 0.000085\n",
      "\tTraining batch 17 Loss: 0.000144\n",
      "\tTraining batch 18 Loss: 0.000004\n",
      "\tTraining batch 19 Loss: 0.000676\n",
      "\tTraining batch 20 Loss: 0.000115\n",
      "Training set: Average loss: 0.000095\n",
      "Validation set: Average loss: 3.441320, Accuracy: 1366/1959 (69.73%)\n",
      "\n",
      "Epoch: 20\n",
      "\tTraining batch 1 Loss: 0.000041\n",
      "\tTraining batch 2 Loss: 0.000019\n",
      "\tTraining batch 3 Loss: 0.000010\n",
      "\tTraining batch 4 Loss: 0.000026\n",
      "\tTraining batch 5 Loss: 0.000003\n",
      "\tTraining batch 6 Loss: 0.000034\n",
      "\tTraining batch 7 Loss: 0.000017\n",
      "\tTraining batch 8 Loss: 0.000019\n",
      "\tTraining batch 9 Loss: 0.000003\n",
      "\tTraining batch 10 Loss: 0.000028\n",
      "\tTraining batch 11 Loss: 0.000109\n",
      "\tTraining batch 12 Loss: 0.000004\n",
      "\tTraining batch 13 Loss: 0.000066\n",
      "\tTraining batch 14 Loss: 0.000032\n",
      "\tTraining batch 15 Loss: 0.000369\n",
      "\tTraining batch 16 Loss: 0.000075\n",
      "\tTraining batch 17 Loss: 0.000128\n",
      "\tTraining batch 18 Loss: 0.000003\n",
      "\tTraining batch 19 Loss: 0.000623\n",
      "\tTraining batch 20 Loss: 0.000105\n",
      "Training set: Average loss: 0.000086\n",
      "Validation set: Average loss: 3.460727, Accuracy: 1365/1959 (69.68%)\n",
      "\n",
      "Epoch: 21\n",
      "\tTraining batch 1 Loss: 0.000037\n",
      "\tTraining batch 2 Loss: 0.000019\n",
      "\tTraining batch 3 Loss: 0.000009\n",
      "\tTraining batch 4 Loss: 0.000024\n",
      "\tTraining batch 5 Loss: 0.000003\n",
      "\tTraining batch 6 Loss: 0.000032\n",
      "\tTraining batch 7 Loss: 0.000015\n",
      "\tTraining batch 8 Loss: 0.000017\n",
      "\tTraining batch 9 Loss: 0.000003\n",
      "\tTraining batch 10 Loss: 0.000027\n",
      "\tTraining batch 11 Loss: 0.000098\n",
      "\tTraining batch 12 Loss: 0.000004\n",
      "\tTraining batch 13 Loss: 0.000060\n",
      "\tTraining batch 14 Loss: 0.000030\n",
      "\tTraining batch 15 Loss: 0.000326\n",
      "\tTraining batch 16 Loss: 0.000066\n",
      "\tTraining batch 17 Loss: 0.000115\n",
      "\tTraining batch 18 Loss: 0.000003\n",
      "\tTraining batch 19 Loss: 0.000577\n",
      "\tTraining batch 20 Loss: 0.000096\n",
      "Training set: Average loss: 0.000078\n",
      "Validation set: Average loss: 3.478697, Accuracy: 1365/1959 (69.68%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 47.0%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000034\n",
      "\tTraining batch 2 Loss: 0.000019\n",
      "\tTraining batch 3 Loss: 0.000008\n",
      "\tTraining batch 4 Loss: 0.000023\n",
      "\tTraining batch 5 Loss: 0.000003\n",
      "\tTraining batch 6 Loss: 0.000029\n",
      "\tTraining batch 7 Loss: 0.000014\n",
      "\tTraining batch 8 Loss: 0.000016\n",
      "\tTraining batch 9 Loss: 0.000003\n",
      "\tTraining batch 10 Loss: 0.000025\n",
      "\tTraining batch 11 Loss: 0.000089\n",
      "\tTraining batch 12 Loss: 0.000004\n",
      "\tTraining batch 13 Loss: 0.000055\n",
      "\tTraining batch 14 Loss: 0.000029\n",
      "\tTraining batch 15 Loss: 0.000292\n",
      "\tTraining batch 16 Loss: 0.000059\n",
      "\tTraining batch 17 Loss: 0.000104\n",
      "\tTraining batch 18 Loss: 0.000003\n",
      "\tTraining batch 19 Loss: 0.000536\n",
      "\tTraining batch 20 Loss: 0.000089\n",
      "\tTraining batch 21 Loss: 6.685206\n",
      "Training set: Average loss: 0.318411\n",
      "Validation set: Average loss: 2.970020, Accuracy: 1358/1959 (69.32%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000505\n",
      "\tTraining batch 2 Loss: 0.013833\n",
      "\tTraining batch 3 Loss: 0.000290\n",
      "\tTraining batch 4 Loss: 0.026985\n",
      "\tTraining batch 5 Loss: 0.072879\n",
      "\tTraining batch 6 Loss: 0.085441\n",
      "\tTraining batch 7 Loss: 0.083781\n",
      "\tTraining batch 8 Loss: 0.095884\n",
      "\tTraining batch 9 Loss: 0.083241\n",
      "\tTraining batch 10 Loss: 0.024430\n",
      "\tTraining batch 11 Loss: 0.159545\n",
      "\tTraining batch 12 Loss: 0.130272\n",
      "\tTraining batch 13 Loss: 0.160118\n",
      "\tTraining batch 14 Loss: 0.096358\n",
      "\tTraining batch 15 Loss: 0.057402\n",
      "\tTraining batch 16 Loss: 0.063278\n",
      "\tTraining batch 17 Loss: 0.136704\n",
      "\tTraining batch 18 Loss: 0.076893\n",
      "\tTraining batch 19 Loss: 0.234648\n",
      "\tTraining batch 20 Loss: 0.031522\n",
      "\tTraining batch 21 Loss: 0.040012\n",
      "Training set: Average loss: 0.079715\n",
      "Validation set: Average loss: 1.701862, Accuracy: 1381/1959 (70.50%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.089402\n",
      "\tTraining batch 2 Loss: 0.016487\n",
      "\tTraining batch 3 Loss: 0.006346\n",
      "\tTraining batch 4 Loss: 0.003834\n",
      "\tTraining batch 5 Loss: 0.004374\n",
      "\tTraining batch 6 Loss: 0.001018\n",
      "\tTraining batch 7 Loss: 0.003372\n",
      "\tTraining batch 8 Loss: 0.000718\n",
      "\tTraining batch 9 Loss: 0.000128\n",
      "\tTraining batch 10 Loss: 0.001946\n",
      "\tTraining batch 11 Loss: 0.001545\n",
      "\tTraining batch 12 Loss: 0.002670\n",
      "\tTraining batch 13 Loss: 0.004372\n",
      "\tTraining batch 14 Loss: 0.002106\n",
      "\tTraining batch 15 Loss: 0.001255\n",
      "\tTraining batch 16 Loss: 0.000139\n",
      "\tTraining batch 17 Loss: 0.000084\n",
      "\tTraining batch 18 Loss: 0.000545\n",
      "\tTraining batch 19 Loss: 0.010389\n",
      "\tTraining batch 20 Loss: 0.221250\n",
      "\tTraining batch 21 Loss: 0.002170\n",
      "Training set: Average loss: 0.017817\n",
      "Validation set: Average loss: 2.924490, Accuracy: 1356/1959 (69.22%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000094\n",
      "\tTraining batch 2 Loss: 0.000494\n",
      "\tTraining batch 3 Loss: 0.000075\n",
      "\tTraining batch 4 Loss: 0.001522\n",
      "\tTraining batch 5 Loss: 0.000234\n",
      "\tTraining batch 6 Loss: 0.000043\n",
      "\tTraining batch 7 Loss: 0.008596\n",
      "\tTraining batch 8 Loss: 0.000081\n",
      "\tTraining batch 9 Loss: 0.000010\n",
      "\tTraining batch 10 Loss: 0.000773\n",
      "\tTraining batch 11 Loss: 0.000389\n",
      "\tTraining batch 12 Loss: 0.000438\n",
      "\tTraining batch 13 Loss: 0.000852\n",
      "\tTraining batch 14 Loss: 0.000584\n",
      "\tTraining batch 15 Loss: 0.004047\n",
      "\tTraining batch 16 Loss: 0.000141\n",
      "\tTraining batch 17 Loss: 0.000278\n",
      "\tTraining batch 18 Loss: 0.000065\n",
      "\tTraining batch 19 Loss: 0.001916\n",
      "\tTraining batch 20 Loss: 0.001827\n",
      "\tTraining batch 21 Loss: 0.001325\n",
      "Training set: Average loss: 0.001133\n",
      "Validation set: Average loss: 2.922228, Accuracy: 1374/1959 (70.14%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.001051\n",
      "\tTraining batch 2 Loss: 0.001270\n",
      "\tTraining batch 3 Loss: 0.000053\n",
      "\tTraining batch 4 Loss: 0.001481\n",
      "\tTraining batch 5 Loss: 0.000296\n",
      "\tTraining batch 6 Loss: 0.000020\n",
      "\tTraining batch 7 Loss: 0.000217\n",
      "\tTraining batch 8 Loss: 0.000053\n",
      "\tTraining batch 9 Loss: 0.000005\n",
      "\tTraining batch 10 Loss: 0.000605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 11 Loss: 0.000234\n",
      "\tTraining batch 12 Loss: 0.000257\n",
      "\tTraining batch 13 Loss: 0.000562\n",
      "\tTraining batch 14 Loss: 0.000286\n",
      "\tTraining batch 15 Loss: 0.000348\n",
      "\tTraining batch 16 Loss: 0.000064\n",
      "\tTraining batch 17 Loss: 0.000115\n",
      "\tTraining batch 18 Loss: 0.000056\n",
      "\tTraining batch 19 Loss: 0.001297\n",
      "\tTraining batch 20 Loss: 0.000547\n",
      "\tTraining batch 21 Loss: 0.000728\n",
      "Training set: Average loss: 0.000455\n",
      "Validation set: Average loss: 2.997851, Accuracy: 1375/1959 (70.19%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000090\n",
      "\tTraining batch 2 Loss: 0.000262\n",
      "\tTraining batch 3 Loss: 0.000035\n",
      "\tTraining batch 4 Loss: 0.000425\n",
      "\tTraining batch 5 Loss: 0.000268\n",
      "\tTraining batch 6 Loss: 0.000017\n",
      "\tTraining batch 7 Loss: 0.000060\n",
      "\tTraining batch 8 Loss: 0.000044\n",
      "\tTraining batch 9 Loss: 0.000004\n",
      "\tTraining batch 10 Loss: 0.000451\n",
      "\tTraining batch 11 Loss: 0.000158\n",
      "\tTraining batch 12 Loss: 0.000160\n",
      "\tTraining batch 13 Loss: 0.000401\n",
      "\tTraining batch 14 Loss: 0.000193\n",
      "\tTraining batch 15 Loss: 0.000246\n",
      "\tTraining batch 16 Loss: 0.000046\n",
      "\tTraining batch 17 Loss: 0.000077\n",
      "\tTraining batch 18 Loss: 0.000045\n",
      "\tTraining batch 19 Loss: 0.000980\n",
      "\tTraining batch 20 Loss: 0.000321\n",
      "\tTraining batch 21 Loss: 0.000433\n",
      "Training set: Average loss: 0.000225\n",
      "Validation set: Average loss: 3.027469, Accuracy: 1374/1959 (70.14%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000055\n",
      "\tTraining batch 2 Loss: 0.000147\n",
      "\tTraining batch 3 Loss: 0.000027\n",
      "\tTraining batch 4 Loss: 0.000235\n",
      "\tTraining batch 5 Loss: 0.000223\n",
      "\tTraining batch 6 Loss: 0.000016\n",
      "\tTraining batch 7 Loss: 0.000039\n",
      "\tTraining batch 8 Loss: 0.000038\n",
      "\tTraining batch 9 Loss: 0.000004\n",
      "\tTraining batch 10 Loss: 0.000379\n",
      "\tTraining batch 11 Loss: 0.000126\n",
      "\tTraining batch 12 Loss: 0.000129\n",
      "\tTraining batch 13 Loss: 0.000323\n",
      "\tTraining batch 14 Loss: 0.000152\n",
      "\tTraining batch 15 Loss: 0.000198\n",
      "\tTraining batch 16 Loss: 0.000039\n",
      "\tTraining batch 17 Loss: 0.000062\n",
      "\tTraining batch 18 Loss: 0.000039\n",
      "\tTraining batch 19 Loss: 0.000805\n",
      "\tTraining batch 20 Loss: 0.000241\n",
      "\tTraining batch 21 Loss: 0.000328\n",
      "Training set: Average loss: 0.000172\n",
      "Validation set: Average loss: 3.050133, Accuracy: 1372/1959 (70.04%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000044\n",
      "\tTraining batch 2 Loss: 0.000107\n",
      "\tTraining batch 3 Loss: 0.000023\n",
      "\tTraining batch 4 Loss: 0.000172\n",
      "\tTraining batch 5 Loss: 0.000190\n",
      "\tTraining batch 6 Loss: 0.000014\n",
      "\tTraining batch 7 Loss: 0.000031\n",
      "\tTraining batch 8 Loss: 0.000035\n",
      "\tTraining batch 9 Loss: 0.000004\n",
      "\tTraining batch 10 Loss: 0.000328\n",
      "\tTraining batch 11 Loss: 0.000107\n",
      "\tTraining batch 12 Loss: 0.000111\n",
      "\tTraining batch 13 Loss: 0.000275\n",
      "\tTraining batch 14 Loss: 0.000126\n",
      "\tTraining batch 15 Loss: 0.000167\n",
      "\tTraining batch 16 Loss: 0.000034\n",
      "\tTraining batch 17 Loss: 0.000053\n",
      "\tTraining batch 18 Loss: 0.000035\n",
      "\tTraining batch 19 Loss: 0.000687\n",
      "\tTraining batch 20 Loss: 0.000198\n",
      "\tTraining batch 21 Loss: 0.000270\n",
      "Training set: Average loss: 0.000143\n",
      "Validation set: Average loss: 3.070735, Accuracy: 1373/1959 (70.09%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000037\n",
      "\tTraining batch 2 Loss: 0.000085\n",
      "\tTraining batch 3 Loss: 0.000021\n",
      "\tTraining batch 4 Loss: 0.000138\n",
      "\tTraining batch 5 Loss: 0.000165\n",
      "\tTraining batch 6 Loss: 0.000014\n",
      "\tTraining batch 7 Loss: 0.000026\n",
      "\tTraining batch 8 Loss: 0.000031\n",
      "\tTraining batch 9 Loss: 0.000003\n",
      "\tTraining batch 10 Loss: 0.000286\n",
      "\tTraining batch 11 Loss: 0.000092\n",
      "\tTraining batch 12 Loss: 0.000100\n",
      "\tTraining batch 13 Loss: 0.000240\n",
      "\tTraining batch 14 Loss: 0.000108\n",
      "\tTraining batch 15 Loss: 0.000144\n",
      "\tTraining batch 16 Loss: 0.000031\n",
      "\tTraining batch 17 Loss: 0.000047\n",
      "\tTraining batch 18 Loss: 0.000031\n",
      "\tTraining batch 19 Loss: 0.000601\n",
      "\tTraining batch 20 Loss: 0.000171\n",
      "\tTraining batch 21 Loss: 0.000230\n",
      "Training set: Average loss: 0.000124\n",
      "Validation set: Average loss: 3.089989, Accuracy: 1371/1959 (69.98%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000032\n",
      "\tTraining batch 2 Loss: 0.000070\n",
      "\tTraining batch 3 Loss: 0.000019\n",
      "\tTraining batch 4 Loss: 0.000117\n",
      "\tTraining batch 5 Loss: 0.000147\n",
      "\tTraining batch 6 Loss: 0.000013\n",
      "\tTraining batch 7 Loss: 0.000023\n",
      "\tTraining batch 8 Loss: 0.000028\n",
      "\tTraining batch 9 Loss: 0.000003\n",
      "\tTraining batch 10 Loss: 0.000254\n",
      "\tTraining batch 11 Loss: 0.000082\n",
      "\tTraining batch 12 Loss: 0.000091\n",
      "\tTraining batch 13 Loss: 0.000214\n",
      "\tTraining batch 14 Loss: 0.000093\n",
      "\tTraining batch 15 Loss: 0.000126\n",
      "\tTraining batch 16 Loss: 0.000028\n",
      "\tTraining batch 17 Loss: 0.000042\n",
      "\tTraining batch 18 Loss: 0.000029\n",
      "\tTraining batch 19 Loss: 0.000534\n",
      "\tTraining batch 20 Loss: 0.000152\n",
      "\tTraining batch 21 Loss: 0.000200\n",
      "Training set: Average loss: 0.000109\n",
      "Validation set: Average loss: 3.108763, Accuracy: 1371/1959 (69.98%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 46.5%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000028\n",
      "\tTraining batch 2 Loss: 0.000059\n",
      "\tTraining batch 3 Loss: 0.000017\n",
      "\tTraining batch 4 Loss: 0.000102\n",
      "\tTraining batch 5 Loss: 0.000132\n",
      "\tTraining batch 6 Loss: 0.000012\n",
      "\tTraining batch 7 Loss: 0.000020\n",
      "\tTraining batch 8 Loss: 0.000026\n",
      "\tTraining batch 9 Loss: 0.000003\n",
      "\tTraining batch 10 Loss: 0.000228\n",
      "\tTraining batch 11 Loss: 0.000073\n",
      "\tTraining batch 12 Loss: 0.000083\n",
      "\tTraining batch 13 Loss: 0.000193\n",
      "\tTraining batch 14 Loss: 0.000082\n",
      "\tTraining batch 15 Loss: 0.000112\n",
      "\tTraining batch 16 Loss: 0.000026\n",
      "\tTraining batch 17 Loss: 0.000038\n",
      "\tTraining batch 18 Loss: 0.000026\n",
      "\tTraining batch 19 Loss: 0.000480\n",
      "\tTraining batch 20 Loss: 0.000136\n",
      "\tTraining batch 21 Loss: 0.000178\n",
      "\tTraining batch 22 Loss: 4.276609\n",
      "Training set: Average loss: 0.194485\n",
      "Validation set: Average loss: 2.466517, Accuracy: 1358/1959 (69.32%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000319\n",
      "\tTraining batch 2 Loss: 0.006106\n",
      "\tTraining batch 3 Loss: 0.001663\n",
      "\tTraining batch 4 Loss: 0.006109\n",
      "\tTraining batch 5 Loss: 0.026369\n",
      "\tTraining batch 6 Loss: 0.074458\n",
      "\tTraining batch 7 Loss: 0.048547\n",
      "\tTraining batch 8 Loss: 0.067033\n",
      "\tTraining batch 9 Loss: 0.086754\n",
      "\tTraining batch 10 Loss: 0.171200\n",
      "\tTraining batch 11 Loss: 0.222241\n",
      "\tTraining batch 12 Loss: 0.075887\n",
      "\tTraining batch 13 Loss: 0.192927\n",
      "\tTraining batch 14 Loss: 0.064189\n",
      "\tTraining batch 15 Loss: 0.098121\n",
      "\tTraining batch 16 Loss: 0.041373\n",
      "\tTraining batch 17 Loss: 0.046298\n",
      "\tTraining batch 18 Loss: 0.017387\n",
      "\tTraining batch 19 Loss: 0.063383\n",
      "\tTraining batch 20 Loss: 0.095973\n",
      "\tTraining batch 21 Loss: 0.024962\n",
      "\tTraining batch 22 Loss: 0.112100\n",
      "Training set: Average loss: 0.070155\n",
      "Validation set: Average loss: 1.942637, Accuracy: 1346/1959 (68.71%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.012322\n",
      "\tTraining batch 2 Loss: 0.022634\n",
      "\tTraining batch 3 Loss: 0.025574\n",
      "\tTraining batch 4 Loss: 0.029591\n",
      "\tTraining batch 5 Loss: 0.012169\n",
      "\tTraining batch 6 Loss: 0.003759\n",
      "\tTraining batch 7 Loss: 0.028662\n",
      "\tTraining batch 8 Loss: 0.006762\n",
      "\tTraining batch 9 Loss: 0.000651\n",
      "\tTraining batch 10 Loss: 0.000668\n",
      "\tTraining batch 11 Loss: 0.004797\n",
      "\tTraining batch 12 Loss: 0.000075\n",
      "\tTraining batch 13 Loss: 0.004825\n",
      "\tTraining batch 14 Loss: 0.000631\n",
      "\tTraining batch 15 Loss: 0.004487\n",
      "\tTraining batch 16 Loss: 0.000156\n",
      "\tTraining batch 17 Loss: 0.000070\n",
      "\tTraining batch 18 Loss: 0.001641\n",
      "\tTraining batch 19 Loss: 0.002879\n",
      "\tTraining batch 20 Loss: 0.001443\n",
      "\tTraining batch 21 Loss: 0.000846\n",
      "\tTraining batch 22 Loss: 0.012591\n",
      "Training set: Average loss: 0.008056\n",
      "Validation set: Average loss: 3.674275, Accuracy: 1373/1959 (70.09%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000015\n",
      "\tTraining batch 2 Loss: 0.000076\n",
      "\tTraining batch 3 Loss: 0.001171\n",
      "\tTraining batch 4 Loss: 0.027323\n",
      "\tTraining batch 5 Loss: 0.000047\n",
      "\tTraining batch 6 Loss: 0.000099\n",
      "\tTraining batch 7 Loss: 0.000422\n",
      "\tTraining batch 8 Loss: 0.001004\n",
      "\tTraining batch 9 Loss: 0.000283\n",
      "\tTraining batch 10 Loss: 0.000167\n",
      "\tTraining batch 11 Loss: 0.000222\n",
      "\tTraining batch 12 Loss: 0.000044\n",
      "\tTraining batch 13 Loss: 0.000507\n",
      "\tTraining batch 14 Loss: 0.000032\n",
      "\tTraining batch 15 Loss: 0.001069\n",
      "\tTraining batch 16 Loss: 0.000067\n",
      "\tTraining batch 17 Loss: 0.000332\n",
      "\tTraining batch 18 Loss: 0.000089\n",
      "\tTraining batch 19 Loss: 0.000801\n",
      "\tTraining batch 20 Loss: 0.002200\n",
      "\tTraining batch 21 Loss: 0.000259\n",
      "\tTraining batch 22 Loss: 0.001237\n",
      "Training set: Average loss: 0.001703\n",
      "Validation set: Average loss: 3.141699, Accuracy: 1349/1959 (68.86%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000040\n",
      "\tTraining batch 2 Loss: 0.001916\n",
      "\tTraining batch 3 Loss: 0.000670\n",
      "\tTraining batch 4 Loss: 0.000046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 5 Loss: 0.000440\n",
      "\tTraining batch 6 Loss: 0.000206\n",
      "\tTraining batch 7 Loss: 0.000257\n",
      "\tTraining batch 8 Loss: 0.000752\n",
      "\tTraining batch 9 Loss: 0.000996\n",
      "\tTraining batch 10 Loss: 0.000361\n",
      "\tTraining batch 11 Loss: 0.000290\n",
      "\tTraining batch 12 Loss: 0.000065\n",
      "\tTraining batch 13 Loss: 0.000416\n",
      "\tTraining batch 14 Loss: 0.000081\n",
      "\tTraining batch 15 Loss: 0.000844\n",
      "\tTraining batch 16 Loss: 0.000088\n",
      "\tTraining batch 17 Loss: 0.000310\n",
      "\tTraining batch 18 Loss: 0.000050\n",
      "\tTraining batch 19 Loss: 0.000321\n",
      "\tTraining batch 20 Loss: 0.000766\n",
      "\tTraining batch 21 Loss: 0.000211\n",
      "\tTraining batch 22 Loss: 0.000501\n",
      "Training set: Average loss: 0.000438\n",
      "Validation set: Average loss: 3.212896, Accuracy: 1357/1959 (69.27%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000025\n",
      "\tTraining batch 2 Loss: 0.000682\n",
      "\tTraining batch 3 Loss: 0.000220\n",
      "\tTraining batch 4 Loss: 0.000041\n",
      "\tTraining batch 5 Loss: 0.000256\n",
      "\tTraining batch 6 Loss: 0.000035\n",
      "\tTraining batch 7 Loss: 0.000144\n",
      "\tTraining batch 8 Loss: 0.000248\n",
      "\tTraining batch 9 Loss: 0.000119\n",
      "\tTraining batch 10 Loss: 0.000243\n",
      "\tTraining batch 11 Loss: 0.000216\n",
      "\tTraining batch 12 Loss: 0.000032\n",
      "\tTraining batch 13 Loss: 0.000230\n",
      "\tTraining batch 14 Loss: 0.000065\n",
      "\tTraining batch 15 Loss: 0.000612\n",
      "\tTraining batch 16 Loss: 0.000058\n",
      "\tTraining batch 17 Loss: 0.000237\n",
      "\tTraining batch 18 Loss: 0.000014\n",
      "\tTraining batch 19 Loss: 0.000207\n",
      "\tTraining batch 20 Loss: 0.000408\n",
      "\tTraining batch 21 Loss: 0.000177\n",
      "\tTraining batch 22 Loss: 0.000248\n",
      "Training set: Average loss: 0.000205\n",
      "Validation set: Average loss: 3.264038, Accuracy: 1365/1959 (69.68%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000015\n",
      "\tTraining batch 2 Loss: 0.000332\n",
      "\tTraining batch 3 Loss: 0.000092\n",
      "\tTraining batch 4 Loss: 0.000039\n",
      "\tTraining batch 5 Loss: 0.000143\n",
      "\tTraining batch 6 Loss: 0.000021\n",
      "\tTraining batch 7 Loss: 0.000094\n",
      "\tTraining batch 8 Loss: 0.000157\n",
      "\tTraining batch 9 Loss: 0.000049\n",
      "\tTraining batch 10 Loss: 0.000185\n",
      "\tTraining batch 11 Loss: 0.000137\n",
      "\tTraining batch 12 Loss: 0.000020\n",
      "\tTraining batch 13 Loss: 0.000129\n",
      "\tTraining batch 14 Loss: 0.000053\n",
      "\tTraining batch 15 Loss: 0.000432\n",
      "\tTraining batch 16 Loss: 0.000044\n",
      "\tTraining batch 17 Loss: 0.000197\n",
      "\tTraining batch 18 Loss: 0.000007\n",
      "\tTraining batch 19 Loss: 0.000165\n",
      "\tTraining batch 20 Loss: 0.000249\n",
      "\tTraining batch 21 Loss: 0.000158\n",
      "\tTraining batch 22 Loss: 0.000148\n",
      "Training set: Average loss: 0.000130\n",
      "Validation set: Average loss: 3.300071, Accuracy: 1364/1959 (69.63%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000011\n",
      "\tTraining batch 2 Loss: 0.000223\n",
      "\tTraining batch 3 Loss: 0.000045\n",
      "\tTraining batch 4 Loss: 0.000037\n",
      "\tTraining batch 5 Loss: 0.000069\n",
      "\tTraining batch 6 Loss: 0.000017\n",
      "\tTraining batch 7 Loss: 0.000065\n",
      "\tTraining batch 8 Loss: 0.000098\n",
      "\tTraining batch 9 Loss: 0.000031\n",
      "\tTraining batch 10 Loss: 0.000152\n",
      "\tTraining batch 11 Loss: 0.000081\n",
      "\tTraining batch 12 Loss: 0.000016\n",
      "\tTraining batch 13 Loss: 0.000092\n",
      "\tTraining batch 14 Loss: 0.000046\n",
      "\tTraining batch 15 Loss: 0.000290\n",
      "\tTraining batch 16 Loss: 0.000037\n",
      "\tTraining batch 17 Loss: 0.000170\n",
      "\tTraining batch 18 Loss: 0.000004\n",
      "\tTraining batch 19 Loss: 0.000143\n",
      "\tTraining batch 20 Loss: 0.000150\n",
      "\tTraining batch 21 Loss: 0.000145\n",
      "\tTraining batch 22 Loss: 0.000090\n",
      "Training set: Average loss: 0.000092\n",
      "Validation set: Average loss: 3.328976, Accuracy: 1365/1959 (69.68%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000010\n",
      "\tTraining batch 2 Loss: 0.000170\n",
      "\tTraining batch 3 Loss: 0.000025\n",
      "\tTraining batch 4 Loss: 0.000035\n",
      "\tTraining batch 5 Loss: 0.000035\n",
      "\tTraining batch 6 Loss: 0.000015\n",
      "\tTraining batch 7 Loss: 0.000049\n",
      "\tTraining batch 8 Loss: 0.000068\n",
      "\tTraining batch 9 Loss: 0.000023\n",
      "\tTraining batch 10 Loss: 0.000131\n",
      "\tTraining batch 11 Loss: 0.000047\n",
      "\tTraining batch 12 Loss: 0.000014\n",
      "\tTraining batch 13 Loss: 0.000078\n",
      "\tTraining batch 14 Loss: 0.000041\n",
      "\tTraining batch 15 Loss: 0.000205\n",
      "\tTraining batch 16 Loss: 0.000031\n",
      "\tTraining batch 17 Loss: 0.000150\n",
      "\tTraining batch 18 Loss: 0.000003\n",
      "\tTraining batch 19 Loss: 0.000130\n",
      "\tTraining batch 20 Loss: 0.000093\n",
      "\tTraining batch 21 Loss: 0.000136\n",
      "\tTraining batch 22 Loss: 0.000057\n",
      "Training set: Average loss: 0.000070\n",
      "Validation set: Average loss: 3.352763, Accuracy: 1368/1959 (69.83%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000009\n",
      "\tTraining batch 2 Loss: 0.000138\n",
      "\tTraining batch 3 Loss: 0.000016\n",
      "\tTraining batch 4 Loss: 0.000033\n",
      "\tTraining batch 5 Loss: 0.000022\n",
      "\tTraining batch 6 Loss: 0.000013\n",
      "\tTraining batch 7 Loss: 0.000041\n",
      "\tTraining batch 8 Loss: 0.000051\n",
      "\tTraining batch 9 Loss: 0.000018\n",
      "\tTraining batch 10 Loss: 0.000116\n",
      "\tTraining batch 11 Loss: 0.000030\n",
      "\tTraining batch 12 Loss: 0.000012\n",
      "\tTraining batch 13 Loss: 0.000068\n",
      "\tTraining batch 14 Loss: 0.000037\n",
      "\tTraining batch 15 Loss: 0.000154\n",
      "\tTraining batch 16 Loss: 0.000026\n",
      "\tTraining batch 17 Loss: 0.000135\n",
      "\tTraining batch 18 Loss: 0.000002\n",
      "\tTraining batch 19 Loss: 0.000120\n",
      "\tTraining batch 20 Loss: 0.000062\n",
      "\tTraining batch 21 Loss: 0.000128\n",
      "\tTraining batch 22 Loss: 0.000038\n",
      "Training set: Average loss: 0.000058\n",
      "Validation set: Average loss: 3.373077, Accuracy: 1371/1959 (69.98%)\n",
      "\n",
      "Epoch: 11\n",
      "\tTraining batch 1 Loss: 0.000009\n",
      "\tTraining batch 2 Loss: 0.000116\n",
      "\tTraining batch 3 Loss: 0.000012\n",
      "\tTraining batch 4 Loss: 0.000031\n",
      "\tTraining batch 5 Loss: 0.000016\n",
      "\tTraining batch 6 Loss: 0.000012\n",
      "\tTraining batch 7 Loss: 0.000037\n",
      "\tTraining batch 8 Loss: 0.000041\n",
      "\tTraining batch 9 Loss: 0.000015\n",
      "\tTraining batch 10 Loss: 0.000104\n",
      "\tTraining batch 11 Loss: 0.000021\n",
      "\tTraining batch 12 Loss: 0.000011\n",
      "\tTraining batch 13 Loss: 0.000061\n",
      "\tTraining batch 14 Loss: 0.000034\n",
      "\tTraining batch 15 Loss: 0.000125\n",
      "\tTraining batch 16 Loss: 0.000024\n",
      "\tTraining batch 17 Loss: 0.000124\n",
      "\tTraining batch 18 Loss: 0.000002\n",
      "\tTraining batch 19 Loss: 0.000110\n",
      "\tTraining batch 20 Loss: 0.000046\n",
      "\tTraining batch 21 Loss: 0.000120\n",
      "\tTraining batch 22 Loss: 0.000027\n",
      "Training set: Average loss: 0.000050\n",
      "Validation set: Average loss: 3.390523, Accuracy: 1371/1959 (69.98%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 47.0%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000009\n",
      "\tTraining batch 2 Loss: 0.000099\n",
      "\tTraining batch 3 Loss: 0.000009\n",
      "\tTraining batch 4 Loss: 0.000029\n",
      "\tTraining batch 5 Loss: 0.000012\n",
      "\tTraining batch 6 Loss: 0.000011\n",
      "\tTraining batch 7 Loss: 0.000033\n",
      "\tTraining batch 8 Loss: 0.000035\n",
      "\tTraining batch 9 Loss: 0.000013\n",
      "\tTraining batch 10 Loss: 0.000095\n",
      "\tTraining batch 11 Loss: 0.000016\n",
      "\tTraining batch 12 Loss: 0.000009\n",
      "\tTraining batch 13 Loss: 0.000057\n",
      "\tTraining batch 14 Loss: 0.000032\n",
      "\tTraining batch 15 Loss: 0.000103\n",
      "\tTraining batch 16 Loss: 0.000022\n",
      "\tTraining batch 17 Loss: 0.000114\n",
      "\tTraining batch 18 Loss: 0.000002\n",
      "\tTraining batch 19 Loss: 0.000101\n",
      "\tTraining batch 20 Loss: 0.000036\n",
      "\tTraining batch 21 Loss: 0.000113\n",
      "\tTraining batch 22 Loss: 0.000020\n",
      "\tTraining batch 23 Loss: 2.852281\n",
      "Training set: Average loss: 0.124054\n",
      "Validation set: Average loss: 3.324036, Accuracy: 1357/1959 (69.27%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000018\n",
      "\tTraining batch 2 Loss: 0.000419\n",
      "\tTraining batch 3 Loss: 0.000407\n",
      "\tTraining batch 4 Loss: 0.014425\n",
      "\tTraining batch 5 Loss: 0.178033\n",
      "\tTraining batch 6 Loss: 0.006654\n",
      "\tTraining batch 7 Loss: 0.108720\n",
      "\tTraining batch 8 Loss: 0.003791\n",
      "\tTraining batch 9 Loss: 0.011142\n",
      "\tTraining batch 10 Loss: 0.019386\n",
      "\tTraining batch 11 Loss: 0.023119\n",
      "\tTraining batch 12 Loss: 0.014729\n",
      "\tTraining batch 13 Loss: 0.070166\n",
      "\tTraining batch 14 Loss: 0.049770\n",
      "\tTraining batch 15 Loss: 0.052903\n",
      "\tTraining batch 16 Loss: 0.028683\n",
      "\tTraining batch 17 Loss: 0.026700\n",
      "\tTraining batch 18 Loss: 0.028378\n",
      "\tTraining batch 19 Loss: 0.071818\n",
      "\tTraining batch 20 Loss: 0.012657\n",
      "\tTraining batch 21 Loss: 0.071198\n",
      "\tTraining batch 22 Loss: 0.040725\n",
      "\tTraining batch 23 Loss: 0.043871\n",
      "Training set: Average loss: 0.038161\n",
      "Validation set: Average loss: 1.801352, Accuracy: 1374/1959 (70.14%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.001950\n",
      "\tTraining batch 2 Loss: 0.015189\n",
      "\tTraining batch 3 Loss: 0.000965\n",
      "\tTraining batch 4 Loss: 0.002320\n",
      "\tTraining batch 5 Loss: 0.006447\n",
      "\tTraining batch 6 Loss: 0.002561\n",
      "\tTraining batch 7 Loss: 0.004861\n",
      "\tTraining batch 8 Loss: 0.001396\n",
      "\tTraining batch 9 Loss: 0.000966\n",
      "\tTraining batch 10 Loss: 0.001262\n",
      "\tTraining batch 11 Loss: 0.002419\n",
      "\tTraining batch 12 Loss: 0.000351\n",
      "\tTraining batch 13 Loss: 0.015005\n",
      "\tTraining batch 14 Loss: 0.000355\n",
      "\tTraining batch 15 Loss: 0.011813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 16 Loss: 0.001166\n",
      "\tTraining batch 17 Loss: 0.009380\n",
      "\tTraining batch 18 Loss: 0.000305\n",
      "\tTraining batch 19 Loss: 0.012665\n",
      "\tTraining batch 20 Loss: 0.003014\n",
      "\tTraining batch 21 Loss: 0.000165\n",
      "\tTraining batch 22 Loss: 0.014602\n",
      "\tTraining batch 23 Loss: 0.004489\n",
      "Training set: Average loss: 0.004941\n",
      "Validation set: Average loss: 2.504029, Accuracy: 1376/1959 (70.24%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000336\n",
      "\tTraining batch 2 Loss: 0.000500\n",
      "\tTraining batch 3 Loss: 0.000037\n",
      "\tTraining batch 4 Loss: 0.002358\n",
      "\tTraining batch 5 Loss: 0.000182\n",
      "\tTraining batch 6 Loss: 0.000418\n",
      "\tTraining batch 7 Loss: 0.000164\n",
      "\tTraining batch 8 Loss: 0.000413\n",
      "\tTraining batch 9 Loss: 0.000114\n",
      "\tTraining batch 10 Loss: 0.000382\n",
      "\tTraining batch 11 Loss: 0.000691\n",
      "\tTraining batch 12 Loss: 0.000068\n",
      "\tTraining batch 13 Loss: 0.001217\n",
      "\tTraining batch 14 Loss: 0.000067\n",
      "\tTraining batch 15 Loss: 0.004122\n",
      "\tTraining batch 16 Loss: 0.000288\n",
      "\tTraining batch 17 Loss: 0.001461\n",
      "\tTraining batch 18 Loss: 0.000450\n",
      "\tTraining batch 19 Loss: 0.003779\n",
      "\tTraining batch 20 Loss: 0.000290\n",
      "\tTraining batch 21 Loss: 0.000056\n",
      "\tTraining batch 22 Loss: 0.004315\n",
      "\tTraining batch 23 Loss: 0.001777\n",
      "Training set: Average loss: 0.001021\n",
      "Validation set: Average loss: 2.765641, Accuracy: 1379/1959 (70.39%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000095\n",
      "\tTraining batch 2 Loss: 0.000185\n",
      "\tTraining batch 3 Loss: 0.000022\n",
      "\tTraining batch 4 Loss: 0.000824\n",
      "\tTraining batch 5 Loss: 0.000071\n",
      "\tTraining batch 6 Loss: 0.000164\n",
      "\tTraining batch 7 Loss: 0.000066\n",
      "\tTraining batch 8 Loss: 0.000186\n",
      "\tTraining batch 9 Loss: 0.000052\n",
      "\tTraining batch 10 Loss: 0.000294\n",
      "\tTraining batch 11 Loss: 0.000443\n",
      "\tTraining batch 12 Loss: 0.000059\n",
      "\tTraining batch 13 Loss: 0.000687\n",
      "\tTraining batch 14 Loss: 0.000041\n",
      "\tTraining batch 15 Loss: 0.002214\n",
      "\tTraining batch 16 Loss: 0.000175\n",
      "\tTraining batch 17 Loss: 0.000674\n",
      "\tTraining batch 18 Loss: 0.000196\n",
      "\tTraining batch 19 Loss: 0.002182\n",
      "\tTraining batch 20 Loss: 0.000153\n",
      "\tTraining batch 21 Loss: 0.000039\n",
      "\tTraining batch 22 Loss: 0.002000\n",
      "\tTraining batch 23 Loss: 0.001194\n",
      "Training set: Average loss: 0.000522\n",
      "Validation set: Average loss: 2.852426, Accuracy: 1386/1959 (70.75%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000051\n",
      "\tTraining batch 2 Loss: 0.000118\n",
      "\tTraining batch 3 Loss: 0.000018\n",
      "\tTraining batch 4 Loss: 0.000513\n",
      "\tTraining batch 5 Loss: 0.000055\n",
      "\tTraining batch 6 Loss: 0.000105\n",
      "\tTraining batch 7 Loss: 0.000045\n",
      "\tTraining batch 8 Loss: 0.000122\n",
      "\tTraining batch 9 Loss: 0.000037\n",
      "\tTraining batch 10 Loss: 0.000254\n",
      "\tTraining batch 11 Loss: 0.000343\n",
      "\tTraining batch 12 Loss: 0.000056\n",
      "\tTraining batch 13 Loss: 0.000518\n",
      "\tTraining batch 14 Loss: 0.000032\n",
      "\tTraining batch 15 Loss: 0.001386\n",
      "\tTraining batch 16 Loss: 0.000119\n",
      "\tTraining batch 17 Loss: 0.000444\n",
      "\tTraining batch 18 Loss: 0.000125\n",
      "\tTraining batch 19 Loss: 0.001516\n",
      "\tTraining batch 20 Loss: 0.000110\n",
      "\tTraining batch 21 Loss: 0.000031\n",
      "\tTraining batch 22 Loss: 0.001239\n",
      "\tTraining batch 23 Loss: 0.000914\n",
      "Training set: Average loss: 0.000354\n",
      "Validation set: Average loss: 2.908447, Accuracy: 1390/1959 (70.95%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000036\n",
      "\tTraining batch 2 Loss: 0.000089\n",
      "\tTraining batch 3 Loss: 0.000015\n",
      "\tTraining batch 4 Loss: 0.000398\n",
      "\tTraining batch 5 Loss: 0.000047\n",
      "\tTraining batch 6 Loss: 0.000081\n",
      "\tTraining batch 7 Loss: 0.000035\n",
      "\tTraining batch 8 Loss: 0.000093\n",
      "\tTraining batch 9 Loss: 0.000030\n",
      "\tTraining batch 10 Loss: 0.000222\n",
      "\tTraining batch 11 Loss: 0.000286\n",
      "\tTraining batch 12 Loss: 0.000049\n",
      "\tTraining batch 13 Loss: 0.000413\n",
      "\tTraining batch 14 Loss: 0.000027\n",
      "\tTraining batch 15 Loss: 0.000991\n",
      "\tTraining batch 16 Loss: 0.000089\n",
      "\tTraining batch 17 Loss: 0.000330\n",
      "\tTraining batch 18 Loss: 0.000094\n",
      "\tTraining batch 19 Loss: 0.001172\n",
      "\tTraining batch 20 Loss: 0.000089\n",
      "\tTraining batch 21 Loss: 0.000026\n",
      "\tTraining batch 22 Loss: 0.000895\n",
      "\tTraining batch 23 Loss: 0.000751\n",
      "Training set: Average loss: 0.000272\n",
      "Validation set: Average loss: 2.952478, Accuracy: 1397/1959 (71.31%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000029\n",
      "\tTraining batch 2 Loss: 0.000072\n",
      "\tTraining batch 3 Loss: 0.000013\n",
      "\tTraining batch 4 Loss: 0.000317\n",
      "\tTraining batch 5 Loss: 0.000042\n",
      "\tTraining batch 6 Loss: 0.000068\n",
      "\tTraining batch 7 Loss: 0.000028\n",
      "\tTraining batch 8 Loss: 0.000078\n",
      "\tTraining batch 9 Loss: 0.000025\n",
      "\tTraining batch 10 Loss: 0.000192\n",
      "\tTraining batch 11 Loss: 0.000245\n",
      "\tTraining batch 12 Loss: 0.000042\n",
      "\tTraining batch 13 Loss: 0.000338\n",
      "\tTraining batch 14 Loss: 0.000024\n",
      "\tTraining batch 15 Loss: 0.000767\n",
      "\tTraining batch 16 Loss: 0.000071\n",
      "\tTraining batch 17 Loss: 0.000264\n",
      "\tTraining batch 18 Loss: 0.000074\n",
      "\tTraining batch 19 Loss: 0.000960\n",
      "\tTraining batch 20 Loss: 0.000075\n",
      "\tTraining batch 21 Loss: 0.000023\n",
      "\tTraining batch 22 Loss: 0.000706\n",
      "\tTraining batch 23 Loss: 0.000636\n",
      "Training set: Average loss: 0.000221\n",
      "Validation set: Average loss: 2.989370, Accuracy: 1398/1959 (71.36%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000024\n",
      "\tTraining batch 2 Loss: 0.000060\n",
      "\tTraining batch 3 Loss: 0.000012\n",
      "\tTraining batch 4 Loss: 0.000259\n",
      "\tTraining batch 5 Loss: 0.000037\n",
      "\tTraining batch 6 Loss: 0.000059\n",
      "\tTraining batch 7 Loss: 0.000024\n",
      "\tTraining batch 8 Loss: 0.000067\n",
      "\tTraining batch 9 Loss: 0.000022\n",
      "\tTraining batch 10 Loss: 0.000169\n",
      "\tTraining batch 11 Loss: 0.000209\n",
      "\tTraining batch 12 Loss: 0.000036\n",
      "\tTraining batch 13 Loss: 0.000285\n",
      "\tTraining batch 14 Loss: 0.000021\n",
      "\tTraining batch 15 Loss: 0.000615\n",
      "\tTraining batch 16 Loss: 0.000059\n",
      "\tTraining batch 17 Loss: 0.000219\n",
      "\tTraining batch 18 Loss: 0.000060\n",
      "\tTraining batch 19 Loss: 0.000807\n",
      "\tTraining batch 20 Loss: 0.000065\n",
      "\tTraining batch 21 Loss: 0.000020\n",
      "\tTraining batch 22 Loss: 0.000577\n",
      "\tTraining batch 23 Loss: 0.000550\n",
      "Training set: Average loss: 0.000185\n",
      "Validation set: Average loss: 3.021915, Accuracy: 1397/1959 (71.31%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000021\n",
      "\tTraining batch 2 Loss: 0.000052\n",
      "\tTraining batch 3 Loss: 0.000011\n",
      "\tTraining batch 4 Loss: 0.000217\n",
      "\tTraining batch 5 Loss: 0.000034\n",
      "\tTraining batch 6 Loss: 0.000052\n",
      "\tTraining batch 7 Loss: 0.000021\n",
      "\tTraining batch 8 Loss: 0.000059\n",
      "\tTraining batch 9 Loss: 0.000020\n",
      "\tTraining batch 10 Loss: 0.000150\n",
      "\tTraining batch 11 Loss: 0.000178\n",
      "\tTraining batch 12 Loss: 0.000031\n",
      "\tTraining batch 13 Loss: 0.000244\n",
      "\tTraining batch 14 Loss: 0.000019\n",
      "\tTraining batch 15 Loss: 0.000508\n",
      "\tTraining batch 16 Loss: 0.000050\n",
      "\tTraining batch 17 Loss: 0.000186\n",
      "\tTraining batch 18 Loss: 0.000050\n",
      "\tTraining batch 19 Loss: 0.000695\n",
      "\tTraining batch 20 Loss: 0.000058\n",
      "\tTraining batch 21 Loss: 0.000018\n",
      "\tTraining batch 22 Loss: 0.000485\n",
      "\tTraining batch 23 Loss: 0.000482\n",
      "Training set: Average loss: 0.000158\n",
      "Validation set: Average loss: 3.050958, Accuracy: 1397/1959 (71.31%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 48.5%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000018\n",
      "\tTraining batch 2 Loss: 0.000046\n",
      "\tTraining batch 3 Loss: 0.000010\n",
      "\tTraining batch 4 Loss: 0.000184\n",
      "\tTraining batch 5 Loss: 0.000031\n",
      "\tTraining batch 6 Loss: 0.000047\n",
      "\tTraining batch 7 Loss: 0.000019\n",
      "\tTraining batch 8 Loss: 0.000053\n",
      "\tTraining batch 9 Loss: 0.000018\n",
      "\tTraining batch 10 Loss: 0.000135\n",
      "\tTraining batch 11 Loss: 0.000154\n",
      "\tTraining batch 12 Loss: 0.000027\n",
      "\tTraining batch 13 Loss: 0.000213\n",
      "\tTraining batch 14 Loss: 0.000017\n",
      "\tTraining batch 15 Loss: 0.000425\n",
      "\tTraining batch 16 Loss: 0.000043\n",
      "\tTraining batch 17 Loss: 0.000160\n",
      "\tTraining batch 18 Loss: 0.000043\n",
      "\tTraining batch 19 Loss: 0.000608\n",
      "\tTraining batch 20 Loss: 0.000051\n",
      "\tTraining batch 21 Loss: 0.000017\n",
      "\tTraining batch 22 Loss: 0.000415\n",
      "\tTraining batch 23 Loss: 0.000427\n",
      "\tTraining batch 24 Loss: 3.041752\n",
      "Training set: Average loss: 0.126871\n",
      "Validation set: Average loss: 2.558596, Accuracy: 1396/1959 (71.26%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000067\n",
      "\tTraining batch 2 Loss: 0.000480\n",
      "\tTraining batch 3 Loss: 0.036682\n",
      "\tTraining batch 4 Loss: 0.050049\n",
      "\tTraining batch 5 Loss: 0.007902\n",
      "\tTraining batch 6 Loss: 0.566572\n",
      "\tTraining batch 7 Loss: 0.014040\n",
      "\tTraining batch 8 Loss: 0.071956\n",
      "\tTraining batch 9 Loss: 0.199728\n",
      "\tTraining batch 10 Loss: 0.061850\n",
      "\tTraining batch 11 Loss: 0.130729\n",
      "\tTraining batch 12 Loss: 0.113022\n",
      "\tTraining batch 13 Loss: 0.089350\n",
      "\tTraining batch 14 Loss: 0.113660\n",
      "\tTraining batch 15 Loss: 0.093023\n",
      "\tTraining batch 16 Loss: 0.034978\n",
      "\tTraining batch 17 Loss: 0.113012\n",
      "\tTraining batch 18 Loss: 0.061408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 19 Loss: 0.099176\n",
      "\tTraining batch 20 Loss: 0.125825\n",
      "\tTraining batch 21 Loss: 0.028987\n",
      "\tTraining batch 22 Loss: 0.013380\n",
      "\tTraining batch 23 Loss: 0.009269\n",
      "\tTraining batch 24 Loss: 0.258745\n",
      "Training set: Average loss: 0.095579\n",
      "Validation set: Average loss: 2.988599, Accuracy: 1368/1959 (69.83%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.068271\n",
      "\tTraining batch 2 Loss: 0.015772\n",
      "\tTraining batch 3 Loss: 0.097879\n",
      "\tTraining batch 4 Loss: 0.002827\n",
      "\tTraining batch 5 Loss: 0.123135\n",
      "\tTraining batch 6 Loss: 0.000312\n",
      "\tTraining batch 7 Loss: 0.000304\n",
      "\tTraining batch 8 Loss: 0.000090\n",
      "\tTraining batch 9 Loss: 0.000013\n",
      "\tTraining batch 10 Loss: 0.000289\n",
      "\tTraining batch 11 Loss: 0.000196\n",
      "\tTraining batch 12 Loss: 0.000103\n",
      "\tTraining batch 13 Loss: 0.000290\n",
      "\tTraining batch 14 Loss: 0.001936\n",
      "\tTraining batch 15 Loss: 0.002051\n",
      "\tTraining batch 16 Loss: 0.000003\n",
      "\tTraining batch 17 Loss: 0.000073\n",
      "\tTraining batch 18 Loss: 0.227466\n",
      "\tTraining batch 19 Loss: 0.429269\n",
      "\tTraining batch 20 Loss: 0.000359\n",
      "\tTraining batch 21 Loss: 0.000573\n",
      "\tTraining batch 22 Loss: 0.006528\n",
      "\tTraining batch 23 Loss: 0.001209\n",
      "\tTraining batch 24 Loss: 0.991876\n",
      "Training set: Average loss: 0.082118\n",
      "Validation set: Average loss: 5.650579, Accuracy: 1304/1959 (66.56%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000171\n",
      "\tTraining batch 2 Loss: 0.010592\n",
      "\tTraining batch 3 Loss: 0.000016\n",
      "\tTraining batch 4 Loss: 0.009988\n",
      "\tTraining batch 5 Loss: 0.045426\n",
      "\tTraining batch 6 Loss: 0.150213\n",
      "\tTraining batch 7 Loss: 0.000920\n",
      "\tTraining batch 8 Loss: 0.008073\n",
      "\tTraining batch 9 Loss: 0.273132\n",
      "\tTraining batch 10 Loss: 0.374686\n",
      "\tTraining batch 11 Loss: 0.041318\n",
      "\tTraining batch 12 Loss: 0.001422\n",
      "\tTraining batch 13 Loss: 0.003754\n",
      "\tTraining batch 14 Loss: 0.001805\n",
      "\tTraining batch 15 Loss: 0.076619\n",
      "\tTraining batch 16 Loss: 0.000097\n",
      "\tTraining batch 17 Loss: 0.017339\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.003511\n",
      "\tTraining batch 20 Loss: 0.000443\n",
      "\tTraining batch 21 Loss: 0.000060\n",
      "\tTraining batch 22 Loss: 0.000052\n",
      "\tTraining batch 23 Loss: 0.000216\n",
      "\tTraining batch 24 Loss: 0.076196\n",
      "Training set: Average loss: 0.045669\n",
      "Validation set: Average loss: 7.217141, Accuracy: 1340/1959 (68.40%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000002\n",
      "\tTraining batch 2 Loss: 0.001308\n",
      "\tTraining batch 3 Loss: 0.117550\n",
      "\tTraining batch 4 Loss: 0.000034\n",
      "\tTraining batch 5 Loss: 0.000246\n",
      "\tTraining batch 6 Loss: 0.000236\n",
      "\tTraining batch 7 Loss: 0.000044\n",
      "\tTraining batch 8 Loss: 0.000091\n",
      "\tTraining batch 9 Loss: 0.000002\n",
      "\tTraining batch 10 Loss: 0.000608\n",
      "\tTraining batch 11 Loss: 0.000002\n",
      "\tTraining batch 12 Loss: 0.018498\n",
      "\tTraining batch 13 Loss: 0.036431\n",
      "\tTraining batch 14 Loss: 0.000425\n",
      "\tTraining batch 15 Loss: 0.000622\n",
      "\tTraining batch 16 Loss: 0.000029\n",
      "\tTraining batch 17 Loss: 0.000680\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.004904\n",
      "\tTraining batch 20 Loss: 0.000135\n",
      "\tTraining batch 21 Loss: 0.000146\n",
      "\tTraining batch 22 Loss: 0.000065\n",
      "\tTraining batch 23 Loss: 0.000017\n",
      "\tTraining batch 24 Loss: 0.000624\n",
      "Training set: Average loss: 0.007612\n",
      "Validation set: Average loss: 4.804669, Accuracy: 1344/1959 (68.61%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000010\n",
      "\tTraining batch 2 Loss: 0.000053\n",
      "\tTraining batch 3 Loss: 0.000005\n",
      "\tTraining batch 4 Loss: 0.003702\n",
      "\tTraining batch 5 Loss: 0.000001\n",
      "\tTraining batch 6 Loss: 0.000012\n",
      "\tTraining batch 7 Loss: 0.000274\n",
      "\tTraining batch 8 Loss: 0.000078\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000073\n",
      "\tTraining batch 11 Loss: 0.000090\n",
      "\tTraining batch 12 Loss: 0.000023\n",
      "\tTraining batch 13 Loss: 0.021443\n",
      "\tTraining batch 14 Loss: 0.000004\n",
      "\tTraining batch 15 Loss: 0.000387\n",
      "\tTraining batch 16 Loss: 0.000001\n",
      "\tTraining batch 17 Loss: 0.001160\n",
      "\tTraining batch 18 Loss: 0.000001\n",
      "\tTraining batch 19 Loss: 0.003211\n",
      "\tTraining batch 20 Loss: 0.001005\n",
      "\tTraining batch 21 Loss: 0.001460\n",
      "\tTraining batch 22 Loss: 0.000024\n",
      "\tTraining batch 23 Loss: 0.000062\n",
      "\tTraining batch 24 Loss: 0.000190\n",
      "Training set: Average loss: 0.001386\n",
      "Validation set: Average loss: 4.959818, Accuracy: 1362/1959 (69.53%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000006\n",
      "\tTraining batch 2 Loss: 0.000004\n",
      "\tTraining batch 3 Loss: 0.000007\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000001\n",
      "\tTraining batch 6 Loss: 0.000012\n",
      "\tTraining batch 7 Loss: 0.000199\n",
      "\tTraining batch 8 Loss: 0.000064\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000048\n",
      "\tTraining batch 11 Loss: 0.000022\n",
      "\tTraining batch 12 Loss: 0.000009\n",
      "\tTraining batch 13 Loss: 0.003736\n",
      "\tTraining batch 14 Loss: 0.000003\n",
      "\tTraining batch 15 Loss: 0.000128\n",
      "\tTraining batch 16 Loss: 0.000003\n",
      "\tTraining batch 17 Loss: 0.000103\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001694\n",
      "\tTraining batch 20 Loss: 0.000006\n",
      "\tTraining batch 21 Loss: 0.000029\n",
      "\tTraining batch 22 Loss: 0.000010\n",
      "\tTraining batch 23 Loss: 0.000013\n",
      "\tTraining batch 24 Loss: 0.000052\n",
      "Training set: Average loss: 0.000256\n",
      "Validation set: Average loss: 5.153169, Accuracy: 1371/1959 (69.98%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000004\n",
      "\tTraining batch 2 Loss: 0.000001\n",
      "\tTraining batch 3 Loss: 0.000005\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000001\n",
      "\tTraining batch 6 Loss: 0.000014\n",
      "\tTraining batch 7 Loss: 0.000118\n",
      "\tTraining batch 8 Loss: 0.000035\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000038\n",
      "\tTraining batch 11 Loss: 0.000014\n",
      "\tTraining batch 12 Loss: 0.000007\n",
      "\tTraining batch 13 Loss: 0.001359\n",
      "\tTraining batch 14 Loss: 0.000002\n",
      "\tTraining batch 15 Loss: 0.000071\n",
      "\tTraining batch 16 Loss: 0.000003\n",
      "\tTraining batch 17 Loss: 0.000074\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001348\n",
      "\tTraining batch 20 Loss: 0.000005\n",
      "\tTraining batch 21 Loss: 0.000027\n",
      "\tTraining batch 22 Loss: 0.000008\n",
      "\tTraining batch 23 Loss: 0.000011\n",
      "\tTraining batch 24 Loss: 0.000041\n",
      "Training set: Average loss: 0.000133\n",
      "Validation set: Average loss: 5.202026, Accuracy: 1372/1959 (70.04%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000003\n",
      "\tTraining batch 2 Loss: 0.000001\n",
      "\tTraining batch 3 Loss: 0.000005\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000001\n",
      "\tTraining batch 6 Loss: 0.000013\n",
      "\tTraining batch 7 Loss: 0.000085\n",
      "\tTraining batch 8 Loss: 0.000026\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000034\n",
      "\tTraining batch 11 Loss: 0.000012\n",
      "\tTraining batch 12 Loss: 0.000007\n",
      "\tTraining batch 13 Loss: 0.000836\n",
      "\tTraining batch 14 Loss: 0.000002\n",
      "\tTraining batch 15 Loss: 0.000053\n",
      "\tTraining batch 16 Loss: 0.000003\n",
      "\tTraining batch 17 Loss: 0.000062\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001118\n",
      "\tTraining batch 20 Loss: 0.000004\n",
      "\tTraining batch 21 Loss: 0.000026\n",
      "\tTraining batch 22 Loss: 0.000007\n",
      "\tTraining batch 23 Loss: 0.000010\n",
      "\tTraining batch 24 Loss: 0.000036\n",
      "Training set: Average loss: 0.000098\n",
      "Validation set: Average loss: 5.231549, Accuracy: 1374/1959 (70.14%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000002\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000004\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000001\n",
      "\tTraining batch 6 Loss: 0.000012\n",
      "\tTraining batch 7 Loss: 0.000067\n",
      "\tTraining batch 8 Loss: 0.000020\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000031\n",
      "\tTraining batch 11 Loss: 0.000011\n",
      "\tTraining batch 12 Loss: 0.000007\n",
      "\tTraining batch 13 Loss: 0.000577\n",
      "\tTraining batch 14 Loss: 0.000002\n",
      "\tTraining batch 15 Loss: 0.000043\n",
      "\tTraining batch 16 Loss: 0.000003\n",
      "\tTraining batch 17 Loss: 0.000054\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000938\n",
      "\tTraining batch 20 Loss: 0.000004\n",
      "\tTraining batch 21 Loss: 0.000025\n",
      "\tTraining batch 22 Loss: 0.000007\n",
      "\tTraining batch 23 Loss: 0.000009\n",
      "\tTraining batch 24 Loss: 0.000033\n",
      "Training set: Average loss: 0.000077\n",
      "Validation set: Average loss: 5.255944, Accuracy: 1375/1959 (70.19%)\n",
      "\n",
      "Epoch: 11\n",
      "\tTraining batch 1 Loss: 0.000002\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000004\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000001\n",
      "\tTraining batch 6 Loss: 0.000011\n",
      "\tTraining batch 7 Loss: 0.000056\n",
      "\tTraining batch 8 Loss: 0.000016\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000029\n",
      "\tTraining batch 11 Loss: 0.000010\n",
      "\tTraining batch 12 Loss: 0.000006\n",
      "\tTraining batch 13 Loss: 0.000434\n",
      "\tTraining batch 14 Loss: 0.000001\n",
      "\tTraining batch 15 Loss: 0.000036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 16 Loss: 0.000003\n",
      "\tTraining batch 17 Loss: 0.000048\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000805\n",
      "\tTraining batch 20 Loss: 0.000004\n",
      "\tTraining batch 21 Loss: 0.000024\n",
      "\tTraining batch 22 Loss: 0.000006\n",
      "\tTraining batch 23 Loss: 0.000009\n",
      "\tTraining batch 24 Loss: 0.000030\n",
      "Training set: Average loss: 0.000064\n",
      "Validation set: Average loss: 5.277289, Accuracy: 1375/1959 (70.19%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 47.0%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000002\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000003\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000001\n",
      "\tTraining batch 6 Loss: 0.000010\n",
      "\tTraining batch 7 Loss: 0.000048\n",
      "\tTraining batch 8 Loss: 0.000014\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000027\n",
      "\tTraining batch 11 Loss: 0.000009\n",
      "\tTraining batch 12 Loss: 0.000006\n",
      "\tTraining batch 13 Loss: 0.000345\n",
      "\tTraining batch 14 Loss: 0.000001\n",
      "\tTraining batch 15 Loss: 0.000032\n",
      "\tTraining batch 16 Loss: 0.000003\n",
      "\tTraining batch 17 Loss: 0.000043\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000701\n",
      "\tTraining batch 20 Loss: 0.000004\n",
      "\tTraining batch 21 Loss: 0.000023\n",
      "\tTraining batch 22 Loss: 0.000006\n",
      "\tTraining batch 23 Loss: 0.000009\n",
      "\tTraining batch 24 Loss: 0.000028\n",
      "\tTraining batch 25 Loss: 8.460011\n",
      "Training set: Average loss: 0.338453\n",
      "Validation set: Average loss: 4.099842, Accuracy: 1354/1959 (69.12%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000023\n",
      "\tTraining batch 2 Loss: 0.091995\n",
      "\tTraining batch 3 Loss: 0.001151\n",
      "\tTraining batch 4 Loss: 0.008232\n",
      "\tTraining batch 5 Loss: 0.006221\n",
      "\tTraining batch 6 Loss: 0.142913\n",
      "\tTraining batch 7 Loss: 0.270810\n",
      "\tTraining batch 8 Loss: 0.108010\n",
      "\tTraining batch 9 Loss: 0.211191\n",
      "\tTraining batch 10 Loss: 0.036171\n",
      "\tTraining batch 11 Loss: 0.024653\n",
      "\tTraining batch 12 Loss: 0.072244\n",
      "\tTraining batch 13 Loss: 0.162097\n",
      "\tTraining batch 14 Loss: 0.070777\n",
      "\tTraining batch 15 Loss: 0.057937\n",
      "\tTraining batch 16 Loss: 0.129030\n",
      "\tTraining batch 17 Loss: 0.101424\n",
      "\tTraining batch 18 Loss: 0.034278\n",
      "\tTraining batch 19 Loss: 0.081524\n",
      "\tTraining batch 20 Loss: 0.142614\n",
      "\tTraining batch 21 Loss: 0.083065\n",
      "\tTraining batch 22 Loss: 0.219357\n",
      "\tTraining batch 23 Loss: 0.001829\n",
      "\tTraining batch 24 Loss: 0.058698\n",
      "\tTraining batch 25 Loss: 0.305912\n",
      "Training set: Average loss: 0.096886\n",
      "Validation set: Average loss: 3.372318, Accuracy: 1309/1959 (66.82%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.076128\n",
      "\tTraining batch 2 Loss: 0.002823\n",
      "\tTraining batch 3 Loss: 0.066569\n",
      "\tTraining batch 4 Loss: 0.090905\n",
      "\tTraining batch 5 Loss: 0.025099\n",
      "\tTraining batch 6 Loss: 0.040256\n",
      "\tTraining batch 7 Loss: 0.057550\n",
      "\tTraining batch 8 Loss: 0.017522\n",
      "\tTraining batch 9 Loss: 0.000040\n",
      "\tTraining batch 10 Loss: 0.006354\n",
      "\tTraining batch 11 Loss: 0.003377\n",
      "\tTraining batch 12 Loss: 0.000451\n",
      "\tTraining batch 13 Loss: 0.053315\n",
      "\tTraining batch 14 Loss: 0.001187\n",
      "\tTraining batch 15 Loss: 0.010986\n",
      "\tTraining batch 16 Loss: 0.005450\n",
      "\tTraining batch 17 Loss: 0.016311\n",
      "\tTraining batch 18 Loss: 0.004614\n",
      "\tTraining batch 19 Loss: 0.012607\n",
      "\tTraining batch 20 Loss: 0.000646\n",
      "\tTraining batch 21 Loss: 0.059622\n",
      "\tTraining batch 22 Loss: 0.074732\n",
      "\tTraining batch 23 Loss: 0.012617\n",
      "\tTraining batch 24 Loss: 0.001864\n",
      "\tTraining batch 25 Loss: 0.000018\n",
      "Training set: Average loss: 0.025642\n",
      "Validation set: Average loss: 3.818900, Accuracy: 1309/1959 (66.82%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 45.5%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.001214\n",
      "\tTraining batch 2 Loss: 0.072158\n",
      "\tTraining batch 3 Loss: 0.024586\n",
      "\tTraining batch 4 Loss: 0.004752\n",
      "\tTraining batch 5 Loss: 0.000608\n",
      "\tTraining batch 6 Loss: 0.000099\n",
      "\tTraining batch 7 Loss: 0.000195\n",
      "\tTraining batch 8 Loss: 0.000008\n",
      "\tTraining batch 9 Loss: 0.000001\n",
      "\tTraining batch 10 Loss: 0.001002\n",
      "\tTraining batch 11 Loss: 0.000498\n",
      "\tTraining batch 12 Loss: 0.010050\n",
      "\tTraining batch 13 Loss: 0.086431\n",
      "\tTraining batch 14 Loss: 0.002768\n",
      "\tTraining batch 15 Loss: 0.000062\n",
      "\tTraining batch 16 Loss: 0.003507\n",
      "\tTraining batch 17 Loss: 0.001465\n",
      "\tTraining batch 18 Loss: 0.000333\n",
      "\tTraining batch 19 Loss: 0.360092\n",
      "\tTraining batch 20 Loss: 0.000101\n",
      "\tTraining batch 21 Loss: 0.000093\n",
      "\tTraining batch 22 Loss: 0.000043\n",
      "\tTraining batch 23 Loss: 0.000079\n",
      "\tTraining batch 24 Loss: 0.000143\n",
      "\tTraining batch 25 Loss: 0.000002\n",
      "\tTraining batch 26 Loss: 4.800640\n",
      "Training set: Average loss: 0.206574\n",
      "Validation set: Average loss: 3.771675, Accuracy: 1335/1959 (68.15%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000061\n",
      "\tTraining batch 2 Loss: 0.001169\n",
      "\tTraining batch 3 Loss: 0.000782\n",
      "\tTraining batch 4 Loss: 0.000241\n",
      "\tTraining batch 5 Loss: 0.023652\n",
      "\tTraining batch 6 Loss: 0.002889\n",
      "\tTraining batch 7 Loss: 0.030072\n",
      "\tTraining batch 8 Loss: 0.032543\n",
      "\tTraining batch 9 Loss: 0.068776\n",
      "\tTraining batch 10 Loss: 0.054244\n",
      "\tTraining batch 11 Loss: 0.039599\n",
      "\tTraining batch 12 Loss: 0.024251\n",
      "\tTraining batch 13 Loss: 0.082588\n",
      "\tTraining batch 14 Loss: 0.052802\n",
      "\tTraining batch 15 Loss: 0.450746\n",
      "\tTraining batch 16 Loss: 0.043645\n",
      "\tTraining batch 17 Loss: 0.054774\n",
      "\tTraining batch 18 Loss: 0.158399\n",
      "\tTraining batch 19 Loss: 0.077619\n",
      "\tTraining batch 20 Loss: 0.026533\n",
      "\tTraining batch 21 Loss: 0.172934\n",
      "\tTraining batch 22 Loss: 0.040832\n",
      "\tTraining batch 23 Loss: 0.030975\n",
      "\tTraining batch 24 Loss: 0.020357\n",
      "\tTraining batch 25 Loss: 0.002552\n",
      "\tTraining batch 26 Loss: 0.594849\n",
      "Training set: Average loss: 0.080303\n",
      "Validation set: Average loss: 1.738068, Accuracy: 1320/1959 (67.38%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.036884\n",
      "\tTraining batch 2 Loss: 0.043993\n",
      "\tTraining batch 3 Loss: 0.073290\n",
      "\tTraining batch 4 Loss: 0.059647\n",
      "\tTraining batch 5 Loss: 0.006944\n",
      "\tTraining batch 6 Loss: 0.011372\n",
      "\tTraining batch 7 Loss: 0.014484\n",
      "\tTraining batch 8 Loss: 0.003996\n",
      "\tTraining batch 9 Loss: 0.008919\n",
      "\tTraining batch 10 Loss: 0.003038\n",
      "\tTraining batch 11 Loss: 0.007869\n",
      "\tTraining batch 12 Loss: 0.001199\n",
      "\tTraining batch 13 Loss: 0.001295\n",
      "\tTraining batch 14 Loss: 0.020048\n",
      "\tTraining batch 15 Loss: 0.026015\n",
      "\tTraining batch 16 Loss: 0.028586\n",
      "\tTraining batch 17 Loss: 0.003437\n",
      "\tTraining batch 18 Loss: 0.000103\n",
      "\tTraining batch 19 Loss: 0.108269\n",
      "\tTraining batch 20 Loss: 0.020601\n",
      "\tTraining batch 21 Loss: 0.000199\n",
      "\tTraining batch 22 Loss: 0.000721\n",
      "\tTraining batch 23 Loss: 0.003015\n",
      "\tTraining batch 24 Loss: 0.020087\n",
      "\tTraining batch 25 Loss: 0.000011\n",
      "\tTraining batch 26 Loss: 0.229597\n",
      "Training set: Average loss: 0.028216\n",
      "Validation set: Average loss: 3.557684, Accuracy: 1314/1959 (67.08%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.034410\n",
      "\tTraining batch 2 Loss: 0.000076\n",
      "\tTraining batch 3 Loss: 0.000137\n",
      "\tTraining batch 4 Loss: 0.000075\n",
      "\tTraining batch 5 Loss: 0.000989\n",
      "\tTraining batch 6 Loss: 0.003094\n",
      "\tTraining batch 7 Loss: 0.001023\n",
      "\tTraining batch 8 Loss: 0.000046\n",
      "\tTraining batch 9 Loss: 0.000001\n",
      "\tTraining batch 10 Loss: 0.000291\n",
      "\tTraining batch 11 Loss: 0.000290\n",
      "\tTraining batch 12 Loss: 0.001487\n",
      "\tTraining batch 13 Loss: 0.001676\n",
      "\tTraining batch 14 Loss: 0.000329\n",
      "\tTraining batch 15 Loss: 0.002429\n",
      "\tTraining batch 16 Loss: 0.000153\n",
      "\tTraining batch 17 Loss: 0.034084\n",
      "\tTraining batch 18 Loss: 0.000490\n",
      "\tTraining batch 19 Loss: 0.016500\n",
      "\tTraining batch 20 Loss: 0.174325\n",
      "\tTraining batch 21 Loss: 0.000658\n",
      "\tTraining batch 22 Loss: 0.000428\n",
      "\tTraining batch 23 Loss: 0.000941\n",
      "\tTraining batch 24 Loss: 0.001221\n",
      "\tTraining batch 25 Loss: 0.000003\n",
      "\tTraining batch 26 Loss: 0.002861\n",
      "Training set: Average loss: 0.010693\n",
      "Validation set: Average loss: 3.228222, Accuracy: 1367/1959 (69.78%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000260\n",
      "\tTraining batch 2 Loss: 0.000067\n",
      "\tTraining batch 3 Loss: 0.001166\n",
      "\tTraining batch 4 Loss: 0.000025\n",
      "\tTraining batch 5 Loss: 0.000324\n",
      "\tTraining batch 6 Loss: 0.000338\n",
      "\tTraining batch 7 Loss: 0.001834\n",
      "\tTraining batch 8 Loss: 0.000085\n",
      "\tTraining batch 9 Loss: 0.000004\n",
      "\tTraining batch 10 Loss: 0.000423\n",
      "\tTraining batch 11 Loss: 0.000089\n",
      "\tTraining batch 12 Loss: 0.002839\n",
      "\tTraining batch 13 Loss: 0.001732\n",
      "\tTraining batch 14 Loss: 0.000030\n",
      "\tTraining batch 15 Loss: 0.000462\n",
      "\tTraining batch 16 Loss: 0.000200\n",
      "\tTraining batch 17 Loss: 0.022984\n",
      "\tTraining batch 18 Loss: 0.000031\n",
      "\tTraining batch 19 Loss: 0.003300\n",
      "\tTraining batch 20 Loss: 0.000395\n",
      "\tTraining batch 21 Loss: 0.000321\n",
      "\tTraining batch 22 Loss: 0.002024\n",
      "\tTraining batch 23 Loss: 0.001516\n",
      "\tTraining batch 24 Loss: 0.000924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 25 Loss: 0.000001\n",
      "\tTraining batch 26 Loss: 0.002029\n",
      "Training set: Average loss: 0.001669\n",
      "Validation set: Average loss: 3.409650, Accuracy: 1325/1959 (67.64%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000351\n",
      "\tTraining batch 2 Loss: 0.000147\n",
      "\tTraining batch 3 Loss: 0.000931\n",
      "\tTraining batch 4 Loss: 0.000268\n",
      "\tTraining batch 5 Loss: 0.000179\n",
      "\tTraining batch 6 Loss: 0.000250\n",
      "\tTraining batch 7 Loss: 0.001799\n",
      "\tTraining batch 8 Loss: 0.000050\n",
      "\tTraining batch 9 Loss: 0.000004\n",
      "\tTraining batch 10 Loss: 0.000110\n",
      "\tTraining batch 11 Loss: 0.000026\n",
      "\tTraining batch 12 Loss: 0.000095\n",
      "\tTraining batch 13 Loss: 0.000714\n",
      "\tTraining batch 14 Loss: 0.000007\n",
      "\tTraining batch 15 Loss: 0.000069\n",
      "\tTraining batch 16 Loss: 0.000093\n",
      "\tTraining batch 17 Loss: 0.000134\n",
      "\tTraining batch 18 Loss: 0.000006\n",
      "\tTraining batch 19 Loss: 0.001478\n",
      "\tTraining batch 20 Loss: 0.000213\n",
      "\tTraining batch 21 Loss: 0.000166\n",
      "\tTraining batch 22 Loss: 0.000155\n",
      "\tTraining batch 23 Loss: 0.000245\n",
      "\tTraining batch 24 Loss: 0.000159\n",
      "\tTraining batch 25 Loss: 0.000001\n",
      "\tTraining batch 26 Loss: 0.000954\n",
      "Training set: Average loss: 0.000331\n",
      "Validation set: Average loss: 3.582211, Accuracy: 1338/1959 (68.30%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000147\n",
      "\tTraining batch 2 Loss: 0.000055\n",
      "\tTraining batch 3 Loss: 0.000140\n",
      "\tTraining batch 4 Loss: 0.000127\n",
      "\tTraining batch 5 Loss: 0.000111\n",
      "\tTraining batch 6 Loss: 0.000089\n",
      "\tTraining batch 7 Loss: 0.001061\n",
      "\tTraining batch 8 Loss: 0.000036\n",
      "\tTraining batch 9 Loss: 0.000003\n",
      "\tTraining batch 10 Loss: 0.000064\n",
      "\tTraining batch 11 Loss: 0.000017\n",
      "\tTraining batch 12 Loss: 0.000046\n",
      "\tTraining batch 13 Loss: 0.000520\n",
      "\tTraining batch 14 Loss: 0.000004\n",
      "\tTraining batch 15 Loss: 0.000048\n",
      "\tTraining batch 16 Loss: 0.000072\n",
      "\tTraining batch 17 Loss: 0.000112\n",
      "\tTraining batch 18 Loss: 0.000004\n",
      "\tTraining batch 19 Loss: 0.001123\n",
      "\tTraining batch 20 Loss: 0.000130\n",
      "\tTraining batch 21 Loss: 0.000141\n",
      "\tTraining batch 22 Loss: 0.000100\n",
      "\tTraining batch 23 Loss: 0.000185\n",
      "\tTraining batch 24 Loss: 0.000111\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000667\n",
      "Training set: Average loss: 0.000197\n",
      "Validation set: Average loss: 3.648492, Accuracy: 1340/1959 (68.40%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000099\n",
      "\tTraining batch 2 Loss: 0.000041\n",
      "\tTraining batch 3 Loss: 0.000104\n",
      "\tTraining batch 4 Loss: 0.000092\n",
      "\tTraining batch 5 Loss: 0.000086\n",
      "\tTraining batch 6 Loss: 0.000066\n",
      "\tTraining batch 7 Loss: 0.000698\n",
      "\tTraining batch 8 Loss: 0.000026\n",
      "\tTraining batch 9 Loss: 0.000002\n",
      "\tTraining batch 10 Loss: 0.000052\n",
      "\tTraining batch 11 Loss: 0.000015\n",
      "\tTraining batch 12 Loss: 0.000035\n",
      "\tTraining batch 13 Loss: 0.000434\n",
      "\tTraining batch 14 Loss: 0.000003\n",
      "\tTraining batch 15 Loss: 0.000040\n",
      "\tTraining batch 16 Loss: 0.000064\n",
      "\tTraining batch 17 Loss: 0.000097\n",
      "\tTraining batch 18 Loss: 0.000004\n",
      "\tTraining batch 19 Loss: 0.000925\n",
      "\tTraining batch 20 Loss: 0.000098\n",
      "\tTraining batch 21 Loss: 0.000127\n",
      "\tTraining batch 22 Loss: 0.000078\n",
      "\tTraining batch 23 Loss: 0.000160\n",
      "\tTraining batch 24 Loss: 0.000089\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000516\n",
      "Training set: Average loss: 0.000152\n",
      "Validation set: Average loss: 3.696504, Accuracy: 1344/1959 (68.61%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000076\n",
      "\tTraining batch 2 Loss: 0.000034\n",
      "\tTraining batch 3 Loss: 0.000086\n",
      "\tTraining batch 4 Loss: 0.000074\n",
      "\tTraining batch 5 Loss: 0.000071\n",
      "\tTraining batch 6 Loss: 0.000053\n",
      "\tTraining batch 7 Loss: 0.000508\n",
      "\tTraining batch 8 Loss: 0.000020\n",
      "\tTraining batch 9 Loss: 0.000002\n",
      "\tTraining batch 10 Loss: 0.000046\n",
      "\tTraining batch 11 Loss: 0.000013\n",
      "\tTraining batch 12 Loss: 0.000028\n",
      "\tTraining batch 13 Loss: 0.000373\n",
      "\tTraining batch 14 Loss: 0.000003\n",
      "\tTraining batch 15 Loss: 0.000035\n",
      "\tTraining batch 16 Loss: 0.000057\n",
      "\tTraining batch 17 Loss: 0.000086\n",
      "\tTraining batch 18 Loss: 0.000003\n",
      "\tTraining batch 19 Loss: 0.000783\n",
      "\tTraining batch 20 Loss: 0.000078\n",
      "\tTraining batch 21 Loss: 0.000116\n",
      "\tTraining batch 22 Loss: 0.000063\n",
      "\tTraining batch 23 Loss: 0.000143\n",
      "\tTraining batch 24 Loss: 0.000075\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000419\n",
      "Training set: Average loss: 0.000125\n",
      "Validation set: Average loss: 3.737321, Accuracy: 1343/1959 (68.56%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000061\n",
      "\tTraining batch 2 Loss: 0.000029\n",
      "\tTraining batch 3 Loss: 0.000073\n",
      "\tTraining batch 4 Loss: 0.000063\n",
      "\tTraining batch 5 Loss: 0.000062\n",
      "\tTraining batch 6 Loss: 0.000045\n",
      "\tTraining batch 7 Loss: 0.000400\n",
      "\tTraining batch 8 Loss: 0.000017\n",
      "\tTraining batch 9 Loss: 0.000001\n",
      "\tTraining batch 10 Loss: 0.000041\n",
      "\tTraining batch 11 Loss: 0.000012\n",
      "\tTraining batch 12 Loss: 0.000023\n",
      "\tTraining batch 13 Loss: 0.000327\n",
      "\tTraining batch 14 Loss: 0.000002\n",
      "\tTraining batch 15 Loss: 0.000031\n",
      "\tTraining batch 16 Loss: 0.000052\n",
      "\tTraining batch 17 Loss: 0.000077\n",
      "\tTraining batch 18 Loss: 0.000003\n",
      "\tTraining batch 19 Loss: 0.000680\n",
      "\tTraining batch 20 Loss: 0.000064\n",
      "\tTraining batch 21 Loss: 0.000107\n",
      "\tTraining batch 22 Loss: 0.000053\n",
      "\tTraining batch 23 Loss: 0.000128\n",
      "\tTraining batch 24 Loss: 0.000064\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000352\n",
      "Training set: Average loss: 0.000106\n",
      "Validation set: Average loss: 3.772097, Accuracy: 1342/1959 (68.50%)\n",
      "\n",
      "Epoch: 11\n",
      "\tTraining batch 1 Loss: 0.000051\n",
      "\tTraining batch 2 Loss: 0.000026\n",
      "\tTraining batch 3 Loss: 0.000063\n",
      "\tTraining batch 4 Loss: 0.000054\n",
      "\tTraining batch 5 Loss: 0.000055\n",
      "\tTraining batch 6 Loss: 0.000039\n",
      "\tTraining batch 7 Loss: 0.000329\n",
      "\tTraining batch 8 Loss: 0.000014\n",
      "\tTraining batch 9 Loss: 0.000001\n",
      "\tTraining batch 10 Loss: 0.000037\n",
      "\tTraining batch 11 Loss: 0.000011\n",
      "\tTraining batch 12 Loss: 0.000019\n",
      "\tTraining batch 13 Loss: 0.000290\n",
      "\tTraining batch 14 Loss: 0.000002\n",
      "\tTraining batch 15 Loss: 0.000028\n",
      "\tTraining batch 16 Loss: 0.000047\n",
      "\tTraining batch 17 Loss: 0.000071\n",
      "\tTraining batch 18 Loss: 0.000003\n",
      "\tTraining batch 19 Loss: 0.000604\n",
      "\tTraining batch 20 Loss: 0.000054\n",
      "\tTraining batch 21 Loss: 0.000099\n",
      "\tTraining batch 22 Loss: 0.000046\n",
      "\tTraining batch 23 Loss: 0.000116\n",
      "\tTraining batch 24 Loss: 0.000056\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000303\n",
      "Training set: Average loss: 0.000093\n",
      "Validation set: Average loss: 3.802922, Accuracy: 1343/1959 (68.56%)\n",
      "\n",
      "Epoch: 12\n",
      "\tTraining batch 1 Loss: 0.000044\n",
      "\tTraining batch 2 Loss: 0.000023\n",
      "\tTraining batch 3 Loss: 0.000055\n",
      "\tTraining batch 4 Loss: 0.000048\n",
      "\tTraining batch 5 Loss: 0.000050\n",
      "\tTraining batch 6 Loss: 0.000034\n",
      "\tTraining batch 7 Loss: 0.000276\n",
      "\tTraining batch 8 Loss: 0.000012\n",
      "\tTraining batch 9 Loss: 0.000001\n",
      "\tTraining batch 10 Loss: 0.000034\n",
      "\tTraining batch 11 Loss: 0.000010\n",
      "\tTraining batch 12 Loss: 0.000017\n",
      "\tTraining batch 13 Loss: 0.000261\n",
      "\tTraining batch 14 Loss: 0.000002\n",
      "\tTraining batch 15 Loss: 0.000026\n",
      "\tTraining batch 16 Loss: 0.000044\n",
      "\tTraining batch 17 Loss: 0.000066\n",
      "\tTraining batch 18 Loss: 0.000003\n",
      "\tTraining batch 19 Loss: 0.000542\n",
      "\tTraining batch 20 Loss: 0.000046\n",
      "\tTraining batch 21 Loss: 0.000092\n",
      "\tTraining batch 22 Loss: 0.000040\n",
      "\tTraining batch 23 Loss: 0.000107\n",
      "\tTraining batch 24 Loss: 0.000049\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000265\n",
      "Training set: Average loss: 0.000083\n",
      "Validation set: Average loss: 3.830938, Accuracy: 1344/1959 (68.61%)\n",
      "\n",
      "Epoch: 13\n",
      "\tTraining batch 1 Loss: 0.000038\n",
      "\tTraining batch 2 Loss: 0.000020\n",
      "\tTraining batch 3 Loss: 0.000048\n",
      "\tTraining batch 4 Loss: 0.000043\n",
      "\tTraining batch 5 Loss: 0.000046\n",
      "\tTraining batch 6 Loss: 0.000031\n",
      "\tTraining batch 7 Loss: 0.000237\n",
      "\tTraining batch 8 Loss: 0.000011\n",
      "\tTraining batch 9 Loss: 0.000001\n",
      "\tTraining batch 10 Loss: 0.000032\n",
      "\tTraining batch 11 Loss: 0.000010\n",
      "\tTraining batch 12 Loss: 0.000015\n",
      "\tTraining batch 13 Loss: 0.000237\n",
      "\tTraining batch 14 Loss: 0.000002\n",
      "\tTraining batch 15 Loss: 0.000024\n",
      "\tTraining batch 16 Loss: 0.000040\n",
      "\tTraining batch 17 Loss: 0.000062\n",
      "\tTraining batch 18 Loss: 0.000002\n",
      "\tTraining batch 19 Loss: 0.000491\n",
      "\tTraining batch 20 Loss: 0.000041\n",
      "\tTraining batch 21 Loss: 0.000086\n",
      "\tTraining batch 22 Loss: 0.000035\n",
      "\tTraining batch 23 Loss: 0.000098\n",
      "\tTraining batch 24 Loss: 0.000044\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000235\n",
      "Training set: Average loss: 0.000074\n",
      "Validation set: Average loss: 3.856186, Accuracy: 1345/1959 (68.66%)\n",
      "\n",
      "Epoch: 14\n",
      "\tTraining batch 1 Loss: 0.000034\n",
      "\tTraining batch 2 Loss: 0.000018\n",
      "\tTraining batch 3 Loss: 0.000043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 4 Loss: 0.000039\n",
      "\tTraining batch 5 Loss: 0.000042\n",
      "\tTraining batch 6 Loss: 0.000028\n",
      "\tTraining batch 7 Loss: 0.000208\n",
      "\tTraining batch 8 Loss: 0.000009\n",
      "\tTraining batch 9 Loss: 0.000001\n",
      "\tTraining batch 10 Loss: 0.000030\n",
      "\tTraining batch 11 Loss: 0.000009\n",
      "\tTraining batch 12 Loss: 0.000013\n",
      "\tTraining batch 13 Loss: 0.000217\n",
      "\tTraining batch 14 Loss: 0.000002\n",
      "\tTraining batch 15 Loss: 0.000022\n",
      "\tTraining batch 16 Loss: 0.000037\n",
      "\tTraining batch 17 Loss: 0.000058\n",
      "\tTraining batch 18 Loss: 0.000002\n",
      "\tTraining batch 19 Loss: 0.000447\n",
      "\tTraining batch 20 Loss: 0.000036\n",
      "\tTraining batch 21 Loss: 0.000081\n",
      "\tTraining batch 22 Loss: 0.000031\n",
      "\tTraining batch 23 Loss: 0.000092\n",
      "\tTraining batch 24 Loss: 0.000040\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000211\n",
      "Training set: Average loss: 0.000067\n",
      "Validation set: Average loss: 3.879548, Accuracy: 1345/1959 (68.66%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 48.0%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000031\n",
      "\tTraining batch 2 Loss: 0.000017\n",
      "\tTraining batch 3 Loss: 0.000039\n",
      "\tTraining batch 4 Loss: 0.000035\n",
      "\tTraining batch 5 Loss: 0.000040\n",
      "\tTraining batch 6 Loss: 0.000025\n",
      "\tTraining batch 7 Loss: 0.000184\n",
      "\tTraining batch 8 Loss: 0.000008\n",
      "\tTraining batch 9 Loss: 0.000001\n",
      "\tTraining batch 10 Loss: 0.000028\n",
      "\tTraining batch 11 Loss: 0.000009\n",
      "\tTraining batch 12 Loss: 0.000012\n",
      "\tTraining batch 13 Loss: 0.000199\n",
      "\tTraining batch 14 Loss: 0.000001\n",
      "\tTraining batch 15 Loss: 0.000021\n",
      "\tTraining batch 16 Loss: 0.000035\n",
      "\tTraining batch 17 Loss: 0.000055\n",
      "\tTraining batch 18 Loss: 0.000002\n",
      "\tTraining batch 19 Loss: 0.000409\n",
      "\tTraining batch 20 Loss: 0.000033\n",
      "\tTraining batch 21 Loss: 0.000077\n",
      "\tTraining batch 22 Loss: 0.000028\n",
      "\tTraining batch 23 Loss: 0.000086\n",
      "\tTraining batch 24 Loss: 0.000037\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000190\n",
      "\tTraining batch 27 Loss: 7.686775\n",
      "Training set: Average loss: 0.284755\n",
      "Validation set: Average loss: 2.473239, Accuracy: 1389/1959 (70.90%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.002882\n",
      "\tTraining batch 2 Loss: 0.116459\n",
      "\tTraining batch 3 Loss: 0.299633\n",
      "\tTraining batch 4 Loss: 0.054757\n",
      "\tTraining batch 5 Loss: 0.284717\n",
      "\tTraining batch 6 Loss: 0.238065\n",
      "\tTraining batch 7 Loss: 0.154196\n",
      "\tTraining batch 8 Loss: 0.385174\n",
      "\tTraining batch 9 Loss: 0.273095\n",
      "\tTraining batch 10 Loss: 0.504891\n",
      "\tTraining batch 11 Loss: 0.216447\n",
      "\tTraining batch 12 Loss: 0.233077\n",
      "\tTraining batch 13 Loss: 0.157923\n",
      "\tTraining batch 14 Loss: 0.116024\n",
      "\tTraining batch 15 Loss: 0.147981\n",
      "\tTraining batch 16 Loss: 0.140489\n",
      "\tTraining batch 17 Loss: 0.168615\n",
      "\tTraining batch 18 Loss: 0.290439\n",
      "\tTraining batch 19 Loss: 0.090065\n",
      "\tTraining batch 20 Loss: 0.030270\n",
      "\tTraining batch 21 Loss: 0.013394\n",
      "\tTraining batch 22 Loss: 0.082995\n",
      "\tTraining batch 23 Loss: 0.089877\n",
      "\tTraining batch 24 Loss: 0.036232\n",
      "\tTraining batch 25 Loss: 0.002166\n",
      "\tTraining batch 26 Loss: 0.018794\n",
      "\tTraining batch 27 Loss: 0.327301\n",
      "Training set: Average loss: 0.165776\n",
      "Validation set: Average loss: 3.226730, Accuracy: 1409/1959 (71.92%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.001018\n",
      "\tTraining batch 2 Loss: 0.075220\n",
      "\tTraining batch 3 Loss: 0.000022\n",
      "\tTraining batch 4 Loss: 0.000413\n",
      "\tTraining batch 5 Loss: 0.039267\n",
      "\tTraining batch 6 Loss: 0.002816\n",
      "\tTraining batch 7 Loss: 0.241320\n",
      "\tTraining batch 8 Loss: 0.000011\n",
      "\tTraining batch 9 Loss: 0.000004\n",
      "\tTraining batch 10 Loss: 0.002180\n",
      "\tTraining batch 11 Loss: 0.001656\n",
      "\tTraining batch 12 Loss: 0.000066\n",
      "\tTraining batch 13 Loss: 0.009527\n",
      "\tTraining batch 14 Loss: 0.004574\n",
      "\tTraining batch 15 Loss: 0.079372\n",
      "\tTraining batch 16 Loss: 0.000177\n",
      "\tTraining batch 17 Loss: 0.000722\n",
      "\tTraining batch 18 Loss: 0.000148\n",
      "\tTraining batch 19 Loss: 0.025369\n",
      "\tTraining batch 20 Loss: 0.003130\n",
      "\tTraining batch 21 Loss: 0.000133\n",
      "\tTraining batch 22 Loss: 0.000972\n",
      "\tTraining batch 23 Loss: 0.000540\n",
      "\tTraining batch 24 Loss: 0.006189\n",
      "\tTraining batch 25 Loss: 0.000006\n",
      "\tTraining batch 26 Loss: 0.025102\n",
      "\tTraining batch 27 Loss: 0.055923\n",
      "Training set: Average loss: 0.021329\n",
      "Validation set: Average loss: 3.956191, Accuracy: 1361/1959 (69.47%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.001166\n",
      "\tTraining batch 2 Loss: 0.000025\n",
      "\tTraining batch 3 Loss: 0.000035\n",
      "\tTraining batch 4 Loss: 0.000011\n",
      "\tTraining batch 5 Loss: 0.023974\n",
      "\tTraining batch 6 Loss: 0.001830\n",
      "\tTraining batch 7 Loss: 0.002278\n",
      "\tTraining batch 8 Loss: 0.000888\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000495\n",
      "\tTraining batch 11 Loss: 0.000005\n",
      "\tTraining batch 12 Loss: 0.116787\n",
      "\tTraining batch 13 Loss: 0.000120\n",
      "\tTraining batch 14 Loss: 0.000009\n",
      "\tTraining batch 15 Loss: 0.000080\n",
      "\tTraining batch 16 Loss: 0.014969\n",
      "\tTraining batch 17 Loss: 0.000023\n",
      "\tTraining batch 18 Loss: 0.000468\n",
      "\tTraining batch 19 Loss: 0.000026\n",
      "\tTraining batch 20 Loss: 0.000374\n",
      "\tTraining batch 21 Loss: 0.003776\n",
      "\tTraining batch 22 Loss: 0.000038\n",
      "\tTraining batch 23 Loss: 0.021102\n",
      "\tTraining batch 24 Loss: 0.000001\n",
      "\tTraining batch 25 Loss: 0.000034\n",
      "\tTraining batch 26 Loss: 0.000475\n",
      "\tTraining batch 27 Loss: 0.009800\n",
      "Training set: Average loss: 0.007363\n",
      "Validation set: Average loss: 4.379710, Accuracy: 1353/1959 (69.07%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000076\n",
      "\tTraining batch 2 Loss: 0.000003\n",
      "\tTraining batch 3 Loss: 0.005775\n",
      "\tTraining batch 4 Loss: 0.000097\n",
      "\tTraining batch 5 Loss: 0.000008\n",
      "\tTraining batch 6 Loss: 0.000016\n",
      "\tTraining batch 7 Loss: 0.003170\n",
      "\tTraining batch 8 Loss: 0.000120\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000032\n",
      "\tTraining batch 11 Loss: 0.000075\n",
      "\tTraining batch 12 Loss: 0.000023\n",
      "\tTraining batch 13 Loss: 0.000175\n",
      "\tTraining batch 14 Loss: 0.003847\n",
      "\tTraining batch 15 Loss: 0.000013\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000021\n",
      "\tTraining batch 18 Loss: 0.000024\n",
      "\tTraining batch 19 Loss: 0.000230\n",
      "\tTraining batch 20 Loss: 0.000012\n",
      "\tTraining batch 21 Loss: 0.000051\n",
      "\tTraining batch 22 Loss: 0.000030\n",
      "\tTraining batch 23 Loss: 0.000053\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000021\n",
      "\tTraining batch 26 Loss: 0.000184\n",
      "\tTraining batch 27 Loss: 0.003002\n",
      "Training set: Average loss: 0.000632\n",
      "Validation set: Average loss: 4.178022, Accuracy: 1346/1959 (68.71%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000072\n",
      "\tTraining batch 2 Loss: 0.000014\n",
      "\tTraining batch 3 Loss: 0.000066\n",
      "\tTraining batch 4 Loss: 0.000006\n",
      "\tTraining batch 5 Loss: 0.000014\n",
      "\tTraining batch 6 Loss: 0.000005\n",
      "\tTraining batch 7 Loss: 0.000160\n",
      "\tTraining batch 8 Loss: 0.000054\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000034\n",
      "\tTraining batch 11 Loss: 0.000007\n",
      "\tTraining batch 12 Loss: 0.000006\n",
      "\tTraining batch 13 Loss: 0.000033\n",
      "\tTraining batch 14 Loss: 0.000005\n",
      "\tTraining batch 15 Loss: 0.000011\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000015\n",
      "\tTraining batch 18 Loss: 0.000017\n",
      "\tTraining batch 19 Loss: 0.000102\n",
      "\tTraining batch 20 Loss: 0.000010\n",
      "\tTraining batch 21 Loss: 0.000047\n",
      "\tTraining batch 22 Loss: 0.000018\n",
      "\tTraining batch 23 Loss: 0.000042\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000019\n",
      "\tTraining batch 26 Loss: 0.000122\n",
      "\tTraining batch 27 Loss: 0.000895\n",
      "Training set: Average loss: 0.000066\n",
      "Validation set: Average loss: 4.209166, Accuracy: 1359/1959 (69.37%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000048\n",
      "\tTraining batch 2 Loss: 0.000017\n",
      "\tTraining batch 3 Loss: 0.000042\n",
      "\tTraining batch 4 Loss: 0.000003\n",
      "\tTraining batch 5 Loss: 0.000011\n",
      "\tTraining batch 6 Loss: 0.000003\n",
      "\tTraining batch 7 Loss: 0.000115\n",
      "\tTraining batch 8 Loss: 0.000047\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000026\n",
      "\tTraining batch 11 Loss: 0.000006\n",
      "\tTraining batch 12 Loss: 0.000005\n",
      "\tTraining batch 13 Loss: 0.000026\n",
      "\tTraining batch 14 Loss: 0.000004\n",
      "\tTraining batch 15 Loss: 0.000010\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000013\n",
      "\tTraining batch 18 Loss: 0.000015\n",
      "\tTraining batch 19 Loss: 0.000083\n",
      "\tTraining batch 20 Loss: 0.000009\n",
      "\tTraining batch 21 Loss: 0.000045\n",
      "\tTraining batch 22 Loss: 0.000015\n",
      "\tTraining batch 23 Loss: 0.000039\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000017\n",
      "\tTraining batch 26 Loss: 0.000103\n",
      "\tTraining batch 27 Loss: 0.000606\n",
      "Training set: Average loss: 0.000049\n",
      "Validation set: Average loss: 4.227685, Accuracy: 1361/1959 (69.47%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000044\n",
      "\tTraining batch 2 Loss: 0.000016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 3 Loss: 0.000038\n",
      "\tTraining batch 4 Loss: 0.000002\n",
      "\tTraining batch 5 Loss: 0.000010\n",
      "\tTraining batch 6 Loss: 0.000003\n",
      "\tTraining batch 7 Loss: 0.000104\n",
      "\tTraining batch 8 Loss: 0.000044\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000023\n",
      "\tTraining batch 11 Loss: 0.000005\n",
      "\tTraining batch 12 Loss: 0.000004\n",
      "\tTraining batch 13 Loss: 0.000024\n",
      "\tTraining batch 14 Loss: 0.000004\n",
      "\tTraining batch 15 Loss: 0.000009\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000012\n",
      "\tTraining batch 18 Loss: 0.000013\n",
      "\tTraining batch 19 Loss: 0.000074\n",
      "\tTraining batch 20 Loss: 0.000008\n",
      "\tTraining batch 21 Loss: 0.000043\n",
      "\tTraining batch 22 Loss: 0.000014\n",
      "\tTraining batch 23 Loss: 0.000037\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000015\n",
      "\tTraining batch 26 Loss: 0.000091\n",
      "\tTraining batch 27 Loss: 0.000461\n",
      "Training set: Average loss: 0.000041\n",
      "Validation set: Average loss: 4.242630, Accuracy: 1361/1959 (69.47%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 48.0%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000041\n",
      "\tTraining batch 2 Loss: 0.000014\n",
      "\tTraining batch 3 Loss: 0.000034\n",
      "\tTraining batch 4 Loss: 0.000002\n",
      "\tTraining batch 5 Loss: 0.000009\n",
      "\tTraining batch 6 Loss: 0.000003\n",
      "\tTraining batch 7 Loss: 0.000095\n",
      "\tTraining batch 8 Loss: 0.000041\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000022\n",
      "\tTraining batch 11 Loss: 0.000005\n",
      "\tTraining batch 12 Loss: 0.000004\n",
      "\tTraining batch 13 Loss: 0.000022\n",
      "\tTraining batch 14 Loss: 0.000004\n",
      "\tTraining batch 15 Loss: 0.000009\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000011\n",
      "\tTraining batch 18 Loss: 0.000012\n",
      "\tTraining batch 19 Loss: 0.000067\n",
      "\tTraining batch 20 Loss: 0.000008\n",
      "\tTraining batch 21 Loss: 0.000041\n",
      "\tTraining batch 22 Loss: 0.000013\n",
      "\tTraining batch 23 Loss: 0.000036\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000014\n",
      "\tTraining batch 26 Loss: 0.000082\n",
      "\tTraining batch 27 Loss: 0.000373\n",
      "\tTraining batch 28 Loss: 6.542745\n",
      "Training set: Average loss: 0.233704\n",
      "Validation set: Average loss: 3.471250, Accuracy: 1382/1959 (70.55%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000158\n",
      "\tTraining batch 2 Loss: 0.002383\n",
      "\tTraining batch 3 Loss: 0.000695\n",
      "\tTraining batch 4 Loss: 0.000072\n",
      "\tTraining batch 5 Loss: 0.001391\n",
      "\tTraining batch 6 Loss: 0.053962\n",
      "\tTraining batch 7 Loss: 0.012275\n",
      "\tTraining batch 8 Loss: 0.242497\n",
      "\tTraining batch 9 Loss: 0.036216\n",
      "\tTraining batch 10 Loss: 0.009060\n",
      "\tTraining batch 11 Loss: 0.078555\n",
      "\tTraining batch 12 Loss: 0.033251\n",
      "\tTraining batch 13 Loss: 0.143142\n",
      "\tTraining batch 14 Loss: 0.051301\n",
      "\tTraining batch 15 Loss: 0.043230\n",
      "\tTraining batch 16 Loss: 0.005554\n",
      "\tTraining batch 17 Loss: 0.094558\n",
      "\tTraining batch 18 Loss: 0.037902\n",
      "\tTraining batch 19 Loss: 0.003418\n",
      "\tTraining batch 20 Loss: 0.004123\n",
      "\tTraining batch 21 Loss: 0.013198\n",
      "\tTraining batch 22 Loss: 0.007012\n",
      "\tTraining batch 23 Loss: 0.028872\n",
      "\tTraining batch 24 Loss: 0.011967\n",
      "\tTraining batch 25 Loss: 0.078666\n",
      "\tTraining batch 26 Loss: 0.123128\n",
      "\tTraining batch 27 Loss: 0.082368\n",
      "\tTraining batch 28 Loss: 0.232871\n",
      "Training set: Average loss: 0.051137\n",
      "Validation set: Average loss: 2.242946, Accuracy: 1388/1959 (70.85%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000604\n",
      "\tTraining batch 2 Loss: 0.001324\n",
      "\tTraining batch 3 Loss: 0.011064\n",
      "\tTraining batch 4 Loss: 0.015128\n",
      "\tTraining batch 5 Loss: 0.000637\n",
      "\tTraining batch 6 Loss: 0.000574\n",
      "\tTraining batch 7 Loss: 0.000378\n",
      "\tTraining batch 8 Loss: 0.000565\n",
      "\tTraining batch 9 Loss: 0.000100\n",
      "\tTraining batch 10 Loss: 0.005777\n",
      "\tTraining batch 11 Loss: 0.004900\n",
      "\tTraining batch 12 Loss: 0.004710\n",
      "\tTraining batch 13 Loss: 0.003052\n",
      "\tTraining batch 14 Loss: 0.014602\n",
      "\tTraining batch 15 Loss: 0.002989\n",
      "\tTraining batch 16 Loss: 0.000745\n",
      "\tTraining batch 17 Loss: 0.002288\n",
      "\tTraining batch 18 Loss: 0.004558\n",
      "\tTraining batch 19 Loss: 0.043333\n",
      "\tTraining batch 20 Loss: 0.000011\n",
      "\tTraining batch 21 Loss: 0.000160\n",
      "\tTraining batch 22 Loss: 0.039020\n",
      "\tTraining batch 23 Loss: 0.008975\n",
      "\tTraining batch 24 Loss: 0.000535\n",
      "\tTraining batch 25 Loss: 0.000004\n",
      "\tTraining batch 26 Loss: 0.000260\n",
      "\tTraining batch 27 Loss: 0.001547\n",
      "\tTraining batch 28 Loss: 0.024538\n",
      "Training set: Average loss: 0.006871\n",
      "Validation set: Average loss: 2.881970, Accuracy: 1386/1959 (70.75%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000211\n",
      "\tTraining batch 2 Loss: 0.000152\n",
      "\tTraining batch 3 Loss: 0.000063\n",
      "\tTraining batch 4 Loss: 0.000918\n",
      "\tTraining batch 5 Loss: 0.000148\n",
      "\tTraining batch 6 Loss: 0.000294\n",
      "\tTraining batch 7 Loss: 0.001147\n",
      "\tTraining batch 8 Loss: 0.000910\n",
      "\tTraining batch 9 Loss: 0.001573\n",
      "\tTraining batch 10 Loss: 0.003893\n",
      "\tTraining batch 11 Loss: 0.000702\n",
      "\tTraining batch 12 Loss: 0.001185\n",
      "\tTraining batch 13 Loss: 0.017174\n",
      "\tTraining batch 14 Loss: 0.131979\n",
      "\tTraining batch 15 Loss: 0.000322\n",
      "\tTraining batch 16 Loss: 0.001458\n",
      "\tTraining batch 17 Loss: 0.000174\n",
      "\tTraining batch 18 Loss: 0.000015\n",
      "\tTraining batch 19 Loss: 0.013701\n",
      "\tTraining batch 20 Loss: 0.000005\n",
      "\tTraining batch 21 Loss: 0.000195\n",
      "\tTraining batch 22 Loss: 0.000820\n",
      "\tTraining batch 23 Loss: 0.022625\n",
      "\tTraining batch 24 Loss: 0.000314\n",
      "\tTraining batch 25 Loss: 0.208673\n",
      "\tTraining batch 26 Loss: 0.002608\n",
      "\tTraining batch 27 Loss: 0.025774\n",
      "\tTraining batch 28 Loss: 0.004399\n",
      "Training set: Average loss: 0.015765\n",
      "Validation set: Average loss: 2.873481, Accuracy: 1394/1959 (71.16%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000151\n",
      "\tTraining batch 2 Loss: 0.000531\n",
      "\tTraining batch 3 Loss: 0.000053\n",
      "\tTraining batch 4 Loss: 0.000975\n",
      "\tTraining batch 5 Loss: 0.000337\n",
      "\tTraining batch 6 Loss: 0.000252\n",
      "\tTraining batch 7 Loss: 0.000431\n",
      "\tTraining batch 8 Loss: 0.000038\n",
      "\tTraining batch 9 Loss: 0.013342\n",
      "\tTraining batch 10 Loss: 0.002837\n",
      "\tTraining batch 11 Loss: 0.000215\n",
      "\tTraining batch 12 Loss: 0.002243\n",
      "\tTraining batch 13 Loss: 0.000865\n",
      "\tTraining batch 14 Loss: 0.001948\n",
      "\tTraining batch 15 Loss: 0.009676\n",
      "\tTraining batch 16 Loss: 0.004681\n",
      "\tTraining batch 17 Loss: 0.000306\n",
      "\tTraining batch 18 Loss: 0.004870\n",
      "\tTraining batch 19 Loss: 0.000559\n",
      "\tTraining batch 20 Loss: 0.000006\n",
      "\tTraining batch 21 Loss: 0.000152\n",
      "\tTraining batch 22 Loss: 0.000018\n",
      "\tTraining batch 23 Loss: 0.000006\n",
      "\tTraining batch 24 Loss: 0.000727\n",
      "\tTraining batch 25 Loss: 0.000013\n",
      "\tTraining batch 26 Loss: 0.000082\n",
      "\tTraining batch 27 Loss: 0.000008\n",
      "\tTraining batch 28 Loss: 0.000219\n",
      "Training set: Average loss: 0.001626\n",
      "Validation set: Average loss: 3.817100, Accuracy: 1386/1959 (70.75%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000022\n",
      "\tTraining batch 2 Loss: 0.000010\n",
      "\tTraining batch 3 Loss: 0.000031\n",
      "\tTraining batch 4 Loss: 0.000230\n",
      "\tTraining batch 5 Loss: 0.000042\n",
      "\tTraining batch 6 Loss: 0.000022\n",
      "\tTraining batch 7 Loss: 0.000090\n",
      "\tTraining batch 8 Loss: 0.000002\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000051\n",
      "\tTraining batch 11 Loss: 0.000029\n",
      "\tTraining batch 12 Loss: 0.000008\n",
      "\tTraining batch 13 Loss: 0.000075\n",
      "\tTraining batch 14 Loss: 0.000046\n",
      "\tTraining batch 15 Loss: 0.000017\n",
      "\tTraining batch 16 Loss: 0.000300\n",
      "\tTraining batch 17 Loss: 0.000081\n",
      "\tTraining batch 18 Loss: 0.000014\n",
      "\tTraining batch 19 Loss: 0.000502\n",
      "\tTraining batch 20 Loss: 0.000001\n",
      "\tTraining batch 21 Loss: 0.000057\n",
      "\tTraining batch 22 Loss: 0.000007\n",
      "\tTraining batch 23 Loss: 0.000007\n",
      "\tTraining batch 24 Loss: 0.000054\n",
      "\tTraining batch 25 Loss: 0.000006\n",
      "\tTraining batch 26 Loss: 0.000021\n",
      "\tTraining batch 27 Loss: 0.000005\n",
      "\tTraining batch 28 Loss: 0.000674\n",
      "Training set: Average loss: 0.000086\n",
      "Validation set: Average loss: 3.936560, Accuracy: 1395/1959 (71.21%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000010\n",
      "\tTraining batch 2 Loss: 0.000004\n",
      "\tTraining batch 3 Loss: 0.000007\n",
      "\tTraining batch 4 Loss: 0.000195\n",
      "\tTraining batch 5 Loss: 0.000016\n",
      "\tTraining batch 6 Loss: 0.000019\n",
      "\tTraining batch 7 Loss: 0.000057\n",
      "\tTraining batch 8 Loss: 0.000001\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000033\n",
      "\tTraining batch 11 Loss: 0.000021\n",
      "\tTraining batch 12 Loss: 0.000005\n",
      "\tTraining batch 13 Loss: 0.000055\n",
      "\tTraining batch 14 Loss: 0.000034\n",
      "\tTraining batch 15 Loss: 0.000014\n",
      "\tTraining batch 16 Loss: 0.000177\n",
      "\tTraining batch 17 Loss: 0.000059\n",
      "\tTraining batch 18 Loss: 0.000011\n",
      "\tTraining batch 19 Loss: 0.000372\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000054\n",
      "\tTraining batch 22 Loss: 0.000006\n",
      "\tTraining batch 23 Loss: 0.000006\n",
      "\tTraining batch 24 Loss: 0.000036\n",
      "\tTraining batch 25 Loss: 0.000005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 26 Loss: 0.000020\n",
      "\tTraining batch 27 Loss: 0.000004\n",
      "\tTraining batch 28 Loss: 0.000020\n",
      "Training set: Average loss: 0.000044\n",
      "Validation set: Average loss: 3.922075, Accuracy: 1395/1959 (71.21%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 47.75%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000010\n",
      "\tTraining batch 2 Loss: 0.000004\n",
      "\tTraining batch 3 Loss: 0.000008\n",
      "\tTraining batch 4 Loss: 0.000095\n",
      "\tTraining batch 5 Loss: 0.000015\n",
      "\tTraining batch 6 Loss: 0.000013\n",
      "\tTraining batch 7 Loss: 0.000057\n",
      "\tTraining batch 8 Loss: 0.000001\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000031\n",
      "\tTraining batch 11 Loss: 0.000019\n",
      "\tTraining batch 12 Loss: 0.000005\n",
      "\tTraining batch 13 Loss: 0.000045\n",
      "\tTraining batch 14 Loss: 0.000031\n",
      "\tTraining batch 15 Loss: 0.000012\n",
      "\tTraining batch 16 Loss: 0.000121\n",
      "\tTraining batch 17 Loss: 0.000052\n",
      "\tTraining batch 18 Loss: 0.000010\n",
      "\tTraining batch 19 Loss: 0.000309\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000052\n",
      "\tTraining batch 22 Loss: 0.000005\n",
      "\tTraining batch 23 Loss: 0.000006\n",
      "\tTraining batch 24 Loss: 0.000031\n",
      "\tTraining batch 25 Loss: 0.000004\n",
      "\tTraining batch 26 Loss: 0.000020\n",
      "\tTraining batch 27 Loss: 0.000003\n",
      "\tTraining batch 28 Loss: 0.000017\n",
      "\tTraining batch 29 Loss: 4.822155\n",
      "Training set: Average loss: 0.166315\n",
      "Validation set: Average loss: 3.509365, Accuracy: 1388/1959 (70.85%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000122\n",
      "\tTraining batch 2 Loss: 0.002668\n",
      "\tTraining batch 3 Loss: 0.120856\n",
      "\tTraining batch 4 Loss: 0.012701\n",
      "\tTraining batch 5 Loss: 0.088931\n",
      "\tTraining batch 6 Loss: 0.059198\n",
      "\tTraining batch 7 Loss: 0.003364\n",
      "\tTraining batch 8 Loss: 0.062446\n",
      "\tTraining batch 9 Loss: 0.005265\n",
      "\tTraining batch 10 Loss: 0.251784\n",
      "\tTraining batch 11 Loss: 0.073137\n",
      "\tTraining batch 12 Loss: 0.097150\n",
      "\tTraining batch 13 Loss: 0.105724\n",
      "\tTraining batch 14 Loss: 0.042421\n",
      "\tTraining batch 15 Loss: 0.048832\n",
      "\tTraining batch 16 Loss: 0.150380\n",
      "\tTraining batch 17 Loss: 0.096908\n",
      "\tTraining batch 18 Loss: 0.011199\n",
      "\tTraining batch 19 Loss: 0.050582\n",
      "\tTraining batch 20 Loss: 0.047515\n",
      "\tTraining batch 21 Loss: 0.202303\n",
      "\tTraining batch 22 Loss: 0.307242\n",
      "\tTraining batch 23 Loss: 0.007511\n",
      "\tTraining batch 24 Loss: 0.067531\n",
      "\tTraining batch 25 Loss: 0.021700\n",
      "\tTraining batch 26 Loss: 0.039322\n",
      "\tTraining batch 27 Loss: 0.001486\n",
      "\tTraining batch 28 Loss: 0.009659\n",
      "\tTraining batch 29 Loss: 0.200158\n",
      "Training set: Average loss: 0.075452\n",
      "Validation set: Average loss: 2.428435, Accuracy: 1366/1959 (69.73%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.027300\n",
      "\tTraining batch 2 Loss: 0.006751\n",
      "\tTraining batch 3 Loss: 0.000318\n",
      "\tTraining batch 4 Loss: 0.003889\n",
      "\tTraining batch 5 Loss: 0.152887\n",
      "\tTraining batch 6 Loss: 0.002126\n",
      "\tTraining batch 7 Loss: 0.000007\n",
      "\tTraining batch 8 Loss: 0.004212\n",
      "\tTraining batch 9 Loss: 0.000003\n",
      "\tTraining batch 10 Loss: 0.005413\n",
      "\tTraining batch 11 Loss: 0.003170\n",
      "\tTraining batch 12 Loss: 0.006273\n",
      "\tTraining batch 13 Loss: 0.043869\n",
      "\tTraining batch 14 Loss: 0.000073\n",
      "\tTraining batch 15 Loss: 0.000844\n",
      "\tTraining batch 16 Loss: 0.000105\n",
      "\tTraining batch 17 Loss: 0.000110\n",
      "\tTraining batch 18 Loss: 0.000295\n",
      "\tTraining batch 19 Loss: 0.005770\n",
      "\tTraining batch 20 Loss: 0.014232\n",
      "\tTraining batch 21 Loss: 0.000806\n",
      "\tTraining batch 22 Loss: 0.003876\n",
      "\tTraining batch 23 Loss: 0.000038\n",
      "\tTraining batch 24 Loss: 0.002416\n",
      "\tTraining batch 25 Loss: 0.000010\n",
      "\tTraining batch 26 Loss: 0.024004\n",
      "\tTraining batch 27 Loss: 0.000090\n",
      "\tTraining batch 28 Loss: 0.000120\n",
      "\tTraining batch 29 Loss: 0.000173\n",
      "Training set: Average loss: 0.010661\n",
      "Validation set: Average loss: 3.593140, Accuracy: 1400/1959 (71.47%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.001247\n",
      "\tTraining batch 2 Loss: 0.000013\n",
      "\tTraining batch 3 Loss: 0.000003\n",
      "\tTraining batch 4 Loss: 0.000308\n",
      "\tTraining batch 5 Loss: 0.000037\n",
      "\tTraining batch 6 Loss: 0.000002\n",
      "\tTraining batch 7 Loss: 0.000039\n",
      "\tTraining batch 8 Loss: 0.000053\n",
      "\tTraining batch 9 Loss: 0.000280\n",
      "\tTraining batch 10 Loss: 0.000934\n",
      "\tTraining batch 11 Loss: 0.000158\n",
      "\tTraining batch 12 Loss: 0.000097\n",
      "\tTraining batch 13 Loss: 0.000699\n",
      "\tTraining batch 14 Loss: 0.000002\n",
      "\tTraining batch 15 Loss: 0.000006\n",
      "\tTraining batch 16 Loss: 0.000119\n",
      "\tTraining batch 17 Loss: 0.000148\n",
      "\tTraining batch 18 Loss: 0.000052\n",
      "\tTraining batch 19 Loss: 0.000177\n",
      "\tTraining batch 20 Loss: 0.000003\n",
      "\tTraining batch 21 Loss: 0.000395\n",
      "\tTraining batch 22 Loss: 0.003695\n",
      "\tTraining batch 23 Loss: 0.000027\n",
      "\tTraining batch 24 Loss: 0.000094\n",
      "\tTraining batch 25 Loss: 0.000037\n",
      "\tTraining batch 26 Loss: 0.001047\n",
      "\tTraining batch 27 Loss: 0.000037\n",
      "\tTraining batch 28 Loss: 0.000131\n",
      "\tTraining batch 29 Loss: 0.000671\n",
      "Training set: Average loss: 0.000363\n",
      "Validation set: Average loss: 3.661849, Accuracy: 1400/1959 (71.47%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 51.0%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000410\n",
      "\tTraining batch 2 Loss: 0.000051\n",
      "\tTraining batch 3 Loss: 0.000002\n",
      "\tTraining batch 4 Loss: 0.000006\n",
      "\tTraining batch 5 Loss: 0.000007\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000013\n",
      "\tTraining batch 8 Loss: 0.000246\n",
      "\tTraining batch 9 Loss: 0.000037\n",
      "\tTraining batch 10 Loss: 0.000037\n",
      "\tTraining batch 11 Loss: 0.000008\n",
      "\tTraining batch 12 Loss: 0.000004\n",
      "\tTraining batch 13 Loss: 0.000016\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000003\n",
      "\tTraining batch 16 Loss: 0.000005\n",
      "\tTraining batch 17 Loss: 0.000034\n",
      "\tTraining batch 18 Loss: 0.000053\n",
      "\tTraining batch 19 Loss: 0.000027\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000084\n",
      "\tTraining batch 22 Loss: 0.000013\n",
      "\tTraining batch 23 Loss: 0.000035\n",
      "\tTraining batch 24 Loss: 0.000026\n",
      "\tTraining batch 25 Loss: 0.000008\n",
      "\tTraining batch 26 Loss: 0.000015\n",
      "\tTraining batch 27 Loss: 0.000036\n",
      "\tTraining batch 28 Loss: 0.000085\n",
      "\tTraining batch 29 Loss: 0.000193\n",
      "\tTraining batch 30 Loss: 6.538435\n",
      "Training set: Average loss: 0.217996\n",
      "Validation set: Average loss: 3.499006, Accuracy: 1412/1959 (72.08%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000071\n",
      "\tTraining batch 2 Loss: 0.000274\n",
      "\tTraining batch 3 Loss: 0.000634\n",
      "\tTraining batch 4 Loss: 0.055786\n",
      "\tTraining batch 5 Loss: 0.114998\n",
      "\tTraining batch 6 Loss: 0.129243\n",
      "\tTraining batch 7 Loss: 0.019536\n",
      "\tTraining batch 8 Loss: 0.138049\n",
      "\tTraining batch 9 Loss: 0.001576\n",
      "\tTraining batch 10 Loss: 0.055511\n",
      "\tTraining batch 11 Loss: 0.116958\n",
      "\tTraining batch 12 Loss: 0.084331\n",
      "\tTraining batch 13 Loss: 0.110814\n",
      "\tTraining batch 14 Loss: 0.076897\n",
      "\tTraining batch 15 Loss: 0.123680\n",
      "\tTraining batch 16 Loss: 0.004564\n",
      "\tTraining batch 17 Loss: 0.037229\n",
      "\tTraining batch 18 Loss: 0.006894\n",
      "\tTraining batch 19 Loss: 0.023386\n",
      "\tTraining batch 20 Loss: 0.002197\n",
      "\tTraining batch 21 Loss: 0.077990\n",
      "\tTraining batch 22 Loss: 0.004206\n",
      "\tTraining batch 23 Loss: 0.030619\n",
      "\tTraining batch 24 Loss: 0.059638\n",
      "\tTraining batch 25 Loss: 0.148249\n",
      "\tTraining batch 26 Loss: 0.008840\n",
      "\tTraining batch 27 Loss: 0.001389\n",
      "\tTraining batch 28 Loss: 0.019167\n",
      "\tTraining batch 29 Loss: 0.172052\n",
      "\tTraining batch 30 Loss: 0.244279\n",
      "Training set: Average loss: 0.062302\n",
      "Validation set: Average loss: 2.572571, Accuracy: 1349/1959 (68.86%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000496\n",
      "\tTraining batch 2 Loss: 0.028321\n",
      "\tTraining batch 3 Loss: 0.000502\n",
      "\tTraining batch 4 Loss: 0.000317\n",
      "\tTraining batch 5 Loss: 0.000219\n",
      "\tTraining batch 6 Loss: 0.000912\n",
      "\tTraining batch 7 Loss: 0.000057\n",
      "\tTraining batch 8 Loss: 0.001476\n",
      "\tTraining batch 9 Loss: 0.000442\n",
      "\tTraining batch 10 Loss: 0.000579\n",
      "\tTraining batch 11 Loss: 0.011145\n",
      "\tTraining batch 12 Loss: 0.000031\n",
      "\tTraining batch 13 Loss: 0.000085\n",
      "\tTraining batch 14 Loss: 0.000303\n",
      "\tTraining batch 15 Loss: 0.001235\n",
      "\tTraining batch 16 Loss: 0.000027\n",
      "\tTraining batch 17 Loss: 0.005702\n",
      "\tTraining batch 18 Loss: 0.000368\n",
      "\tTraining batch 19 Loss: 0.000468\n",
      "\tTraining batch 20 Loss: 0.000184\n",
      "\tTraining batch 21 Loss: 0.000264\n",
      "\tTraining batch 22 Loss: 0.004566\n",
      "\tTraining batch 23 Loss: 0.000558\n",
      "\tTraining batch 24 Loss: 0.000308\n",
      "\tTraining batch 25 Loss: 0.000787\n",
      "\tTraining batch 26 Loss: 0.000243\n",
      "\tTraining batch 27 Loss: 0.064302\n",
      "\tTraining batch 28 Loss: 0.031599\n",
      "\tTraining batch 29 Loss: 0.003351\n",
      "\tTraining batch 30 Loss: 0.025606\n",
      "Training set: Average loss: 0.006148\n",
      "Validation set: Average loss: 3.477074, Accuracy: 1396/1959 (71.26%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000211\n",
      "\tTraining batch 2 Loss: 0.000028\n",
      "\tTraining batch 3 Loss: 0.000043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 4 Loss: 0.000050\n",
      "\tTraining batch 5 Loss: 0.000022\n",
      "\tTraining batch 6 Loss: 0.004549\n",
      "\tTraining batch 7 Loss: 0.000256\n",
      "\tTraining batch 8 Loss: 0.000652\n",
      "\tTraining batch 9 Loss: 0.046135\n",
      "\tTraining batch 10 Loss: 0.000281\n",
      "\tTraining batch 11 Loss: 0.001244\n",
      "\tTraining batch 12 Loss: 0.000016\n",
      "\tTraining batch 13 Loss: 0.000010\n",
      "\tTraining batch 14 Loss: 0.000031\n",
      "\tTraining batch 15 Loss: 0.000151\n",
      "\tTraining batch 16 Loss: 0.000010\n",
      "\tTraining batch 17 Loss: 0.000612\n",
      "\tTraining batch 18 Loss: 0.000217\n",
      "\tTraining batch 19 Loss: 0.000340\n",
      "\tTraining batch 20 Loss: 0.000002\n",
      "\tTraining batch 21 Loss: 0.000134\n",
      "\tTraining batch 22 Loss: 0.000014\n",
      "\tTraining batch 23 Loss: 0.000221\n",
      "\tTraining batch 24 Loss: 0.000115\n",
      "\tTraining batch 25 Loss: 0.000008\n",
      "\tTraining batch 26 Loss: 0.000014\n",
      "\tTraining batch 27 Loss: 0.000025\n",
      "\tTraining batch 28 Loss: 0.000102\n",
      "\tTraining batch 29 Loss: 0.000679\n",
      "\tTraining batch 30 Loss: 0.001846\n",
      "Training set: Average loss: 0.001934\n",
      "Validation set: Average loss: 3.752125, Accuracy: 1379/1959 (70.39%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000058\n",
      "\tTraining batch 2 Loss: 0.000174\n",
      "\tTraining batch 3 Loss: 0.000036\n",
      "\tTraining batch 4 Loss: 0.000892\n",
      "\tTraining batch 5 Loss: 0.000016\n",
      "\tTraining batch 6 Loss: 0.000002\n",
      "\tTraining batch 7 Loss: 0.000084\n",
      "\tTraining batch 8 Loss: 0.000144\n",
      "\tTraining batch 9 Loss: 0.000036\n",
      "\tTraining batch 10 Loss: 0.000133\n",
      "\tTraining batch 11 Loss: 0.000098\n",
      "\tTraining batch 12 Loss: 0.000004\n",
      "\tTraining batch 13 Loss: 0.000009\n",
      "\tTraining batch 14 Loss: 0.000174\n",
      "\tTraining batch 15 Loss: 0.000133\n",
      "\tTraining batch 16 Loss: 0.000007\n",
      "\tTraining batch 17 Loss: 0.002543\n",
      "\tTraining batch 18 Loss: 0.000100\n",
      "\tTraining batch 19 Loss: 0.000261\n",
      "\tTraining batch 20 Loss: 0.000002\n",
      "\tTraining batch 21 Loss: 0.000089\n",
      "\tTraining batch 22 Loss: 0.000009\n",
      "\tTraining batch 23 Loss: 0.000083\n",
      "\tTraining batch 24 Loss: 0.000069\n",
      "\tTraining batch 25 Loss: 0.000009\n",
      "\tTraining batch 26 Loss: 0.000012\n",
      "\tTraining batch 27 Loss: 0.000020\n",
      "\tTraining batch 28 Loss: 0.000070\n",
      "\tTraining batch 29 Loss: 0.000161\n",
      "\tTraining batch 30 Loss: 0.001267\n",
      "Training set: Average loss: 0.000223\n",
      "Validation set: Average loss: 3.800104, Accuracy: 1390/1959 (70.95%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000016\n",
      "\tTraining batch 2 Loss: 0.000034\n",
      "\tTraining batch 3 Loss: 0.000014\n",
      "\tTraining batch 4 Loss: 0.000444\n",
      "\tTraining batch 5 Loss: 0.000007\n",
      "\tTraining batch 6 Loss: 0.000002\n",
      "\tTraining batch 7 Loss: 0.000056\n",
      "\tTraining batch 8 Loss: 0.000106\n",
      "\tTraining batch 9 Loss: 0.000018\n",
      "\tTraining batch 10 Loss: 0.000078\n",
      "\tTraining batch 11 Loss: 0.000073\n",
      "\tTraining batch 12 Loss: 0.000002\n",
      "\tTraining batch 13 Loss: 0.000010\n",
      "\tTraining batch 14 Loss: 0.000062\n",
      "\tTraining batch 15 Loss: 0.000073\n",
      "\tTraining batch 16 Loss: 0.000002\n",
      "\tTraining batch 17 Loss: 0.000267\n",
      "\tTraining batch 18 Loss: 0.000044\n",
      "\tTraining batch 19 Loss: 0.000204\n",
      "\tTraining batch 20 Loss: 0.000001\n",
      "\tTraining batch 21 Loss: 0.000068\n",
      "\tTraining batch 22 Loss: 0.000009\n",
      "\tTraining batch 23 Loss: 0.000057\n",
      "\tTraining batch 24 Loss: 0.000054\n",
      "\tTraining batch 25 Loss: 0.000009\n",
      "\tTraining batch 26 Loss: 0.000011\n",
      "\tTraining batch 27 Loss: 0.000017\n",
      "\tTraining batch 28 Loss: 0.000052\n",
      "\tTraining batch 29 Loss: 0.000098\n",
      "\tTraining batch 30 Loss: 0.001013\n",
      "Training set: Average loss: 0.000097\n",
      "Validation set: Average loss: 3.816204, Accuracy: 1390/1959 (70.95%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 48.25%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000011\n",
      "\tTraining batch 2 Loss: 0.000020\n",
      "\tTraining batch 3 Loss: 0.000010\n",
      "\tTraining batch 4 Loss: 0.000234\n",
      "\tTraining batch 5 Loss: 0.000006\n",
      "\tTraining batch 6 Loss: 0.000002\n",
      "\tTraining batch 7 Loss: 0.000049\n",
      "\tTraining batch 8 Loss: 0.000085\n",
      "\tTraining batch 9 Loss: 0.000014\n",
      "\tTraining batch 10 Loss: 0.000065\n",
      "\tTraining batch 11 Loss: 0.000059\n",
      "\tTraining batch 12 Loss: 0.000002\n",
      "\tTraining batch 13 Loss: 0.000009\n",
      "\tTraining batch 14 Loss: 0.000052\n",
      "\tTraining batch 15 Loss: 0.000060\n",
      "\tTraining batch 16 Loss: 0.000002\n",
      "\tTraining batch 17 Loss: 0.000210\n",
      "\tTraining batch 18 Loss: 0.000037\n",
      "\tTraining batch 19 Loss: 0.000179\n",
      "\tTraining batch 20 Loss: 0.000001\n",
      "\tTraining batch 21 Loss: 0.000060\n",
      "\tTraining batch 22 Loss: 0.000009\n",
      "\tTraining batch 23 Loss: 0.000047\n",
      "\tTraining batch 24 Loss: 0.000047\n",
      "\tTraining batch 25 Loss: 0.000008\n",
      "\tTraining batch 26 Loss: 0.000010\n",
      "\tTraining batch 27 Loss: 0.000016\n",
      "\tTraining batch 28 Loss: 0.000046\n",
      "\tTraining batch 29 Loss: 0.000081\n",
      "\tTraining batch 30 Loss: 0.000831\n",
      "\tTraining batch 31 Loss: 4.870914\n",
      "Training set: Average loss: 0.157199\n",
      "Validation set: Average loss: 3.506639, Accuracy: 1413/1959 (72.13%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000014\n",
      "\tTraining batch 2 Loss: 0.000015\n",
      "\tTraining batch 3 Loss: 0.000103\n",
      "\tTraining batch 4 Loss: 0.002799\n",
      "\tTraining batch 5 Loss: 0.014658\n",
      "\tTraining batch 6 Loss: 0.001035\n",
      "\tTraining batch 7 Loss: 0.001934\n",
      "\tTraining batch 8 Loss: 0.003209\n",
      "\tTraining batch 9 Loss: 0.005732\n",
      "\tTraining batch 10 Loss: 0.021141\n",
      "\tTraining batch 11 Loss: 0.005976\n",
      "\tTraining batch 12 Loss: 0.001630\n",
      "\tTraining batch 13 Loss: 0.083704\n",
      "\tTraining batch 14 Loss: 0.007193\n",
      "\tTraining batch 15 Loss: 0.138944\n",
      "\tTraining batch 16 Loss: 0.001298\n",
      "\tTraining batch 17 Loss: 0.008675\n",
      "\tTraining batch 18 Loss: 0.001220\n",
      "\tTraining batch 19 Loss: 0.044411\n",
      "\tTraining batch 20 Loss: 0.000952\n",
      "\tTraining batch 21 Loss: 0.009312\n",
      "\tTraining batch 22 Loss: 0.007576\n",
      "\tTraining batch 23 Loss: 0.007670\n",
      "\tTraining batch 24 Loss: 0.000815\n",
      "\tTraining batch 25 Loss: 0.253469\n",
      "\tTraining batch 26 Loss: 0.050172\n",
      "\tTraining batch 27 Loss: 0.002967\n",
      "\tTraining batch 28 Loss: 0.015766\n",
      "\tTraining batch 29 Loss: 0.058325\n",
      "\tTraining batch 30 Loss: 0.024633\n",
      "\tTraining batch 31 Loss: 0.046721\n",
      "Training set: Average loss: 0.026518\n",
      "Validation set: Average loss: 2.318628, Accuracy: 1436/1959 (73.30%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.058893\n",
      "\tTraining batch 2 Loss: 0.000516\n",
      "\tTraining batch 3 Loss: 0.000133\n",
      "\tTraining batch 4 Loss: 0.002561\n",
      "\tTraining batch 5 Loss: 0.000349\n",
      "\tTraining batch 6 Loss: 0.001216\n",
      "\tTraining batch 7 Loss: 0.000549\n",
      "\tTraining batch 8 Loss: 0.000826\n",
      "\tTraining batch 9 Loss: 0.000918\n",
      "\tTraining batch 10 Loss: 0.000047\n",
      "\tTraining batch 11 Loss: 0.000024\n",
      "\tTraining batch 12 Loss: 0.000447\n",
      "\tTraining batch 13 Loss: 0.007766\n",
      "\tTraining batch 14 Loss: 0.001726\n",
      "\tTraining batch 15 Loss: 0.040885\n",
      "\tTraining batch 16 Loss: 0.001184\n",
      "\tTraining batch 17 Loss: 0.001694\n",
      "\tTraining batch 18 Loss: 0.002487\n",
      "\tTraining batch 19 Loss: 0.117383\n",
      "\tTraining batch 20 Loss: 0.001167\n",
      "\tTraining batch 21 Loss: 0.000656\n",
      "\tTraining batch 22 Loss: 0.000129\n",
      "\tTraining batch 23 Loss: 0.003605\n",
      "\tTraining batch 24 Loss: 0.013524\n",
      "\tTraining batch 25 Loss: 0.000227\n",
      "\tTraining batch 26 Loss: 0.002560\n",
      "\tTraining batch 27 Loss: 0.074250\n",
      "\tTraining batch 28 Loss: 0.028262\n",
      "\tTraining batch 29 Loss: 0.000237\n",
      "\tTraining batch 30 Loss: 0.075537\n",
      "\tTraining batch 31 Loss: 0.002674\n",
      "Training set: Average loss: 0.014272\n",
      "Validation set: Average loss: 3.181042, Accuracy: 1417/1959 (72.33%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.003406\n",
      "\tTraining batch 2 Loss: 0.000005\n",
      "\tTraining batch 3 Loss: 0.000076\n",
      "\tTraining batch 4 Loss: 0.000014\n",
      "\tTraining batch 5 Loss: 0.000013\n",
      "\tTraining batch 6 Loss: 0.000007\n",
      "\tTraining batch 7 Loss: 0.003301\n",
      "\tTraining batch 8 Loss: 0.000045\n",
      "\tTraining batch 9 Loss: 0.000037\n",
      "\tTraining batch 10 Loss: 0.000001\n",
      "\tTraining batch 11 Loss: 0.000117\n",
      "\tTraining batch 12 Loss: 0.000006\n",
      "\tTraining batch 13 Loss: 0.000011\n",
      "\tTraining batch 14 Loss: 0.000006\n",
      "\tTraining batch 15 Loss: 0.001219\n",
      "\tTraining batch 16 Loss: 0.000008\n",
      "\tTraining batch 17 Loss: 0.000200\n",
      "\tTraining batch 18 Loss: 0.000001\n",
      "\tTraining batch 19 Loss: 0.000402\n",
      "\tTraining batch 20 Loss: 0.000385\n",
      "\tTraining batch 21 Loss: 0.000149\n",
      "\tTraining batch 22 Loss: 0.000021\n",
      "\tTraining batch 23 Loss: 0.001181\n",
      "\tTraining batch 24 Loss: 0.000063\n",
      "\tTraining batch 25 Loss: 0.000016\n",
      "\tTraining batch 26 Loss: 0.002581\n",
      "\tTraining batch 27 Loss: 0.000380\n",
      "\tTraining batch 28 Loss: 0.001168\n",
      "\tTraining batch 29 Loss: 0.013089\n",
      "\tTraining batch 30 Loss: 0.014778\n",
      "\tTraining batch 31 Loss: 0.020349\n",
      "Training set: Average loss: 0.002033\n",
      "Validation set: Average loss: 3.849968, Accuracy: 1432/1959 (73.10%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000004\n",
      "\tTraining batch 2 Loss: 0.000177\n",
      "\tTraining batch 3 Loss: 0.000073\n",
      "\tTraining batch 4 Loss: 0.000008\n",
      "\tTraining batch 5 Loss: 0.000015\n",
      "\tTraining batch 6 Loss: 0.000058\n",
      "\tTraining batch 7 Loss: 0.000409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 8 Loss: 0.002261\n",
      "\tTraining batch 9 Loss: 0.000006\n",
      "\tTraining batch 10 Loss: 0.113707\n",
      "\tTraining batch 11 Loss: 0.000008\n",
      "\tTraining batch 12 Loss: 0.000011\n",
      "\tTraining batch 13 Loss: 0.000075\n",
      "\tTraining batch 14 Loss: 0.002544\n",
      "\tTraining batch 15 Loss: 0.000080\n",
      "\tTraining batch 16 Loss: 0.000008\n",
      "\tTraining batch 17 Loss: 0.002637\n",
      "\tTraining batch 18 Loss: 0.000705\n",
      "\tTraining batch 19 Loss: 0.000021\n",
      "\tTraining batch 20 Loss: 0.000001\n",
      "\tTraining batch 21 Loss: 0.000016\n",
      "\tTraining batch 22 Loss: 0.000010\n",
      "\tTraining batch 23 Loss: 0.000079\n",
      "\tTraining batch 24 Loss: 0.000003\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000022\n",
      "\tTraining batch 27 Loss: 0.000012\n",
      "\tTraining batch 28 Loss: 0.000004\n",
      "\tTraining batch 29 Loss: 0.000042\n",
      "\tTraining batch 30 Loss: 0.003399\n",
      "\tTraining batch 31 Loss: 0.000037\n",
      "Training set: Average loss: 0.004078\n",
      "Validation set: Average loss: 3.935279, Accuracy: 1425/1959 (72.74%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000079\n",
      "\tTraining batch 2 Loss: 0.000004\n",
      "\tTraining batch 3 Loss: 0.000087\n",
      "\tTraining batch 4 Loss: 0.000018\n",
      "\tTraining batch 5 Loss: 0.000066\n",
      "\tTraining batch 6 Loss: 0.000003\n",
      "\tTraining batch 7 Loss: 0.001860\n",
      "\tTraining batch 8 Loss: 0.000003\n",
      "\tTraining batch 9 Loss: 0.000003\n",
      "\tTraining batch 10 Loss: 0.000011\n",
      "\tTraining batch 11 Loss: 0.000017\n",
      "\tTraining batch 12 Loss: 0.000001\n",
      "\tTraining batch 13 Loss: 0.000071\n",
      "\tTraining batch 14 Loss: 0.000010\n",
      "\tTraining batch 15 Loss: 0.000007\n",
      "\tTraining batch 16 Loss: 0.000001\n",
      "\tTraining batch 17 Loss: 0.000086\n",
      "\tTraining batch 18 Loss: 0.000032\n",
      "\tTraining batch 19 Loss: 0.000120\n",
      "\tTraining batch 20 Loss: 0.000001\n",
      "\tTraining batch 21 Loss: 0.000016\n",
      "\tTraining batch 22 Loss: 0.000004\n",
      "\tTraining batch 23 Loss: 0.000360\n",
      "\tTraining batch 24 Loss: 0.000005\n",
      "\tTraining batch 25 Loss: 0.000002\n",
      "\tTraining batch 26 Loss: 0.000019\n",
      "\tTraining batch 27 Loss: 0.000020\n",
      "\tTraining batch 28 Loss: 0.000008\n",
      "\tTraining batch 29 Loss: 0.000031\n",
      "\tTraining batch 30 Loss: 0.002603\n",
      "\tTraining batch 31 Loss: 0.000025\n",
      "Training set: Average loss: 0.000180\n",
      "Validation set: Average loss: 4.004517, Accuracy: 1415/1959 (72.23%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000101\n",
      "\tTraining batch 2 Loss: 0.000004\n",
      "\tTraining batch 3 Loss: 0.000064\n",
      "\tTraining batch 4 Loss: 0.000019\n",
      "\tTraining batch 5 Loss: 0.000051\n",
      "\tTraining batch 6 Loss: 0.000003\n",
      "\tTraining batch 7 Loss: 0.000921\n",
      "\tTraining batch 8 Loss: 0.000002\n",
      "\tTraining batch 9 Loss: 0.000002\n",
      "\tTraining batch 10 Loss: 0.000009\n",
      "\tTraining batch 11 Loss: 0.000017\n",
      "\tTraining batch 12 Loss: 0.000001\n",
      "\tTraining batch 13 Loss: 0.000038\n",
      "\tTraining batch 14 Loss: 0.000008\n",
      "\tTraining batch 15 Loss: 0.000006\n",
      "\tTraining batch 16 Loss: 0.000001\n",
      "\tTraining batch 17 Loss: 0.000080\n",
      "\tTraining batch 18 Loss: 0.000029\n",
      "\tTraining batch 19 Loss: 0.000117\n",
      "\tTraining batch 20 Loss: 0.000001\n",
      "\tTraining batch 21 Loss: 0.000014\n",
      "\tTraining batch 22 Loss: 0.000003\n",
      "\tTraining batch 23 Loss: 0.000170\n",
      "\tTraining batch 24 Loss: 0.000005\n",
      "\tTraining batch 25 Loss: 0.000002\n",
      "\tTraining batch 26 Loss: 0.000016\n",
      "\tTraining batch 27 Loss: 0.000018\n",
      "\tTraining batch 28 Loss: 0.000007\n",
      "\tTraining batch 29 Loss: 0.000028\n",
      "\tTraining batch 30 Loss: 0.002010\n",
      "\tTraining batch 31 Loss: 0.000025\n",
      "Training set: Average loss: 0.000122\n",
      "Validation set: Average loss: 4.030360, Accuracy: 1416/1959 (72.28%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000061\n",
      "\tTraining batch 2 Loss: 0.000003\n",
      "\tTraining batch 3 Loss: 0.000051\n",
      "\tTraining batch 4 Loss: 0.000017\n",
      "\tTraining batch 5 Loss: 0.000042\n",
      "\tTraining batch 6 Loss: 0.000002\n",
      "\tTraining batch 7 Loss: 0.000605\n",
      "\tTraining batch 8 Loss: 0.000002\n",
      "\tTraining batch 9 Loss: 0.000002\n",
      "\tTraining batch 10 Loss: 0.000007\n",
      "\tTraining batch 11 Loss: 0.000015\n",
      "\tTraining batch 12 Loss: 0.000001\n",
      "\tTraining batch 13 Loss: 0.000024\n",
      "\tTraining batch 14 Loss: 0.000007\n",
      "\tTraining batch 15 Loss: 0.000006\n",
      "\tTraining batch 16 Loss: 0.000001\n",
      "\tTraining batch 17 Loss: 0.000078\n",
      "\tTraining batch 18 Loss: 0.000027\n",
      "\tTraining batch 19 Loss: 0.000108\n",
      "\tTraining batch 20 Loss: 0.000001\n",
      "\tTraining batch 21 Loss: 0.000012\n",
      "\tTraining batch 22 Loss: 0.000003\n",
      "\tTraining batch 23 Loss: 0.000108\n",
      "\tTraining batch 24 Loss: 0.000005\n",
      "\tTraining batch 25 Loss: 0.000001\n",
      "\tTraining batch 26 Loss: 0.000015\n",
      "\tTraining batch 27 Loss: 0.000016\n",
      "\tTraining batch 28 Loss: 0.000006\n",
      "\tTraining batch 29 Loss: 0.000024\n",
      "\tTraining batch 30 Loss: 0.001609\n",
      "\tTraining batch 31 Loss: 0.000025\n",
      "Training set: Average loss: 0.000093\n",
      "Validation set: Average loss: 4.051757, Accuracy: 1415/1959 (72.23%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000043\n",
      "\tTraining batch 2 Loss: 0.000003\n",
      "\tTraining batch 3 Loss: 0.000042\n",
      "\tTraining batch 4 Loss: 0.000015\n",
      "\tTraining batch 5 Loss: 0.000036\n",
      "\tTraining batch 6 Loss: 0.000002\n",
      "\tTraining batch 7 Loss: 0.000448\n",
      "\tTraining batch 8 Loss: 0.000001\n",
      "\tTraining batch 9 Loss: 0.000002\n",
      "\tTraining batch 10 Loss: 0.000006\n",
      "\tTraining batch 11 Loss: 0.000014\n",
      "\tTraining batch 12 Loss: 0.000001\n",
      "\tTraining batch 13 Loss: 0.000017\n",
      "\tTraining batch 14 Loss: 0.000007\n",
      "\tTraining batch 15 Loss: 0.000006\n",
      "\tTraining batch 16 Loss: 0.000001\n",
      "\tTraining batch 17 Loss: 0.000075\n",
      "\tTraining batch 18 Loss: 0.000026\n",
      "\tTraining batch 19 Loss: 0.000101\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000011\n",
      "\tTraining batch 22 Loss: 0.000002\n",
      "\tTraining batch 23 Loss: 0.000081\n",
      "\tTraining batch 24 Loss: 0.000005\n",
      "\tTraining batch 25 Loss: 0.000001\n",
      "\tTraining batch 26 Loss: 0.000014\n",
      "\tTraining batch 27 Loss: 0.000015\n",
      "\tTraining batch 28 Loss: 0.000006\n",
      "\tTraining batch 29 Loss: 0.000022\n",
      "\tTraining batch 30 Loss: 0.001343\n",
      "\tTraining batch 31 Loss: 0.000024\n",
      "Training set: Average loss: 0.000076\n",
      "Validation set: Average loss: 4.070273, Accuracy: 1416/1959 (72.28%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000033\n",
      "\tTraining batch 2 Loss: 0.000003\n",
      "\tTraining batch 3 Loss: 0.000036\n",
      "\tTraining batch 4 Loss: 0.000013\n",
      "\tTraining batch 5 Loss: 0.000032\n",
      "\tTraining batch 6 Loss: 0.000002\n",
      "\tTraining batch 7 Loss: 0.000357\n",
      "\tTraining batch 8 Loss: 0.000001\n",
      "\tTraining batch 9 Loss: 0.000002\n",
      "\tTraining batch 10 Loss: 0.000005\n",
      "\tTraining batch 11 Loss: 0.000013\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000014\n",
      "\tTraining batch 14 Loss: 0.000006\n",
      "\tTraining batch 15 Loss: 0.000006\n",
      "\tTraining batch 16 Loss: 0.000001\n",
      "\tTraining batch 17 Loss: 0.000072\n",
      "\tTraining batch 18 Loss: 0.000025\n",
      "\tTraining batch 19 Loss: 0.000094\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000011\n",
      "\tTraining batch 22 Loss: 0.000002\n",
      "\tTraining batch 23 Loss: 0.000066\n",
      "\tTraining batch 24 Loss: 0.000004\n",
      "\tTraining batch 25 Loss: 0.000001\n",
      "\tTraining batch 26 Loss: 0.000013\n",
      "\tTraining batch 27 Loss: 0.000014\n",
      "\tTraining batch 28 Loss: 0.000005\n",
      "\tTraining batch 29 Loss: 0.000020\n",
      "\tTraining batch 30 Loss: 0.001147\n",
      "\tTraining batch 31 Loss: 0.000024\n",
      "Training set: Average loss: 0.000065\n",
      "Validation set: Average loss: 4.087435, Accuracy: 1416/1959 (72.28%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 49.25%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000027\n",
      "\tTraining batch 2 Loss: 0.000003\n",
      "\tTraining batch 3 Loss: 0.000031\n",
      "\tTraining batch 4 Loss: 0.000012\n",
      "\tTraining batch 5 Loss: 0.000028\n",
      "\tTraining batch 6 Loss: 0.000002\n",
      "\tTraining batch 7 Loss: 0.000295\n",
      "\tTraining batch 8 Loss: 0.000001\n",
      "\tTraining batch 9 Loss: 0.000002\n",
      "\tTraining batch 10 Loss: 0.000005\n",
      "\tTraining batch 11 Loss: 0.000012\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000011\n",
      "\tTraining batch 14 Loss: 0.000006\n",
      "\tTraining batch 15 Loss: 0.000006\n",
      "\tTraining batch 16 Loss: 0.000001\n",
      "\tTraining batch 17 Loss: 0.000069\n",
      "\tTraining batch 18 Loss: 0.000024\n",
      "\tTraining batch 19 Loss: 0.000088\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000010\n",
      "\tTraining batch 22 Loss: 0.000002\n",
      "\tTraining batch 23 Loss: 0.000056\n",
      "\tTraining batch 24 Loss: 0.000004\n",
      "\tTraining batch 25 Loss: 0.000001\n",
      "\tTraining batch 26 Loss: 0.000013\n",
      "\tTraining batch 27 Loss: 0.000013\n",
      "\tTraining batch 28 Loss: 0.000005\n",
      "\tTraining batch 29 Loss: 0.000018\n",
      "\tTraining batch 30 Loss: 0.001001\n",
      "\tTraining batch 31 Loss: 0.000023\n",
      "\tTraining batch 32 Loss: 4.708232\n",
      "Training set: Average loss: 0.147188\n",
      "Validation set: Average loss: 3.593501, Accuracy: 1378/1959 (70.34%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000049\n",
      "\tTraining batch 2 Loss: 0.000022\n",
      "\tTraining batch 3 Loss: 0.023211\n",
      "\tTraining batch 4 Loss: 0.000809\n",
      "\tTraining batch 5 Loss: 0.080464\n",
      "\tTraining batch 6 Loss: 0.420480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 7 Loss: 0.809265\n",
      "\tTraining batch 8 Loss: 0.008246\n",
      "\tTraining batch 9 Loss: 0.041175\n",
      "\tTraining batch 10 Loss: 0.006258\n",
      "\tTraining batch 11 Loss: 0.189882\n",
      "\tTraining batch 12 Loss: 0.022803\n",
      "\tTraining batch 13 Loss: 0.053108\n",
      "\tTraining batch 14 Loss: 0.053055\n",
      "\tTraining batch 15 Loss: 0.023808\n",
      "\tTraining batch 16 Loss: 0.023969\n",
      "\tTraining batch 17 Loss: 0.049152\n",
      "\tTraining batch 18 Loss: 0.044465\n",
      "\tTraining batch 19 Loss: 0.038610\n",
      "\tTraining batch 20 Loss: 0.026262\n",
      "\tTraining batch 21 Loss: 0.170137\n",
      "\tTraining batch 22 Loss: 0.081748\n",
      "\tTraining batch 23 Loss: 0.096274\n",
      "\tTraining batch 24 Loss: 0.074096\n",
      "\tTraining batch 25 Loss: 0.010829\n",
      "\tTraining batch 26 Loss: 0.041789\n",
      "\tTraining batch 27 Loss: 0.066406\n",
      "\tTraining batch 28 Loss: 0.176911\n",
      "\tTraining batch 29 Loss: 0.156743\n",
      "\tTraining batch 30 Loss: 0.103201\n",
      "\tTraining batch 31 Loss: 0.122661\n",
      "\tTraining batch 32 Loss: 0.290605\n",
      "Training set: Average loss: 0.103328\n",
      "Validation set: Average loss: 3.282412, Accuracy: 1370/1959 (69.93%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000242\n",
      "\tTraining batch 2 Loss: 0.003827\n",
      "\tTraining batch 3 Loss: 0.000699\n",
      "\tTraining batch 4 Loss: 0.017295\n",
      "\tTraining batch 5 Loss: 0.000129\n",
      "\tTraining batch 6 Loss: 0.009996\n",
      "\tTraining batch 7 Loss: 0.000163\n",
      "\tTraining batch 8 Loss: 0.000062\n",
      "\tTraining batch 9 Loss: 0.000596\n",
      "\tTraining batch 10 Loss: 0.001991\n",
      "\tTraining batch 11 Loss: 0.076895\n",
      "\tTraining batch 12 Loss: 0.001360\n",
      "\tTraining batch 13 Loss: 0.001088\n",
      "\tTraining batch 14 Loss: 0.017469\n",
      "\tTraining batch 15 Loss: 0.006866\n",
      "\tTraining batch 16 Loss: 0.028516\n",
      "\tTraining batch 17 Loss: 0.009330\n",
      "\tTraining batch 18 Loss: 0.000001\n",
      "\tTraining batch 19 Loss: 0.001568\n",
      "\tTraining batch 20 Loss: 0.005675\n",
      "\tTraining batch 21 Loss: 0.000010\n",
      "\tTraining batch 22 Loss: 0.032775\n",
      "\tTraining batch 23 Loss: 0.000103\n",
      "\tTraining batch 24 Loss: 0.000121\n",
      "\tTraining batch 25 Loss: 0.003357\n",
      "\tTraining batch 26 Loss: 0.001236\n",
      "\tTraining batch 27 Loss: 0.011088\n",
      "\tTraining batch 28 Loss: 0.000141\n",
      "\tTraining batch 29 Loss: 0.016673\n",
      "\tTraining batch 30 Loss: 0.039304\n",
      "\tTraining batch 31 Loss: 0.078136\n",
      "\tTraining batch 32 Loss: 0.000038\n",
      "Training set: Average loss: 0.011461\n",
      "Validation set: Average loss: 5.056823, Accuracy: 1383/1959 (70.60%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000343\n",
      "\tTraining batch 2 Loss: 0.002867\n",
      "\tTraining batch 3 Loss: 0.051567\n",
      "\tTraining batch 4 Loss: 0.000014\n",
      "\tTraining batch 5 Loss: 0.038923\n",
      "\tTraining batch 6 Loss: 0.000005\n",
      "\tTraining batch 7 Loss: 0.000216\n",
      "\tTraining batch 8 Loss: 0.000362\n",
      "\tTraining batch 9 Loss: 0.297182\n",
      "\tTraining batch 10 Loss: 0.000120\n",
      "\tTraining batch 11 Loss: 0.118092\n",
      "\tTraining batch 12 Loss: 0.001029\n",
      "\tTraining batch 13 Loss: 0.080068\n",
      "\tTraining batch 14 Loss: 0.112446\n",
      "\tTraining batch 15 Loss: 0.000019\n",
      "\tTraining batch 16 Loss: 0.000001\n",
      "\tTraining batch 17 Loss: 0.000005\n",
      "\tTraining batch 18 Loss: 0.000119\n",
      "\tTraining batch 19 Loss: 0.002806\n",
      "\tTraining batch 20 Loss: 0.000508\n",
      "\tTraining batch 21 Loss: 0.000145\n",
      "\tTraining batch 22 Loss: 0.038973\n",
      "\tTraining batch 23 Loss: 0.001700\n",
      "\tTraining batch 24 Loss: 0.000001\n",
      "\tTraining batch 25 Loss: 0.000071\n",
      "\tTraining batch 26 Loss: 0.000043\n",
      "\tTraining batch 27 Loss: 0.007178\n",
      "\tTraining batch 28 Loss: 0.028859\n",
      "\tTraining batch 29 Loss: 0.002045\n",
      "\tTraining batch 30 Loss: 0.001313\n",
      "\tTraining batch 31 Loss: 0.084898\n",
      "\tTraining batch 32 Loss: 0.392057\n",
      "Training set: Average loss: 0.039499\n",
      "Validation set: Average loss: 6.491073, Accuracy: 1364/1959 (69.63%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.322988\n",
      "\tTraining batch 2 Loss: 0.008592\n",
      "\tTraining batch 3 Loss: 0.052730\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.070954\n",
      "\tTraining batch 6 Loss: 0.000200\n",
      "\tTraining batch 7 Loss: 0.001214\n",
      "\tTraining batch 8 Loss: 0.016035\n",
      "\tTraining batch 9 Loss: 0.001464\n",
      "\tTraining batch 10 Loss: 0.028772\n",
      "\tTraining batch 11 Loss: 0.000104\n",
      "\tTraining batch 12 Loss: 0.000067\n",
      "\tTraining batch 13 Loss: 0.021583\n",
      "\tTraining batch 14 Loss: 0.061979\n",
      "\tTraining batch 15 Loss: 0.000084\n",
      "\tTraining batch 16 Loss: 0.519927\n",
      "\tTraining batch 17 Loss: 0.008052\n",
      "\tTraining batch 18 Loss: 0.014129\n",
      "\tTraining batch 19 Loss: 0.099107\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.090332\n",
      "\tTraining batch 22 Loss: 0.000038\n",
      "\tTraining batch 23 Loss: 0.000891\n",
      "\tTraining batch 24 Loss: 0.139847\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000001\n",
      "\tTraining batch 27 Loss: 0.364294\n",
      "\tTraining batch 28 Loss: 0.004910\n",
      "\tTraining batch 29 Loss: 0.196144\n",
      "\tTraining batch 30 Loss: 0.004715\n",
      "\tTraining batch 31 Loss: 0.056679\n",
      "\tTraining batch 32 Loss: 0.000295\n",
      "Training set: Average loss: 0.065191\n",
      "Validation set: Average loss: 7.171104, Accuracy: 1357/1959 (69.27%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.070717\n",
      "\tTraining batch 2 Loss: 0.149976\n",
      "\tTraining batch 3 Loss: 0.348517\n",
      "\tTraining batch 4 Loss: 2.817457\n",
      "\tTraining batch 5 Loss: 0.000684\n",
      "\tTraining batch 6 Loss: 0.000225\n",
      "\tTraining batch 7 Loss: 0.177407\n",
      "\tTraining batch 8 Loss: 0.149133\n",
      "\tTraining batch 9 Loss: 0.192409\n",
      "\tTraining batch 10 Loss: 0.005222\n",
      "\tTraining batch 11 Loss: 0.211064\n",
      "\tTraining batch 12 Loss: 0.021489\n",
      "\tTraining batch 13 Loss: 0.000812\n",
      "\tTraining batch 14 Loss: 0.139195\n",
      "\tTraining batch 15 Loss: 0.930649\n",
      "\tTraining batch 16 Loss: 0.021266\n",
      "\tTraining batch 17 Loss: 0.864874\n",
      "\tTraining batch 18 Loss: 0.015760\n",
      "\tTraining batch 19 Loss: 0.234093\n",
      "\tTraining batch 20 Loss: 0.753020\n",
      "\tTraining batch 21 Loss: 0.124884\n",
      "\tTraining batch 22 Loss: 0.510683\n",
      "\tTraining batch 23 Loss: 0.451507\n",
      "\tTraining batch 24 Loss: 0.014026\n",
      "\tTraining batch 25 Loss: 0.341799\n",
      "\tTraining batch 26 Loss: 0.445144\n",
      "\tTraining batch 27 Loss: 0.499512\n",
      "\tTraining batch 28 Loss: 0.358910\n",
      "\tTraining batch 29 Loss: 0.982834\n",
      "\tTraining batch 30 Loss: 0.269842\n",
      "\tTraining batch 31 Loss: 0.416621\n",
      "\tTraining batch 32 Loss: 0.044383\n",
      "Training set: Average loss: 0.361379\n",
      "Validation set: Average loss: 6.134776, Accuracy: 1369/1959 (69.88%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.250239\n",
      "\tTraining batch 2 Loss: 0.373906\n",
      "\tTraining batch 3 Loss: 0.709255\n",
      "\tTraining batch 4 Loss: 0.269690\n",
      "\tTraining batch 5 Loss: 0.150090\n",
      "\tTraining batch 6 Loss: 0.024030\n",
      "\tTraining batch 7 Loss: 0.120510\n",
      "\tTraining batch 8 Loss: 0.946053\n",
      "\tTraining batch 9 Loss: 0.264527\n",
      "\tTraining batch 10 Loss: 0.401070\n",
      "\tTraining batch 11 Loss: 0.023622\n",
      "\tTraining batch 12 Loss: 0.000014\n",
      "\tTraining batch 13 Loss: 0.002140\n",
      "\tTraining batch 14 Loss: 0.193838\n",
      "\tTraining batch 15 Loss: 0.219868\n",
      "\tTraining batch 16 Loss: 0.166236\n",
      "\tTraining batch 17 Loss: 2.206383\n",
      "\tTraining batch 18 Loss: 0.388776\n",
      "\tTraining batch 19 Loss: 0.086174\n",
      "\tTraining batch 20 Loss: 0.088254\n",
      "\tTraining batch 21 Loss: 0.235704\n",
      "\tTraining batch 22 Loss: 1.891613\n",
      "\tTraining batch 23 Loss: 0.074353\n",
      "\tTraining batch 24 Loss: 0.065391\n",
      "\tTraining batch 25 Loss: 0.385912\n",
      "\tTraining batch 26 Loss: 0.120955\n",
      "\tTraining batch 27 Loss: 0.197564\n",
      "\tTraining batch 28 Loss: 0.040042\n",
      "\tTraining batch 29 Loss: 0.185907\n",
      "\tTraining batch 30 Loss: 0.704268\n",
      "\tTraining batch 31 Loss: 0.248461\n",
      "\tTraining batch 32 Loss: 0.144665\n",
      "Training set: Average loss: 0.349360\n",
      "Validation set: Average loss: 4.948993, Accuracy: 1310/1959 (66.87%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.434259\n",
      "\tTraining batch 2 Loss: 0.066054\n",
      "\tTraining batch 3 Loss: 0.135987\n",
      "\tTraining batch 4 Loss: 0.370887\n",
      "\tTraining batch 5 Loss: 0.007207\n",
      "\tTraining batch 6 Loss: 0.242100\n",
      "\tTraining batch 7 Loss: 0.000052\n",
      "\tTraining batch 8 Loss: 0.000037\n",
      "\tTraining batch 9 Loss: 0.000015\n",
      "\tTraining batch 10 Loss: 0.231858\n",
      "\tTraining batch 11 Loss: 0.000770\n",
      "\tTraining batch 12 Loss: 0.030403\n",
      "\tTraining batch 13 Loss: 0.260216\n",
      "\tTraining batch 14 Loss: 0.067296\n",
      "\tTraining batch 15 Loss: 0.000015\n",
      "\tTraining batch 16 Loss: 0.068673\n",
      "\tTraining batch 17 Loss: 0.009407\n",
      "\tTraining batch 18 Loss: 0.313272\n",
      "\tTraining batch 19 Loss: 0.040661\n",
      "\tTraining batch 20 Loss: 0.081254\n",
      "\tTraining batch 21 Loss: 0.028998\n",
      "\tTraining batch 22 Loss: 0.000589\n",
      "\tTraining batch 23 Loss: 0.005259\n",
      "\tTraining batch 24 Loss: 0.176333\n",
      "\tTraining batch 25 Loss: 0.420859\n",
      "\tTraining batch 26 Loss: 0.061396\n",
      "\tTraining batch 27 Loss: 0.000194\n",
      "\tTraining batch 28 Loss: 0.003654\n",
      "\tTraining batch 29 Loss: 0.183621\n",
      "\tTraining batch 30 Loss: 0.087050\n",
      "\tTraining batch 31 Loss: 0.001767\n",
      "\tTraining batch 32 Loss: 0.196638\n",
      "Training set: Average loss: 0.110212\n",
      "Validation set: Average loss: 5.611003, Accuracy: 1304/1959 (66.56%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.305926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 2 Loss: 0.001640\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.006851\n",
      "\tTraining batch 5 Loss: 0.130503\n",
      "\tTraining batch 6 Loss: 0.122198\n",
      "\tTraining batch 7 Loss: 0.000085\n",
      "\tTraining batch 8 Loss: 0.000223\n",
      "\tTraining batch 9 Loss: 0.000131\n",
      "\tTraining batch 10 Loss: 0.004984\n",
      "\tTraining batch 11 Loss: 0.000016\n",
      "\tTraining batch 12 Loss: 0.643798\n",
      "\tTraining batch 13 Loss: 0.005664\n",
      "\tTraining batch 14 Loss: 0.069389\n",
      "\tTraining batch 15 Loss: 0.244207\n",
      "\tTraining batch 16 Loss: 0.000837\n",
      "\tTraining batch 17 Loss: 0.000081\n",
      "\tTraining batch 18 Loss: 0.266433\n",
      "\tTraining batch 19 Loss: 0.002361\n",
      "\tTraining batch 20 Loss: 0.037133\n",
      "\tTraining batch 21 Loss: 0.016372\n",
      "\tTraining batch 22 Loss: 0.043303\n",
      "\tTraining batch 23 Loss: 0.047451\n",
      "\tTraining batch 24 Loss: 0.000384\n",
      "\tTraining batch 25 Loss: 0.242300\n",
      "\tTraining batch 26 Loss: 0.095327\n",
      "\tTraining batch 27 Loss: 0.000039\n",
      "\tTraining batch 28 Loss: 0.009679\n",
      "\tTraining batch 29 Loss: 0.000097\n",
      "\tTraining batch 30 Loss: 0.023830\n",
      "\tTraining batch 31 Loss: 0.000453\n",
      "\tTraining batch 32 Loss: 0.001586\n",
      "Training set: Average loss: 0.072603\n",
      "Validation set: Average loss: 5.490529, Accuracy: 1280/1959 (65.34%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.058058\n",
      "\tTraining batch 2 Loss: 0.001679\n",
      "\tTraining batch 3 Loss: 0.000729\n",
      "\tTraining batch 4 Loss: 0.350894\n",
      "\tTraining batch 5 Loss: 0.000170\n",
      "\tTraining batch 6 Loss: 0.002620\n",
      "\tTraining batch 7 Loss: 0.002314\n",
      "\tTraining batch 8 Loss: 0.000750\n",
      "\tTraining batch 9 Loss: 0.143702\n",
      "\tTraining batch 10 Loss: 0.006165\n",
      "\tTraining batch 11 Loss: 0.121405\n",
      "\tTraining batch 12 Loss: 0.000025\n",
      "\tTraining batch 13 Loss: 0.001990\n",
      "\tTraining batch 14 Loss: 0.062364\n",
      "\tTraining batch 15 Loss: 0.000034\n",
      "\tTraining batch 16 Loss: 0.095657\n",
      "\tTraining batch 17 Loss: 0.004563\n",
      "\tTraining batch 18 Loss: 0.001042\n",
      "\tTraining batch 19 Loss: 0.003691\n",
      "\tTraining batch 20 Loss: 0.000340\n",
      "\tTraining batch 21 Loss: 0.015948\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000218\n",
      "\tTraining batch 24 Loss: 0.000697\n",
      "\tTraining batch 25 Loss: 0.000037\n",
      "\tTraining batch 26 Loss: 0.012190\n",
      "\tTraining batch 27 Loss: 0.000003\n",
      "\tTraining batch 28 Loss: 0.001751\n",
      "\tTraining batch 29 Loss: 0.003738\n",
      "\tTraining batch 30 Loss: 0.179217\n",
      "\tTraining batch 31 Loss: 0.000514\n",
      "\tTraining batch 32 Loss: 0.072589\n",
      "Training set: Average loss: 0.035784\n",
      "Validation set: Average loss: 7.207885, Accuracy: 1343/1959 (68.56%)\n",
      "\n",
      "Epoch: 11\n",
      "\tTraining batch 1 Loss: 0.000001\n",
      "\tTraining batch 2 Loss: 0.000050\n",
      "\tTraining batch 3 Loss: 0.000001\n",
      "\tTraining batch 4 Loss: 0.000038\n",
      "\tTraining batch 5 Loss: 0.000282\n",
      "\tTraining batch 6 Loss: 0.000806\n",
      "\tTraining batch 7 Loss: 0.000031\n",
      "\tTraining batch 8 Loss: 0.000024\n",
      "\tTraining batch 9 Loss: 0.000005\n",
      "\tTraining batch 10 Loss: 0.000262\n",
      "\tTraining batch 11 Loss: 0.000036\n",
      "\tTraining batch 12 Loss: 0.001860\n",
      "\tTraining batch 13 Loss: 0.000087\n",
      "\tTraining batch 14 Loss: 0.059079\n",
      "\tTraining batch 15 Loss: 0.000002\n",
      "\tTraining batch 16 Loss: 0.000058\n",
      "\tTraining batch 17 Loss: 0.000495\n",
      "\tTraining batch 18 Loss: 0.000198\n",
      "\tTraining batch 19 Loss: 0.009783\n",
      "\tTraining batch 20 Loss: 0.000019\n",
      "\tTraining batch 21 Loss: 0.327404\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000061\n",
      "\tTraining batch 24 Loss: 0.000323\n",
      "\tTraining batch 25 Loss: 0.000110\n",
      "\tTraining batch 26 Loss: 0.007920\n",
      "\tTraining batch 27 Loss: 0.000004\n",
      "\tTraining batch 28 Loss: 0.001961\n",
      "\tTraining batch 29 Loss: 0.000010\n",
      "\tTraining batch 30 Loss: 0.002398\n",
      "\tTraining batch 31 Loss: 0.000163\n",
      "\tTraining batch 32 Loss: 0.000154\n",
      "Training set: Average loss: 0.012926\n",
      "Validation set: Average loss: 6.858215, Accuracy: 1312/1959 (66.97%)\n",
      "\n",
      "Epoch: 12\n",
      "\tTraining batch 1 Loss: 0.000035\n",
      "\tTraining batch 2 Loss: 0.000045\n",
      "\tTraining batch 3 Loss: 0.000007\n",
      "\tTraining batch 4 Loss: 0.000078\n",
      "\tTraining batch 5 Loss: 0.000224\n",
      "\tTraining batch 6 Loss: 0.001658\n",
      "\tTraining batch 7 Loss: 0.000034\n",
      "\tTraining batch 8 Loss: 0.000620\n",
      "\tTraining batch 9 Loss: 0.002321\n",
      "\tTraining batch 10 Loss: 0.000031\n",
      "\tTraining batch 11 Loss: 0.000002\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000025\n",
      "\tTraining batch 14 Loss: 0.069198\n",
      "\tTraining batch 15 Loss: 0.000002\n",
      "\tTraining batch 16 Loss: 0.000018\n",
      "\tTraining batch 17 Loss: 0.000359\n",
      "\tTraining batch 18 Loss: 0.000352\n",
      "\tTraining batch 19 Loss: 0.000122\n",
      "\tTraining batch 20 Loss: 0.000112\n",
      "\tTraining batch 21 Loss: 0.000155\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000983\n",
      "\tTraining batch 24 Loss: 0.034172\n",
      "\tTraining batch 25 Loss: 0.000013\n",
      "\tTraining batch 26 Loss: 0.008518\n",
      "\tTraining batch 27 Loss: 0.000479\n",
      "\tTraining batch 28 Loss: 0.000161\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.331786\n",
      "\tTraining batch 31 Loss: 0.000211\n",
      "\tTraining batch 32 Loss: 0.000457\n",
      "Training set: Average loss: 0.014131\n",
      "Validation set: Average loss: 7.284938, Accuracy: 1347/1959 (68.76%)\n",
      "\n",
      "Epoch: 13\n",
      "\tTraining batch 1 Loss: 0.000001\n",
      "\tTraining batch 2 Loss: 0.000262\n",
      "\tTraining batch 3 Loss: 0.000011\n",
      "\tTraining batch 4 Loss: 0.000245\n",
      "\tTraining batch 5 Loss: 0.001553\n",
      "\tTraining batch 6 Loss: 0.001237\n",
      "\tTraining batch 7 Loss: 0.000176\n",
      "\tTraining batch 8 Loss: 0.000003\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000017\n",
      "\tTraining batch 11 Loss: 0.006736\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000724\n",
      "\tTraining batch 14 Loss: 0.056371\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000005\n",
      "\tTraining batch 17 Loss: 0.000051\n",
      "\tTraining batch 18 Loss: 0.005624\n",
      "\tTraining batch 19 Loss: 0.000998\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000257\n",
      "\tTraining batch 22 Loss: 0.000003\n",
      "\tTraining batch 23 Loss: 0.000054\n",
      "\tTraining batch 24 Loss: 0.000145\n",
      "\tTraining batch 25 Loss: 0.000002\n",
      "\tTraining batch 26 Loss: 0.007427\n",
      "\tTraining batch 27 Loss: 0.000001\n",
      "\tTraining batch 28 Loss: 0.327665\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000726\n",
      "\tTraining batch 31 Loss: 0.000619\n",
      "\tTraining batch 32 Loss: 0.000216\n",
      "Training set: Average loss: 0.012848\n",
      "Validation set: Average loss: 6.998677, Accuracy: 1333/1959 (68.04%)\n",
      "\n",
      "Epoch: 14\n",
      "\tTraining batch 1 Loss: 0.000003\n",
      "\tTraining batch 2 Loss: 0.000724\n",
      "\tTraining batch 3 Loss: 0.000091\n",
      "\tTraining batch 4 Loss: 0.000140\n",
      "\tTraining batch 5 Loss: 0.000067\n",
      "\tTraining batch 6 Loss: 0.000741\n",
      "\tTraining batch 7 Loss: 0.000003\n",
      "\tTraining batch 8 Loss: 0.000003\n",
      "\tTraining batch 9 Loss: 0.129830\n",
      "\tTraining batch 10 Loss: 0.000090\n",
      "\tTraining batch 11 Loss: 0.000002\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000268\n",
      "\tTraining batch 14 Loss: 0.054618\n",
      "\tTraining batch 15 Loss: 0.000005\n",
      "\tTraining batch 16 Loss: 0.000014\n",
      "\tTraining batch 17 Loss: 0.001699\n",
      "\tTraining batch 18 Loss: 0.000039\n",
      "\tTraining batch 19 Loss: 0.001180\n",
      "\tTraining batch 20 Loss: 0.000034\n",
      "\tTraining batch 21 Loss: 0.001552\n",
      "\tTraining batch 22 Loss: 0.000001\n",
      "\tTraining batch 23 Loss: 0.000171\n",
      "\tTraining batch 24 Loss: 0.000443\n",
      "\tTraining batch 25 Loss: 0.000166\n",
      "\tTraining batch 26 Loss: 0.003492\n",
      "\tTraining batch 27 Loss: 0.000245\n",
      "\tTraining batch 28 Loss: 0.000583\n",
      "\tTraining batch 29 Loss: 0.000040\n",
      "\tTraining batch 30 Loss: 0.002442\n",
      "\tTraining batch 31 Loss: 0.000429\n",
      "\tTraining batch 32 Loss: 0.006039\n",
      "Training set: Average loss: 0.006411\n",
      "Validation set: Average loss: 6.737478, Accuracy: 1332/1959 (67.99%)\n",
      "\n",
      "Epoch: 15\n",
      "\tTraining batch 1 Loss: 0.000371\n",
      "\tTraining batch 2 Loss: 0.000031\n",
      "\tTraining batch 3 Loss: 0.000079\n",
      "\tTraining batch 4 Loss: 0.000182\n",
      "\tTraining batch 5 Loss: 0.002426\n",
      "\tTraining batch 6 Loss: 0.001327\n",
      "\tTraining batch 7 Loss: 0.000001\n",
      "\tTraining batch 8 Loss: 0.000011\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000068\n",
      "\tTraining batch 11 Loss: 0.000001\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000556\n",
      "\tTraining batch 14 Loss: 0.051748\n",
      "\tTraining batch 15 Loss: 0.000003\n",
      "\tTraining batch 16 Loss: 0.000007\n",
      "\tTraining batch 17 Loss: 0.000320\n",
      "\tTraining batch 18 Loss: 0.000074\n",
      "\tTraining batch 19 Loss: 0.000696\n",
      "\tTraining batch 20 Loss: 0.000044\n",
      "\tTraining batch 21 Loss: 0.000149\n",
      "\tTraining batch 22 Loss: 0.000003\n",
      "\tTraining batch 23 Loss: 0.000104\n",
      "\tTraining batch 24 Loss: 0.000314\n",
      "\tTraining batch 25 Loss: 0.000079\n",
      "\tTraining batch 26 Loss: 0.003132\n",
      "\tTraining batch 27 Loss: 0.000159\n",
      "\tTraining batch 28 Loss: 0.000269\n",
      "\tTraining batch 29 Loss: 0.000019\n",
      "\tTraining batch 30 Loss: 0.001871\n",
      "\tTraining batch 31 Loss: 0.000033\n",
      "\tTraining batch 32 Loss: 0.000261\n",
      "Training set: Average loss: 0.002011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set: Average loss: 6.709929, Accuracy: 1342/1959 (68.50%)\n",
      "\n",
      "Epoch: 16\n",
      "\tTraining batch 1 Loss: 0.000012\n",
      "\tTraining batch 2 Loss: 0.000032\n",
      "\tTraining batch 3 Loss: 0.000012\n",
      "\tTraining batch 4 Loss: 0.000092\n",
      "\tTraining batch 5 Loss: 0.000169\n",
      "\tTraining batch 6 Loss: 0.000328\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000022\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000058\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000255\n",
      "\tTraining batch 14 Loss: 0.045955\n",
      "\tTraining batch 15 Loss: 0.000002\n",
      "\tTraining batch 16 Loss: 0.000007\n",
      "\tTraining batch 17 Loss: 0.000117\n",
      "\tTraining batch 18 Loss: 0.000058\n",
      "\tTraining batch 19 Loss: 0.000497\n",
      "\tTraining batch 20 Loss: 0.000031\n",
      "\tTraining batch 21 Loss: 0.000166\n",
      "\tTraining batch 22 Loss: 0.000003\n",
      "\tTraining batch 23 Loss: 0.000084\n",
      "\tTraining batch 24 Loss: 0.000226\n",
      "\tTraining batch 25 Loss: 0.000057\n",
      "\tTraining batch 26 Loss: 0.002690\n",
      "\tTraining batch 27 Loss: 0.000123\n",
      "\tTraining batch 28 Loss: 0.000216\n",
      "\tTraining batch 29 Loss: 0.000013\n",
      "\tTraining batch 30 Loss: 0.001478\n",
      "\tTraining batch 31 Loss: 0.000037\n",
      "\tTraining batch 32 Loss: 0.000150\n",
      "Training set: Average loss: 0.001653\n",
      "Validation set: Average loss: 6.737349, Accuracy: 1342/1959 (68.50%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 47.5%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000009\n",
      "\tTraining batch 2 Loss: 0.000030\n",
      "\tTraining batch 3 Loss: 0.000009\n",
      "\tTraining batch 4 Loss: 0.000077\n",
      "\tTraining batch 5 Loss: 0.000125\n",
      "\tTraining batch 6 Loss: 0.000228\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000016\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000051\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000187\n",
      "\tTraining batch 14 Loss: 0.039250\n",
      "\tTraining batch 15 Loss: 0.000002\n",
      "\tTraining batch 16 Loss: 0.000007\n",
      "\tTraining batch 17 Loss: 0.000080\n",
      "\tTraining batch 18 Loss: 0.000050\n",
      "\tTraining batch 19 Loss: 0.000371\n",
      "\tTraining batch 20 Loss: 0.000025\n",
      "\tTraining batch 21 Loss: 0.000163\n",
      "\tTraining batch 22 Loss: 0.000002\n",
      "\tTraining batch 23 Loss: 0.000070\n",
      "\tTraining batch 24 Loss: 0.000184\n",
      "\tTraining batch 25 Loss: 0.000049\n",
      "\tTraining batch 26 Loss: 0.002359\n",
      "\tTraining batch 27 Loss: 0.000096\n",
      "\tTraining batch 28 Loss: 0.000184\n",
      "\tTraining batch 29 Loss: 0.000010\n",
      "\tTraining batch 30 Loss: 0.001195\n",
      "\tTraining batch 31 Loss: 0.000039\n",
      "\tTraining batch 32 Loss: 0.000098\n",
      "\tTraining batch 33 Loss: 10.175305\n",
      "Training set: Average loss: 0.309705\n",
      "Validation set: Average loss: 5.768236, Accuracy: 1335/1959 (68.15%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000035\n",
      "\tTraining batch 2 Loss: 0.006792\n",
      "\tTraining batch 3 Loss: 0.031608\n",
      "\tTraining batch 4 Loss: 0.160716\n",
      "\tTraining batch 5 Loss: 0.009084\n",
      "\tTraining batch 6 Loss: 0.143681\n",
      "\tTraining batch 7 Loss: 0.075312\n",
      "\tTraining batch 8 Loss: 0.240978\n",
      "\tTraining batch 9 Loss: 0.138636\n",
      "\tTraining batch 10 Loss: 0.182868\n",
      "\tTraining batch 11 Loss: 0.187011\n",
      "\tTraining batch 12 Loss: 0.235209\n",
      "\tTraining batch 13 Loss: 0.252933\n",
      "\tTraining batch 14 Loss: 0.262298\n",
      "\tTraining batch 15 Loss: 0.579201\n",
      "\tTraining batch 16 Loss: 0.033558\n",
      "\tTraining batch 17 Loss: 0.020389\n",
      "\tTraining batch 18 Loss: 0.214572\n",
      "\tTraining batch 19 Loss: 0.082105\n",
      "\tTraining batch 20 Loss: 0.117097\n",
      "\tTraining batch 21 Loss: 0.122960\n",
      "\tTraining batch 22 Loss: 0.008686\n",
      "\tTraining batch 23 Loss: 0.038193\n",
      "\tTraining batch 24 Loss: 0.020032\n",
      "\tTraining batch 25 Loss: 0.036540\n",
      "\tTraining batch 26 Loss: 0.067574\n",
      "\tTraining batch 27 Loss: 0.234516\n",
      "\tTraining batch 28 Loss: 0.111353\n",
      "\tTraining batch 29 Loss: 0.007414\n",
      "\tTraining batch 30 Loss: 0.021826\n",
      "\tTraining batch 31 Loss: 0.098116\n",
      "\tTraining batch 32 Loss: 0.072383\n",
      "\tTraining batch 33 Loss: 0.415131\n",
      "Training set: Average loss: 0.128146\n",
      "Validation set: Average loss: 3.199668, Accuracy: 1340/1959 (68.40%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.022861\n",
      "\tTraining batch 2 Loss: 0.202160\n",
      "\tTraining batch 3 Loss: 0.002100\n",
      "\tTraining batch 4 Loss: 0.052441\n",
      "\tTraining batch 5 Loss: 0.006986\n",
      "\tTraining batch 6 Loss: 0.001554\n",
      "\tTraining batch 7 Loss: 0.001227\n",
      "\tTraining batch 8 Loss: 0.000802\n",
      "\tTraining batch 9 Loss: 0.004145\n",
      "\tTraining batch 10 Loss: 0.046163\n",
      "\tTraining batch 11 Loss: 0.001114\n",
      "\tTraining batch 12 Loss: 0.000145\n",
      "\tTraining batch 13 Loss: 0.001976\n",
      "\tTraining batch 14 Loss: 0.035856\n",
      "\tTraining batch 15 Loss: 0.021511\n",
      "\tTraining batch 16 Loss: 0.063015\n",
      "\tTraining batch 17 Loss: 0.005818\n",
      "\tTraining batch 18 Loss: 0.008316\n",
      "\tTraining batch 19 Loss: 0.036765\n",
      "\tTraining batch 20 Loss: 0.016549\n",
      "\tTraining batch 21 Loss: 0.084830\n",
      "\tTraining batch 22 Loss: 0.000004\n",
      "\tTraining batch 23 Loss: 0.001647\n",
      "\tTraining batch 24 Loss: 0.004088\n",
      "\tTraining batch 25 Loss: 0.004896\n",
      "\tTraining batch 26 Loss: 0.004224\n",
      "\tTraining batch 27 Loss: 0.000077\n",
      "\tTraining batch 28 Loss: 0.003195\n",
      "\tTraining batch 29 Loss: 0.000016\n",
      "\tTraining batch 30 Loss: 0.001315\n",
      "\tTraining batch 31 Loss: 0.001154\n",
      "\tTraining batch 32 Loss: 0.000947\n",
      "\tTraining batch 33 Loss: 0.030774\n",
      "Training set: Average loss: 0.020263\n",
      "Validation set: Average loss: 4.943686, Accuracy: 1322/1959 (67.48%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000203\n",
      "\tTraining batch 2 Loss: 0.000557\n",
      "\tTraining batch 3 Loss: 0.102292\n",
      "\tTraining batch 4 Loss: 0.004547\n",
      "\tTraining batch 5 Loss: 0.008263\n",
      "\tTraining batch 6 Loss: 0.000291\n",
      "\tTraining batch 7 Loss: 0.000133\n",
      "\tTraining batch 8 Loss: 0.000004\n",
      "\tTraining batch 9 Loss: 0.000038\n",
      "\tTraining batch 10 Loss: 0.000011\n",
      "\tTraining batch 11 Loss: 0.000940\n",
      "\tTraining batch 12 Loss: 0.000066\n",
      "\tTraining batch 13 Loss: 0.000475\n",
      "\tTraining batch 14 Loss: 0.009784\n",
      "\tTraining batch 15 Loss: 0.000007\n",
      "\tTraining batch 16 Loss: 0.003129\n",
      "\tTraining batch 17 Loss: 0.000039\n",
      "\tTraining batch 18 Loss: 0.000404\n",
      "\tTraining batch 19 Loss: 0.015112\n",
      "\tTraining batch 20 Loss: 0.000107\n",
      "\tTraining batch 21 Loss: 0.006230\n",
      "\tTraining batch 22 Loss: 0.000033\n",
      "\tTraining batch 23 Loss: 0.001364\n",
      "\tTraining batch 24 Loss: 0.001177\n",
      "\tTraining batch 25 Loss: 0.001265\n",
      "\tTraining batch 26 Loss: 0.005578\n",
      "\tTraining batch 27 Loss: 0.000616\n",
      "\tTraining batch 28 Loss: 0.000428\n",
      "\tTraining batch 29 Loss: 0.000007\n",
      "\tTraining batch 30 Loss: 0.001130\n",
      "\tTraining batch 31 Loss: 0.008114\n",
      "\tTraining batch 32 Loss: 0.001503\n",
      "\tTraining batch 33 Loss: 0.026449\n",
      "Training set: Average loss: 0.006070\n",
      "Validation set: Average loss: 5.043777, Accuracy: 1365/1959 (69.68%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000084\n",
      "\tTraining batch 2 Loss: 0.001308\n",
      "\tTraining batch 3 Loss: 0.000003\n",
      "\tTraining batch 4 Loss: 0.001535\n",
      "\tTraining batch 5 Loss: 0.000265\n",
      "\tTraining batch 6 Loss: 0.000154\n",
      "\tTraining batch 7 Loss: 0.000100\n",
      "\tTraining batch 8 Loss: 0.000002\n",
      "\tTraining batch 9 Loss: 0.000031\n",
      "\tTraining batch 10 Loss: 0.000009\n",
      "\tTraining batch 11 Loss: 0.003476\n",
      "\tTraining batch 12 Loss: 0.000074\n",
      "\tTraining batch 13 Loss: 0.000972\n",
      "\tTraining batch 14 Loss: 0.006594\n",
      "\tTraining batch 15 Loss: 0.000004\n",
      "\tTraining batch 16 Loss: 0.000729\n",
      "\tTraining batch 17 Loss: 0.000025\n",
      "\tTraining batch 18 Loss: 0.000336\n",
      "\tTraining batch 19 Loss: 0.008063\n",
      "\tTraining batch 20 Loss: 0.000003\n",
      "\tTraining batch 21 Loss: 0.000373\n",
      "\tTraining batch 22 Loss: 0.000025\n",
      "\tTraining batch 23 Loss: 0.000253\n",
      "\tTraining batch 24 Loss: 0.000361\n",
      "\tTraining batch 25 Loss: 0.000361\n",
      "\tTraining batch 26 Loss: 0.004848\n",
      "\tTraining batch 27 Loss: 0.000032\n",
      "\tTraining batch 28 Loss: 0.000667\n",
      "\tTraining batch 29 Loss: 0.000031\n",
      "\tTraining batch 30 Loss: 0.000546\n",
      "\tTraining batch 31 Loss: 0.000176\n",
      "\tTraining batch 32 Loss: 0.000312\n",
      "\tTraining batch 33 Loss: 0.000842\n",
      "Training set: Average loss: 0.000988\n",
      "Validation set: Average loss: 5.243687, Accuracy: 1354/1959 (69.12%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000043\n",
      "\tTraining batch 2 Loss: 0.000352\n",
      "\tTraining batch 3 Loss: 0.000001\n",
      "\tTraining batch 4 Loss: 0.000864\n",
      "\tTraining batch 5 Loss: 0.000291\n",
      "\tTraining batch 6 Loss: 0.000106\n",
      "\tTraining batch 7 Loss: 0.000067\n",
      "\tTraining batch 8 Loss: 0.000001\n",
      "\tTraining batch 9 Loss: 0.000764\n",
      "\tTraining batch 10 Loss: 0.000005\n",
      "\tTraining batch 11 Loss: 0.000146\n",
      "\tTraining batch 12 Loss: 0.000146\n",
      "\tTraining batch 13 Loss: 0.000206\n",
      "\tTraining batch 14 Loss: 0.004038\n",
      "\tTraining batch 15 Loss: 0.000002\n",
      "\tTraining batch 16 Loss: 0.000511\n",
      "\tTraining batch 17 Loss: 0.000018\n",
      "\tTraining batch 18 Loss: 0.000077\n",
      "\tTraining batch 19 Loss: 0.005108\n",
      "\tTraining batch 20 Loss: 0.000002\n",
      "\tTraining batch 21 Loss: 0.000255\n",
      "\tTraining batch 22 Loss: 0.000006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 23 Loss: 0.000313\n",
      "\tTraining batch 24 Loss: 0.000311\n",
      "\tTraining batch 25 Loss: 0.000204\n",
      "\tTraining batch 26 Loss: 0.004058\n",
      "\tTraining batch 27 Loss: 0.000025\n",
      "\tTraining batch 28 Loss: 0.000446\n",
      "\tTraining batch 29 Loss: 0.000019\n",
      "\tTraining batch 30 Loss: 0.000448\n",
      "\tTraining batch 31 Loss: 0.000153\n",
      "\tTraining batch 32 Loss: 0.000220\n",
      "\tTraining batch 33 Loss: 0.000700\n",
      "Training set: Average loss: 0.000603\n",
      "Validation set: Average loss: 5.313918, Accuracy: 1354/1959 (69.12%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 47.5%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000041\n",
      "\tTraining batch 2 Loss: 0.000271\n",
      "\tTraining batch 3 Loss: 0.000001\n",
      "\tTraining batch 4 Loss: 0.000770\n",
      "\tTraining batch 5 Loss: 0.000206\n",
      "\tTraining batch 6 Loss: 0.000095\n",
      "\tTraining batch 7 Loss: 0.000054\n",
      "\tTraining batch 8 Loss: 0.000001\n",
      "\tTraining batch 9 Loss: 0.000008\n",
      "\tTraining batch 10 Loss: 0.000004\n",
      "\tTraining batch 11 Loss: 0.000102\n",
      "\tTraining batch 12 Loss: 0.000062\n",
      "\tTraining batch 13 Loss: 0.000194\n",
      "\tTraining batch 14 Loss: 0.002814\n",
      "\tTraining batch 15 Loss: 0.000002\n",
      "\tTraining batch 16 Loss: 0.000362\n",
      "\tTraining batch 17 Loss: 0.000016\n",
      "\tTraining batch 18 Loss: 0.000055\n",
      "\tTraining batch 19 Loss: 0.004212\n",
      "\tTraining batch 20 Loss: 0.000001\n",
      "\tTraining batch 21 Loss: 0.000222\n",
      "\tTraining batch 22 Loss: 0.000005\n",
      "\tTraining batch 23 Loss: 0.000291\n",
      "\tTraining batch 24 Loss: 0.000264\n",
      "\tTraining batch 25 Loss: 0.000151\n",
      "\tTraining batch 26 Loss: 0.003800\n",
      "\tTraining batch 27 Loss: 0.000021\n",
      "\tTraining batch 28 Loss: 0.000310\n",
      "\tTraining batch 29 Loss: 0.000013\n",
      "\tTraining batch 30 Loss: 0.000389\n",
      "\tTraining batch 31 Loss: 0.000136\n",
      "\tTraining batch 32 Loss: 0.000171\n",
      "\tTraining batch 33 Loss: 0.000602\n",
      "\tTraining batch 34 Loss: 9.706553\n",
      "Training set: Average loss: 0.285947\n",
      "Validation set: Average loss: 4.453409, Accuracy: 1351/1959 (68.96%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000078\n",
      "\tTraining batch 2 Loss: 0.003765\n",
      "\tTraining batch 3 Loss: 0.001381\n",
      "\tTraining batch 4 Loss: 0.045439\n",
      "\tTraining batch 5 Loss: 0.021772\n",
      "\tTraining batch 6 Loss: 0.042268\n",
      "\tTraining batch 7 Loss: 0.305619\n",
      "\tTraining batch 8 Loss: 0.219364\n",
      "\tTraining batch 9 Loss: 0.028987\n",
      "\tTraining batch 10 Loss: 0.145780\n",
      "\tTraining batch 11 Loss: 0.043944\n",
      "\tTraining batch 12 Loss: 0.033966\n",
      "\tTraining batch 13 Loss: 0.038774\n",
      "\tTraining batch 14 Loss: 0.061593\n",
      "\tTraining batch 15 Loss: 0.034963\n",
      "\tTraining batch 16 Loss: 0.068585\n",
      "\tTraining batch 17 Loss: 0.103900\n",
      "\tTraining batch 18 Loss: 0.070716\n",
      "\tTraining batch 19 Loss: 0.181005\n",
      "\tTraining batch 20 Loss: 0.063080\n",
      "\tTraining batch 21 Loss: 0.235005\n",
      "\tTraining batch 22 Loss: 0.038751\n",
      "\tTraining batch 23 Loss: 0.036495\n",
      "\tTraining batch 24 Loss: 0.029305\n",
      "\tTraining batch 25 Loss: 0.109380\n",
      "\tTraining batch 26 Loss: 0.018674\n",
      "\tTraining batch 27 Loss: 0.013007\n",
      "\tTraining batch 28 Loss: 0.052387\n",
      "\tTraining batch 29 Loss: 0.008572\n",
      "\tTraining batch 30 Loss: 0.041150\n",
      "\tTraining batch 31 Loss: 0.049375\n",
      "\tTraining batch 32 Loss: 0.031994\n",
      "\tTraining batch 33 Loss: 0.123249\n",
      "\tTraining batch 34 Loss: 0.303214\n",
      "Training set: Average loss: 0.076633\n",
      "Validation set: Average loss: 2.858208, Accuracy: 1350/1959 (68.91%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.003165\n",
      "\tTraining batch 2 Loss: 0.009365\n",
      "\tTraining batch 3 Loss: 0.000618\n",
      "\tTraining batch 4 Loss: 0.013959\n",
      "\tTraining batch 5 Loss: 0.007994\n",
      "\tTraining batch 6 Loss: 0.037825\n",
      "\tTraining batch 7 Loss: 0.005862\n",
      "\tTraining batch 8 Loss: 0.000094\n",
      "\tTraining batch 9 Loss: 0.000670\n",
      "\tTraining batch 10 Loss: 0.002183\n",
      "\tTraining batch 11 Loss: 0.002861\n",
      "\tTraining batch 12 Loss: 0.000470\n",
      "\tTraining batch 13 Loss: 0.018869\n",
      "\tTraining batch 14 Loss: 0.004059\n",
      "\tTraining batch 15 Loss: 0.215579\n",
      "\tTraining batch 16 Loss: 0.021889\n",
      "\tTraining batch 17 Loss: 0.000666\n",
      "\tTraining batch 18 Loss: 0.001847\n",
      "\tTraining batch 19 Loss: 0.062655\n",
      "\tTraining batch 20 Loss: 0.000088\n",
      "\tTraining batch 21 Loss: 0.033615\n",
      "\tTraining batch 22 Loss: 0.000046\n",
      "\tTraining batch 23 Loss: 0.014477\n",
      "\tTraining batch 24 Loss: 0.000462\n",
      "\tTraining batch 25 Loss: 0.000983\n",
      "\tTraining batch 26 Loss: 0.004070\n",
      "\tTraining batch 27 Loss: 0.000042\n",
      "\tTraining batch 28 Loss: 0.003918\n",
      "\tTraining batch 29 Loss: 0.000017\n",
      "\tTraining batch 30 Loss: 0.027817\n",
      "\tTraining batch 31 Loss: 0.001134\n",
      "\tTraining batch 32 Loss: 0.000855\n",
      "\tTraining batch 33 Loss: 0.001354\n",
      "\tTraining batch 34 Loss: 0.006009\n",
      "Training set: Average loss: 0.014868\n",
      "Validation set: Average loss: 4.321696, Accuracy: 1312/1959 (66.97%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.009117\n",
      "\tTraining batch 2 Loss: 0.000168\n",
      "\tTraining batch 3 Loss: 0.000011\n",
      "\tTraining batch 4 Loss: 0.005877\n",
      "\tTraining batch 5 Loss: 0.021967\n",
      "\tTraining batch 6 Loss: 0.000145\n",
      "\tTraining batch 7 Loss: 0.000007\n",
      "\tTraining batch 8 Loss: 0.000006\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000016\n",
      "\tTraining batch 11 Loss: 0.026784\n",
      "\tTraining batch 12 Loss: 0.131258\n",
      "\tTraining batch 13 Loss: 0.000303\n",
      "\tTraining batch 14 Loss: 0.064975\n",
      "\tTraining batch 15 Loss: 0.000373\n",
      "\tTraining batch 16 Loss: 0.001146\n",
      "\tTraining batch 17 Loss: 0.000079\n",
      "\tTraining batch 18 Loss: 0.000207\n",
      "\tTraining batch 19 Loss: 0.035859\n",
      "\tTraining batch 20 Loss: 0.000572\n",
      "\tTraining batch 21 Loss: 0.001817\n",
      "\tTraining batch 22 Loss: 0.000064\n",
      "\tTraining batch 23 Loss: 0.000258\n",
      "\tTraining batch 24 Loss: 0.000085\n",
      "\tTraining batch 25 Loss: 0.000661\n",
      "\tTraining batch 26 Loss: 0.008648\n",
      "\tTraining batch 27 Loss: 0.000780\n",
      "\tTraining batch 28 Loss: 0.000446\n",
      "\tTraining batch 29 Loss: 0.000140\n",
      "\tTraining batch 30 Loss: 0.045554\n",
      "\tTraining batch 31 Loss: 0.016976\n",
      "\tTraining batch 32 Loss: 0.055751\n",
      "\tTraining batch 33 Loss: 0.023178\n",
      "\tTraining batch 34 Loss: 0.006200\n",
      "Training set: Average loss: 0.013513\n",
      "Validation set: Average loss: 4.483693, Accuracy: 1365/1959 (69.68%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000080\n",
      "\tTraining batch 2 Loss: 0.000025\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000766\n",
      "\tTraining batch 5 Loss: 0.000598\n",
      "\tTraining batch 6 Loss: 0.000221\n",
      "\tTraining batch 7 Loss: 0.000019\n",
      "\tTraining batch 8 Loss: 0.000018\n",
      "\tTraining batch 9 Loss: 0.000003\n",
      "\tTraining batch 10 Loss: 0.000127\n",
      "\tTraining batch 11 Loss: 0.000542\n",
      "\tTraining batch 12 Loss: 0.000119\n",
      "\tTraining batch 13 Loss: 0.000572\n",
      "\tTraining batch 14 Loss: 0.031443\n",
      "\tTraining batch 15 Loss: 0.000137\n",
      "\tTraining batch 16 Loss: 0.000266\n",
      "\tTraining batch 17 Loss: 0.003245\n",
      "\tTraining batch 18 Loss: 0.001724\n",
      "\tTraining batch 19 Loss: 0.007159\n",
      "\tTraining batch 20 Loss: 0.000288\n",
      "\tTraining batch 21 Loss: 0.000077\n",
      "\tTraining batch 22 Loss: 0.000030\n",
      "\tTraining batch 23 Loss: 0.001031\n",
      "\tTraining batch 24 Loss: 0.000248\n",
      "\tTraining batch 25 Loss: 0.000015\n",
      "\tTraining batch 26 Loss: 0.007315\n",
      "\tTraining batch 27 Loss: 0.000205\n",
      "\tTraining batch 28 Loss: 0.000192\n",
      "\tTraining batch 29 Loss: 0.000076\n",
      "\tTraining batch 30 Loss: 0.002164\n",
      "\tTraining batch 31 Loss: 0.000092\n",
      "\tTraining batch 32 Loss: 0.001600\n",
      "\tTraining batch 33 Loss: 0.002679\n",
      "\tTraining batch 34 Loss: 0.020973\n",
      "Training set: Average loss: 0.002472\n",
      "Validation set: Average loss: 5.250440, Accuracy: 1330/1959 (67.89%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000209\n",
      "\tTraining batch 2 Loss: 0.000299\n",
      "\tTraining batch 3 Loss: 0.000018\n",
      "\tTraining batch 4 Loss: 0.000519\n",
      "\tTraining batch 5 Loss: 0.000609\n",
      "\tTraining batch 6 Loss: 0.000335\n",
      "\tTraining batch 7 Loss: 0.000021\n",
      "\tTraining batch 8 Loss: 0.000011\n",
      "\tTraining batch 9 Loss: 0.000001\n",
      "\tTraining batch 10 Loss: 0.000081\n",
      "\tTraining batch 11 Loss: 0.000025\n",
      "\tTraining batch 12 Loss: 0.000114\n",
      "\tTraining batch 13 Loss: 0.000473\n",
      "\tTraining batch 14 Loss: 0.000640\n",
      "\tTraining batch 15 Loss: 0.000062\n",
      "\tTraining batch 16 Loss: 0.000659\n",
      "\tTraining batch 17 Loss: 0.000191\n",
      "\tTraining batch 18 Loss: 0.000013\n",
      "\tTraining batch 19 Loss: 0.017949\n",
      "\tTraining batch 20 Loss: 0.000121\n",
      "\tTraining batch 21 Loss: 0.000660\n",
      "\tTraining batch 22 Loss: 0.000003\n",
      "\tTraining batch 23 Loss: 0.000873\n",
      "\tTraining batch 24 Loss: 0.000136\n",
      "\tTraining batch 25 Loss: 0.000280\n",
      "\tTraining batch 26 Loss: 0.003090\n",
      "\tTraining batch 27 Loss: 0.000008\n",
      "\tTraining batch 28 Loss: 0.000349\n",
      "\tTraining batch 29 Loss: 0.000009\n",
      "\tTraining batch 30 Loss: 0.001386\n",
      "\tTraining batch 31 Loss: 0.000634\n",
      "\tTraining batch 32 Loss: 0.000602\n",
      "\tTraining batch 33 Loss: 0.000966\n",
      "\tTraining batch 34 Loss: 0.001986\n",
      "Training set: Average loss: 0.000980\n",
      "Validation set: Average loss: 4.449368, Accuracy: 1360/1959 (69.42%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000012\n",
      "\tTraining batch 2 Loss: 0.000014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 3 Loss: 0.000001\n",
      "\tTraining batch 4 Loss: 0.000407\n",
      "\tTraining batch 5 Loss: 0.000090\n",
      "\tTraining batch 6 Loss: 0.000510\n",
      "\tTraining batch 7 Loss: 0.000016\n",
      "\tTraining batch 8 Loss: 0.000009\n",
      "\tTraining batch 9 Loss: 0.000001\n",
      "\tTraining batch 10 Loss: 0.000064\n",
      "\tTraining batch 11 Loss: 0.000009\n",
      "\tTraining batch 12 Loss: 0.000046\n",
      "\tTraining batch 13 Loss: 0.000272\n",
      "\tTraining batch 14 Loss: 0.000487\n",
      "\tTraining batch 15 Loss: 0.000037\n",
      "\tTraining batch 16 Loss: 0.000463\n",
      "\tTraining batch 17 Loss: 0.000186\n",
      "\tTraining batch 18 Loss: 0.000007\n",
      "\tTraining batch 19 Loss: 0.012966\n",
      "\tTraining batch 20 Loss: 0.000080\n",
      "\tTraining batch 21 Loss: 0.000406\n",
      "\tTraining batch 22 Loss: 0.000001\n",
      "\tTraining batch 23 Loss: 0.000475\n",
      "\tTraining batch 24 Loss: 0.000073\n",
      "\tTraining batch 25 Loss: 0.000125\n",
      "\tTraining batch 26 Loss: 0.003120\n",
      "\tTraining batch 27 Loss: 0.000005\n",
      "\tTraining batch 28 Loss: 0.000222\n",
      "\tTraining batch 29 Loss: 0.000006\n",
      "\tTraining batch 30 Loss: 0.000941\n",
      "\tTraining batch 31 Loss: 0.000336\n",
      "\tTraining batch 32 Loss: 0.000425\n",
      "\tTraining batch 33 Loss: 0.000586\n",
      "\tTraining batch 34 Loss: 0.001679\n",
      "Training set: Average loss: 0.000708\n",
      "Validation set: Average loss: 4.574244, Accuracy: 1356/1959 (69.22%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000009\n",
      "\tTraining batch 2 Loss: 0.000010\n",
      "\tTraining batch 3 Loss: 0.000001\n",
      "\tTraining batch 4 Loss: 0.000341\n",
      "\tTraining batch 5 Loss: 0.000060\n",
      "\tTraining batch 6 Loss: 0.000313\n",
      "\tTraining batch 7 Loss: 0.000008\n",
      "\tTraining batch 8 Loss: 0.000005\n",
      "\tTraining batch 9 Loss: 0.000001\n",
      "\tTraining batch 10 Loss: 0.000051\n",
      "\tTraining batch 11 Loss: 0.000007\n",
      "\tTraining batch 12 Loss: 0.000051\n",
      "\tTraining batch 13 Loss: 0.000212\n",
      "\tTraining batch 14 Loss: 0.000452\n",
      "\tTraining batch 15 Loss: 0.000032\n",
      "\tTraining batch 16 Loss: 0.000244\n",
      "\tTraining batch 17 Loss: 0.000171\n",
      "\tTraining batch 18 Loss: 0.000005\n",
      "\tTraining batch 19 Loss: 0.009104\n",
      "\tTraining batch 20 Loss: 0.000054\n",
      "\tTraining batch 21 Loss: 0.000215\n",
      "\tTraining batch 22 Loss: 0.000001\n",
      "\tTraining batch 23 Loss: 0.000327\n",
      "\tTraining batch 24 Loss: 0.000049\n",
      "\tTraining batch 25 Loss: 0.000065\n",
      "\tTraining batch 26 Loss: 0.003275\n",
      "\tTraining batch 27 Loss: 0.000003\n",
      "\tTraining batch 28 Loss: 0.000165\n",
      "\tTraining batch 29 Loss: 0.000005\n",
      "\tTraining batch 30 Loss: 0.000702\n",
      "\tTraining batch 31 Loss: 0.000197\n",
      "\tTraining batch 32 Loss: 0.000336\n",
      "\tTraining batch 33 Loss: 0.000404\n",
      "\tTraining batch 34 Loss: 0.001417\n",
      "Training set: Average loss: 0.000538\n",
      "Validation set: Average loss: 4.671462, Accuracy: 1356/1959 (69.22%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 48.0%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000007\n",
      "\tTraining batch 2 Loss: 0.000008\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000287\n",
      "\tTraining batch 5 Loss: 0.000051\n",
      "\tTraining batch 6 Loss: 0.000236\n",
      "\tTraining batch 7 Loss: 0.000005\n",
      "\tTraining batch 8 Loss: 0.000003\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000044\n",
      "\tTraining batch 11 Loss: 0.000006\n",
      "\tTraining batch 12 Loss: 0.000046\n",
      "\tTraining batch 13 Loss: 0.000183\n",
      "\tTraining batch 14 Loss: 0.000417\n",
      "\tTraining batch 15 Loss: 0.000028\n",
      "\tTraining batch 16 Loss: 0.000163\n",
      "\tTraining batch 17 Loss: 0.000156\n",
      "\tTraining batch 18 Loss: 0.000004\n",
      "\tTraining batch 19 Loss: 0.007166\n",
      "\tTraining batch 20 Loss: 0.000042\n",
      "\tTraining batch 21 Loss: 0.000152\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000257\n",
      "\tTraining batch 24 Loss: 0.000039\n",
      "\tTraining batch 25 Loss: 0.000042\n",
      "\tTraining batch 26 Loss: 0.003292\n",
      "\tTraining batch 27 Loss: 0.000002\n",
      "\tTraining batch 28 Loss: 0.000147\n",
      "\tTraining batch 29 Loss: 0.000005\n",
      "\tTraining batch 30 Loss: 0.000580\n",
      "\tTraining batch 31 Loss: 0.000144\n",
      "\tTraining batch 32 Loss: 0.000273\n",
      "\tTraining batch 33 Loss: 0.000335\n",
      "\tTraining batch 34 Loss: 0.001189\n",
      "\tTraining batch 35 Loss: 7.685709\n",
      "Training set: Average loss: 0.220029\n",
      "Validation set: Average loss: 4.143626, Accuracy: 1364/1959 (69.63%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000499\n",
      "\tTraining batch 2 Loss: 0.041307\n",
      "\tTraining batch 3 Loss: 0.220738\n",
      "\tTraining batch 4 Loss: 0.206350\n",
      "\tTraining batch 5 Loss: 0.455608\n",
      "\tTraining batch 6 Loss: 0.195313\n",
      "\tTraining batch 7 Loss: 0.213344\n",
      "\tTraining batch 8 Loss: 0.635496\n",
      "\tTraining batch 9 Loss: 0.108999\n",
      "\tTraining batch 10 Loss: 0.062051\n",
      "\tTraining batch 11 Loss: 0.043639\n",
      "\tTraining batch 12 Loss: 0.083369\n",
      "\tTraining batch 13 Loss: 0.108200\n",
      "\tTraining batch 14 Loss: 0.014683\n",
      "\tTraining batch 15 Loss: 0.057266\n",
      "\tTraining batch 16 Loss: 0.153463\n",
      "\tTraining batch 17 Loss: 0.030990\n",
      "\tTraining batch 18 Loss: 0.416975\n",
      "\tTraining batch 19 Loss: 0.070232\n",
      "\tTraining batch 20 Loss: 0.009274\n",
      "\tTraining batch 21 Loss: 0.114823\n",
      "\tTraining batch 22 Loss: 0.085219\n",
      "\tTraining batch 23 Loss: 0.040141\n",
      "\tTraining batch 24 Loss: 0.007363\n",
      "\tTraining batch 25 Loss: 0.074951\n",
      "\tTraining batch 26 Loss: 0.011365\n",
      "\tTraining batch 27 Loss: 0.160852\n",
      "\tTraining batch 28 Loss: 0.109952\n",
      "\tTraining batch 29 Loss: 0.003269\n",
      "\tTraining batch 30 Loss: 0.010333\n",
      "\tTraining batch 31 Loss: 0.277815\n",
      "\tTraining batch 32 Loss: 0.136864\n",
      "\tTraining batch 33 Loss: 0.017857\n",
      "\tTraining batch 34 Loss: 0.291597\n",
      "\tTraining batch 35 Loss: 0.232232\n",
      "Training set: Average loss: 0.134355\n",
      "Validation set: Average loss: 4.666123, Accuracy: 1330/1959 (67.89%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000479\n",
      "\tTraining batch 2 Loss: 0.754086\n",
      "\tTraining batch 3 Loss: 0.066412\n",
      "\tTraining batch 4 Loss: 0.017135\n",
      "\tTraining batch 5 Loss: 0.001436\n",
      "\tTraining batch 6 Loss: 0.002893\n",
      "\tTraining batch 7 Loss: 0.084419\n",
      "\tTraining batch 8 Loss: 0.000002\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.039644\n",
      "\tTraining batch 11 Loss: 0.022323\n",
      "\tTraining batch 12 Loss: 0.000166\n",
      "\tTraining batch 13 Loss: 0.003799\n",
      "\tTraining batch 14 Loss: 0.000534\n",
      "\tTraining batch 15 Loss: 0.000032\n",
      "\tTraining batch 16 Loss: 0.000011\n",
      "\tTraining batch 17 Loss: 0.030500\n",
      "\tTraining batch 18 Loss: 0.000004\n",
      "\tTraining batch 19 Loss: 0.008310\n",
      "\tTraining batch 20 Loss: 0.029126\n",
      "\tTraining batch 21 Loss: 0.035412\n",
      "\tTraining batch 22 Loss: 0.003591\n",
      "\tTraining batch 23 Loss: 0.000741\n",
      "\tTraining batch 24 Loss: 0.000142\n",
      "\tTraining batch 25 Loss: 0.000127\n",
      "\tTraining batch 26 Loss: 0.011444\n",
      "\tTraining batch 27 Loss: 0.041398\n",
      "\tTraining batch 28 Loss: 0.104637\n",
      "\tTraining batch 29 Loss: 0.005728\n",
      "\tTraining batch 30 Loss: 0.001150\n",
      "\tTraining batch 31 Loss: 0.001830\n",
      "\tTraining batch 32 Loss: 0.013518\n",
      "\tTraining batch 33 Loss: 0.008567\n",
      "\tTraining batch 34 Loss: 0.154516\n",
      "\tTraining batch 35 Loss: 0.239700\n",
      "Training set: Average loss: 0.048109\n",
      "Validation set: Average loss: 4.532273, Accuracy: 1327/1959 (67.74%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000438\n",
      "\tTraining batch 2 Loss: 0.000020\n",
      "\tTraining batch 3 Loss: 0.000001\n",
      "\tTraining batch 4 Loss: 0.000060\n",
      "\tTraining batch 5 Loss: 0.000034\n",
      "\tTraining batch 6 Loss: 0.000997\n",
      "\tTraining batch 7 Loss: 0.000044\n",
      "\tTraining batch 8 Loss: 0.000009\n",
      "\tTraining batch 9 Loss: 0.000042\n",
      "\tTraining batch 10 Loss: 0.000376\n",
      "\tTraining batch 11 Loss: 0.000002\n",
      "\tTraining batch 12 Loss: 0.000013\n",
      "\tTraining batch 13 Loss: 0.008437\n",
      "\tTraining batch 14 Loss: 0.001188\n",
      "\tTraining batch 15 Loss: 0.000040\n",
      "\tTraining batch 16 Loss: 0.000127\n",
      "\tTraining batch 17 Loss: 0.000237\n",
      "\tTraining batch 18 Loss: 0.000149\n",
      "\tTraining batch 19 Loss: 0.001582\n",
      "\tTraining batch 20 Loss: 0.000939\n",
      "\tTraining batch 21 Loss: 0.013120\n",
      "\tTraining batch 22 Loss: 0.002067\n",
      "\tTraining batch 23 Loss: 0.000715\n",
      "\tTraining batch 24 Loss: 0.003367\n",
      "\tTraining batch 25 Loss: 0.000041\n",
      "\tTraining batch 26 Loss: 0.012215\n",
      "\tTraining batch 27 Loss: 0.012174\n",
      "\tTraining batch 28 Loss: 0.009887\n",
      "\tTraining batch 29 Loss: 0.000290\n",
      "\tTraining batch 30 Loss: 0.002741\n",
      "\tTraining batch 31 Loss: 0.000784\n",
      "\tTraining batch 32 Loss: 0.001319\n",
      "\tTraining batch 33 Loss: 0.011851\n",
      "\tTraining batch 34 Loss: 0.079219\n",
      "\tTraining batch 35 Loss: 0.004892\n",
      "Training set: Average loss: 0.004841\n",
      "Validation set: Average loss: 4.751660, Accuracy: 1348/1959 (68.81%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000274\n",
      "\tTraining batch 2 Loss: 0.005599\n",
      "\tTraining batch 3 Loss: 0.000005\n",
      "\tTraining batch 4 Loss: 0.000367\n",
      "\tTraining batch 5 Loss: 0.000133\n",
      "\tTraining batch 6 Loss: 0.005568\n",
      "\tTraining batch 7 Loss: 0.000019\n",
      "\tTraining batch 8 Loss: 0.000008\n",
      "\tTraining batch 9 Loss: 0.000074\n",
      "\tTraining batch 10 Loss: 0.000105\n",
      "\tTraining batch 11 Loss: 0.000010\n",
      "\tTraining batch 12 Loss: 0.000019\n",
      "\tTraining batch 13 Loss: 0.001463\n",
      "\tTraining batch 14 Loss: 0.001242\n",
      "\tTraining batch 15 Loss: 0.000077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 16 Loss: 0.000063\n",
      "\tTraining batch 17 Loss: 0.000075\n",
      "\tTraining batch 18 Loss: 0.000048\n",
      "\tTraining batch 19 Loss: 0.000234\n",
      "\tTraining batch 20 Loss: 0.000545\n",
      "\tTraining batch 21 Loss: 0.001052\n",
      "\tTraining batch 22 Loss: 0.000009\n",
      "\tTraining batch 23 Loss: 0.000559\n",
      "\tTraining batch 24 Loss: 0.000541\n",
      "\tTraining batch 25 Loss: 0.000079\n",
      "\tTraining batch 26 Loss: 0.001820\n",
      "\tTraining batch 27 Loss: 0.001086\n",
      "\tTraining batch 28 Loss: 0.000696\n",
      "\tTraining batch 29 Loss: 0.000012\n",
      "\tTraining batch 30 Loss: 0.000412\n",
      "\tTraining batch 31 Loss: 0.000037\n",
      "\tTraining batch 32 Loss: 0.000129\n",
      "\tTraining batch 33 Loss: 0.003246\n",
      "\tTraining batch 34 Loss: 0.074322\n",
      "\tTraining batch 35 Loss: 0.003644\n",
      "Training set: Average loss: 0.002959\n",
      "Validation set: Average loss: 5.132573, Accuracy: 1336/1959 (68.20%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000045\n",
      "\tTraining batch 2 Loss: 0.000021\n",
      "\tTraining batch 3 Loss: 0.000001\n",
      "\tTraining batch 4 Loss: 0.000685\n",
      "\tTraining batch 5 Loss: 0.000057\n",
      "\tTraining batch 6 Loss: 0.000133\n",
      "\tTraining batch 7 Loss: 0.000013\n",
      "\tTraining batch 8 Loss: 0.000007\n",
      "\tTraining batch 9 Loss: 0.000039\n",
      "\tTraining batch 10 Loss: 0.000041\n",
      "\tTraining batch 11 Loss: 0.000006\n",
      "\tTraining batch 12 Loss: 0.000024\n",
      "\tTraining batch 13 Loss: 0.000607\n",
      "\tTraining batch 14 Loss: 0.000476\n",
      "\tTraining batch 15 Loss: 0.000038\n",
      "\tTraining batch 16 Loss: 0.000026\n",
      "\tTraining batch 17 Loss: 0.000072\n",
      "\tTraining batch 18 Loss: 0.000035\n",
      "\tTraining batch 19 Loss: 0.000267\n",
      "\tTraining batch 20 Loss: 0.000217\n",
      "\tTraining batch 21 Loss: 0.000725\n",
      "\tTraining batch 22 Loss: 0.000006\n",
      "\tTraining batch 23 Loss: 0.000331\n",
      "\tTraining batch 24 Loss: 0.000255\n",
      "\tTraining batch 25 Loss: 0.000022\n",
      "\tTraining batch 26 Loss: 0.001087\n",
      "\tTraining batch 27 Loss: 0.001010\n",
      "\tTraining batch 28 Loss: 0.000819\n",
      "\tTraining batch 29 Loss: 0.000010\n",
      "\tTraining batch 30 Loss: 0.000350\n",
      "\tTraining batch 31 Loss: 0.000058\n",
      "\tTraining batch 32 Loss: 0.000053\n",
      "\tTraining batch 33 Loss: 0.000774\n",
      "\tTraining batch 34 Loss: 0.067822\n",
      "\tTraining batch 35 Loss: 0.000172\n",
      "Training set: Average loss: 0.002180\n",
      "Validation set: Average loss: 5.155576, Accuracy: 1342/1959 (68.50%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000017\n",
      "\tTraining batch 2 Loss: 0.000016\n",
      "\tTraining batch 3 Loss: 0.000001\n",
      "\tTraining batch 4 Loss: 0.000263\n",
      "\tTraining batch 5 Loss: 0.000034\n",
      "\tTraining batch 6 Loss: 0.000099\n",
      "\tTraining batch 7 Loss: 0.000012\n",
      "\tTraining batch 8 Loss: 0.000004\n",
      "\tTraining batch 9 Loss: 0.000037\n",
      "\tTraining batch 10 Loss: 0.000034\n",
      "\tTraining batch 11 Loss: 0.000004\n",
      "\tTraining batch 12 Loss: 0.000021\n",
      "\tTraining batch 13 Loss: 0.000449\n",
      "\tTraining batch 14 Loss: 0.000360\n",
      "\tTraining batch 15 Loss: 0.000029\n",
      "\tTraining batch 16 Loss: 0.000022\n",
      "\tTraining batch 17 Loss: 0.000067\n",
      "\tTraining batch 18 Loss: 0.000032\n",
      "\tTraining batch 19 Loss: 0.000233\n",
      "\tTraining batch 20 Loss: 0.000113\n",
      "\tTraining batch 21 Loss: 0.000590\n",
      "\tTraining batch 22 Loss: 0.000005\n",
      "\tTraining batch 23 Loss: 0.000300\n",
      "\tTraining batch 24 Loss: 0.000220\n",
      "\tTraining batch 25 Loss: 0.000017\n",
      "\tTraining batch 26 Loss: 0.001027\n",
      "\tTraining batch 27 Loss: 0.000869\n",
      "\tTraining batch 28 Loss: 0.000673\n",
      "\tTraining batch 29 Loss: 0.000008\n",
      "\tTraining batch 30 Loss: 0.000296\n",
      "\tTraining batch 31 Loss: 0.000050\n",
      "\tTraining batch 32 Loss: 0.000047\n",
      "\tTraining batch 33 Loss: 0.000629\n",
      "\tTraining batch 34 Loss: 0.064159\n",
      "\tTraining batch 35 Loss: 0.000151\n",
      "Training set: Average loss: 0.002025\n",
      "Validation set: Average loss: 5.189416, Accuracy: 1344/1959 (68.61%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000015\n",
      "\tTraining batch 2 Loss: 0.000015\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000232\n",
      "\tTraining batch 5 Loss: 0.000030\n",
      "\tTraining batch 6 Loss: 0.000080\n",
      "\tTraining batch 7 Loss: 0.000010\n",
      "\tTraining batch 8 Loss: 0.000003\n",
      "\tTraining batch 9 Loss: 0.000028\n",
      "\tTraining batch 10 Loss: 0.000029\n",
      "\tTraining batch 11 Loss: 0.000004\n",
      "\tTraining batch 12 Loss: 0.000022\n",
      "\tTraining batch 13 Loss: 0.000370\n",
      "\tTraining batch 14 Loss: 0.000320\n",
      "\tTraining batch 15 Loss: 0.000026\n",
      "\tTraining batch 16 Loss: 0.000020\n",
      "\tTraining batch 17 Loss: 0.000062\n",
      "\tTraining batch 18 Loss: 0.000030\n",
      "\tTraining batch 19 Loss: 0.000199\n",
      "\tTraining batch 20 Loss: 0.000077\n",
      "\tTraining batch 21 Loss: 0.000473\n",
      "\tTraining batch 22 Loss: 0.000004\n",
      "\tTraining batch 23 Loss: 0.000279\n",
      "\tTraining batch 24 Loss: 0.000204\n",
      "\tTraining batch 25 Loss: 0.000015\n",
      "\tTraining batch 26 Loss: 0.000977\n",
      "\tTraining batch 27 Loss: 0.000728\n",
      "\tTraining batch 28 Loss: 0.000547\n",
      "\tTraining batch 29 Loss: 0.000007\n",
      "\tTraining batch 30 Loss: 0.000250\n",
      "\tTraining batch 31 Loss: 0.000040\n",
      "\tTraining batch 32 Loss: 0.000041\n",
      "\tTraining batch 33 Loss: 0.000527\n",
      "\tTraining batch 34 Loss: 0.060333\n",
      "\tTraining batch 35 Loss: 0.000138\n",
      "Training set: Average loss: 0.001890\n",
      "Validation set: Average loss: 5.225597, Accuracy: 1345/1959 (68.66%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000014\n",
      "\tTraining batch 2 Loss: 0.000014\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000212\n",
      "\tTraining batch 5 Loss: 0.000028\n",
      "\tTraining batch 6 Loss: 0.000066\n",
      "\tTraining batch 7 Loss: 0.000009\n",
      "\tTraining batch 8 Loss: 0.000003\n",
      "\tTraining batch 9 Loss: 0.000023\n",
      "\tTraining batch 10 Loss: 0.000025\n",
      "\tTraining batch 11 Loss: 0.000004\n",
      "\tTraining batch 12 Loss: 0.000023\n",
      "\tTraining batch 13 Loss: 0.000309\n",
      "\tTraining batch 14 Loss: 0.000288\n",
      "\tTraining batch 15 Loss: 0.000025\n",
      "\tTraining batch 16 Loss: 0.000019\n",
      "\tTraining batch 17 Loss: 0.000057\n",
      "\tTraining batch 18 Loss: 0.000027\n",
      "\tTraining batch 19 Loss: 0.000172\n",
      "\tTraining batch 20 Loss: 0.000055\n",
      "\tTraining batch 21 Loss: 0.000390\n",
      "\tTraining batch 22 Loss: 0.000003\n",
      "\tTraining batch 23 Loss: 0.000255\n",
      "\tTraining batch 24 Loss: 0.000192\n",
      "\tTraining batch 25 Loss: 0.000013\n",
      "\tTraining batch 26 Loss: 0.000960\n",
      "\tTraining batch 27 Loss: 0.000604\n",
      "\tTraining batch 28 Loss: 0.000446\n",
      "\tTraining batch 29 Loss: 0.000006\n",
      "\tTraining batch 30 Loss: 0.000210\n",
      "\tTraining batch 31 Loss: 0.000032\n",
      "\tTraining batch 32 Loss: 0.000037\n",
      "\tTraining batch 33 Loss: 0.000457\n",
      "\tTraining batch 34 Loss: 0.056164\n",
      "\tTraining batch 35 Loss: 0.000128\n",
      "Training set: Average loss: 0.001751\n",
      "Validation set: Average loss: 5.264622, Accuracy: 1343/1959 (68.56%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000014\n",
      "\tTraining batch 2 Loss: 0.000013\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000194\n",
      "\tTraining batch 5 Loss: 0.000027\n",
      "\tTraining batch 6 Loss: 0.000054\n",
      "\tTraining batch 7 Loss: 0.000008\n",
      "\tTraining batch 8 Loss: 0.000003\n",
      "\tTraining batch 9 Loss: 0.000020\n",
      "\tTraining batch 10 Loss: 0.000022\n",
      "\tTraining batch 11 Loss: 0.000004\n",
      "\tTraining batch 12 Loss: 0.000025\n",
      "\tTraining batch 13 Loss: 0.000259\n",
      "\tTraining batch 14 Loss: 0.000260\n",
      "\tTraining batch 15 Loss: 0.000025\n",
      "\tTraining batch 16 Loss: 0.000018\n",
      "\tTraining batch 17 Loss: 0.000053\n",
      "\tTraining batch 18 Loss: 0.000025\n",
      "\tTraining batch 19 Loss: 0.000144\n",
      "\tTraining batch 20 Loss: 0.000040\n",
      "\tTraining batch 21 Loss: 0.000319\n",
      "\tTraining batch 22 Loss: 0.000003\n",
      "\tTraining batch 23 Loss: 0.000234\n",
      "\tTraining batch 24 Loss: 0.000179\n",
      "\tTraining batch 25 Loss: 0.000011\n",
      "\tTraining batch 26 Loss: 0.000964\n",
      "\tTraining batch 27 Loss: 0.000495\n",
      "\tTraining batch 28 Loss: 0.000357\n",
      "\tTraining batch 29 Loss: 0.000005\n",
      "\tTraining batch 30 Loss: 0.000175\n",
      "\tTraining batch 31 Loss: 0.000026\n",
      "\tTraining batch 32 Loss: 0.000032\n",
      "\tTraining batch 33 Loss: 0.000405\n",
      "\tTraining batch 34 Loss: 0.051536\n",
      "\tTraining batch 35 Loss: 0.000119\n",
      "Training set: Average loss: 0.001602\n",
      "Validation set: Average loss: 5.305871, Accuracy: 1341/1959 (68.45%)\n",
      "\n",
      "Epoch: 11\n",
      "\tTraining batch 1 Loss: 0.000013\n",
      "\tTraining batch 2 Loss: 0.000011\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000178\n",
      "\tTraining batch 5 Loss: 0.000027\n",
      "\tTraining batch 6 Loss: 0.000045\n",
      "\tTraining batch 7 Loss: 0.000007\n",
      "\tTraining batch 8 Loss: 0.000002\n",
      "\tTraining batch 9 Loss: 0.000019\n",
      "\tTraining batch 10 Loss: 0.000019\n",
      "\tTraining batch 11 Loss: 0.000004\n",
      "\tTraining batch 12 Loss: 0.000026\n",
      "\tTraining batch 13 Loss: 0.000213\n",
      "\tTraining batch 14 Loss: 0.000234\n",
      "\tTraining batch 15 Loss: 0.000025\n",
      "\tTraining batch 16 Loss: 0.000016\n",
      "\tTraining batch 17 Loss: 0.000049\n",
      "\tTraining batch 18 Loss: 0.000023\n",
      "\tTraining batch 19 Loss: 0.000123\n",
      "\tTraining batch 20 Loss: 0.000031\n",
      "\tTraining batch 21 Loss: 0.000265\n",
      "\tTraining batch 22 Loss: 0.000003\n",
      "\tTraining batch 23 Loss: 0.000215\n",
      "\tTraining batch 24 Loss: 0.000163\n",
      "\tTraining batch 25 Loss: 0.000009\n",
      "\tTraining batch 26 Loss: 0.000959\n",
      "\tTraining batch 27 Loss: 0.000405\n",
      "\tTraining batch 28 Loss: 0.000276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 29 Loss: 0.000004\n",
      "\tTraining batch 30 Loss: 0.000145\n",
      "\tTraining batch 31 Loss: 0.000023\n",
      "\tTraining batch 32 Loss: 0.000029\n",
      "\tTraining batch 33 Loss: 0.000363\n",
      "\tTraining batch 34 Loss: 0.046321\n",
      "\tTraining batch 35 Loss: 0.000111\n",
      "Training set: Average loss: 0.001439\n",
      "Validation set: Average loss: 5.351405, Accuracy: 1337/1959 (68.25%)\n",
      "\n",
      "Epoch: 12\n",
      "\tTraining batch 1 Loss: 0.000012\n",
      "\tTraining batch 2 Loss: 0.000011\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000167\n",
      "\tTraining batch 5 Loss: 0.000028\n",
      "\tTraining batch 6 Loss: 0.000038\n",
      "\tTraining batch 7 Loss: 0.000007\n",
      "\tTraining batch 8 Loss: 0.000002\n",
      "\tTraining batch 9 Loss: 0.000018\n",
      "\tTraining batch 10 Loss: 0.000017\n",
      "\tTraining batch 11 Loss: 0.000004\n",
      "\tTraining batch 12 Loss: 0.000028\n",
      "\tTraining batch 13 Loss: 0.000177\n",
      "\tTraining batch 14 Loss: 0.000215\n",
      "\tTraining batch 15 Loss: 0.000028\n",
      "\tTraining batch 16 Loss: 0.000014\n",
      "\tTraining batch 17 Loss: 0.000046\n",
      "\tTraining batch 18 Loss: 0.000020\n",
      "\tTraining batch 19 Loss: 0.000104\n",
      "\tTraining batch 20 Loss: 0.000024\n",
      "\tTraining batch 21 Loss: 0.000219\n",
      "\tTraining batch 22 Loss: 0.000002\n",
      "\tTraining batch 23 Loss: 0.000200\n",
      "\tTraining batch 24 Loss: 0.000151\n",
      "\tTraining batch 25 Loss: 0.000008\n",
      "\tTraining batch 26 Loss: 0.001000\n",
      "\tTraining batch 27 Loss: 0.000333\n",
      "\tTraining batch 28 Loss: 0.000209\n",
      "\tTraining batch 29 Loss: 0.000004\n",
      "\tTraining batch 30 Loss: 0.000120\n",
      "\tTraining batch 31 Loss: 0.000019\n",
      "\tTraining batch 32 Loss: 0.000026\n",
      "\tTraining batch 33 Loss: 0.000329\n",
      "\tTraining batch 34 Loss: 0.040455\n",
      "\tTraining batch 35 Loss: 0.000105\n",
      "Training set: Average loss: 0.001261\n",
      "Validation set: Average loss: 5.401873, Accuracy: 1337/1959 (68.25%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 44.75%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000011\n",
      "\tTraining batch 2 Loss: 0.000010\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000157\n",
      "\tTraining batch 5 Loss: 0.000029\n",
      "\tTraining batch 6 Loss: 0.000031\n",
      "\tTraining batch 7 Loss: 0.000006\n",
      "\tTraining batch 8 Loss: 0.000002\n",
      "\tTraining batch 9 Loss: 0.000018\n",
      "\tTraining batch 10 Loss: 0.000015\n",
      "\tTraining batch 11 Loss: 0.000004\n",
      "\tTraining batch 12 Loss: 0.000028\n",
      "\tTraining batch 13 Loss: 0.000146\n",
      "\tTraining batch 14 Loss: 0.000199\n",
      "\tTraining batch 15 Loss: 0.000031\n",
      "\tTraining batch 16 Loss: 0.000012\n",
      "\tTraining batch 17 Loss: 0.000044\n",
      "\tTraining batch 18 Loss: 0.000018\n",
      "\tTraining batch 19 Loss: 0.000088\n",
      "\tTraining batch 20 Loss: 0.000019\n",
      "\tTraining batch 21 Loss: 0.000185\n",
      "\tTraining batch 22 Loss: 0.000002\n",
      "\tTraining batch 23 Loss: 0.000187\n",
      "\tTraining batch 24 Loss: 0.000139\n",
      "\tTraining batch 25 Loss: 0.000006\n",
      "\tTraining batch 26 Loss: 0.001046\n",
      "\tTraining batch 27 Loss: 0.000274\n",
      "\tTraining batch 28 Loss: 0.000156\n",
      "\tTraining batch 29 Loss: 0.000003\n",
      "\tTraining batch 30 Loss: 0.000098\n",
      "\tTraining batch 31 Loss: 0.000016\n",
      "\tTraining batch 32 Loss: 0.000023\n",
      "\tTraining batch 33 Loss: 0.000302\n",
      "\tTraining batch 34 Loss: 0.034011\n",
      "\tTraining batch 35 Loss: 0.000100\n",
      "\tTraining batch 36 Loss: 6.168657\n",
      "Training set: Average loss: 0.172391\n",
      "Validation set: Average loss: 4.670228, Accuracy: 1333/1959 (68.04%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000012\n",
      "\tTraining batch 2 Loss: 0.000149\n",
      "\tTraining batch 3 Loss: 0.000320\n",
      "\tTraining batch 4 Loss: 0.001876\n",
      "\tTraining batch 5 Loss: 0.001968\n",
      "\tTraining batch 6 Loss: 0.006551\n",
      "\tTraining batch 7 Loss: 0.012123\n",
      "\tTraining batch 8 Loss: 0.076296\n",
      "\tTraining batch 9 Loss: 0.015289\n",
      "\tTraining batch 10 Loss: 0.043946\n",
      "\tTraining batch 11 Loss: 0.035400\n",
      "\tTraining batch 12 Loss: 0.193974\n",
      "\tTraining batch 13 Loss: 0.233901\n",
      "\tTraining batch 14 Loss: 0.087542\n",
      "\tTraining batch 15 Loss: 0.142170\n",
      "\tTraining batch 16 Loss: 0.014783\n",
      "\tTraining batch 17 Loss: 0.070148\n",
      "\tTraining batch 18 Loss: 0.078319\n",
      "\tTraining batch 19 Loss: 0.057551\n",
      "\tTraining batch 20 Loss: 0.002999\n",
      "\tTraining batch 21 Loss: 0.091364\n",
      "\tTraining batch 22 Loss: 0.006297\n",
      "\tTraining batch 23 Loss: 0.023549\n",
      "\tTraining batch 24 Loss: 0.001708\n",
      "\tTraining batch 25 Loss: 0.006311\n",
      "\tTraining batch 26 Loss: 0.013790\n",
      "\tTraining batch 27 Loss: 0.033341\n",
      "\tTraining batch 28 Loss: 0.064655\n",
      "\tTraining batch 29 Loss: 0.002287\n",
      "\tTraining batch 30 Loss: 0.102804\n",
      "\tTraining batch 31 Loss: 0.031435\n",
      "\tTraining batch 32 Loss: 0.031396\n",
      "\tTraining batch 33 Loss: 0.011694\n",
      "\tTraining batch 34 Loss: 0.193499\n",
      "\tTraining batch 35 Loss: 0.065846\n",
      "\tTraining batch 36 Loss: 0.115303\n",
      "Training set: Average loss: 0.051961\n",
      "Validation set: Average loss: 3.858136, Accuracy: 1307/1959 (66.72%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.001052\n",
      "\tTraining batch 2 Loss: 0.000426\n",
      "\tTraining batch 3 Loss: 0.000074\n",
      "\tTraining batch 4 Loss: 0.000411\n",
      "\tTraining batch 5 Loss: 0.000296\n",
      "\tTraining batch 6 Loss: 0.000542\n",
      "\tTraining batch 7 Loss: 0.000034\n",
      "\tTraining batch 8 Loss: 0.000010\n",
      "\tTraining batch 9 Loss: 0.019838\n",
      "\tTraining batch 10 Loss: 0.000429\n",
      "\tTraining batch 11 Loss: 0.000015\n",
      "\tTraining batch 12 Loss: 0.000630\n",
      "\tTraining batch 13 Loss: 0.001415\n",
      "\tTraining batch 14 Loss: 0.001244\n",
      "\tTraining batch 15 Loss: 0.000065\n",
      "\tTraining batch 16 Loss: 0.001644\n",
      "\tTraining batch 17 Loss: 0.002887\n",
      "\tTraining batch 18 Loss: 0.000025\n",
      "\tTraining batch 19 Loss: 0.002948\n",
      "\tTraining batch 20 Loss: 0.001377\n",
      "\tTraining batch 21 Loss: 0.037873\n",
      "\tTraining batch 22 Loss: 0.011988\n",
      "\tTraining batch 23 Loss: 0.002258\n",
      "\tTraining batch 24 Loss: 0.064926\n",
      "\tTraining batch 25 Loss: 0.000152\n",
      "\tTraining batch 26 Loss: 0.042162\n",
      "\tTraining batch 27 Loss: 0.065181\n",
      "\tTraining batch 28 Loss: 0.038661\n",
      "\tTraining batch 29 Loss: 0.000096\n",
      "\tTraining batch 30 Loss: 0.029831\n",
      "\tTraining batch 31 Loss: 0.000097\n",
      "\tTraining batch 32 Loss: 0.000078\n",
      "\tTraining batch 33 Loss: 0.001852\n",
      "\tTraining batch 34 Loss: 0.007811\n",
      "\tTraining batch 35 Loss: 0.000063\n",
      "\tTraining batch 36 Loss: 0.210993\n",
      "Training set: Average loss: 0.015261\n",
      "Validation set: Average loss: 5.705872, Accuracy: 1296/1959 (66.16%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.029515\n",
      "\tTraining batch 2 Loss: 0.000106\n",
      "\tTraining batch 3 Loss: 0.000581\n",
      "\tTraining batch 4 Loss: 0.030292\n",
      "\tTraining batch 5 Loss: 0.000166\n",
      "\tTraining batch 6 Loss: 0.015043\n",
      "\tTraining batch 7 Loss: 0.000028\n",
      "\tTraining batch 8 Loss: 0.000002\n",
      "\tTraining batch 9 Loss: 0.000001\n",
      "\tTraining batch 10 Loss: 0.000012\n",
      "\tTraining batch 11 Loss: 0.000001\n",
      "\tTraining batch 12 Loss: 0.000003\n",
      "\tTraining batch 13 Loss: 0.000008\n",
      "\tTraining batch 14 Loss: 0.000644\n",
      "\tTraining batch 15 Loss: 0.000228\n",
      "\tTraining batch 16 Loss: 0.000185\n",
      "\tTraining batch 17 Loss: 0.000905\n",
      "\tTraining batch 18 Loss: 0.000003\n",
      "\tTraining batch 19 Loss: 0.000249\n",
      "\tTraining batch 20 Loss: 0.000098\n",
      "\tTraining batch 21 Loss: 0.002647\n",
      "\tTraining batch 22 Loss: 0.000032\n",
      "\tTraining batch 23 Loss: 0.004535\n",
      "\tTraining batch 24 Loss: 0.000085\n",
      "\tTraining batch 25 Loss: 0.000014\n",
      "\tTraining batch 26 Loss: 0.005214\n",
      "\tTraining batch 27 Loss: 0.000752\n",
      "\tTraining batch 28 Loss: 0.002407\n",
      "\tTraining batch 29 Loss: 0.000013\n",
      "\tTraining batch 30 Loss: 0.002331\n",
      "\tTraining batch 31 Loss: 0.003447\n",
      "\tTraining batch 32 Loss: 0.195832\n",
      "\tTraining batch 33 Loss: 0.001677\n",
      "\tTraining batch 34 Loss: 0.012553\n",
      "\tTraining batch 35 Loss: 0.001085\n",
      "\tTraining batch 36 Loss: 0.069056\n",
      "Training set: Average loss: 0.010549\n",
      "Validation set: Average loss: 5.257158, Accuracy: 1314/1959 (67.08%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000004\n",
      "\tTraining batch 2 Loss: 0.000251\n",
      "\tTraining batch 3 Loss: 0.000022\n",
      "\tTraining batch 4 Loss: 0.000099\n",
      "\tTraining batch 5 Loss: 0.000004\n",
      "\tTraining batch 6 Loss: 0.000009\n",
      "\tTraining batch 7 Loss: 0.000090\n",
      "\tTraining batch 8 Loss: 0.000012\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000005\n",
      "\tTraining batch 11 Loss: 0.000001\n",
      "\tTraining batch 12 Loss: 0.000002\n",
      "\tTraining batch 13 Loss: 0.088564\n",
      "\tTraining batch 14 Loss: 0.001317\n",
      "\tTraining batch 15 Loss: 0.063552\n",
      "\tTraining batch 16 Loss: 0.000013\n",
      "\tTraining batch 17 Loss: 0.000010\n",
      "\tTraining batch 18 Loss: 0.000208\n",
      "\tTraining batch 19 Loss: 0.000056\n",
      "\tTraining batch 20 Loss: 0.000060\n",
      "\tTraining batch 21 Loss: 0.000370\n",
      "\tTraining batch 22 Loss: 0.000488\n",
      "\tTraining batch 23 Loss: 0.000070\n",
      "\tTraining batch 24 Loss: 0.002727\n",
      "\tTraining batch 25 Loss: 0.000102\n",
      "\tTraining batch 26 Loss: 0.001456\n",
      "\tTraining batch 27 Loss: 0.000690\n",
      "\tTraining batch 28 Loss: 0.000236\n",
      "\tTraining batch 29 Loss: 0.000006\n",
      "\tTraining batch 30 Loss: 0.000081\n",
      "\tTraining batch 31 Loss: 0.001529\n",
      "\tTraining batch 32 Loss: 0.005210\n",
      "\tTraining batch 33 Loss: 0.040162\n",
      "\tTraining batch 34 Loss: 0.008648\n",
      "\tTraining batch 35 Loss: 0.000093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 36 Loss: 0.006904\n",
      "Training set: Average loss: 0.006196\n",
      "Validation set: Average loss: 5.660363, Accuracy: 1336/1959 (68.20%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000015\n",
      "\tTraining batch 2 Loss: 0.000185\n",
      "\tTraining batch 3 Loss: 0.000005\n",
      "\tTraining batch 4 Loss: 0.000080\n",
      "\tTraining batch 5 Loss: 0.000004\n",
      "\tTraining batch 6 Loss: 0.000633\n",
      "\tTraining batch 7 Loss: 0.000086\n",
      "\tTraining batch 8 Loss: 0.000002\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000021\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000001\n",
      "\tTraining batch 13 Loss: 0.000029\n",
      "\tTraining batch 14 Loss: 0.000758\n",
      "\tTraining batch 15 Loss: 0.000002\n",
      "\tTraining batch 16 Loss: 0.000026\n",
      "\tTraining batch 17 Loss: 0.000115\n",
      "\tTraining batch 18 Loss: 0.000010\n",
      "\tTraining batch 19 Loss: 0.000101\n",
      "\tTraining batch 20 Loss: 0.000056\n",
      "\tTraining batch 21 Loss: 0.000655\n",
      "\tTraining batch 22 Loss: 0.000001\n",
      "\tTraining batch 23 Loss: 0.000063\n",
      "\tTraining batch 24 Loss: 0.000646\n",
      "\tTraining batch 25 Loss: 0.000023\n",
      "\tTraining batch 26 Loss: 0.003413\n",
      "\tTraining batch 27 Loss: 0.000043\n",
      "\tTraining batch 28 Loss: 0.000471\n",
      "\tTraining batch 29 Loss: 0.000005\n",
      "\tTraining batch 30 Loss: 0.000027\n",
      "\tTraining batch 31 Loss: 0.000044\n",
      "\tTraining batch 32 Loss: 0.000002\n",
      "\tTraining batch 33 Loss: 0.000108\n",
      "\tTraining batch 34 Loss: 0.003513\n",
      "\tTraining batch 35 Loss: 0.000085\n",
      "\tTraining batch 36 Loss: 0.000566\n",
      "Training set: Average loss: 0.000328\n",
      "Validation set: Average loss: 5.841219, Accuracy: 1338/1959 (68.30%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000007\n",
      "\tTraining batch 2 Loss: 0.000160\n",
      "\tTraining batch 3 Loss: 0.000002\n",
      "\tTraining batch 4 Loss: 0.000105\n",
      "\tTraining batch 5 Loss: 0.000006\n",
      "\tTraining batch 6 Loss: 0.000005\n",
      "\tTraining batch 7 Loss: 0.000058\n",
      "\tTraining batch 8 Loss: 0.000002\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000019\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000002\n",
      "\tTraining batch 13 Loss: 0.000030\n",
      "\tTraining batch 14 Loss: 0.000661\n",
      "\tTraining batch 15 Loss: 0.000002\n",
      "\tTraining batch 16 Loss: 0.000016\n",
      "\tTraining batch 17 Loss: 0.000102\n",
      "\tTraining batch 18 Loss: 0.000009\n",
      "\tTraining batch 19 Loss: 0.000087\n",
      "\tTraining batch 20 Loss: 0.000052\n",
      "\tTraining batch 21 Loss: 0.000491\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000054\n",
      "\tTraining batch 24 Loss: 0.000141\n",
      "\tTraining batch 25 Loss: 0.000019\n",
      "\tTraining batch 26 Loss: 0.002418\n",
      "\tTraining batch 27 Loss: 0.000037\n",
      "\tTraining batch 28 Loss: 0.000322\n",
      "\tTraining batch 29 Loss: 0.000004\n",
      "\tTraining batch 30 Loss: 0.000025\n",
      "\tTraining batch 31 Loss: 0.000052\n",
      "\tTraining batch 32 Loss: 0.000002\n",
      "\tTraining batch 33 Loss: 0.000096\n",
      "\tTraining batch 34 Loss: 0.002925\n",
      "\tTraining batch 35 Loss: 0.000080\n",
      "\tTraining batch 36 Loss: 0.000476\n",
      "Training set: Average loss: 0.000235\n",
      "Validation set: Average loss: 5.863356, Accuracy: 1340/1959 (68.40%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000005\n",
      "\tTraining batch 2 Loss: 0.000125\n",
      "\tTraining batch 3 Loss: 0.000002\n",
      "\tTraining batch 4 Loss: 0.000080\n",
      "\tTraining batch 5 Loss: 0.000005\n",
      "\tTraining batch 6 Loss: 0.000004\n",
      "\tTraining batch 7 Loss: 0.000048\n",
      "\tTraining batch 8 Loss: 0.000002\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000017\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000002\n",
      "\tTraining batch 13 Loss: 0.000027\n",
      "\tTraining batch 14 Loss: 0.000510\n",
      "\tTraining batch 15 Loss: 0.000002\n",
      "\tTraining batch 16 Loss: 0.000014\n",
      "\tTraining batch 17 Loss: 0.000075\n",
      "\tTraining batch 18 Loss: 0.000009\n",
      "\tTraining batch 19 Loss: 0.000072\n",
      "\tTraining batch 20 Loss: 0.000045\n",
      "\tTraining batch 21 Loss: 0.000379\n",
      "\tTraining batch 22 Loss: 0.000001\n",
      "\tTraining batch 23 Loss: 0.000048\n",
      "\tTraining batch 24 Loss: 0.000101\n",
      "\tTraining batch 25 Loss: 0.000018\n",
      "\tTraining batch 26 Loss: 0.001855\n",
      "\tTraining batch 27 Loss: 0.000034\n",
      "\tTraining batch 28 Loss: 0.000246\n",
      "\tTraining batch 29 Loss: 0.000004\n",
      "\tTraining batch 30 Loss: 0.000024\n",
      "\tTraining batch 31 Loss: 0.000057\n",
      "\tTraining batch 32 Loss: 0.000001\n",
      "\tTraining batch 33 Loss: 0.000090\n",
      "\tTraining batch 34 Loss: 0.002492\n",
      "\tTraining batch 35 Loss: 0.000074\n",
      "\tTraining batch 36 Loss: 0.000404\n",
      "Training set: Average loss: 0.000191\n",
      "Validation set: Average loss: 5.883824, Accuracy: 1338/1959 (68.30%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000004\n",
      "\tTraining batch 2 Loss: 0.000106\n",
      "\tTraining batch 3 Loss: 0.000002\n",
      "\tTraining batch 4 Loss: 0.000063\n",
      "\tTraining batch 5 Loss: 0.000005\n",
      "\tTraining batch 6 Loss: 0.000004\n",
      "\tTraining batch 7 Loss: 0.000042\n",
      "\tTraining batch 8 Loss: 0.000001\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000015\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000001\n",
      "\tTraining batch 13 Loss: 0.000025\n",
      "\tTraining batch 14 Loss: 0.000411\n",
      "\tTraining batch 15 Loss: 0.000002\n",
      "\tTraining batch 16 Loss: 0.000012\n",
      "\tTraining batch 17 Loss: 0.000059\n",
      "\tTraining batch 18 Loss: 0.000009\n",
      "\tTraining batch 19 Loss: 0.000061\n",
      "\tTraining batch 20 Loss: 0.000040\n",
      "\tTraining batch 21 Loss: 0.000309\n",
      "\tTraining batch 22 Loss: 0.000001\n",
      "\tTraining batch 23 Loss: 0.000043\n",
      "\tTraining batch 24 Loss: 0.000081\n",
      "\tTraining batch 25 Loss: 0.000017\n",
      "\tTraining batch 26 Loss: 0.001520\n",
      "\tTraining batch 27 Loss: 0.000030\n",
      "\tTraining batch 28 Loss: 0.000201\n",
      "\tTraining batch 29 Loss: 0.000003\n",
      "\tTraining batch 30 Loss: 0.000023\n",
      "\tTraining batch 31 Loss: 0.000058\n",
      "\tTraining batch 32 Loss: 0.000001\n",
      "\tTraining batch 33 Loss: 0.000085\n",
      "\tTraining batch 34 Loss: 0.002152\n",
      "\tTraining batch 35 Loss: 0.000068\n",
      "\tTraining batch 36 Loss: 0.000349\n",
      "Training set: Average loss: 0.000161\n",
      "Validation set: Average loss: 5.903701, Accuracy: 1338/1959 (68.30%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 47.5%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000004\n",
      "\tTraining batch 2 Loss: 0.000092\n",
      "\tTraining batch 3 Loss: 0.000001\n",
      "\tTraining batch 4 Loss: 0.000052\n",
      "\tTraining batch 5 Loss: 0.000004\n",
      "\tTraining batch 6 Loss: 0.000004\n",
      "\tTraining batch 7 Loss: 0.000037\n",
      "\tTraining batch 8 Loss: 0.000001\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000014\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000001\n",
      "\tTraining batch 13 Loss: 0.000024\n",
      "\tTraining batch 14 Loss: 0.000342\n",
      "\tTraining batch 15 Loss: 0.000002\n",
      "\tTraining batch 16 Loss: 0.000010\n",
      "\tTraining batch 17 Loss: 0.000049\n",
      "\tTraining batch 18 Loss: 0.000009\n",
      "\tTraining batch 19 Loss: 0.000054\n",
      "\tTraining batch 20 Loss: 0.000036\n",
      "\tTraining batch 21 Loss: 0.000261\n",
      "\tTraining batch 22 Loss: 0.000001\n",
      "\tTraining batch 23 Loss: 0.000039\n",
      "\tTraining batch 24 Loss: 0.000069\n",
      "\tTraining batch 25 Loss: 0.000016\n",
      "\tTraining batch 26 Loss: 0.001295\n",
      "\tTraining batch 27 Loss: 0.000028\n",
      "\tTraining batch 28 Loss: 0.000172\n",
      "\tTraining batch 29 Loss: 0.000002\n",
      "\tTraining batch 30 Loss: 0.000022\n",
      "\tTraining batch 31 Loss: 0.000059\n",
      "\tTraining batch 32 Loss: 0.000001\n",
      "\tTraining batch 33 Loss: 0.000079\n",
      "\tTraining batch 34 Loss: 0.001882\n",
      "\tTraining batch 35 Loss: 0.000064\n",
      "\tTraining batch 36 Loss: 0.000307\n",
      "\tTraining batch 37 Loss: 7.646842\n",
      "Training set: Average loss: 0.206807\n",
      "Validation set: Average loss: 5.099987, Accuracy: 1327/1959 (67.74%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000013\n",
      "\tTraining batch 2 Loss: 0.001000\n",
      "\tTraining batch 3 Loss: 0.000083\n",
      "\tTraining batch 4 Loss: 0.000467\n",
      "\tTraining batch 5 Loss: 0.005466\n",
      "\tTraining batch 6 Loss: 0.052330\n",
      "\tTraining batch 7 Loss: 0.000929\n",
      "\tTraining batch 8 Loss: 0.107238\n",
      "\tTraining batch 9 Loss: 0.028885\n",
      "\tTraining batch 10 Loss: 0.389253\n",
      "\tTraining batch 11 Loss: 1.020851\n",
      "\tTraining batch 12 Loss: 0.105774\n",
      "\tTraining batch 13 Loss: 0.092168\n",
      "\tTraining batch 14 Loss: 0.047327\n",
      "\tTraining batch 15 Loss: 0.101523\n",
      "\tTraining batch 16 Loss: 0.016819\n",
      "\tTraining batch 17 Loss: 0.019137\n",
      "\tTraining batch 18 Loss: 0.072710\n",
      "\tTraining batch 19 Loss: 0.023117\n",
      "\tTraining batch 20 Loss: 0.272865\n",
      "\tTraining batch 21 Loss: 0.135927\n",
      "\tTraining batch 22 Loss: 0.453112\n",
      "\tTraining batch 23 Loss: 0.014470\n",
      "\tTraining batch 24 Loss: 0.168044\n",
      "\tTraining batch 25 Loss: 0.000280\n",
      "\tTraining batch 26 Loss: 0.024922\n",
      "\tTraining batch 27 Loss: 0.010466\n",
      "\tTraining batch 28 Loss: 0.366297\n",
      "\tTraining batch 29 Loss: 0.043007\n",
      "\tTraining batch 30 Loss: 0.183034\n",
      "\tTraining batch 31 Loss: 0.085111\n",
      "\tTraining batch 32 Loss: 0.077488\n",
      "\tTraining batch 33 Loss: 0.098910\n",
      "\tTraining batch 34 Loss: 0.009360\n",
      "\tTraining batch 35 Loss: 0.299951\n",
      "\tTraining batch 36 Loss: 0.146704\n",
      "\tTraining batch 37 Loss: 0.499274\n",
      "Training set: Average loss: 0.134441\n",
      "Validation set: Average loss: 4.542769, Accuracy: 1323/1959 (67.53%)\n",
      "\n",
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 1 Loss: 0.000441\n",
      "\tTraining batch 2 Loss: 0.000910\n",
      "\tTraining batch 3 Loss: 0.000127\n",
      "\tTraining batch 4 Loss: 0.000317\n",
      "\tTraining batch 5 Loss: 0.006807\n",
      "\tTraining batch 6 Loss: 0.000024\n",
      "\tTraining batch 7 Loss: 0.000058\n",
      "\tTraining batch 8 Loss: 0.000004\n",
      "\tTraining batch 9 Loss: 0.000003\n",
      "\tTraining batch 10 Loss: 0.001246\n",
      "\tTraining batch 11 Loss: 0.191562\n",
      "\tTraining batch 12 Loss: 0.033340\n",
      "\tTraining batch 13 Loss: 0.031685\n",
      "\tTraining batch 14 Loss: 0.001042\n",
      "\tTraining batch 15 Loss: 0.038747\n",
      "\tTraining batch 16 Loss: 0.007111\n",
      "\tTraining batch 17 Loss: 0.000230\n",
      "\tTraining batch 18 Loss: 0.000223\n",
      "\tTraining batch 19 Loss: 0.031035\n",
      "\tTraining batch 20 Loss: 0.470018\n",
      "\tTraining batch 21 Loss: 0.004582\n",
      "\tTraining batch 22 Loss: 0.019989\n",
      "\tTraining batch 23 Loss: 0.001403\n",
      "\tTraining batch 24 Loss: 0.007667\n",
      "\tTraining batch 25 Loss: 0.007989\n",
      "\tTraining batch 26 Loss: 0.002673\n",
      "\tTraining batch 27 Loss: 0.000811\n",
      "\tTraining batch 28 Loss: 0.000312\n",
      "\tTraining batch 29 Loss: 0.000029\n",
      "\tTraining batch 30 Loss: 0.001631\n",
      "\tTraining batch 31 Loss: 0.002013\n",
      "\tTraining batch 32 Loss: 0.137755\n",
      "\tTraining batch 33 Loss: 0.001149\n",
      "\tTraining batch 34 Loss: 0.025730\n",
      "\tTraining batch 35 Loss: 0.013148\n",
      "\tTraining batch 36 Loss: 0.016899\n",
      "\tTraining batch 37 Loss: 0.024464\n",
      "Training set: Average loss: 0.029275\n",
      "Validation set: Average loss: 4.662707, Accuracy: 1329/1959 (67.84%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.015240\n",
      "\tTraining batch 2 Loss: 0.000051\n",
      "\tTraining batch 3 Loss: 0.001852\n",
      "\tTraining batch 4 Loss: 0.006939\n",
      "\tTraining batch 5 Loss: 0.144152\n",
      "\tTraining batch 6 Loss: 0.000003\n",
      "\tTraining batch 7 Loss: 0.000473\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000004\n",
      "\tTraining batch 11 Loss: 0.129047\n",
      "\tTraining batch 12 Loss: 0.000105\n",
      "\tTraining batch 13 Loss: 0.000144\n",
      "\tTraining batch 14 Loss: 0.000667\n",
      "\tTraining batch 15 Loss: 0.002092\n",
      "\tTraining batch 16 Loss: 0.000002\n",
      "\tTraining batch 17 Loss: 0.020317\n",
      "\tTraining batch 18 Loss: 0.000011\n",
      "\tTraining batch 19 Loss: 0.009832\n",
      "\tTraining batch 20 Loss: 0.000035\n",
      "\tTraining batch 21 Loss: 0.000032\n",
      "\tTraining batch 22 Loss: 0.000091\n",
      "\tTraining batch 23 Loss: 0.000151\n",
      "\tTraining batch 24 Loss: 0.157219\n",
      "\tTraining batch 25 Loss: 0.000397\n",
      "\tTraining batch 26 Loss: 0.000674\n",
      "\tTraining batch 27 Loss: 0.031178\n",
      "\tTraining batch 28 Loss: 0.005946\n",
      "\tTraining batch 29 Loss: 0.000172\n",
      "\tTraining batch 30 Loss: 0.000299\n",
      "\tTraining batch 31 Loss: 0.000013\n",
      "\tTraining batch 32 Loss: 0.000025\n",
      "\tTraining batch 33 Loss: 0.001429\n",
      "\tTraining batch 34 Loss: 0.087309\n",
      "\tTraining batch 35 Loss: 0.042122\n",
      "\tTraining batch 36 Loss: 0.000271\n",
      "\tTraining batch 37 Loss: 0.086413\n",
      "Training set: Average loss: 0.020127\n",
      "Validation set: Average loss: 5.872430, Accuracy: 1293/1959 (66.00%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.013306\n",
      "\tTraining batch 3 Loss: 0.000005\n",
      "\tTraining batch 4 Loss: 0.000112\n",
      "\tTraining batch 5 Loss: 0.007949\n",
      "\tTraining batch 6 Loss: 0.000001\n",
      "\tTraining batch 7 Loss: 0.000026\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000004\n",
      "\tTraining batch 11 Loss: 0.000041\n",
      "\tTraining batch 12 Loss: 0.000081\n",
      "\tTraining batch 13 Loss: 0.002566\n",
      "\tTraining batch 14 Loss: 0.000010\n",
      "\tTraining batch 15 Loss: 0.002157\n",
      "\tTraining batch 16 Loss: 0.000001\n",
      "\tTraining batch 17 Loss: 0.000335\n",
      "\tTraining batch 18 Loss: 0.000025\n",
      "\tTraining batch 19 Loss: 0.005313\n",
      "\tTraining batch 20 Loss: 0.000035\n",
      "\tTraining batch 21 Loss: 0.001912\n",
      "\tTraining batch 22 Loss: 0.000019\n",
      "\tTraining batch 23 Loss: 0.000027\n",
      "\tTraining batch 24 Loss: 0.001345\n",
      "\tTraining batch 25 Loss: 0.133573\n",
      "\tTraining batch 26 Loss: 0.108632\n",
      "\tTraining batch 27 Loss: 0.000010\n",
      "\tTraining batch 28 Loss: 0.000123\n",
      "\tTraining batch 29 Loss: 0.008670\n",
      "\tTraining batch 30 Loss: 0.028018\n",
      "\tTraining batch 31 Loss: 0.000006\n",
      "\tTraining batch 32 Loss: 0.000004\n",
      "\tTraining batch 33 Loss: 0.000032\n",
      "\tTraining batch 34 Loss: 0.000930\n",
      "\tTraining batch 35 Loss: 0.090351\n",
      "\tTraining batch 36 Loss: 0.000589\n",
      "\tTraining batch 37 Loss: 0.000095\n",
      "Training set: Average loss: 0.010981\n",
      "Validation set: Average loss: 5.334998, Accuracy: 1324/1959 (67.59%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000009\n",
      "\tTraining batch 2 Loss: 0.001251\n",
      "\tTraining batch 3 Loss: 0.000001\n",
      "\tTraining batch 4 Loss: 0.000225\n",
      "\tTraining batch 5 Loss: 0.003036\n",
      "\tTraining batch 6 Loss: 0.000023\n",
      "\tTraining batch 7 Loss: 0.000003\n",
      "\tTraining batch 8 Loss: 0.007069\n",
      "\tTraining batch 9 Loss: 0.013131\n",
      "\tTraining batch 10 Loss: 0.000002\n",
      "\tTraining batch 11 Loss: 0.000001\n",
      "\tTraining batch 12 Loss: 0.000007\n",
      "\tTraining batch 13 Loss: 0.001653\n",
      "\tTraining batch 14 Loss: 0.004539\n",
      "\tTraining batch 15 Loss: 0.000016\n",
      "\tTraining batch 16 Loss: 0.000001\n",
      "\tTraining batch 17 Loss: 0.000125\n",
      "\tTraining batch 18 Loss: 0.000118\n",
      "\tTraining batch 19 Loss: 0.000872\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.002252\n",
      "\tTraining batch 22 Loss: 0.000048\n",
      "\tTraining batch 23 Loss: 0.000178\n",
      "\tTraining batch 24 Loss: 0.004523\n",
      "\tTraining batch 25 Loss: 0.000043\n",
      "\tTraining batch 26 Loss: 0.094164\n",
      "\tTraining batch 27 Loss: 0.000005\n",
      "\tTraining batch 28 Loss: 0.005711\n",
      "\tTraining batch 29 Loss: 0.000080\n",
      "\tTraining batch 30 Loss: 0.002411\n",
      "\tTraining batch 31 Loss: 0.000162\n",
      "\tTraining batch 32 Loss: 0.000042\n",
      "\tTraining batch 33 Loss: 0.042880\n",
      "\tTraining batch 34 Loss: 0.003438\n",
      "\tTraining batch 35 Loss: 0.008355\n",
      "\tTraining batch 36 Loss: 0.073320\n",
      "\tTraining batch 37 Loss: 0.007789\n",
      "Training set: Average loss: 0.007500\n",
      "Validation set: Average loss: 7.118432, Accuracy: 1362/1959 (69.53%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000012\n",
      "\tTraining batch 2 Loss: 0.370100\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000014\n",
      "\tTraining batch 5 Loss: 0.000012\n",
      "\tTraining batch 6 Loss: 0.000001\n",
      "\tTraining batch 7 Loss: 0.000005\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000973\n",
      "\tTraining batch 10 Loss: 0.010459\n",
      "\tTraining batch 11 Loss: 0.000002\n",
      "\tTraining batch 12 Loss: 0.004521\n",
      "\tTraining batch 13 Loss: 0.114257\n",
      "\tTraining batch 14 Loss: 0.004563\n",
      "\tTraining batch 15 Loss: 0.001546\n",
      "\tTraining batch 16 Loss: 0.000007\n",
      "\tTraining batch 17 Loss: 0.297089\n",
      "\tTraining batch 18 Loss: 0.010833\n",
      "\tTraining batch 19 Loss: 0.136665\n",
      "\tTraining batch 20 Loss: 0.100499\n",
      "\tTraining batch 21 Loss: 0.091239\n",
      "\tTraining batch 22 Loss: 0.278075\n",
      "\tTraining batch 23 Loss: 0.040732\n",
      "\tTraining batch 24 Loss: 0.051204\n",
      "\tTraining batch 25 Loss: 0.000032\n",
      "\tTraining batch 26 Loss: 0.012007\n",
      "\tTraining batch 27 Loss: 0.000143\n",
      "\tTraining batch 28 Loss: 0.000602\n",
      "\tTraining batch 29 Loss: 0.000001\n",
      "\tTraining batch 30 Loss: 0.000006\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000001\n",
      "\tTraining batch 33 Loss: 0.000004\n",
      "\tTraining batch 34 Loss: 0.139843\n",
      "\tTraining batch 35 Loss: 0.070667\n",
      "\tTraining batch 36 Loss: 0.000046\n",
      "\tTraining batch 37 Loss: 0.000167\n",
      "Training set: Average loss: 0.046928\n",
      "Validation set: Average loss: 8.907535, Accuracy: 1314/1959 (67.08%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000294\n",
      "\tTraining batch 2 Loss: 0.000145\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.013721\n",
      "\tTraining batch 5 Loss: 0.053176\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000122\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000010\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.016953\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000336\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000176\n",
      "\tTraining batch 20 Loss: 0.000537\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000013\n",
      "\tTraining batch 26 Loss: 0.002280\n",
      "\tTraining batch 27 Loss: 0.000308\n",
      "\tTraining batch 28 Loss: 0.000004\n",
      "\tTraining batch 29 Loss: 0.000167\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000005\n",
      "\tTraining batch 32 Loss: 0.227940\n",
      "\tTraining batch 33 Loss: 0.000010\n",
      "\tTraining batch 34 Loss: 0.003745\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000767\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "Training set: Average loss: 0.008668\n",
      "Validation set: Average loss: 9.118491, Accuracy: 1295/1959 (66.11%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000034\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000002\n",
      "\tTraining batch 13 Loss: 0.000005\n",
      "\tTraining batch 14 Loss: 0.000167\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000002\n",
      "\tTraining batch 17 Loss: 0.000095\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000006\n",
      "\tTraining batch 20 Loss: 0.001658\n",
      "\tTraining batch 21 Loss: 0.000001\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000004\n",
      "\tTraining batch 26 Loss: 0.000573\n",
      "\tTraining batch 27 Loss: 0.000363\n",
      "\tTraining batch 28 Loss: 0.000031\n",
      "\tTraining batch 29 Loss: 0.000001\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000003\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000003\n",
      "\tTraining batch 34 Loss: 0.001569\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000013\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "Training set: Average loss: 0.000122\n",
      "Validation set: Average loss: 9.587871, Accuracy: 1327/1959 (67.74%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000099\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000044\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000414\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000002\n",
      "\tTraining batch 13 Loss: 0.000012\n",
      "\tTraining batch 14 Loss: 0.000181\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000002\n",
      "\tTraining batch 17 Loss: 0.000068\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000005\n",
      "\tTraining batch 20 Loss: 0.000027\n",
      "\tTraining batch 21 Loss: 0.000001\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000004\n",
      "\tTraining batch 26 Loss: 0.000497\n",
      "\tTraining batch 27 Loss: 0.000171\n",
      "\tTraining batch 28 Loss: 0.000023\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000003\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000003\n",
      "\tTraining batch 34 Loss: 0.001372\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000012\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "Training set: Average loss: 0.000080\n",
      "Validation set: Average loss: 9.592650, Accuracy: 1330/1959 (67.89%)\n",
      "\n",
      "Epoch: 11\n",
      "\tTraining batch 1 Loss: 0.000009\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000010\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000001\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000002\n",
      "\tTraining batch 13 Loss: 0.000008\n",
      "\tTraining batch 14 Loss: 0.000165\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000002\n",
      "\tTraining batch 17 Loss: 0.000056\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000005\n",
      "\tTraining batch 20 Loss: 0.000022\n",
      "\tTraining batch 21 Loss: 0.000001\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000004\n",
      "\tTraining batch 26 Loss: 0.000443\n",
      "\tTraining batch 27 Loss: 0.000120\n",
      "\tTraining batch 28 Loss: 0.000018\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000003\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000003\n",
      "\tTraining batch 34 Loss: 0.001167\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000011\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "Training set: Average loss: 0.000055\n",
      "Validation set: Average loss: 9.608168, Accuracy: 1329/1959 (67.84%)\n",
      "\n",
      "Epoch: 12\n",
      "\tTraining batch 1 Loss: 0.000006\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000007\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000001\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000002\n",
      "\tTraining batch 13 Loss: 0.000006\n",
      "\tTraining batch 14 Loss: 0.000153\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000002\n",
      "\tTraining batch 17 Loss: 0.000047\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000005\n",
      "\tTraining batch 20 Loss: 0.000020\n",
      "\tTraining batch 21 Loss: 0.000001\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000004\n",
      "\tTraining batch 26 Loss: 0.000400\n",
      "\tTraining batch 27 Loss: 0.000097\n",
      "\tTraining batch 28 Loss: 0.000014\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000002\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000003\n",
      "\tTraining batch 34 Loss: 0.001017\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000011\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "Training set: Average loss: 0.000049\n",
      "Validation set: Average loss: 9.621049, Accuracy: 1328/1959 (67.79%)\n",
      "\n",
      "Epoch: 13\n",
      "\tTraining batch 1 Loss: 0.000004\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000005\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000001\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000002\n",
      "\tTraining batch 13 Loss: 0.000005\n",
      "\tTraining batch 14 Loss: 0.000143\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000002\n",
      "\tTraining batch 17 Loss: 0.000041\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000005\n",
      "\tTraining batch 20 Loss: 0.000018\n",
      "\tTraining batch 21 Loss: 0.000001\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000004\n",
      "\tTraining batch 26 Loss: 0.000368\n",
      "\tTraining batch 27 Loss: 0.000080\n",
      "\tTraining batch 28 Loss: 0.000012\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000002\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000003\n",
      "\tTraining batch 34 Loss: 0.000902\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000010\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "Training set: Average loss: 0.000043\n",
      "Validation set: Average loss: 9.632300, Accuracy: 1328/1959 (67.79%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 47.5%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000003\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000004\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000001\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000002\n",
      "\tTraining batch 13 Loss: 0.000004\n",
      "\tTraining batch 14 Loss: 0.000134\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000002\n",
      "\tTraining batch 17 Loss: 0.000036\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000005\n",
      "\tTraining batch 20 Loss: 0.000016\n",
      "\tTraining batch 21 Loss: 0.000001\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000003\n",
      "\tTraining batch 26 Loss: 0.000340\n",
      "\tTraining batch 27 Loss: 0.000068\n",
      "\tTraining batch 28 Loss: 0.000010\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000002\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000002\n",
      "\tTraining batch 34 Loss: 0.000808\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000010\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 13.372546\n",
      "Training set: Average loss: 0.351947\n",
      "Validation set: Average loss: 8.415908, Accuracy: 1361/1959 (69.47%)\n",
      "\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000001\n",
      "\tTraining batch 3 Loss: 0.000080\n",
      "\tTraining batch 4 Loss: 0.000120\n",
      "\tTraining batch 5 Loss: 0.000070\n",
      "\tTraining batch 6 Loss: 0.000016\n",
      "\tTraining batch 7 Loss: 0.111589\n",
      "\tTraining batch 8 Loss: 0.000113\n",
      "\tTraining batch 9 Loss: 0.127943\n",
      "\tTraining batch 10 Loss: 0.219055\n",
      "\tTraining batch 11 Loss: 0.273282\n",
      "\tTraining batch 12 Loss: 0.083684\n",
      "\tTraining batch 13 Loss: 0.207803\n",
      "\tTraining batch 14 Loss: 0.024047\n",
      "\tTraining batch 15 Loss: 0.046386\n",
      "\tTraining batch 16 Loss: 0.113758\n",
      "\tTraining batch 17 Loss: 0.113075\n",
      "\tTraining batch 18 Loss: 0.000983\n",
      "\tTraining batch 19 Loss: 0.108518\n",
      "\tTraining batch 20 Loss: 0.004058\n",
      "\tTraining batch 21 Loss: 0.109482\n",
      "\tTraining batch 22 Loss: 0.000017\n",
      "\tTraining batch 23 Loss: 0.037127\n",
      "\tTraining batch 24 Loss: 0.000141\n",
      "\tTraining batch 25 Loss: 0.005536\n",
      "\tTraining batch 26 Loss: 0.012253\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000414\n",
      "\tTraining batch 29 Loss: 0.235217\n",
      "\tTraining batch 30 Loss: 0.375074\n",
      "\tTraining batch 31 Loss: 0.005026\n",
      "\tTraining batch 32 Loss: 0.223314\n",
      "\tTraining batch 33 Loss: 0.033028\n",
      "\tTraining batch 34 Loss: 0.116696\n",
      "\tTraining batch 35 Loss: 0.000142\n",
      "\tTraining batch 36 Loss: 0.000207\n",
      "\tTraining batch 37 Loss: 0.000216\n",
      "\tTraining batch 38 Loss: 0.540625\n",
      "Training set: Average loss: 0.082345\n",
      "Validation set: Average loss: 6.335548, Accuracy: 1300/1959 (66.36%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000560\n",
      "\tTraining batch 2 Loss: 0.002447\n",
      "\tTraining batch 3 Loss: 0.000063\n",
      "\tTraining batch 4 Loss: 0.000028\n",
      "\tTraining batch 5 Loss: 0.000011\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.124682\n",
      "\tTraining batch 8 Loss: 0.000008\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.226145\n",
      "\tTraining batch 11 Loss: 0.000351\n",
      "\tTraining batch 12 Loss: 0.000069\n",
      "\tTraining batch 13 Loss: 0.000055\n",
      "\tTraining batch 14 Loss: 0.000099\n",
      "\tTraining batch 15 Loss: 0.000092\n",
      "\tTraining batch 16 Loss: 0.000033\n",
      "\tTraining batch 17 Loss: 0.000024\n",
      "\tTraining batch 18 Loss: 0.000003\n",
      "\tTraining batch 19 Loss: 0.000002\n",
      "\tTraining batch 20 Loss: 0.004893\n",
      "\tTraining batch 21 Loss: 0.001176\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000096\n",
      "\tTraining batch 24 Loss: 0.000129\n",
      "\tTraining batch 25 Loss: 0.000046\n",
      "\tTraining batch 26 Loss: 0.001045\n",
      "\tTraining batch 27 Loss: 0.549969\n",
      "\tTraining batch 28 Loss: 0.000057\n",
      "\tTraining batch 29 Loss: 0.037358\n",
      "\tTraining batch 30 Loss: 0.000307\n",
      "\tTraining batch 31 Loss: 0.015746\n",
      "\tTraining batch 32 Loss: 0.000862\n",
      "\tTraining batch 33 Loss: 0.000418\n",
      "\tTraining batch 34 Loss: 0.000799\n",
      "\tTraining batch 35 Loss: 0.022278\n",
      "\tTraining batch 36 Loss: 0.000181\n",
      "\tTraining batch 37 Loss: 0.000021\n",
      "\tTraining batch 38 Loss: 0.011315\n",
      "Training set: Average loss: 0.026352\n",
      "Validation set: Average loss: 6.504697, Accuracy: 1330/1959 (67.89%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000003\n",
      "\tTraining batch 2 Loss: 0.010195\n",
      "\tTraining batch 3 Loss: 0.000045\n",
      "\tTraining batch 4 Loss: 0.000115\n",
      "\tTraining batch 5 Loss: 0.000045\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000002\n",
      "\tTraining batch 8 Loss: 0.000009\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000011\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000055\n",
      "\tTraining batch 13 Loss: 0.000014\n",
      "\tTraining batch 14 Loss: 0.002291\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000018\n",
      "\tTraining batch 18 Loss: 0.000001\n",
      "\tTraining batch 19 Loss: 0.000711\n",
      "\tTraining batch 20 Loss: 0.011833\n",
      "\tTraining batch 21 Loss: 0.000008\n",
      "\tTraining batch 22 Loss: 0.001230\n",
      "\tTraining batch 23 Loss: 0.000376\n",
      "\tTraining batch 24 Loss: 0.000039\n",
      "\tTraining batch 25 Loss: 0.000105\n",
      "\tTraining batch 26 Loss: 0.001531\n",
      "\tTraining batch 27 Loss: 0.000188\n",
      "\tTraining batch 28 Loss: 0.000062\n",
      "\tTraining batch 29 Loss: 0.000010\n",
      "\tTraining batch 30 Loss: 0.000759\n",
      "\tTraining batch 31 Loss: 0.000168\n",
      "\tTraining batch 32 Loss: 0.002197\n",
      "\tTraining batch 33 Loss: 0.000039\n",
      "\tTraining batch 34 Loss: 0.000402\n",
      "\tTraining batch 35 Loss: 0.000001\n",
      "\tTraining batch 36 Loss: 0.000247\n",
      "\tTraining batch 37 Loss: 0.000212\n",
      "\tTraining batch 38 Loss: 0.002190\n",
      "Training set: Average loss: 0.000924\n",
      "Validation set: Average loss: 6.670221, Accuracy: 1338/1959 (68.30%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000004\n",
      "\tTraining batch 2 Loss: 0.000009\n",
      "\tTraining batch 3 Loss: 0.000001\n",
      "\tTraining batch 4 Loss: 0.000547\n",
      "\tTraining batch 5 Loss: 0.000036\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000003\n",
      "\tTraining batch 8 Loss: 0.000020\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000037\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000031\n",
      "\tTraining batch 13 Loss: 0.000007\n",
      "\tTraining batch 14 Loss: 0.000154\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000014\n",
      "\tTraining batch 18 Loss: 0.000001\n",
      "\tTraining batch 19 Loss: 0.000044\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000011\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000003\n",
      "\tTraining batch 24 Loss: 0.000033\n",
      "\tTraining batch 25 Loss: 0.000037\n",
      "\tTraining batch 26 Loss: 0.000879\n",
      "\tTraining batch 27 Loss: 0.000030\n",
      "\tTraining batch 28 Loss: 0.000066\n",
      "\tTraining batch 29 Loss: 0.000006\n",
      "\tTraining batch 30 Loss: 0.000175\n",
      "\tTraining batch 31 Loss: 0.000107\n",
      "\tTraining batch 32 Loss: 0.000017\n",
      "\tTraining batch 33 Loss: 0.000030\n",
      "\tTraining batch 34 Loss: 0.000301\n",
      "\tTraining batch 35 Loss: 0.000001\n",
      "\tTraining batch 36 Loss: 0.000081\n",
      "\tTraining batch 37 Loss: 0.000044\n",
      "\tTraining batch 38 Loss: 0.000983\n",
      "Training set: Average loss: 0.000098\n",
      "Validation set: Average loss: 6.705579, Accuracy: 1335/1959 (68.15%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000003\n",
      "\tTraining batch 2 Loss: 0.000008\n",
      "\tTraining batch 3 Loss: 0.000001\n",
      "\tTraining batch 4 Loss: 0.000467\n",
      "\tTraining batch 5 Loss: 0.000045\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000003\n",
      "\tTraining batch 8 Loss: 0.000021\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000035\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000030\n",
      "\tTraining batch 13 Loss: 0.000006\n",
      "\tTraining batch 14 Loss: 0.000137\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000014\n",
      "\tTraining batch 18 Loss: 0.000001\n",
      "\tTraining batch 19 Loss: 0.000044\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000009\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000002\n",
      "\tTraining batch 24 Loss: 0.000030\n",
      "\tTraining batch 25 Loss: 0.000030\n",
      "\tTraining batch 26 Loss: 0.000783\n",
      "\tTraining batch 27 Loss: 0.000022\n",
      "\tTraining batch 28 Loss: 0.000042\n",
      "\tTraining batch 29 Loss: 0.000006\n",
      "\tTraining batch 30 Loss: 0.000142\n",
      "\tTraining batch 31 Loss: 0.000075\n",
      "\tTraining batch 32 Loss: 0.000015\n",
      "\tTraining batch 33 Loss: 0.000025\n",
      "\tTraining batch 34 Loss: 0.000272\n",
      "\tTraining batch 35 Loss: 0.000001\n",
      "\tTraining batch 36 Loss: 0.000055\n",
      "\tTraining batch 37 Loss: 0.000034\n",
      "\tTraining batch 38 Loss: 0.000733\n",
      "Training set: Average loss: 0.000081\n",
      "Validation set: Average loss: 6.717055, Accuracy: 1336/1959 (68.20%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000003\n",
      "\tTraining batch 2 Loss: 0.000008\n",
      "\tTraining batch 3 Loss: 0.000001\n",
      "\tTraining batch 4 Loss: 0.000394\n",
      "\tTraining batch 5 Loss: 0.000031\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000002\n",
      "\tTraining batch 8 Loss: 0.000018\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000031\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000028\n",
      "\tTraining batch 13 Loss: 0.000005\n",
      "\tTraining batch 14 Loss: 0.000124\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000014\n",
      "\tTraining batch 18 Loss: 0.000001\n",
      "\tTraining batch 19 Loss: 0.000043\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000007\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000002\n",
      "\tTraining batch 24 Loss: 0.000028\n",
      "\tTraining batch 25 Loss: 0.000025\n",
      "\tTraining batch 26 Loss: 0.000704\n",
      "\tTraining batch 27 Loss: 0.000018\n",
      "\tTraining batch 28 Loss: 0.000032\n",
      "\tTraining batch 29 Loss: 0.000005\n",
      "\tTraining batch 30 Loss: 0.000121\n",
      "\tTraining batch 31 Loss: 0.000059\n",
      "\tTraining batch 32 Loss: 0.000013\n",
      "\tTraining batch 33 Loss: 0.000021\n",
      "\tTraining batch 34 Loss: 0.000252\n",
      "\tTraining batch 35 Loss: 0.000001\n",
      "\tTraining batch 36 Loss: 0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 37 Loss: 0.000028\n",
      "\tTraining batch 38 Loss: 0.000591\n",
      "Training set: Average loss: 0.000070\n",
      "Validation set: Average loss: 6.726845, Accuracy: 1335/1959 (68.15%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000003\n",
      "\tTraining batch 2 Loss: 0.000007\n",
      "\tTraining batch 3 Loss: 0.000001\n",
      "\tTraining batch 4 Loss: 0.000340\n",
      "\tTraining batch 5 Loss: 0.000025\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000002\n",
      "\tTraining batch 8 Loss: 0.000017\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000029\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000027\n",
      "\tTraining batch 13 Loss: 0.000005\n",
      "\tTraining batch 14 Loss: 0.000113\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000014\n",
      "\tTraining batch 18 Loss: 0.000001\n",
      "\tTraining batch 19 Loss: 0.000043\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000007\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000002\n",
      "\tTraining batch 24 Loss: 0.000026\n",
      "\tTraining batch 25 Loss: 0.000022\n",
      "\tTraining batch 26 Loss: 0.000638\n",
      "\tTraining batch 27 Loss: 0.000016\n",
      "\tTraining batch 28 Loss: 0.000026\n",
      "\tTraining batch 29 Loss: 0.000005\n",
      "\tTraining batch 30 Loss: 0.000106\n",
      "\tTraining batch 31 Loss: 0.000048\n",
      "\tTraining batch 32 Loss: 0.000012\n",
      "\tTraining batch 33 Loss: 0.000019\n",
      "\tTraining batch 34 Loss: 0.000237\n",
      "\tTraining batch 35 Loss: 0.000001\n",
      "\tTraining batch 36 Loss: 0.000039\n",
      "\tTraining batch 37 Loss: 0.000024\n",
      "\tTraining batch 38 Loss: 0.000494\n",
      "Training set: Average loss: 0.000062\n",
      "Validation set: Average loss: 6.735578, Accuracy: 1336/1959 (68.20%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000002\n",
      "\tTraining batch 2 Loss: 0.000007\n",
      "\tTraining batch 3 Loss: 0.000001\n",
      "\tTraining batch 4 Loss: 0.000299\n",
      "\tTraining batch 5 Loss: 0.000021\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000002\n",
      "\tTraining batch 8 Loss: 0.000016\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000027\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000026\n",
      "\tTraining batch 13 Loss: 0.000004\n",
      "\tTraining batch 14 Loss: 0.000104\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000014\n",
      "\tTraining batch 18 Loss: 0.000001\n",
      "\tTraining batch 19 Loss: 0.000042\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000006\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000002\n",
      "\tTraining batch 24 Loss: 0.000024\n",
      "\tTraining batch 25 Loss: 0.000019\n",
      "\tTraining batch 26 Loss: 0.000582\n",
      "\tTraining batch 27 Loss: 0.000014\n",
      "\tTraining batch 28 Loss: 0.000022\n",
      "\tTraining batch 29 Loss: 0.000005\n",
      "\tTraining batch 30 Loss: 0.000095\n",
      "\tTraining batch 31 Loss: 0.000041\n",
      "\tTraining batch 32 Loss: 0.000012\n",
      "\tTraining batch 33 Loss: 0.000017\n",
      "\tTraining batch 34 Loss: 0.000225\n",
      "\tTraining batch 35 Loss: 0.000001\n",
      "\tTraining batch 36 Loss: 0.000035\n",
      "\tTraining batch 37 Loss: 0.000021\n",
      "\tTraining batch 38 Loss: 0.000420\n",
      "Training set: Average loss: 0.000055\n",
      "Validation set: Average loss: 6.743559, Accuracy: 1336/1959 (68.20%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 48.5%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000002\n",
      "\tTraining batch 2 Loss: 0.000007\n",
      "\tTraining batch 3 Loss: 0.000001\n",
      "\tTraining batch 4 Loss: 0.000267\n",
      "\tTraining batch 5 Loss: 0.000018\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000002\n",
      "\tTraining batch 8 Loss: 0.000015\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000026\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000025\n",
      "\tTraining batch 13 Loss: 0.000004\n",
      "\tTraining batch 14 Loss: 0.000096\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000014\n",
      "\tTraining batch 18 Loss: 0.000001\n",
      "\tTraining batch 19 Loss: 0.000041\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000006\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000002\n",
      "\tTraining batch 24 Loss: 0.000022\n",
      "\tTraining batch 25 Loss: 0.000017\n",
      "\tTraining batch 26 Loss: 0.000534\n",
      "\tTraining batch 27 Loss: 0.000012\n",
      "\tTraining batch 28 Loss: 0.000020\n",
      "\tTraining batch 29 Loss: 0.000005\n",
      "\tTraining batch 30 Loss: 0.000086\n",
      "\tTraining batch 31 Loss: 0.000036\n",
      "\tTraining batch 32 Loss: 0.000011\n",
      "\tTraining batch 33 Loss: 0.000015\n",
      "\tTraining batch 34 Loss: 0.000214\n",
      "\tTraining batch 35 Loss: 0.000001\n",
      "\tTraining batch 36 Loss: 0.000032\n",
      "\tTraining batch 37 Loss: 0.000019\n",
      "\tTraining batch 38 Loss: 0.000365\n",
      "\tTraining batch 39 Loss: 5.004638\n",
      "Training set: Average loss: 0.128373\n",
      "Validation set: Average loss: 5.658809, Accuracy: 1347/1959 (68.76%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000021\n",
      "\tTraining batch 2 Loss: 0.000006\n",
      "\tTraining batch 3 Loss: 0.166981\n",
      "\tTraining batch 4 Loss: 0.279417\n",
      "\tTraining batch 5 Loss: 0.089374\n",
      "\tTraining batch 6 Loss: 0.505193\n",
      "\tTraining batch 7 Loss: 0.000348\n",
      "\tTraining batch 8 Loss: 0.381656\n",
      "\tTraining batch 9 Loss: 0.000262\n",
      "\tTraining batch 10 Loss: 0.174819\n",
      "\tTraining batch 11 Loss: 0.003453\n",
      "\tTraining batch 12 Loss: 0.040852\n",
      "\tTraining batch 13 Loss: 0.131826\n",
      "\tTraining batch 14 Loss: 0.000750\n",
      "\tTraining batch 15 Loss: 0.026776\n",
      "\tTraining batch 16 Loss: 0.006393\n",
      "\tTraining batch 17 Loss: 0.020009\n",
      "\tTraining batch 18 Loss: 0.173542\n",
      "\tTraining batch 19 Loss: 0.074089\n",
      "\tTraining batch 20 Loss: 0.000130\n",
      "\tTraining batch 21 Loss: 0.025466\n",
      "\tTraining batch 22 Loss: 0.000128\n",
      "\tTraining batch 23 Loss: 0.005677\n",
      "\tTraining batch 24 Loss: 0.073470\n",
      "\tTraining batch 25 Loss: 0.013177\n",
      "\tTraining batch 26 Loss: 0.030840\n",
      "\tTraining batch 27 Loss: 0.021282\n",
      "\tTraining batch 28 Loss: 0.003974\n",
      "\tTraining batch 29 Loss: 0.008043\n",
      "\tTraining batch 30 Loss: 0.001469\n",
      "\tTraining batch 31 Loss: 0.101310\n",
      "\tTraining batch 32 Loss: 0.000047\n",
      "\tTraining batch 33 Loss: 0.120705\n",
      "\tTraining batch 34 Loss: 0.001467\n",
      "\tTraining batch 35 Loss: 0.117203\n",
      "\tTraining batch 36 Loss: 0.018001\n",
      "\tTraining batch 37 Loss: 0.001082\n",
      "\tTraining batch 38 Loss: 0.061853\n",
      "\tTraining batch 39 Loss: 0.446840\n",
      "Training set: Average loss: 0.080203\n",
      "Validation set: Average loss: 5.093954, Accuracy: 1324/1959 (67.59%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.001658\n",
      "\tTraining batch 2 Loss: 0.002854\n",
      "\tTraining batch 3 Loss: 0.005100\n",
      "\tTraining batch 4 Loss: 0.388460\n",
      "\tTraining batch 5 Loss: 0.000030\n",
      "\tTraining batch 6 Loss: 0.000471\n",
      "\tTraining batch 7 Loss: 0.000047\n",
      "\tTraining batch 8 Loss: 0.000027\n",
      "\tTraining batch 9 Loss: 0.000029\n",
      "\tTraining batch 10 Loss: 0.000175\n",
      "\tTraining batch 11 Loss: 0.000869\n",
      "\tTraining batch 12 Loss: 0.000122\n",
      "\tTraining batch 13 Loss: 0.000876\n",
      "\tTraining batch 14 Loss: 0.000118\n",
      "\tTraining batch 15 Loss: 0.017726\n",
      "\tTraining batch 16 Loss: 0.008346\n",
      "\tTraining batch 17 Loss: 0.000402\n",
      "\tTraining batch 18 Loss: 0.005329\n",
      "\tTraining batch 19 Loss: 0.053385\n",
      "\tTraining batch 20 Loss: 0.008047\n",
      "\tTraining batch 21 Loss: 0.065282\n",
      "\tTraining batch 22 Loss: 0.146856\n",
      "\tTraining batch 23 Loss: 0.082707\n",
      "\tTraining batch 24 Loss: 0.000073\n",
      "\tTraining batch 25 Loss: 0.000006\n",
      "\tTraining batch 26 Loss: 0.001550\n",
      "\tTraining batch 27 Loss: 0.000018\n",
      "\tTraining batch 28 Loss: 0.001285\n",
      "\tTraining batch 29 Loss: 0.004706\n",
      "\tTraining batch 30 Loss: 0.000659\n",
      "\tTraining batch 31 Loss: 0.001094\n",
      "\tTraining batch 32 Loss: 0.000628\n",
      "\tTraining batch 33 Loss: 0.027166\n",
      "\tTraining batch 34 Loss: 0.000495\n",
      "\tTraining batch 35 Loss: 0.006979\n",
      "\tTraining batch 36 Loss: 0.000552\n",
      "\tTraining batch 37 Loss: 0.001839\n",
      "\tTraining batch 38 Loss: 0.058464\n",
      "\tTraining batch 39 Loss: 0.467558\n",
      "Training set: Average loss: 0.034923\n",
      "Validation set: Average loss: 7.420594, Accuracy: 1286/1959 (65.65%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.027900\n",
      "\tTraining batch 2 Loss: 0.005183\n",
      "\tTraining batch 3 Loss: 0.000091\n",
      "\tTraining batch 4 Loss: 0.004959\n",
      "\tTraining batch 5 Loss: 0.006225\n",
      "\tTraining batch 6 Loss: 0.000046\n",
      "\tTraining batch 7 Loss: 0.000018\n",
      "\tTraining batch 8 Loss: 0.000139\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.020409\n",
      "\tTraining batch 11 Loss: 0.000044\n",
      "\tTraining batch 12 Loss: 0.328592\n",
      "\tTraining batch 13 Loss: 0.035200\n",
      "\tTraining batch 14 Loss: 0.002425\n",
      "\tTraining batch 15 Loss: 0.000001\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.020308\n",
      "\tTraining batch 18 Loss: 0.001320\n",
      "\tTraining batch 19 Loss: 0.007209\n",
      "\tTraining batch 20 Loss: 0.000003\n",
      "\tTraining batch 21 Loss: 0.000001\n",
      "\tTraining batch 22 Loss: 0.000001\n",
      "\tTraining batch 23 Loss: 0.000003\n",
      "\tTraining batch 24 Loss: 0.000797\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.001565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 27 Loss: 0.003873\n",
      "\tTraining batch 28 Loss: 0.000551\n",
      "\tTraining batch 29 Loss: 0.002167\n",
      "\tTraining batch 30 Loss: 0.004553\n",
      "\tTraining batch 31 Loss: 0.068669\n",
      "\tTraining batch 32 Loss: 0.000042\n",
      "\tTraining batch 33 Loss: 0.000001\n",
      "\tTraining batch 34 Loss: 0.001521\n",
      "\tTraining batch 35 Loss: 0.000012\n",
      "\tTraining batch 36 Loss: 0.001337\n",
      "\tTraining batch 37 Loss: 0.029670\n",
      "\tTraining batch 38 Loss: 0.003626\n",
      "\tTraining batch 39 Loss: 0.144793\n",
      "Training set: Average loss: 0.018545\n",
      "Validation set: Average loss: 8.521892, Accuracy: 1321/1959 (67.43%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000003\n",
      "\tTraining batch 2 Loss: 0.000883\n",
      "\tTraining batch 3 Loss: 0.000003\n",
      "\tTraining batch 4 Loss: 0.000020\n",
      "\tTraining batch 5 Loss: 0.012618\n",
      "\tTraining batch 6 Loss: 0.002282\n",
      "\tTraining batch 7 Loss: 0.195627\n",
      "\tTraining batch 8 Loss: 0.000063\n",
      "\tTraining batch 9 Loss: 0.000005\n",
      "\tTraining batch 10 Loss: 0.000002\n",
      "\tTraining batch 11 Loss: 0.000498\n",
      "\tTraining batch 12 Loss: 0.000019\n",
      "\tTraining batch 13 Loss: 0.000003\n",
      "\tTraining batch 14 Loss: 0.001155\n",
      "\tTraining batch 15 Loss: 0.308701\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000003\n",
      "\tTraining batch 19 Loss: 0.000101\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000004\n",
      "\tTraining batch 23 Loss: 0.000216\n",
      "\tTraining batch 24 Loss: 0.000089\n",
      "\tTraining batch 25 Loss: 0.000001\n",
      "\tTraining batch 26 Loss: 0.004373\n",
      "\tTraining batch 27 Loss: 0.000009\n",
      "\tTraining batch 28 Loss: 0.000049\n",
      "\tTraining batch 29 Loss: 0.002273\n",
      "\tTraining batch 30 Loss: 0.000002\n",
      "\tTraining batch 31 Loss: 0.000005\n",
      "\tTraining batch 32 Loss: 0.000004\n",
      "\tTraining batch 33 Loss: 0.000001\n",
      "\tTraining batch 34 Loss: 0.000063\n",
      "\tTraining batch 35 Loss: 0.002188\n",
      "\tTraining batch 36 Loss: 0.000057\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.026261\n",
      "\tTraining batch 39 Loss: 0.000259\n",
      "Training set: Average loss: 0.014304\n",
      "Validation set: Average loss: 7.617285, Accuracy: 1323/1959 (67.53%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000001\n",
      "\tTraining batch 2 Loss: 0.000147\n",
      "\tTraining batch 3 Loss: 0.000010\n",
      "\tTraining batch 4 Loss: 0.000023\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000017\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000001\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000001\n",
      "\tTraining batch 11 Loss: 0.000001\n",
      "\tTraining batch 12 Loss: 0.000007\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000018\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000002\n",
      "\tTraining batch 18 Loss: 0.000002\n",
      "\tTraining batch 19 Loss: 0.001650\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000023\n",
      "\tTraining batch 24 Loss: 0.000047\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.001392\n",
      "\tTraining batch 27 Loss: 0.000033\n",
      "\tTraining batch 28 Loss: 0.000003\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000002\n",
      "\tTraining batch 31 Loss: 0.000001\n",
      "\tTraining batch 32 Loss: 0.000021\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000167\n",
      "\tTraining batch 35 Loss: 0.000011\n",
      "\tTraining batch 36 Loss: 0.005248\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000672\n",
      "\tTraining batch 39 Loss: 0.000394\n",
      "Training set: Average loss: 0.000254\n",
      "Validation set: Average loss: 7.373829, Accuracy: 1314/1959 (67.08%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000002\n",
      "\tTraining batch 2 Loss: 0.000007\n",
      "\tTraining batch 3 Loss: 0.000002\n",
      "\tTraining batch 4 Loss: 0.000021\n",
      "\tTraining batch 5 Loss: 0.000001\n",
      "\tTraining batch 6 Loss: 0.000005\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000002\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000002\n",
      "\tTraining batch 11 Loss: 0.000001\n",
      "\tTraining batch 12 Loss: 0.000015\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000012\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000003\n",
      "\tTraining batch 18 Loss: 0.000001\n",
      "\tTraining batch 19 Loss: 0.001314\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000029\n",
      "\tTraining batch 24 Loss: 0.000039\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000903\n",
      "\tTraining batch 27 Loss: 0.000037\n",
      "\tTraining batch 28 Loss: 0.000014\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.000001\n",
      "\tTraining batch 32 Loss: 0.000012\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000184\n",
      "\tTraining batch 35 Loss: 0.000012\n",
      "\tTraining batch 36 Loss: 0.000056\n",
      "\tTraining batch 37 Loss: 0.000001\n",
      "\tTraining batch 38 Loss: 0.000822\n",
      "\tTraining batch 39 Loss: 0.000390\n",
      "Training set: Average loss: 0.000100\n",
      "Validation set: Average loss: 7.265076, Accuracy: 1326/1959 (67.69%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000011\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000018\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000004\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000002\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000002\n",
      "\tTraining batch 11 Loss: 0.000001\n",
      "\tTraining batch 12 Loss: 0.000013\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000012\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000003\n",
      "\tTraining batch 18 Loss: 0.000001\n",
      "\tTraining batch 19 Loss: 0.001163\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000028\n",
      "\tTraining batch 24 Loss: 0.000036\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000754\n",
      "\tTraining batch 27 Loss: 0.000034\n",
      "\tTraining batch 28 Loss: 0.000013\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.000001\n",
      "\tTraining batch 32 Loss: 0.000012\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000176\n",
      "\tTraining batch 35 Loss: 0.000011\n",
      "\tTraining batch 36 Loss: 0.000046\n",
      "\tTraining batch 37 Loss: 0.000001\n",
      "\tTraining batch 38 Loss: 0.000588\n",
      "\tTraining batch 39 Loss: 0.000290\n",
      "Training set: Average loss: 0.000083\n",
      "Validation set: Average loss: 7.294259, Accuracy: 1326/1959 (67.69%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 47.5%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000011\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000018\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000004\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000002\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000002\n",
      "\tTraining batch 11 Loss: 0.000001\n",
      "\tTraining batch 12 Loss: 0.000012\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000012\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000003\n",
      "\tTraining batch 18 Loss: 0.000001\n",
      "\tTraining batch 19 Loss: 0.001055\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000027\n",
      "\tTraining batch 24 Loss: 0.000034\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000652\n",
      "\tTraining batch 27 Loss: 0.000030\n",
      "\tTraining batch 28 Loss: 0.000012\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000011\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000169\n",
      "\tTraining batch 35 Loss: 0.000011\n",
      "\tTraining batch 36 Loss: 0.000040\n",
      "\tTraining batch 37 Loss: 0.000001\n",
      "\tTraining batch 38 Loss: 0.000456\n",
      "\tTraining batch 39 Loss: 0.000229\n",
      "\tTraining batch 40 Loss: 14.268652\n",
      "Training set: Average loss: 0.356786\n",
      "Validation set: Average loss: 5.730811, Accuracy: 1329/1959 (67.84%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000039\n",
      "\tTraining batch 2 Loss: 0.006099\n",
      "\tTraining batch 3 Loss: 0.054451\n",
      "\tTraining batch 4 Loss: 0.333885\n",
      "\tTraining batch 5 Loss: 0.450098\n",
      "\tTraining batch 6 Loss: 0.562375\n",
      "\tTraining batch 7 Loss: 0.122150\n",
      "\tTraining batch 8 Loss: 0.412259\n",
      "\tTraining batch 9 Loss: 1.034285\n",
      "\tTraining batch 10 Loss: 0.246308\n",
      "\tTraining batch 11 Loss: 0.960847\n",
      "\tTraining batch 12 Loss: 0.031369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 13 Loss: 0.298695\n",
      "\tTraining batch 14 Loss: 0.199181\n",
      "\tTraining batch 15 Loss: 0.185354\n",
      "\tTraining batch 16 Loss: 0.082327\n",
      "\tTraining batch 17 Loss: 0.116894\n",
      "\tTraining batch 18 Loss: 0.078566\n",
      "\tTraining batch 19 Loss: 0.044498\n",
      "\tTraining batch 20 Loss: 0.175879\n",
      "\tTraining batch 21 Loss: 0.138058\n",
      "\tTraining batch 22 Loss: 0.669236\n",
      "\tTraining batch 23 Loss: 0.420143\n",
      "\tTraining batch 24 Loss: 0.154529\n",
      "\tTraining batch 25 Loss: 0.003008\n",
      "\tTraining batch 26 Loss: 0.084601\n",
      "\tTraining batch 27 Loss: 0.000109\n",
      "\tTraining batch 28 Loss: 0.019483\n",
      "\tTraining batch 29 Loss: 0.280305\n",
      "\tTraining batch 30 Loss: 0.001022\n",
      "\tTraining batch 31 Loss: 0.000514\n",
      "\tTraining batch 32 Loss: 0.126695\n",
      "\tTraining batch 33 Loss: 0.047602\n",
      "\tTraining batch 34 Loss: 0.104620\n",
      "\tTraining batch 35 Loss: 0.106723\n",
      "\tTraining batch 36 Loss: 0.007216\n",
      "\tTraining batch 37 Loss: 0.000059\n",
      "\tTraining batch 38 Loss: 0.357058\n",
      "\tTraining batch 39 Loss: 0.084285\n",
      "\tTraining batch 40 Loss: 0.396752\n",
      "Training set: Average loss: 0.209939\n",
      "Validation set: Average loss: 5.560925, Accuracy: 1342/1959 (68.50%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.030406\n",
      "\tTraining batch 2 Loss: 0.000130\n",
      "\tTraining batch 3 Loss: 0.017098\n",
      "\tTraining batch 4 Loss: 0.001797\n",
      "\tTraining batch 5 Loss: 0.024923\n",
      "\tTraining batch 6 Loss: 0.000105\n",
      "\tTraining batch 7 Loss: 0.000061\n",
      "\tTraining batch 8 Loss: 0.148212\n",
      "\tTraining batch 9 Loss: 0.000085\n",
      "\tTraining batch 10 Loss: 0.142365\n",
      "\tTraining batch 11 Loss: 0.000175\n",
      "\tTraining batch 12 Loss: 0.000007\n",
      "\tTraining batch 13 Loss: 0.230777\n",
      "\tTraining batch 14 Loss: 0.055366\n",
      "\tTraining batch 15 Loss: 0.000854\n",
      "\tTraining batch 16 Loss: 0.018778\n",
      "\tTraining batch 17 Loss: 0.000509\n",
      "\tTraining batch 18 Loss: 0.007938\n",
      "\tTraining batch 19 Loss: 0.006623\n",
      "\tTraining batch 20 Loss: 0.000433\n",
      "\tTraining batch 21 Loss: 0.000305\n",
      "\tTraining batch 22 Loss: 0.000048\n",
      "\tTraining batch 23 Loss: 0.001230\n",
      "\tTraining batch 24 Loss: 0.048604\n",
      "\tTraining batch 25 Loss: 0.001809\n",
      "\tTraining batch 26 Loss: 0.003873\n",
      "\tTraining batch 27 Loss: 0.005434\n",
      "\tTraining batch 28 Loss: 0.033847\n",
      "\tTraining batch 29 Loss: 0.000074\n",
      "\tTraining batch 30 Loss: 0.000892\n",
      "\tTraining batch 31 Loss: 0.001815\n",
      "\tTraining batch 32 Loss: 0.000234\n",
      "\tTraining batch 33 Loss: 0.018020\n",
      "\tTraining batch 34 Loss: 0.000096\n",
      "\tTraining batch 35 Loss: 0.002737\n",
      "\tTraining batch 36 Loss: 0.002329\n",
      "\tTraining batch 37 Loss: 0.025192\n",
      "\tTraining batch 38 Loss: 0.069589\n",
      "\tTraining batch 39 Loss: 0.005003\n",
      "\tTraining batch 40 Loss: 0.009280\n",
      "Training set: Average loss: 0.022926\n",
      "Validation set: Average loss: 6.211913, Accuracy: 1328/1959 (67.79%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000026\n",
      "\tTraining batch 2 Loss: 0.000197\n",
      "\tTraining batch 3 Loss: 0.011419\n",
      "\tTraining batch 4 Loss: 0.009489\n",
      "\tTraining batch 5 Loss: 0.001083\n",
      "\tTraining batch 6 Loss: 0.000008\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000035\n",
      "\tTraining batch 9 Loss: 0.000007\n",
      "\tTraining batch 10 Loss: 0.000014\n",
      "\tTraining batch 11 Loss: 0.037532\n",
      "\tTraining batch 12 Loss: 0.000434\n",
      "\tTraining batch 13 Loss: 0.004491\n",
      "\tTraining batch 14 Loss: 0.058512\n",
      "\tTraining batch 15 Loss: 0.000432\n",
      "\tTraining batch 16 Loss: 0.000002\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000116\n",
      "\tTraining batch 19 Loss: 0.000302\n",
      "\tTraining batch 20 Loss: 0.000001\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000001\n",
      "\tTraining batch 24 Loss: 0.000719\n",
      "\tTraining batch 25 Loss: 0.000004\n",
      "\tTraining batch 26 Loss: 0.000385\n",
      "\tTraining batch 27 Loss: 0.000001\n",
      "\tTraining batch 28 Loss: 0.000964\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.000004\n",
      "\tTraining batch 32 Loss: 0.000002\n",
      "\tTraining batch 33 Loss: 0.000003\n",
      "\tTraining batch 34 Loss: 0.060799\n",
      "\tTraining batch 35 Loss: 0.002420\n",
      "\tTraining batch 36 Loss: 0.000750\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000117\n",
      "\tTraining batch 39 Loss: 0.000763\n",
      "\tTraining batch 40 Loss: 0.001361\n",
      "Training set: Average loss: 0.004810\n",
      "Validation set: Average loss: 7.619666, Accuracy: 1332/1959 (67.99%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000258\n",
      "\tTraining batch 3 Loss: 0.000002\n",
      "\tTraining batch 4 Loss: 0.000106\n",
      "\tTraining batch 5 Loss: 0.010757\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000001\n",
      "\tTraining batch 8 Loss: 0.000003\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000029\n",
      "\tTraining batch 11 Loss: 0.000004\n",
      "\tTraining batch 12 Loss: 0.000002\n",
      "\tTraining batch 13 Loss: 0.000030\n",
      "\tTraining batch 14 Loss: 0.010341\n",
      "\tTraining batch 15 Loss: 0.000013\n",
      "\tTraining batch 16 Loss: 0.000532\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000009\n",
      "\tTraining batch 19 Loss: 0.002834\n",
      "\tTraining batch 20 Loss: 0.000048\n",
      "\tTraining batch 21 Loss: 0.000007\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000085\n",
      "\tTraining batch 24 Loss: 0.013480\n",
      "\tTraining batch 25 Loss: 0.000003\n",
      "\tTraining batch 26 Loss: 0.000321\n",
      "\tTraining batch 27 Loss: 0.000002\n",
      "\tTraining batch 28 Loss: 0.000074\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000010\n",
      "\tTraining batch 31 Loss: 0.000034\n",
      "\tTraining batch 32 Loss: 0.000025\n",
      "\tTraining batch 33 Loss: 0.000158\n",
      "\tTraining batch 34 Loss: 0.001078\n",
      "\tTraining batch 35 Loss: 0.000080\n",
      "\tTraining batch 36 Loss: 0.001361\n",
      "\tTraining batch 37 Loss: 0.000020\n",
      "\tTraining batch 38 Loss: 0.000085\n",
      "\tTraining batch 39 Loss: 0.147707\n",
      "\tTraining batch 40 Loss: 0.017145\n",
      "Training set: Average loss: 0.005166\n",
      "Validation set: Average loss: 7.239676, Accuracy: 1351/1959 (68.96%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000004\n",
      "\tTraining batch 2 Loss: 0.000011\n",
      "\tTraining batch 3 Loss: 0.000001\n",
      "\tTraining batch 4 Loss: 0.000034\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000001\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000009\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000003\n",
      "\tTraining batch 11 Loss: 0.000005\n",
      "\tTraining batch 12 Loss: 0.000003\n",
      "\tTraining batch 13 Loss: 0.000294\n",
      "\tTraining batch 14 Loss: 0.028839\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000007\n",
      "\tTraining batch 17 Loss: 0.000003\n",
      "\tTraining batch 18 Loss: 0.000028\n",
      "\tTraining batch 19 Loss: 0.002883\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000003\n",
      "\tTraining batch 22 Loss: 0.000002\n",
      "\tTraining batch 23 Loss: 0.000002\n",
      "\tTraining batch 24 Loss: 0.000067\n",
      "\tTraining batch 25 Loss: 0.000001\n",
      "\tTraining batch 26 Loss: 0.000498\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000036\n",
      "\tTraining batch 29 Loss: 0.000001\n",
      "\tTraining batch 30 Loss: 0.000084\n",
      "\tTraining batch 31 Loss: 0.000001\n",
      "\tTraining batch 32 Loss: 0.000211\n",
      "\tTraining batch 33 Loss: 0.000001\n",
      "\tTraining batch 34 Loss: 0.001540\n",
      "\tTraining batch 35 Loss: 0.000001\n",
      "\tTraining batch 36 Loss: 0.000731\n",
      "\tTraining batch 37 Loss: 0.000002\n",
      "\tTraining batch 38 Loss: 0.000205\n",
      "\tTraining batch 39 Loss: 0.000153\n",
      "\tTraining batch 40 Loss: 0.001159\n",
      "Training set: Average loss: 0.000921\n",
      "Validation set: Average loss: 6.855601, Accuracy: 1343/1959 (68.56%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000036\n",
      "\tTraining batch 2 Loss: 0.000027\n",
      "\tTraining batch 3 Loss: 0.000006\n",
      "\tTraining batch 4 Loss: 0.000077\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000029\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.114367\n",
      "\tTraining batch 11 Loss: 0.000003\n",
      "\tTraining batch 12 Loss: 0.000001\n",
      "\tTraining batch 13 Loss: 0.000189\n",
      "\tTraining batch 14 Loss: 0.001246\n",
      "\tTraining batch 15 Loss: 0.000001\n",
      "\tTraining batch 16 Loss: 0.000066\n",
      "\tTraining batch 17 Loss: 0.000001\n",
      "\tTraining batch 18 Loss: 0.000002\n",
      "\tTraining batch 19 Loss: 0.001645\n",
      "\tTraining batch 20 Loss: 0.000001\n",
      "\tTraining batch 21 Loss: 0.000008\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000007\n",
      "\tTraining batch 24 Loss: 0.000094\n",
      "\tTraining batch 25 Loss: 0.000009\n",
      "\tTraining batch 26 Loss: 0.000251\n",
      "\tTraining batch 27 Loss: 0.000001\n",
      "\tTraining batch 28 Loss: 0.000066\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000002\n",
      "\tTraining batch 31 Loss: 0.000017\n",
      "\tTraining batch 32 Loss: 0.000009\n",
      "\tTraining batch 33 Loss: 0.000020\n",
      "\tTraining batch 34 Loss: 0.000079\n",
      "\tTraining batch 35 Loss: 0.000037\n",
      "\tTraining batch 36 Loss: 0.000412\n",
      "\tTraining batch 37 Loss: 0.000001\n",
      "\tTraining batch 38 Loss: 0.000150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 39 Loss: 0.000145\n",
      "\tTraining batch 40 Loss: 0.006701\n",
      "Training set: Average loss: 0.003143\n",
      "Validation set: Average loss: 7.004296, Accuracy: 1341/1959 (68.45%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000070\n",
      "\tTraining batch 2 Loss: 0.000060\n",
      "\tTraining batch 3 Loss: 0.000001\n",
      "\tTraining batch 4 Loss: 0.000158\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000004\n",
      "\tTraining batch 7 Loss: 0.000001\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000025\n",
      "\tTraining batch 11 Loss: 0.000087\n",
      "\tTraining batch 12 Loss: 0.000001\n",
      "\tTraining batch 13 Loss: 0.000318\n",
      "\tTraining batch 14 Loss: 0.000749\n",
      "\tTraining batch 15 Loss: 0.000022\n",
      "\tTraining batch 16 Loss: 0.000314\n",
      "\tTraining batch 17 Loss: 0.000004\n",
      "\tTraining batch 18 Loss: 0.000002\n",
      "\tTraining batch 19 Loss: 0.000192\n",
      "\tTraining batch 20 Loss: 0.000001\n",
      "\tTraining batch 21 Loss: 0.000012\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000005\n",
      "\tTraining batch 24 Loss: 0.000089\n",
      "\tTraining batch 25 Loss: 0.000015\n",
      "\tTraining batch 26 Loss: 0.000244\n",
      "\tTraining batch 27 Loss: 0.000001\n",
      "\tTraining batch 28 Loss: 0.000070\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000003\n",
      "\tTraining batch 31 Loss: 0.000017\n",
      "\tTraining batch 32 Loss: 0.000005\n",
      "\tTraining batch 33 Loss: 0.000015\n",
      "\tTraining batch 34 Loss: 0.000042\n",
      "\tTraining batch 35 Loss: 0.000034\n",
      "\tTraining batch 36 Loss: 0.000288\n",
      "\tTraining batch 37 Loss: 0.000001\n",
      "\tTraining batch 38 Loss: 0.000137\n",
      "\tTraining batch 39 Loss: 0.000097\n",
      "\tTraining batch 40 Loss: 0.002404\n",
      "Training set: Average loss: 0.000137\n",
      "Validation set: Average loss: 7.103374, Accuracy: 1341/1959 (68.45%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 45.25%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000039\n",
      "\tTraining batch 2 Loss: 0.000056\n",
      "\tTraining batch 3 Loss: 0.000001\n",
      "\tTraining batch 4 Loss: 0.000099\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000005\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000009\n",
      "\tTraining batch 11 Loss: 0.000060\n",
      "\tTraining batch 12 Loss: 0.000001\n",
      "\tTraining batch 13 Loss: 0.000040\n",
      "\tTraining batch 14 Loss: 0.000671\n",
      "\tTraining batch 15 Loss: 0.000012\n",
      "\tTraining batch 16 Loss: 0.000106\n",
      "\tTraining batch 17 Loss: 0.000003\n",
      "\tTraining batch 18 Loss: 0.000002\n",
      "\tTraining batch 19 Loss: 0.000160\n",
      "\tTraining batch 20 Loss: 0.000001\n",
      "\tTraining batch 21 Loss: 0.000009\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000002\n",
      "\tTraining batch 24 Loss: 0.000084\n",
      "\tTraining batch 25 Loss: 0.000013\n",
      "\tTraining batch 26 Loss: 0.000254\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000065\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000002\n",
      "\tTraining batch 31 Loss: 0.000009\n",
      "\tTraining batch 32 Loss: 0.000003\n",
      "\tTraining batch 33 Loss: 0.000007\n",
      "\tTraining batch 34 Loss: 0.000030\n",
      "\tTraining batch 35 Loss: 0.000022\n",
      "\tTraining batch 36 Loss: 0.000213\n",
      "\tTraining batch 37 Loss: 0.000001\n",
      "\tTraining batch 38 Loss: 0.000111\n",
      "\tTraining batch 39 Loss: 0.000061\n",
      "\tTraining batch 40 Loss: 0.000975\n",
      "\tTraining batch 41 Loss: 9.922510\n",
      "Training set: Average loss: 0.242089\n",
      "Validation set: Average loss: 6.403524, Accuracy: 1335/1959 (68.15%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000326\n",
      "\tTraining batch 2 Loss: 0.004769\n",
      "\tTraining batch 3 Loss: 0.000044\n",
      "\tTraining batch 4 Loss: 0.018709\n",
      "\tTraining batch 5 Loss: 0.000131\n",
      "\tTraining batch 6 Loss: 0.014817\n",
      "\tTraining batch 7 Loss: 0.000504\n",
      "\tTraining batch 8 Loss: 0.004820\n",
      "\tTraining batch 9 Loss: 0.005078\n",
      "\tTraining batch 10 Loss: 0.159220\n",
      "\tTraining batch 11 Loss: 0.024168\n",
      "\tTraining batch 12 Loss: 0.186353\n",
      "\tTraining batch 13 Loss: 0.111438\n",
      "\tTraining batch 14 Loss: 0.056276\n",
      "\tTraining batch 15 Loss: 0.272835\n",
      "\tTraining batch 16 Loss: 0.058995\n",
      "\tTraining batch 17 Loss: 0.082790\n",
      "\tTraining batch 18 Loss: 0.070467\n",
      "\tTraining batch 19 Loss: 0.124908\n",
      "\tTraining batch 20 Loss: 0.157646\n",
      "\tTraining batch 21 Loss: 0.322110\n",
      "\tTraining batch 22 Loss: 0.498586\n",
      "\tTraining batch 23 Loss: 0.104335\n",
      "\tTraining batch 24 Loss: 0.037210\n",
      "\tTraining batch 25 Loss: 0.061548\n",
      "\tTraining batch 26 Loss: 0.183570\n",
      "\tTraining batch 27 Loss: 0.112154\n",
      "\tTraining batch 28 Loss: 0.003565\n",
      "\tTraining batch 29 Loss: 0.021832\n",
      "\tTraining batch 30 Loss: 0.443436\n",
      "\tTraining batch 31 Loss: 0.092987\n",
      "\tTraining batch 32 Loss: 0.066732\n",
      "\tTraining batch 33 Loss: 0.019722\n",
      "\tTraining batch 34 Loss: 0.000113\n",
      "\tTraining batch 35 Loss: 0.058737\n",
      "\tTraining batch 36 Loss: 0.003456\n",
      "\tTraining batch 37 Loss: 0.000088\n",
      "\tTraining batch 38 Loss: 0.088252\n",
      "\tTraining batch 39 Loss: 0.000936\n",
      "\tTraining batch 40 Loss: 0.004307\n",
      "\tTraining batch 41 Loss: 0.581739\n",
      "Training set: Average loss: 0.099017\n",
      "Validation set: Average loss: 5.376080, Accuracy: 1331/1959 (67.94%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.001397\n",
      "\tTraining batch 2 Loss: 0.000148\n",
      "\tTraining batch 3 Loss: 0.064641\n",
      "\tTraining batch 4 Loss: 0.000224\n",
      "\tTraining batch 5 Loss: 0.000169\n",
      "\tTraining batch 6 Loss: 0.000044\n",
      "\tTraining batch 7 Loss: 0.003599\n",
      "\tTraining batch 8 Loss: 0.000184\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.004124\n",
      "\tTraining batch 11 Loss: 0.121670\n",
      "\tTraining batch 12 Loss: 0.011586\n",
      "\tTraining batch 13 Loss: 0.000044\n",
      "\tTraining batch 14 Loss: 0.000915\n",
      "\tTraining batch 15 Loss: 0.079990\n",
      "\tTraining batch 16 Loss: 0.006120\n",
      "\tTraining batch 17 Loss: 0.003185\n",
      "\tTraining batch 18 Loss: 0.016854\n",
      "\tTraining batch 19 Loss: 0.037875\n",
      "\tTraining batch 20 Loss: 0.018464\n",
      "\tTraining batch 21 Loss: 0.000384\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.003651\n",
      "\tTraining batch 24 Loss: 0.000027\n",
      "\tTraining batch 25 Loss: 0.166582\n",
      "\tTraining batch 26 Loss: 0.006486\n",
      "\tTraining batch 27 Loss: 0.044501\n",
      "\tTraining batch 28 Loss: 0.000377\n",
      "\tTraining batch 29 Loss: 0.000002\n",
      "\tTraining batch 30 Loss: 0.006753\n",
      "\tTraining batch 31 Loss: 0.000118\n",
      "\tTraining batch 32 Loss: 0.387525\n",
      "\tTraining batch 33 Loss: 0.118776\n",
      "\tTraining batch 34 Loss: 0.000002\n",
      "\tTraining batch 35 Loss: 0.000001\n",
      "\tTraining batch 36 Loss: 0.000109\n",
      "\tTraining batch 37 Loss: 0.000060\n",
      "\tTraining batch 38 Loss: 0.000124\n",
      "\tTraining batch 39 Loss: 0.000005\n",
      "\tTraining batch 40 Loss: 0.000284\n",
      "\tTraining batch 41 Loss: 0.076093\n",
      "Training set: Average loss: 0.028856\n",
      "Validation set: Average loss: 7.961954, Accuracy: 1318/1959 (67.28%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000336\n",
      "\tTraining batch 2 Loss: 0.000004\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000115\n",
      "\tTraining batch 5 Loss: 0.000014\n",
      "\tTraining batch 6 Loss: 0.010006\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000026\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000014\n",
      "\tTraining batch 11 Loss: 0.000023\n",
      "\tTraining batch 12 Loss: 0.000001\n",
      "\tTraining batch 13 Loss: 0.000006\n",
      "\tTraining batch 14 Loss: 0.000355\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000232\n",
      "\tTraining batch 17 Loss: 0.000124\n",
      "\tTraining batch 18 Loss: 0.000037\n",
      "\tTraining batch 19 Loss: 0.000763\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000005\n",
      "\tTraining batch 22 Loss: 0.000001\n",
      "\tTraining batch 23 Loss: 0.063457\n",
      "\tTraining batch 24 Loss: 0.000045\n",
      "\tTraining batch 25 Loss: 0.000046\n",
      "\tTraining batch 26 Loss: 0.000216\n",
      "\tTraining batch 27 Loss: 0.002468\n",
      "\tTraining batch 28 Loss: 0.000027\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000004\n",
      "\tTraining batch 31 Loss: 0.000006\n",
      "\tTraining batch 32 Loss: 0.000245\n",
      "\tTraining batch 33 Loss: 0.040028\n",
      "\tTraining batch 34 Loss: 0.000008\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.001214\n",
      "\tTraining batch 37 Loss: 0.000026\n",
      "\tTraining batch 38 Loss: 0.001151\n",
      "\tTraining batch 39 Loss: 0.000006\n",
      "\tTraining batch 40 Loss: 0.020943\n",
      "\tTraining batch 41 Loss: 0.000109\n",
      "Training set: Average loss: 0.003465\n",
      "Validation set: Average loss: 7.718571, Accuracy: 1327/1959 (67.74%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.030008\n",
      "\tTraining batch 2 Loss: 0.000003\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000242\n",
      "\tTraining batch 5 Loss: 0.000004\n",
      "\tTraining batch 6 Loss: 0.000001\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000004\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.025139\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000001\n",
      "\tTraining batch 13 Loss: 0.000002\n",
      "\tTraining batch 14 Loss: 0.000376\n",
      "\tTraining batch 15 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 16 Loss: 0.000125\n",
      "\tTraining batch 17 Loss: 0.004652\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.007176\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000004\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000017\n",
      "\tTraining batch 24 Loss: 0.000046\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000906\n",
      "\tTraining batch 27 Loss: 0.000049\n",
      "\tTraining batch 28 Loss: 0.000007\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000004\n",
      "\tTraining batch 32 Loss: 0.000012\n",
      "\tTraining batch 33 Loss: 0.000010\n",
      "\tTraining batch 34 Loss: 0.000007\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000035\n",
      "\tTraining batch 37 Loss: 0.000019\n",
      "\tTraining batch 38 Loss: 0.000142\n",
      "\tTraining batch 39 Loss: 0.000020\n",
      "\tTraining batch 40 Loss: 0.002723\n",
      "\tTraining batch 41 Loss: 0.000929\n",
      "Training set: Average loss: 0.001772\n",
      "Validation set: Average loss: 7.150787, Accuracy: 1325/1959 (67.64%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000010\n",
      "\tTraining batch 2 Loss: 0.000003\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000103\n",
      "\tTraining batch 5 Loss: 0.000010\n",
      "\tTraining batch 6 Loss: 0.000001\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000001\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000001\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000001\n",
      "\tTraining batch 13 Loss: 0.000001\n",
      "\tTraining batch 14 Loss: 0.000246\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000064\n",
      "\tTraining batch 17 Loss: 0.000001\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.006503\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000003\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000026\n",
      "\tTraining batch 24 Loss: 0.000045\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000861\n",
      "\tTraining batch 27 Loss: 0.000018\n",
      "\tTraining batch 28 Loss: 0.000006\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000001\n",
      "\tTraining batch 32 Loss: 0.000008\n",
      "\tTraining batch 33 Loss: 0.000012\n",
      "\tTraining batch 34 Loss: 0.000005\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000029\n",
      "\tTraining batch 37 Loss: 0.000011\n",
      "\tTraining batch 38 Loss: 0.000124\n",
      "\tTraining batch 39 Loss: 0.000013\n",
      "\tTraining batch 40 Loss: 0.000323\n",
      "\tTraining batch 41 Loss: 0.000027\n",
      "Training set: Average loss: 0.000206\n",
      "Validation set: Average loss: 7.277244, Accuracy: 1325/1959 (67.64%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 47.5%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000010\n",
      "\tTraining batch 2 Loss: 0.000004\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000091\n",
      "\tTraining batch 5 Loss: 0.000009\n",
      "\tTraining batch 6 Loss: 0.000001\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000001\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000001\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000001\n",
      "\tTraining batch 13 Loss: 0.000001\n",
      "\tTraining batch 14 Loss: 0.000246\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000050\n",
      "\tTraining batch 17 Loss: 0.000001\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.005404\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000002\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000029\n",
      "\tTraining batch 24 Loss: 0.000041\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000674\n",
      "\tTraining batch 27 Loss: 0.000013\n",
      "\tTraining batch 28 Loss: 0.000005\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000001\n",
      "\tTraining batch 32 Loss: 0.000006\n",
      "\tTraining batch 33 Loss: 0.000012\n",
      "\tTraining batch 34 Loss: 0.000004\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000027\n",
      "\tTraining batch 37 Loss: 0.000009\n",
      "\tTraining batch 38 Loss: 0.000113\n",
      "\tTraining batch 39 Loss: 0.000012\n",
      "\tTraining batch 40 Loss: 0.000283\n",
      "\tTraining batch 41 Loss: 0.000024\n",
      "\tTraining batch 42 Loss: 10.301207\n",
      "Training set: Average loss: 0.245435\n",
      "Validation set: Average loss: 5.694704, Accuracy: 1360/1959 (69.42%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000010\n",
      "\tTraining batch 2 Loss: 0.000295\n",
      "\tTraining batch 3 Loss: 0.000052\n",
      "\tTraining batch 4 Loss: 0.010257\n",
      "\tTraining batch 5 Loss: 0.024885\n",
      "\tTraining batch 6 Loss: 0.357770\n",
      "\tTraining batch 7 Loss: 0.005776\n",
      "\tTraining batch 8 Loss: 0.037302\n",
      "\tTraining batch 9 Loss: 0.035990\n",
      "\tTraining batch 10 Loss: 0.137853\n",
      "\tTraining batch 11 Loss: 0.077443\n",
      "\tTraining batch 12 Loss: 0.080553\n",
      "\tTraining batch 13 Loss: 0.036464\n",
      "\tTraining batch 14 Loss: 0.291402\n",
      "\tTraining batch 15 Loss: 0.217151\n",
      "\tTraining batch 16 Loss: 0.133171\n",
      "\tTraining batch 17 Loss: 0.122843\n",
      "\tTraining batch 18 Loss: 0.095638\n",
      "\tTraining batch 19 Loss: 0.205880\n",
      "\tTraining batch 20 Loss: 0.070197\n",
      "\tTraining batch 21 Loss: 0.258147\n",
      "\tTraining batch 22 Loss: 0.525834\n",
      "\tTraining batch 23 Loss: 0.074760\n",
      "\tTraining batch 24 Loss: 0.003544\n",
      "\tTraining batch 25 Loss: 0.293545\n",
      "\tTraining batch 26 Loss: 0.019006\n",
      "\tTraining batch 27 Loss: 0.011167\n",
      "\tTraining batch 28 Loss: 0.237920\n",
      "\tTraining batch 29 Loss: 0.029641\n",
      "\tTraining batch 30 Loss: 0.194552\n",
      "\tTraining batch 31 Loss: 0.000405\n",
      "\tTraining batch 32 Loss: 0.095668\n",
      "\tTraining batch 33 Loss: 0.033688\n",
      "\tTraining batch 34 Loss: 0.046560\n",
      "\tTraining batch 35 Loss: 0.000217\n",
      "\tTraining batch 36 Loss: 0.083803\n",
      "\tTraining batch 37 Loss: 0.000062\n",
      "\tTraining batch 38 Loss: 0.010694\n",
      "\tTraining batch 39 Loss: 0.026119\n",
      "\tTraining batch 40 Loss: 0.289824\n",
      "\tTraining batch 41 Loss: 0.065832\n",
      "\tTraining batch 42 Loss: 0.150983\n",
      "Training set: Average loss: 0.104593\n",
      "Validation set: Average loss: 6.467280, Accuracy: 1338/1959 (68.30%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.001526\n",
      "\tTraining batch 2 Loss: 0.000708\n",
      "\tTraining batch 3 Loss: 0.011116\n",
      "\tTraining batch 4 Loss: 0.020519\n",
      "\tTraining batch 5 Loss: 0.075368\n",
      "\tTraining batch 6 Loss: 0.000006\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000645\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.005573\n",
      "\tTraining batch 11 Loss: 0.000049\n",
      "\tTraining batch 12 Loss: 0.004323\n",
      "\tTraining batch 13 Loss: 0.000042\n",
      "\tTraining batch 14 Loss: 0.002016\n",
      "\tTraining batch 15 Loss: 0.182695\n",
      "\tTraining batch 16 Loss: 0.002981\n",
      "\tTraining batch 17 Loss: 0.000095\n",
      "\tTraining batch 18 Loss: 0.000009\n",
      "\tTraining batch 19 Loss: 0.022977\n",
      "\tTraining batch 20 Loss: 0.000024\n",
      "\tTraining batch 21 Loss: 0.002632\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.007289\n",
      "\tTraining batch 24 Loss: 0.000012\n",
      "\tTraining batch 25 Loss: 0.000044\n",
      "\tTraining batch 26 Loss: 0.001655\n",
      "\tTraining batch 27 Loss: 0.000314\n",
      "\tTraining batch 28 Loss: 0.004411\n",
      "\tTraining batch 29 Loss: 0.000061\n",
      "\tTraining batch 30 Loss: 0.010438\n",
      "\tTraining batch 31 Loss: 0.002156\n",
      "\tTraining batch 32 Loss: 0.001366\n",
      "\tTraining batch 33 Loss: 0.000191\n",
      "\tTraining batch 34 Loss: 0.005243\n",
      "\tTraining batch 35 Loss: 0.032152\n",
      "\tTraining batch 36 Loss: 0.000241\n",
      "\tTraining batch 37 Loss: 0.026045\n",
      "\tTraining batch 38 Loss: 0.006382\n",
      "\tTraining batch 39 Loss: 0.039173\n",
      "\tTraining batch 40 Loss: 0.008202\n",
      "\tTraining batch 41 Loss: 0.347627\n",
      "\tTraining batch 42 Loss: 0.075974\n",
      "Training set: Average loss: 0.021483\n",
      "Validation set: Average loss: 6.171500, Accuracy: 1363/1959 (69.58%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000144\n",
      "\tTraining batch 2 Loss: 0.000026\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.001056\n",
      "\tTraining batch 5 Loss: 0.000072\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000002\n",
      "\tTraining batch 8 Loss: 0.197881\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000013\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000012\n",
      "\tTraining batch 13 Loss: 0.000002\n",
      "\tTraining batch 14 Loss: 0.000254\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000001\n",
      "\tTraining batch 17 Loss: 0.000004\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.003957\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000018\n",
      "\tTraining batch 24 Loss: 0.000009\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000820\n",
      "\tTraining batch 27 Loss: 0.000059\n",
      "\tTraining batch 28 Loss: 0.000002\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000017\n",
      "\tTraining batch 31 Loss: 0.000144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 32 Loss: 0.000473\n",
      "\tTraining batch 33 Loss: 0.000001\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000034\n",
      "\tTraining batch 36 Loss: 0.003833\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000068\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000001\n",
      "\tTraining batch 41 Loss: 0.000058\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "Training set: Average loss: 0.004975\n",
      "Validation set: Average loss: 8.562840, Accuracy: 1348/1959 (68.81%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000247\n",
      "\tTraining batch 2 Loss: 0.000040\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000060\n",
      "\tTraining batch 5 Loss: 0.006332\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000617\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000004\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000002\n",
      "\tTraining batch 13 Loss: 0.000008\n",
      "\tTraining batch 14 Loss: 0.000247\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.002113\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000015\n",
      "\tTraining batch 24 Loss: 0.000007\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000577\n",
      "\tTraining batch 27 Loss: 0.000006\n",
      "\tTraining batch 28 Loss: 0.000001\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000012\n",
      "\tTraining batch 31 Loss: 0.000005\n",
      "\tTraining batch 32 Loss: 0.000009\n",
      "\tTraining batch 33 Loss: 0.000001\n",
      "\tTraining batch 34 Loss: 0.000001\n",
      "\tTraining batch 35 Loss: 0.000011\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000044\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000002\n",
      "\tTraining batch 41 Loss: 0.000017\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "Training set: Average loss: 0.000247\n",
      "Validation set: Average loss: 8.470867, Accuracy: 1345/1959 (68.66%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000009\n",
      "\tTraining batch 2 Loss: 0.000006\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000063\n",
      "\tTraining batch 5 Loss: 0.000197\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000020\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000004\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000002\n",
      "\tTraining batch 13 Loss: 0.000006\n",
      "\tTraining batch 14 Loss: 0.000268\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001846\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000007\n",
      "\tTraining batch 24 Loss: 0.000006\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000496\n",
      "\tTraining batch 27 Loss: 0.000004\n",
      "\tTraining batch 28 Loss: 0.000001\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000011\n",
      "\tTraining batch 31 Loss: 0.000004\n",
      "\tTraining batch 32 Loss: 0.000008\n",
      "\tTraining batch 33 Loss: 0.000001\n",
      "\tTraining batch 34 Loss: 0.000001\n",
      "\tTraining batch 35 Loss: 0.000011\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000040\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000001\n",
      "\tTraining batch 41 Loss: 0.000016\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "Training set: Average loss: 0.000072\n",
      "Validation set: Average loss: 8.495640, Accuracy: 1345/1959 (68.66%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 44.75%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000008\n",
      "\tTraining batch 2 Loss: 0.000006\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000060\n",
      "\tTraining batch 5 Loss: 0.000144\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000011\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000003\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000001\n",
      "\tTraining batch 13 Loss: 0.000006\n",
      "\tTraining batch 14 Loss: 0.000237\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001666\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000004\n",
      "\tTraining batch 24 Loss: 0.000005\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000415\n",
      "\tTraining batch 27 Loss: 0.000004\n",
      "\tTraining batch 28 Loss: 0.000001\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000011\n",
      "\tTraining batch 31 Loss: 0.000004\n",
      "\tTraining batch 32 Loss: 0.000008\n",
      "\tTraining batch 33 Loss: 0.000001\n",
      "\tTraining batch 34 Loss: 0.000001\n",
      "\tTraining batch 35 Loss: 0.000010\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000037\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000001\n",
      "\tTraining batch 41 Loss: 0.000015\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 16.101971\n",
      "Training set: Average loss: 0.374526\n",
      "Validation set: Average loss: 7.464167, Accuracy: 1340/1959 (68.40%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000016\n",
      "\tTraining batch 2 Loss: 0.016228\n",
      "\tTraining batch 3 Loss: 0.000003\n",
      "\tTraining batch 4 Loss: 0.010240\n",
      "\tTraining batch 5 Loss: 0.163444\n",
      "\tTraining batch 6 Loss: 0.028419\n",
      "\tTraining batch 7 Loss: 0.541275\n",
      "\tTraining batch 8 Loss: 0.206514\n",
      "\tTraining batch 9 Loss: 0.978862\n",
      "\tTraining batch 10 Loss: 1.735205\n",
      "\tTraining batch 11 Loss: 0.614009\n",
      "\tTraining batch 12 Loss: 0.109278\n",
      "\tTraining batch 13 Loss: 0.002807\n",
      "\tTraining batch 14 Loss: 0.032417\n",
      "\tTraining batch 15 Loss: 1.514119\n",
      "\tTraining batch 16 Loss: 0.172862\n",
      "\tTraining batch 17 Loss: 0.149012\n",
      "\tTraining batch 18 Loss: 0.133047\n",
      "\tTraining batch 19 Loss: 0.118671\n",
      "\tTraining batch 20 Loss: 0.087075\n",
      "\tTraining batch 21 Loss: 0.170267\n",
      "\tTraining batch 22 Loss: 0.048645\n",
      "\tTraining batch 23 Loss: 0.091454\n",
      "\tTraining batch 24 Loss: 0.535523\n",
      "\tTraining batch 25 Loss: 0.042967\n",
      "\tTraining batch 26 Loss: 0.196101\n",
      "\tTraining batch 27 Loss: 0.274697\n",
      "\tTraining batch 28 Loss: 0.228075\n",
      "\tTraining batch 29 Loss: 0.504738\n",
      "\tTraining batch 30 Loss: 0.065197\n",
      "\tTraining batch 31 Loss: 0.324895\n",
      "\tTraining batch 32 Loss: 0.043124\n",
      "\tTraining batch 33 Loss: 0.119538\n",
      "\tTraining batch 34 Loss: 0.145800\n",
      "\tTraining batch 35 Loss: 0.220583\n",
      "\tTraining batch 36 Loss: 0.004847\n",
      "\tTraining batch 37 Loss: 0.191927\n",
      "\tTraining batch 38 Loss: 0.103255\n",
      "\tTraining batch 39 Loss: 0.022809\n",
      "\tTraining batch 40 Loss: 0.018223\n",
      "\tTraining batch 41 Loss: 0.315035\n",
      "\tTraining batch 42 Loss: 0.002030\n",
      "\tTraining batch 43 Loss: 0.580342\n",
      "Training set: Average loss: 0.252641\n",
      "Validation set: Average loss: 5.637550, Accuracy: 1272/1959 (64.93%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.025336\n",
      "\tTraining batch 2 Loss: 0.004296\n",
      "\tTraining batch 3 Loss: 0.000020\n",
      "\tTraining batch 4 Loss: 0.154276\n",
      "\tTraining batch 5 Loss: 0.097110\n",
      "\tTraining batch 6 Loss: 0.207291\n",
      "\tTraining batch 7 Loss: 0.189207\n",
      "\tTraining batch 8 Loss: 0.747292\n",
      "\tTraining batch 9 Loss: 0.000069\n",
      "\tTraining batch 10 Loss: 0.163902\n",
      "\tTraining batch 11 Loss: 0.001491\n",
      "\tTraining batch 12 Loss: 0.001287\n",
      "\tTraining batch 13 Loss: 0.001606\n",
      "\tTraining batch 14 Loss: 0.056476\n",
      "\tTraining batch 15 Loss: 0.000005\n",
      "\tTraining batch 16 Loss: 0.005601\n",
      "\tTraining batch 17 Loss: 0.000002\n",
      "\tTraining batch 18 Loss: 0.000004\n",
      "\tTraining batch 19 Loss: 0.008106\n",
      "\tTraining batch 20 Loss: 0.000346\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000055\n",
      "\tTraining batch 23 Loss: 0.107388\n",
      "\tTraining batch 24 Loss: 0.000724\n",
      "\tTraining batch 25 Loss: 0.000093\n",
      "\tTraining batch 26 Loss: 0.000811\n",
      "\tTraining batch 27 Loss: 0.008501\n",
      "\tTraining batch 28 Loss: 0.000220\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000171\n",
      "\tTraining batch 31 Loss: 0.000096\n",
      "\tTraining batch 32 Loss: 0.030702\n",
      "\tTraining batch 33 Loss: 0.000659\n",
      "\tTraining batch 34 Loss: 0.000919\n",
      "\tTraining batch 35 Loss: 0.000119\n",
      "\tTraining batch 36 Loss: 0.267986\n",
      "\tTraining batch 37 Loss: 0.029884\n",
      "\tTraining batch 38 Loss: 0.005431\n",
      "\tTraining batch 39 Loss: 0.000477\n",
      "\tTraining batch 40 Loss: 0.000129\n",
      "\tTraining batch 41 Loss: 0.071609\n",
      "\tTraining batch 42 Loss: 0.001731\n",
      "\tTraining batch 43 Loss: 0.233787\n",
      "Training set: Average loss: 0.056400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set: Average loss: 7.406253, Accuracy: 1288/1959 (65.75%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000001\n",
      "\tTraining batch 2 Loss: 0.000854\n",
      "\tTraining batch 3 Loss: 0.000001\n",
      "\tTraining batch 4 Loss: 0.000318\n",
      "\tTraining batch 5 Loss: 0.000236\n",
      "\tTraining batch 6 Loss: 0.000234\n",
      "\tTraining batch 7 Loss: 0.001957\n",
      "\tTraining batch 8 Loss: 0.023495\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.004039\n",
      "\tTraining batch 11 Loss: 0.000002\n",
      "\tTraining batch 12 Loss: 0.000018\n",
      "\tTraining batch 13 Loss: 0.000771\n",
      "\tTraining batch 14 Loss: 0.000838\n",
      "\tTraining batch 15 Loss: 0.011372\n",
      "\tTraining batch 16 Loss: 0.000650\n",
      "\tTraining batch 17 Loss: 0.000272\n",
      "\tTraining batch 18 Loss: 0.000015\n",
      "\tTraining batch 19 Loss: 0.000103\n",
      "\tTraining batch 20 Loss: 0.000025\n",
      "\tTraining batch 21 Loss: 0.000029\n",
      "\tTraining batch 22 Loss: 0.008566\n",
      "\tTraining batch 23 Loss: 0.000004\n",
      "\tTraining batch 24 Loss: 0.000016\n",
      "\tTraining batch 25 Loss: 0.000025\n",
      "\tTraining batch 26 Loss: 0.035695\n",
      "\tTraining batch 27 Loss: 0.000123\n",
      "\tTraining batch 28 Loss: 0.000054\n",
      "\tTraining batch 29 Loss: 0.000001\n",
      "\tTraining batch 30 Loss: 0.000004\n",
      "\tTraining batch 31 Loss: 0.063041\n",
      "\tTraining batch 32 Loss: 0.000006\n",
      "\tTraining batch 33 Loss: 0.001232\n",
      "\tTraining batch 34 Loss: 0.000047\n",
      "\tTraining batch 35 Loss: 0.001167\n",
      "\tTraining batch 36 Loss: 0.002619\n",
      "\tTraining batch 37 Loss: 0.000004\n",
      "\tTraining batch 38 Loss: 0.000152\n",
      "\tTraining batch 39 Loss: 0.000069\n",
      "\tTraining batch 40 Loss: 0.001375\n",
      "\tTraining batch 41 Loss: 0.000003\n",
      "\tTraining batch 42 Loss: 0.022255\n",
      "\tTraining batch 43 Loss: 0.026748\n",
      "Training set: Average loss: 0.004847\n",
      "Validation set: Average loss: 7.206323, Accuracy: 1271/1959 (64.88%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000004\n",
      "\tTraining batch 2 Loss: 0.000057\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000010\n",
      "\tTraining batch 5 Loss: 0.000107\n",
      "\tTraining batch 6 Loss: 0.000327\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000003\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000013\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000001\n",
      "\tTraining batch 13 Loss: 0.000024\n",
      "\tTraining batch 14 Loss: 0.001747\n",
      "\tTraining batch 15 Loss: 0.000013\n",
      "\tTraining batch 16 Loss: 0.000007\n",
      "\tTraining batch 17 Loss: 0.000020\n",
      "\tTraining batch 18 Loss: 0.000013\n",
      "\tTraining batch 19 Loss: 0.000032\n",
      "\tTraining batch 20 Loss: 0.000012\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000019\n",
      "\tTraining batch 25 Loss: 0.000001\n",
      "\tTraining batch 26 Loss: 0.000514\n",
      "\tTraining batch 27 Loss: 0.000032\n",
      "\tTraining batch 28 Loss: 0.000062\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000002\n",
      "\tTraining batch 31 Loss: 0.000004\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000002\n",
      "\tTraining batch 34 Loss: 0.000002\n",
      "\tTraining batch 35 Loss: 0.000451\n",
      "\tTraining batch 36 Loss: 0.000043\n",
      "\tTraining batch 37 Loss: 0.000001\n",
      "\tTraining batch 38 Loss: 0.000102\n",
      "\tTraining batch 39 Loss: 0.000047\n",
      "\tTraining batch 40 Loss: 0.000003\n",
      "\tTraining batch 41 Loss: 0.000003\n",
      "\tTraining batch 42 Loss: 0.000001\n",
      "\tTraining batch 43 Loss: 0.027491\n",
      "Training set: Average loss: 0.000725\n",
      "Validation set: Average loss: 8.164978, Accuracy: 1284/1959 (65.54%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000097\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000027\n",
      "\tTraining batch 5 Loss: 0.000101\n",
      "\tTraining batch 6 Loss: 0.000003\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000001\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000405\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000001\n",
      "\tTraining batch 13 Loss: 0.000032\n",
      "\tTraining batch 14 Loss: 0.001592\n",
      "\tTraining batch 15 Loss: 0.000008\n",
      "\tTraining batch 16 Loss: 0.000005\n",
      "\tTraining batch 17 Loss: 0.000012\n",
      "\tTraining batch 18 Loss: 0.000010\n",
      "\tTraining batch 19 Loss: 0.000027\n",
      "\tTraining batch 20 Loss: 0.000010\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000017\n",
      "\tTraining batch 25 Loss: 0.000001\n",
      "\tTraining batch 26 Loss: 0.000433\n",
      "\tTraining batch 27 Loss: 0.000024\n",
      "\tTraining batch 28 Loss: 0.000052\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000002\n",
      "\tTraining batch 31 Loss: 0.000003\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000002\n",
      "\tTraining batch 34 Loss: 0.000001\n",
      "\tTraining batch 35 Loss: 0.000023\n",
      "\tTraining batch 36 Loss: 0.000031\n",
      "\tTraining batch 37 Loss: 0.000001\n",
      "\tTraining batch 38 Loss: 0.000084\n",
      "\tTraining batch 39 Loss: 0.000032\n",
      "\tTraining batch 40 Loss: 0.000006\n",
      "\tTraining batch 41 Loss: 0.000003\n",
      "\tTraining batch 42 Loss: 0.000001\n",
      "\tTraining batch 43 Loss: 0.021552\n",
      "Training set: Average loss: 0.000572\n",
      "Validation set: Average loss: 8.148143, Accuracy: 1283/1959 (65.49%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000075\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000023\n",
      "\tTraining batch 5 Loss: 0.000086\n",
      "\tTraining batch 6 Loss: 0.000005\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000002\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000003\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000001\n",
      "\tTraining batch 13 Loss: 0.000029\n",
      "\tTraining batch 14 Loss: 0.000774\n",
      "\tTraining batch 15 Loss: 0.000011\n",
      "\tTraining batch 16 Loss: 0.000005\n",
      "\tTraining batch 17 Loss: 0.000017\n",
      "\tTraining batch 18 Loss: 0.000006\n",
      "\tTraining batch 19 Loss: 0.000027\n",
      "\tTraining batch 20 Loss: 0.000008\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000001\n",
      "\tTraining batch 24 Loss: 0.000015\n",
      "\tTraining batch 25 Loss: 0.000001\n",
      "\tTraining batch 26 Loss: 0.000392\n",
      "\tTraining batch 27 Loss: 0.000023\n",
      "\tTraining batch 28 Loss: 0.000045\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000002\n",
      "\tTraining batch 31 Loss: 0.000001\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000003\n",
      "\tTraining batch 34 Loss: 0.000001\n",
      "\tTraining batch 35 Loss: 0.000024\n",
      "\tTraining batch 36 Loss: 0.000030\n",
      "\tTraining batch 37 Loss: 0.000001\n",
      "\tTraining batch 38 Loss: 0.000070\n",
      "\tTraining batch 39 Loss: 0.000022\n",
      "\tTraining batch 40 Loss: 0.000019\n",
      "\tTraining batch 41 Loss: 0.000007\n",
      "\tTraining batch 42 Loss: 0.000001\n",
      "\tTraining batch 43 Loss: 0.021301\n",
      "Training set: Average loss: 0.000536\n",
      "Validation set: Average loss: 8.100834, Accuracy: 1286/1959 (65.65%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000049\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000021\n",
      "\tTraining batch 5 Loss: 0.000059\n",
      "\tTraining batch 6 Loss: 0.000007\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000001\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000003\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000001\n",
      "\tTraining batch 13 Loss: 0.000026\n",
      "\tTraining batch 14 Loss: 0.000559\n",
      "\tTraining batch 15 Loss: 0.000011\n",
      "\tTraining batch 16 Loss: 0.000004\n",
      "\tTraining batch 17 Loss: 0.000015\n",
      "\tTraining batch 18 Loss: 0.000005\n",
      "\tTraining batch 19 Loss: 0.000027\n",
      "\tTraining batch 20 Loss: 0.000007\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000001\n",
      "\tTraining batch 24 Loss: 0.000014\n",
      "\tTraining batch 25 Loss: 0.000001\n",
      "\tTraining batch 26 Loss: 0.000332\n",
      "\tTraining batch 27 Loss: 0.000024\n",
      "\tTraining batch 28 Loss: 0.000042\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000002\n",
      "\tTraining batch 31 Loss: 0.000001\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000003\n",
      "\tTraining batch 34 Loss: 0.000001\n",
      "\tTraining batch 35 Loss: 0.000023\n",
      "\tTraining batch 36 Loss: 0.000027\n",
      "\tTraining batch 37 Loss: 0.000001\n",
      "\tTraining batch 38 Loss: 0.000062\n",
      "\tTraining batch 39 Loss: 0.000019\n",
      "\tTraining batch 40 Loss: 0.000024\n",
      "\tTraining batch 41 Loss: 0.000007\n",
      "\tTraining batch 42 Loss: 0.000001\n",
      "\tTraining batch 43 Loss: 0.020101\n",
      "Training set: Average loss: 0.000500\n",
      "Validation set: Average loss: 8.095569, Accuracy: 1286/1959 (65.65%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 45.75%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000040\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000019\n",
      "\tTraining batch 5 Loss: 0.000046\n",
      "\tTraining batch 6 Loss: 0.000008\n",
      "\tTraining batch 7 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 8 Loss: 0.000001\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000003\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000001\n",
      "\tTraining batch 13 Loss: 0.000024\n",
      "\tTraining batch 14 Loss: 0.000451\n",
      "\tTraining batch 15 Loss: 0.000011\n",
      "\tTraining batch 16 Loss: 0.000004\n",
      "\tTraining batch 17 Loss: 0.000012\n",
      "\tTraining batch 18 Loss: 0.000005\n",
      "\tTraining batch 19 Loss: 0.000026\n",
      "\tTraining batch 20 Loss: 0.000006\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000001\n",
      "\tTraining batch 24 Loss: 0.000013\n",
      "\tTraining batch 25 Loss: 0.000001\n",
      "\tTraining batch 26 Loss: 0.000293\n",
      "\tTraining batch 27 Loss: 0.000024\n",
      "\tTraining batch 28 Loss: 0.000039\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000002\n",
      "\tTraining batch 31 Loss: 0.000001\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000003\n",
      "\tTraining batch 34 Loss: 0.000001\n",
      "\tTraining batch 35 Loss: 0.000021\n",
      "\tTraining batch 36 Loss: 0.000025\n",
      "\tTraining batch 37 Loss: 0.000001\n",
      "\tTraining batch 38 Loss: 0.000055\n",
      "\tTraining batch 39 Loss: 0.000017\n",
      "\tTraining batch 40 Loss: 0.000031\n",
      "\tTraining batch 41 Loss: 0.000008\n",
      "\tTraining batch 42 Loss: 0.000001\n",
      "\tTraining batch 43 Loss: 0.018897\n",
      "\tTraining batch 44 Loss: 11.320806\n",
      "Training set: Average loss: 0.257748\n",
      "Validation set: Average loss: 6.377368, Accuracy: 1330/1959 (67.89%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000001\n",
      "\tTraining batch 2 Loss: 0.004058\n",
      "\tTraining batch 3 Loss: 0.000332\n",
      "\tTraining batch 4 Loss: 0.000149\n",
      "\tTraining batch 5 Loss: 0.000338\n",
      "\tTraining batch 6 Loss: 0.090886\n",
      "\tTraining batch 7 Loss: 0.012808\n",
      "\tTraining batch 8 Loss: 0.056974\n",
      "\tTraining batch 9 Loss: 0.117895\n",
      "\tTraining batch 10 Loss: 0.230817\n",
      "\tTraining batch 11 Loss: 0.043328\n",
      "\tTraining batch 12 Loss: 0.066078\n",
      "\tTraining batch 13 Loss: 0.024172\n",
      "\tTraining batch 14 Loss: 0.012568\n",
      "\tTraining batch 15 Loss: 0.408895\n",
      "\tTraining batch 16 Loss: 0.080487\n",
      "\tTraining batch 17 Loss: 0.171470\n",
      "\tTraining batch 18 Loss: 0.016204\n",
      "\tTraining batch 19 Loss: 0.062658\n",
      "\tTraining batch 20 Loss: 0.133287\n",
      "\tTraining batch 21 Loss: 0.067573\n",
      "\tTraining batch 22 Loss: 0.064825\n",
      "\tTraining batch 23 Loss: 0.168270\n",
      "\tTraining batch 24 Loss: 0.129734\n",
      "\tTraining batch 25 Loss: 0.021700\n",
      "\tTraining batch 26 Loss: 0.124816\n",
      "\tTraining batch 27 Loss: 0.028517\n",
      "\tTraining batch 28 Loss: 0.019077\n",
      "\tTraining batch 29 Loss: 0.000061\n",
      "\tTraining batch 30 Loss: 0.003141\n",
      "\tTraining batch 31 Loss: 0.281250\n",
      "\tTraining batch 32 Loss: 0.030607\n",
      "\tTraining batch 33 Loss: 0.029538\n",
      "\tTraining batch 34 Loss: 0.005176\n",
      "\tTraining batch 35 Loss: 0.032508\n",
      "\tTraining batch 36 Loss: 0.056704\n",
      "\tTraining batch 37 Loss: 0.187757\n",
      "\tTraining batch 38 Loss: 0.005635\n",
      "\tTraining batch 39 Loss: 0.033253\n",
      "\tTraining batch 40 Loss: 0.023706\n",
      "\tTraining batch 41 Loss: 0.001621\n",
      "\tTraining batch 42 Loss: 0.168580\n",
      "\tTraining batch 43 Loss: 0.080057\n",
      "\tTraining batch 44 Loss: 0.441660\n",
      "Training set: Average loss: 0.080436\n",
      "Validation set: Average loss: 5.149788, Accuracy: 1312/1959 (66.97%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000737\n",
      "\tTraining batch 2 Loss: 0.000014\n",
      "\tTraining batch 3 Loss: 0.000068\n",
      "\tTraining batch 4 Loss: 0.000136\n",
      "\tTraining batch 5 Loss: 0.000100\n",
      "\tTraining batch 6 Loss: 0.000033\n",
      "\tTraining batch 7 Loss: 0.000001\n",
      "\tTraining batch 8 Loss: 0.000002\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.137945\n",
      "\tTraining batch 11 Loss: 0.155409\n",
      "\tTraining batch 12 Loss: 0.002138\n",
      "\tTraining batch 13 Loss: 0.006257\n",
      "\tTraining batch 14 Loss: 0.000054\n",
      "\tTraining batch 15 Loss: 0.000006\n",
      "\tTraining batch 16 Loss: 0.000003\n",
      "\tTraining batch 17 Loss: 0.000239\n",
      "\tTraining batch 18 Loss: 0.000007\n",
      "\tTraining batch 19 Loss: 0.069450\n",
      "\tTraining batch 20 Loss: 0.002157\n",
      "\tTraining batch 21 Loss: 0.000032\n",
      "\tTraining batch 22 Loss: 0.000003\n",
      "\tTraining batch 23 Loss: 0.000015\n",
      "\tTraining batch 24 Loss: 0.000001\n",
      "\tTraining batch 25 Loss: 0.000330\n",
      "\tTraining batch 26 Loss: 0.001373\n",
      "\tTraining batch 27 Loss: 0.000008\n",
      "\tTraining batch 28 Loss: 0.001909\n",
      "\tTraining batch 29 Loss: 0.001189\n",
      "\tTraining batch 30 Loss: 0.042036\n",
      "\tTraining batch 31 Loss: 0.000046\n",
      "\tTraining batch 32 Loss: 0.030546\n",
      "\tTraining batch 33 Loss: 0.000037\n",
      "\tTraining batch 34 Loss: 0.000966\n",
      "\tTraining batch 35 Loss: 0.000002\n",
      "\tTraining batch 36 Loss: 0.011786\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000058\n",
      "\tTraining batch 39 Loss: 0.000004\n",
      "\tTraining batch 40 Loss: 0.001545\n",
      "\tTraining batch 41 Loss: 0.000100\n",
      "\tTraining batch 42 Loss: 0.000002\n",
      "\tTraining batch 43 Loss: 0.013411\n",
      "\tTraining batch 44 Loss: 0.016845\n",
      "Training set: Average loss: 0.011295\n",
      "Validation set: Average loss: 6.428863, Accuracy: 1346/1959 (68.71%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.001540\n",
      "\tTraining batch 2 Loss: 0.000001\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000203\n",
      "\tTraining batch 5 Loss: 0.000005\n",
      "\tTraining batch 6 Loss: 0.000110\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.022308\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000048\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000401\n",
      "\tTraining batch 13 Loss: 0.000062\n",
      "\tTraining batch 14 Loss: 0.000170\n",
      "\tTraining batch 15 Loss: 0.000001\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000150\n",
      "\tTraining batch 18 Loss: 0.000016\n",
      "\tTraining batch 19 Loss: 0.000022\n",
      "\tTraining batch 20 Loss: 0.000003\n",
      "\tTraining batch 21 Loss: 0.000001\n",
      "\tTraining batch 22 Loss: 0.000001\n",
      "\tTraining batch 23 Loss: 0.000001\n",
      "\tTraining batch 24 Loss: 0.000006\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000948\n",
      "\tTraining batch 27 Loss: 0.000011\n",
      "\tTraining batch 28 Loss: 0.000004\n",
      "\tTraining batch 29 Loss: 0.000003\n",
      "\tTraining batch 30 Loss: 0.000014\n",
      "\tTraining batch 31 Loss: 0.000003\n",
      "\tTraining batch 32 Loss: 0.000044\n",
      "\tTraining batch 33 Loss: 0.003282\n",
      "\tTraining batch 34 Loss: 0.000198\n",
      "\tTraining batch 35 Loss: 0.000006\n",
      "\tTraining batch 36 Loss: 0.000013\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000015\n",
      "\tTraining batch 39 Loss: 0.000001\n",
      "\tTraining batch 40 Loss: 0.001109\n",
      "\tTraining batch 41 Loss: 0.000008\n",
      "\tTraining batch 42 Loss: 0.000003\n",
      "\tTraining batch 43 Loss: 0.009729\n",
      "\tTraining batch 44 Loss: 0.000970\n",
      "Training set: Average loss: 0.000941\n",
      "Validation set: Average loss: 6.051678, Accuracy: 1347/1959 (68.76%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000013\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000156\n",
      "\tTraining batch 5 Loss: 0.000009\n",
      "\tTraining batch 6 Loss: 0.000045\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000021\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000174\n",
      "\tTraining batch 13 Loss: 0.000024\n",
      "\tTraining batch 14 Loss: 0.000107\n",
      "\tTraining batch 15 Loss: 0.000001\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000015\n",
      "\tTraining batch 18 Loss: 0.000020\n",
      "\tTraining batch 19 Loss: 0.000005\n",
      "\tTraining batch 20 Loss: 0.000001\n",
      "\tTraining batch 21 Loss: 0.000001\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000001\n",
      "\tTraining batch 24 Loss: 0.000006\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000865\n",
      "\tTraining batch 27 Loss: 0.000010\n",
      "\tTraining batch 28 Loss: 0.000002\n",
      "\tTraining batch 29 Loss: 0.000005\n",
      "\tTraining batch 30 Loss: 0.000013\n",
      "\tTraining batch 31 Loss: 0.000001\n",
      "\tTraining batch 32 Loss: 0.000023\n",
      "\tTraining batch 33 Loss: 0.000013\n",
      "\tTraining batch 34 Loss: 0.000139\n",
      "\tTraining batch 35 Loss: 0.000006\n",
      "\tTraining batch 36 Loss: 0.000010\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000012\n",
      "\tTraining batch 39 Loss: 0.000001\n",
      "\tTraining batch 40 Loss: 0.000512\n",
      "\tTraining batch 41 Loss: 0.000008\n",
      "\tTraining batch 42 Loss: 0.000002\n",
      "\tTraining batch 43 Loss: 0.007468\n",
      "\tTraining batch 44 Loss: 0.000581\n",
      "Training set: Average loss: 0.000233\n",
      "Validation set: Average loss: 6.072319, Accuracy: 1346/1959 (68.71%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000011\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000125\n",
      "\tTraining batch 5 Loss: 0.000007\n",
      "\tTraining batch 6 Loss: 0.000017\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000016\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000095\n",
      "\tTraining batch 13 Loss: 0.000019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 14 Loss: 0.000104\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000009\n",
      "\tTraining batch 18 Loss: 0.000014\n",
      "\tTraining batch 19 Loss: 0.000004\n",
      "\tTraining batch 20 Loss: 0.000001\n",
      "\tTraining batch 21 Loss: 0.000001\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000001\n",
      "\tTraining batch 24 Loss: 0.000007\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000753\n",
      "\tTraining batch 27 Loss: 0.000009\n",
      "\tTraining batch 28 Loss: 0.000002\n",
      "\tTraining batch 29 Loss: 0.000006\n",
      "\tTraining batch 30 Loss: 0.000012\n",
      "\tTraining batch 31 Loss: 0.000001\n",
      "\tTraining batch 32 Loss: 0.000023\n",
      "\tTraining batch 33 Loss: 0.000012\n",
      "\tTraining batch 34 Loss: 0.000110\n",
      "\tTraining batch 35 Loss: 0.000005\n",
      "\tTraining batch 36 Loss: 0.000010\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000011\n",
      "\tTraining batch 39 Loss: 0.000001\n",
      "\tTraining batch 40 Loss: 0.000336\n",
      "\tTraining batch 41 Loss: 0.000008\n",
      "\tTraining batch 42 Loss: 0.000002\n",
      "\tTraining batch 43 Loss: 0.005535\n",
      "\tTraining batch 44 Loss: 0.000423\n",
      "Training set: Average loss: 0.000175\n",
      "Validation set: Average loss: 6.102249, Accuracy: 1347/1959 (68.76%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000011\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000130\n",
      "\tTraining batch 5 Loss: 0.000007\n",
      "\tTraining batch 6 Loss: 0.000012\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000013\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000067\n",
      "\tTraining batch 13 Loss: 0.000019\n",
      "\tTraining batch 14 Loss: 0.000103\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000006\n",
      "\tTraining batch 18 Loss: 0.000013\n",
      "\tTraining batch 19 Loss: 0.000003\n",
      "\tTraining batch 20 Loss: 0.000001\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000001\n",
      "\tTraining batch 24 Loss: 0.000008\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000701\n",
      "\tTraining batch 27 Loss: 0.000009\n",
      "\tTraining batch 28 Loss: 0.000001\n",
      "\tTraining batch 29 Loss: 0.000007\n",
      "\tTraining batch 30 Loss: 0.000011\n",
      "\tTraining batch 31 Loss: 0.000001\n",
      "\tTraining batch 32 Loss: 0.000032\n",
      "\tTraining batch 33 Loss: 0.000010\n",
      "\tTraining batch 34 Loss: 0.000094\n",
      "\tTraining batch 35 Loss: 0.000004\n",
      "\tTraining batch 36 Loss: 0.000009\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000011\n",
      "\tTraining batch 39 Loss: 0.000001\n",
      "\tTraining batch 40 Loss: 0.000192\n",
      "\tTraining batch 41 Loss: 0.000009\n",
      "\tTraining batch 42 Loss: 0.000001\n",
      "\tTraining batch 43 Loss: 0.004054\n",
      "\tTraining batch 44 Loss: 0.000304\n",
      "Training set: Average loss: 0.000133\n",
      "Validation set: Average loss: 6.132774, Accuracy: 1346/1959 (68.71%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000011\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000139\n",
      "\tTraining batch 5 Loss: 0.000007\n",
      "\tTraining batch 6 Loss: 0.000010\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000011\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000056\n",
      "\tTraining batch 13 Loss: 0.000020\n",
      "\tTraining batch 14 Loss: 0.000099\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000004\n",
      "\tTraining batch 18 Loss: 0.000013\n",
      "\tTraining batch 19 Loss: 0.000003\n",
      "\tTraining batch 20 Loss: 0.000001\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000001\n",
      "\tTraining batch 24 Loss: 0.000009\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000648\n",
      "\tTraining batch 27 Loss: 0.000009\n",
      "\tTraining batch 28 Loss: 0.000001\n",
      "\tTraining batch 29 Loss: 0.000008\n",
      "\tTraining batch 30 Loss: 0.000011\n",
      "\tTraining batch 31 Loss: 0.000001\n",
      "\tTraining batch 32 Loss: 0.000035\n",
      "\tTraining batch 33 Loss: 0.000010\n",
      "\tTraining batch 34 Loss: 0.000078\n",
      "\tTraining batch 35 Loss: 0.000004\n",
      "\tTraining batch 36 Loss: 0.000009\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000010\n",
      "\tTraining batch 39 Loss: 0.000001\n",
      "\tTraining batch 40 Loss: 0.000143\n",
      "\tTraining batch 41 Loss: 0.000009\n",
      "\tTraining batch 42 Loss: 0.000001\n",
      "\tTraining batch 43 Loss: 0.003233\n",
      "\tTraining batch 44 Loss: 0.000242\n",
      "Training set: Average loss: 0.000110\n",
      "Validation set: Average loss: 6.157042, Accuracy: 1349/1959 (68.86%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000012\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000143\n",
      "\tTraining batch 5 Loss: 0.000006\n",
      "\tTraining batch 6 Loss: 0.000008\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000010\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000048\n",
      "\tTraining batch 13 Loss: 0.000022\n",
      "\tTraining batch 14 Loss: 0.000092\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000003\n",
      "\tTraining batch 18 Loss: 0.000013\n",
      "\tTraining batch 19 Loss: 0.000003\n",
      "\tTraining batch 20 Loss: 0.000001\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000001\n",
      "\tTraining batch 24 Loss: 0.000009\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000577\n",
      "\tTraining batch 27 Loss: 0.000010\n",
      "\tTraining batch 28 Loss: 0.000001\n",
      "\tTraining batch 29 Loss: 0.000009\n",
      "\tTraining batch 30 Loss: 0.000010\n",
      "\tTraining batch 31 Loss: 0.000001\n",
      "\tTraining batch 32 Loss: 0.000031\n",
      "\tTraining batch 33 Loss: 0.000009\n",
      "\tTraining batch 34 Loss: 0.000070\n",
      "\tTraining batch 35 Loss: 0.000003\n",
      "\tTraining batch 36 Loss: 0.000009\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000010\n",
      "\tTraining batch 39 Loss: 0.000001\n",
      "\tTraining batch 40 Loss: 0.000121\n",
      "\tTraining batch 41 Loss: 0.000009\n",
      "\tTraining batch 42 Loss: 0.000001\n",
      "\tTraining batch 43 Loss: 0.002634\n",
      "\tTraining batch 44 Loss: 0.000207\n",
      "Training set: Average loss: 0.000093\n",
      "Validation set: Average loss: 6.177202, Accuracy: 1349/1959 (68.86%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 46.25%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000012\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000144\n",
      "\tTraining batch 5 Loss: 0.000006\n",
      "\tTraining batch 6 Loss: 0.000007\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000010\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000042\n",
      "\tTraining batch 13 Loss: 0.000023\n",
      "\tTraining batch 14 Loss: 0.000086\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000003\n",
      "\tTraining batch 18 Loss: 0.000012\n",
      "\tTraining batch 19 Loss: 0.000003\n",
      "\tTraining batch 20 Loss: 0.000001\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000001\n",
      "\tTraining batch 24 Loss: 0.000009\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000515\n",
      "\tTraining batch 27 Loss: 0.000010\n",
      "\tTraining batch 28 Loss: 0.000001\n",
      "\tTraining batch 29 Loss: 0.000009\n",
      "\tTraining batch 30 Loss: 0.000010\n",
      "\tTraining batch 31 Loss: 0.000001\n",
      "\tTraining batch 32 Loss: 0.000029\n",
      "\tTraining batch 33 Loss: 0.000008\n",
      "\tTraining batch 34 Loss: 0.000063\n",
      "\tTraining batch 35 Loss: 0.000003\n",
      "\tTraining batch 36 Loss: 0.000008\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000010\n",
      "\tTraining batch 39 Loss: 0.000001\n",
      "\tTraining batch 40 Loss: 0.000103\n",
      "\tTraining batch 41 Loss: 0.000009\n",
      "\tTraining batch 42 Loss: 0.000001\n",
      "\tTraining batch 43 Loss: 0.002387\n",
      "\tTraining batch 44 Loss: 0.000182\n",
      "\tTraining batch 45 Loss: 9.088579\n",
      "Training set: Average loss: 0.202051\n",
      "Validation set: Average loss: 5.447418, Accuracy: 1341/1959 (68.45%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000048\n",
      "\tTraining batch 2 Loss: 0.000123\n",
      "\tTraining batch 3 Loss: 0.000001\n",
      "\tTraining batch 4 Loss: 0.005215\n",
      "\tTraining batch 5 Loss: 0.000007\n",
      "\tTraining batch 6 Loss: 0.003019\n",
      "\tTraining batch 7 Loss: 0.000001\n",
      "\tTraining batch 8 Loss: 0.016384\n",
      "\tTraining batch 9 Loss: 0.068654\n",
      "\tTraining batch 10 Loss: 0.018789\n",
      "\tTraining batch 11 Loss: 0.118664\n",
      "\tTraining batch 12 Loss: 0.096769\n",
      "\tTraining batch 13 Loss: 0.006196\n",
      "\tTraining batch 14 Loss: 0.000749\n",
      "\tTraining batch 15 Loss: 0.119917\n",
      "\tTraining batch 16 Loss: 0.621117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 17 Loss: 0.050792\n",
      "\tTraining batch 18 Loss: 0.025544\n",
      "\tTraining batch 19 Loss: 0.254473\n",
      "\tTraining batch 20 Loss: 0.036442\n",
      "\tTraining batch 21 Loss: 0.045565\n",
      "\tTraining batch 22 Loss: 0.021315\n",
      "\tTraining batch 23 Loss: 0.072580\n",
      "\tTraining batch 24 Loss: 0.000173\n",
      "\tTraining batch 25 Loss: 0.273026\n",
      "\tTraining batch 26 Loss: 0.028844\n",
      "\tTraining batch 27 Loss: 0.024537\n",
      "\tTraining batch 28 Loss: 0.174823\n",
      "\tTraining batch 29 Loss: 0.005421\n",
      "\tTraining batch 30 Loss: 0.142523\n",
      "\tTraining batch 31 Loss: 0.001992\n",
      "\tTraining batch 32 Loss: 0.105320\n",
      "\tTraining batch 33 Loss: 0.007472\n",
      "\tTraining batch 34 Loss: 0.135293\n",
      "\tTraining batch 35 Loss: 0.129174\n",
      "\tTraining batch 36 Loss: 0.002738\n",
      "\tTraining batch 37 Loss: 0.114188\n",
      "\tTraining batch 38 Loss: 0.000740\n",
      "\tTraining batch 39 Loss: 0.015698\n",
      "\tTraining batch 40 Loss: 0.005740\n",
      "\tTraining batch 41 Loss: 0.067055\n",
      "\tTraining batch 42 Loss: 0.000983\n",
      "\tTraining batch 43 Loss: 0.149133\n",
      "\tTraining batch 44 Loss: 0.060920\n",
      "\tTraining batch 45 Loss: 0.210513\n",
      "Training set: Average loss: 0.071971\n",
      "Validation set: Average loss: 5.222858, Accuracy: 1350/1959 (68.91%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.005385\n",
      "\tTraining batch 2 Loss: 0.000009\n",
      "\tTraining batch 3 Loss: 0.000066\n",
      "\tTraining batch 4 Loss: 0.000883\n",
      "\tTraining batch 5 Loss: 0.003070\n",
      "\tTraining batch 6 Loss: 0.001929\n",
      "\tTraining batch 7 Loss: 0.000002\n",
      "\tTraining batch 8 Loss: 0.000002\n",
      "\tTraining batch 9 Loss: 0.000002\n",
      "\tTraining batch 10 Loss: 0.061546\n",
      "\tTraining batch 11 Loss: 0.000007\n",
      "\tTraining batch 12 Loss: 0.000578\n",
      "\tTraining batch 13 Loss: 0.003090\n",
      "\tTraining batch 14 Loss: 0.000362\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000006\n",
      "\tTraining batch 17 Loss: 0.000037\n",
      "\tTraining batch 18 Loss: 0.004628\n",
      "\tTraining batch 19 Loss: 0.000109\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000002\n",
      "\tTraining batch 22 Loss: 0.036251\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000712\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000769\n",
      "\tTraining batch 27 Loss: 0.000138\n",
      "\tTraining batch 28 Loss: 0.004357\n",
      "\tTraining batch 29 Loss: 0.000013\n",
      "\tTraining batch 30 Loss: 0.000013\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.001699\n",
      "\tTraining batch 34 Loss: 0.000353\n",
      "\tTraining batch 35 Loss: 0.000309\n",
      "\tTraining batch 36 Loss: 0.000185\n",
      "\tTraining batch 37 Loss: 0.000002\n",
      "\tTraining batch 38 Loss: 0.000412\n",
      "\tTraining batch 39 Loss: 0.000018\n",
      "\tTraining batch 40 Loss: 0.000422\n",
      "\tTraining batch 41 Loss: 0.000023\n",
      "\tTraining batch 42 Loss: 0.000021\n",
      "\tTraining batch 43 Loss: 0.002753\n",
      "\tTraining batch 44 Loss: 0.000154\n",
      "\tTraining batch 45 Loss: 0.341734\n",
      "Training set: Average loss: 0.010490\n",
      "Validation set: Average loss: 6.432868, Accuracy: 1344/1959 (68.61%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.083252\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000003\n",
      "\tTraining batch 4 Loss: 0.009280\n",
      "\tTraining batch 5 Loss: 0.000054\n",
      "\tTraining batch 6 Loss: 0.005900\n",
      "\tTraining batch 7 Loss: 0.000040\n",
      "\tTraining batch 8 Loss: 0.000001\n",
      "\tTraining batch 9 Loss: 0.000041\n",
      "\tTraining batch 10 Loss: 0.001146\n",
      "\tTraining batch 11 Loss: 0.000613\n",
      "\tTraining batch 12 Loss: 0.219861\n",
      "\tTraining batch 13 Loss: 0.001428\n",
      "\tTraining batch 14 Loss: 0.187429\n",
      "\tTraining batch 15 Loss: 0.000001\n",
      "\tTraining batch 16 Loss: 0.000143\n",
      "\tTraining batch 17 Loss: 0.001149\n",
      "\tTraining batch 18 Loss: 0.061313\n",
      "\tTraining batch 19 Loss: 0.000002\n",
      "\tTraining batch 20 Loss: 0.012668\n",
      "\tTraining batch 21 Loss: 0.000009\n",
      "\tTraining batch 22 Loss: 0.000001\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000004\n",
      "\tTraining batch 25 Loss: 0.000018\n",
      "\tTraining batch 26 Loss: 0.001779\n",
      "\tTraining batch 27 Loss: 0.000729\n",
      "\tTraining batch 28 Loss: 0.000604\n",
      "\tTraining batch 29 Loss: 0.000435\n",
      "\tTraining batch 30 Loss: 0.000375\n",
      "\tTraining batch 31 Loss: 0.462232\n",
      "\tTraining batch 32 Loss: 0.004739\n",
      "\tTraining batch 33 Loss: 0.000332\n",
      "\tTraining batch 34 Loss: 0.000206\n",
      "\tTraining batch 35 Loss: 0.000060\n",
      "\tTraining batch 36 Loss: 0.015949\n",
      "\tTraining batch 37 Loss: 0.000008\n",
      "\tTraining batch 38 Loss: 0.000568\n",
      "\tTraining batch 39 Loss: 0.014705\n",
      "\tTraining batch 40 Loss: 0.000119\n",
      "\tTraining batch 41 Loss: 0.000002\n",
      "\tTraining batch 42 Loss: 0.001572\n",
      "\tTraining batch 43 Loss: 0.056501\n",
      "\tTraining batch 44 Loss: 0.002884\n",
      "\tTraining batch 45 Loss: 0.027816\n",
      "Training set: Average loss: 0.026133\n",
      "Validation set: Average loss: 6.178928, Accuracy: 1374/1959 (70.14%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.002084\n",
      "\tTraining batch 2 Loss: 0.000008\n",
      "\tTraining batch 3 Loss: 0.000283\n",
      "\tTraining batch 4 Loss: 0.000052\n",
      "\tTraining batch 5 Loss: 0.000068\n",
      "\tTraining batch 6 Loss: 0.000006\n",
      "\tTraining batch 7 Loss: 0.004624\n",
      "\tTraining batch 8 Loss: 0.000001\n",
      "\tTraining batch 9 Loss: 0.000027\n",
      "\tTraining batch 10 Loss: 0.000026\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000133\n",
      "\tTraining batch 14 Loss: 0.000006\n",
      "\tTraining batch 15 Loss: 0.000004\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000143\n",
      "\tTraining batch 18 Loss: 0.000004\n",
      "\tTraining batch 19 Loss: 0.000002\n",
      "\tTraining batch 20 Loss: 0.000005\n",
      "\tTraining batch 21 Loss: 0.000052\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000012\n",
      "\tTraining batch 24 Loss: 0.000012\n",
      "\tTraining batch 25 Loss: 0.000073\n",
      "\tTraining batch 26 Loss: 0.005886\n",
      "\tTraining batch 27 Loss: 0.000924\n",
      "\tTraining batch 28 Loss: 0.005827\n",
      "\tTraining batch 29 Loss: 0.000004\n",
      "\tTraining batch 30 Loss: 0.000104\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.019657\n",
      "\tTraining batch 33 Loss: 0.000141\n",
      "\tTraining batch 34 Loss: 0.000039\n",
      "\tTraining batch 35 Loss: 0.000101\n",
      "\tTraining batch 36 Loss: 0.000070\n",
      "\tTraining batch 37 Loss: 0.002192\n",
      "\tTraining batch 38 Loss: 0.000067\n",
      "\tTraining batch 39 Loss: 0.000007\n",
      "\tTraining batch 40 Loss: 0.001280\n",
      "\tTraining batch 41 Loss: 0.000034\n",
      "\tTraining batch 42 Loss: 0.000978\n",
      "\tTraining batch 43 Loss: 0.007815\n",
      "\tTraining batch 44 Loss: 0.079822\n",
      "\tTraining batch 45 Loss: 0.000005\n",
      "Training set: Average loss: 0.002946\n",
      "Validation set: Average loss: 5.740907, Accuracy: 1357/1959 (69.27%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000023\n",
      "\tTraining batch 2 Loss: 0.000003\n",
      "\tTraining batch 3 Loss: 0.000010\n",
      "\tTraining batch 4 Loss: 0.000055\n",
      "\tTraining batch 5 Loss: 0.000090\n",
      "\tTraining batch 6 Loss: 0.000003\n",
      "\tTraining batch 7 Loss: 0.000004\n",
      "\tTraining batch 8 Loss: 0.000001\n",
      "\tTraining batch 9 Loss: 0.000258\n",
      "\tTraining batch 10 Loss: 0.000232\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000025\n",
      "\tTraining batch 13 Loss: 0.000081\n",
      "\tTraining batch 14 Loss: 0.000452\n",
      "\tTraining batch 15 Loss: 0.000001\n",
      "\tTraining batch 16 Loss: 0.000001\n",
      "\tTraining batch 17 Loss: 0.000155\n",
      "\tTraining batch 18 Loss: 0.000010\n",
      "\tTraining batch 19 Loss: 0.000017\n",
      "\tTraining batch 20 Loss: 0.000002\n",
      "\tTraining batch 21 Loss: 0.000054\n",
      "\tTraining batch 22 Loss: 0.000001\n",
      "\tTraining batch 23 Loss: 0.000002\n",
      "\tTraining batch 24 Loss: 0.000055\n",
      "\tTraining batch 25 Loss: 0.030156\n",
      "\tTraining batch 26 Loss: 0.000715\n",
      "\tTraining batch 27 Loss: 0.000967\n",
      "\tTraining batch 28 Loss: 0.000016\n",
      "\tTraining batch 29 Loss: 0.000002\n",
      "\tTraining batch 30 Loss: 0.000086\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000462\n",
      "\tTraining batch 34 Loss: 0.000057\n",
      "\tTraining batch 35 Loss: 0.000049\n",
      "\tTraining batch 36 Loss: 0.000019\n",
      "\tTraining batch 37 Loss: 0.000005\n",
      "\tTraining batch 38 Loss: 0.000109\n",
      "\tTraining batch 39 Loss: 0.000039\n",
      "\tTraining batch 40 Loss: 0.001264\n",
      "\tTraining batch 41 Loss: 0.000038\n",
      "\tTraining batch 42 Loss: 0.000325\n",
      "\tTraining batch 43 Loss: 0.006751\n",
      "\tTraining batch 44 Loss: 0.000738\n",
      "\tTraining batch 45 Loss: 0.000419\n",
      "Training set: Average loss: 0.000972\n",
      "Validation set: Average loss: 5.694245, Accuracy: 1345/1959 (68.66%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.001379\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000001\n",
      "\tTraining batch 4 Loss: 0.000169\n",
      "\tTraining batch 5 Loss: 0.000007\n",
      "\tTraining batch 6 Loss: 0.000022\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000001\n",
      "\tTraining batch 10 Loss: 0.000023\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000123\n",
      "\tTraining batch 14 Loss: 0.000003\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000001\n",
      "\tTraining batch 17 Loss: 0.000228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 18 Loss: 0.000009\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000003\n",
      "\tTraining batch 21 Loss: 0.000047\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000001\n",
      "\tTraining batch 25 Loss: 0.000018\n",
      "\tTraining batch 26 Loss: 0.000722\n",
      "\tTraining batch 27 Loss: 0.000005\n",
      "\tTraining batch 28 Loss: 0.000006\n",
      "\tTraining batch 29 Loss: 0.000008\n",
      "\tTraining batch 30 Loss: 0.000052\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000038\n",
      "\tTraining batch 34 Loss: 0.000040\n",
      "\tTraining batch 35 Loss: 0.000036\n",
      "\tTraining batch 36 Loss: 0.000005\n",
      "\tTraining batch 37 Loss: 0.000002\n",
      "\tTraining batch 38 Loss: 0.000062\n",
      "\tTraining batch 39 Loss: 0.000024\n",
      "\tTraining batch 40 Loss: 0.000449\n",
      "\tTraining batch 41 Loss: 0.000011\n",
      "\tTraining batch 42 Loss: 0.000044\n",
      "\tTraining batch 43 Loss: 0.005496\n",
      "\tTraining batch 44 Loss: 0.000285\n",
      "\tTraining batch 45 Loss: 0.000037\n",
      "Training set: Average loss: 0.000208\n",
      "Validation set: Average loss: 5.755040, Accuracy: 1339/1959 (68.35%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000116\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000001\n",
      "\tTraining batch 4 Loss: 0.000179\n",
      "\tTraining batch 5 Loss: 0.000006\n",
      "\tTraining batch 6 Loss: 0.000014\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000001\n",
      "\tTraining batch 10 Loss: 0.000018\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000097\n",
      "\tTraining batch 14 Loss: 0.000003\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000001\n",
      "\tTraining batch 17 Loss: 0.000102\n",
      "\tTraining batch 18 Loss: 0.000007\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000002\n",
      "\tTraining batch 21 Loss: 0.000028\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000001\n",
      "\tTraining batch 25 Loss: 0.000019\n",
      "\tTraining batch 26 Loss: 0.000577\n",
      "\tTraining batch 27 Loss: 0.000003\n",
      "\tTraining batch 28 Loss: 0.000005\n",
      "\tTraining batch 29 Loss: 0.000007\n",
      "\tTraining batch 30 Loss: 0.000038\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000033\n",
      "\tTraining batch 34 Loss: 0.000032\n",
      "\tTraining batch 35 Loss: 0.000026\n",
      "\tTraining batch 36 Loss: 0.000004\n",
      "\tTraining batch 37 Loss: 0.000001\n",
      "\tTraining batch 38 Loss: 0.000050\n",
      "\tTraining batch 39 Loss: 0.000020\n",
      "\tTraining batch 40 Loss: 0.000321\n",
      "\tTraining batch 41 Loss: 0.000009\n",
      "\tTraining batch 42 Loss: 0.000032\n",
      "\tTraining batch 43 Loss: 0.004633\n",
      "\tTraining batch 44 Loss: 0.000190\n",
      "\tTraining batch 45 Loss: 0.000023\n",
      "Training set: Average loss: 0.000147\n",
      "Validation set: Average loss: 5.787082, Accuracy: 1342/1959 (68.50%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000076\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000001\n",
      "\tTraining batch 4 Loss: 0.000174\n",
      "\tTraining batch 5 Loss: 0.000006\n",
      "\tTraining batch 6 Loss: 0.000011\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000016\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000080\n",
      "\tTraining batch 14 Loss: 0.000003\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000001\n",
      "\tTraining batch 17 Loss: 0.000073\n",
      "\tTraining batch 18 Loss: 0.000005\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000002\n",
      "\tTraining batch 21 Loss: 0.000022\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000001\n",
      "\tTraining batch 25 Loss: 0.000019\n",
      "\tTraining batch 26 Loss: 0.000490\n",
      "\tTraining batch 27 Loss: 0.000002\n",
      "\tTraining batch 28 Loss: 0.000005\n",
      "\tTraining batch 29 Loss: 0.000006\n",
      "\tTraining batch 30 Loss: 0.000030\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000031\n",
      "\tTraining batch 34 Loss: 0.000028\n",
      "\tTraining batch 35 Loss: 0.000021\n",
      "\tTraining batch 36 Loss: 0.000003\n",
      "\tTraining batch 37 Loss: 0.000001\n",
      "\tTraining batch 38 Loss: 0.000042\n",
      "\tTraining batch 39 Loss: 0.000018\n",
      "\tTraining batch 40 Loss: 0.000255\n",
      "\tTraining batch 41 Loss: 0.000008\n",
      "\tTraining batch 42 Loss: 0.000025\n",
      "\tTraining batch 43 Loss: 0.003937\n",
      "\tTraining batch 44 Loss: 0.000145\n",
      "\tTraining batch 45 Loss: 0.000016\n",
      "Training set: Average loss: 0.000123\n",
      "Validation set: Average loss: 5.814641, Accuracy: 1342/1959 (68.50%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 48.25%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000055\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000001\n",
      "\tTraining batch 4 Loss: 0.000176\n",
      "\tTraining batch 5 Loss: 0.000005\n",
      "\tTraining batch 6 Loss: 0.000010\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000015\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000067\n",
      "\tTraining batch 14 Loss: 0.000002\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000055\n",
      "\tTraining batch 18 Loss: 0.000004\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000001\n",
      "\tTraining batch 21 Loss: 0.000018\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000001\n",
      "\tTraining batch 25 Loss: 0.000018\n",
      "\tTraining batch 26 Loss: 0.000425\n",
      "\tTraining batch 27 Loss: 0.000002\n",
      "\tTraining batch 28 Loss: 0.000005\n",
      "\tTraining batch 29 Loss: 0.000006\n",
      "\tTraining batch 30 Loss: 0.000025\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000028\n",
      "\tTraining batch 34 Loss: 0.000024\n",
      "\tTraining batch 35 Loss: 0.000017\n",
      "\tTraining batch 36 Loss: 0.000003\n",
      "\tTraining batch 37 Loss: 0.000001\n",
      "\tTraining batch 38 Loss: 0.000036\n",
      "\tTraining batch 39 Loss: 0.000018\n",
      "\tTraining batch 40 Loss: 0.000211\n",
      "\tTraining batch 41 Loss: 0.000008\n",
      "\tTraining batch 42 Loss: 0.000021\n",
      "\tTraining batch 43 Loss: 0.003383\n",
      "\tTraining batch 44 Loss: 0.000117\n",
      "\tTraining batch 45 Loss: 0.000013\n",
      "\tTraining batch 46 Loss: 10.659292\n",
      "Training set: Average loss: 0.231828\n",
      "Validation set: Average loss: 4.970825, Accuracy: 1334/1959 (68.10%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000278\n",
      "\tTraining batch 2 Loss: 0.063219\n",
      "\tTraining batch 3 Loss: 0.000300\n",
      "\tTraining batch 4 Loss: 0.001711\n",
      "\tTraining batch 5 Loss: 0.021168\n",
      "\tTraining batch 6 Loss: 0.013961\n",
      "\tTraining batch 7 Loss: 0.001277\n",
      "\tTraining batch 8 Loss: 0.018881\n",
      "\tTraining batch 9 Loss: 0.104089\n",
      "\tTraining batch 10 Loss: 0.033507\n",
      "\tTraining batch 11 Loss: 0.032169\n",
      "\tTraining batch 12 Loss: 0.051949\n",
      "\tTraining batch 13 Loss: 0.098925\n",
      "\tTraining batch 14 Loss: 0.026629\n",
      "\tTraining batch 15 Loss: 0.378110\n",
      "\tTraining batch 16 Loss: 0.468080\n",
      "\tTraining batch 17 Loss: 0.220159\n",
      "\tTraining batch 18 Loss: 0.133558\n",
      "\tTraining batch 19 Loss: 0.088915\n",
      "\tTraining batch 20 Loss: 0.063096\n",
      "\tTraining batch 21 Loss: 0.135975\n",
      "\tTraining batch 22 Loss: 0.017754\n",
      "\tTraining batch 23 Loss: 0.145505\n",
      "\tTraining batch 24 Loss: 0.071868\n",
      "\tTraining batch 25 Loss: 0.022414\n",
      "\tTraining batch 26 Loss: 0.140630\n",
      "\tTraining batch 27 Loss: 0.045059\n",
      "\tTraining batch 28 Loss: 0.021988\n",
      "\tTraining batch 29 Loss: 0.011603\n",
      "\tTraining batch 30 Loss: 0.173251\n",
      "\tTraining batch 31 Loss: 0.000743\n",
      "\tTraining batch 32 Loss: 0.003150\n",
      "\tTraining batch 33 Loss: 0.004755\n",
      "\tTraining batch 34 Loss: 0.033137\n",
      "\tTraining batch 35 Loss: 0.107427\n",
      "\tTraining batch 36 Loss: 0.024853\n",
      "\tTraining batch 37 Loss: 0.057709\n",
      "\tTraining batch 38 Loss: 0.000323\n",
      "\tTraining batch 39 Loss: 0.000862\n",
      "\tTraining batch 40 Loss: 0.079900\n",
      "\tTraining batch 41 Loss: 0.000073\n",
      "\tTraining batch 42 Loss: 0.341359\n",
      "\tTraining batch 43 Loss: 0.030720\n",
      "\tTraining batch 44 Loss: 0.183145\n",
      "\tTraining batch 45 Loss: 0.300891\n",
      "\tTraining batch 46 Loss: 2.195086\n",
      "Training set: Average loss: 0.129786\n",
      "Validation set: Average loss: 6.807977, Accuracy: 1294/1959 (66.05%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000099\n",
      "\tTraining batch 2 Loss: 0.000275\n",
      "\tTraining batch 3 Loss: 0.004370\n",
      "\tTraining batch 4 Loss: 0.139191\n",
      "\tTraining batch 5 Loss: 0.000126\n",
      "\tTraining batch 6 Loss: 0.000007\n",
      "\tTraining batch 7 Loss: 0.001221\n",
      "\tTraining batch 8 Loss: 0.000044\n",
      "\tTraining batch 9 Loss: 0.297507\n",
      "\tTraining batch 10 Loss: 0.000121\n",
      "\tTraining batch 11 Loss: 0.115036\n",
      "\tTraining batch 12 Loss: 0.000032\n",
      "\tTraining batch 13 Loss: 0.000234\n",
      "\tTraining batch 14 Loss: 0.000314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 15 Loss: 0.258722\n",
      "\tTraining batch 16 Loss: 0.002588\n",
      "\tTraining batch 17 Loss: 0.000105\n",
      "\tTraining batch 18 Loss: 0.000528\n",
      "\tTraining batch 19 Loss: 0.006207\n",
      "\tTraining batch 20 Loss: 0.274928\n",
      "\tTraining batch 21 Loss: 0.033711\n",
      "\tTraining batch 22 Loss: 0.008236\n",
      "\tTraining batch 23 Loss: 0.419013\n",
      "\tTraining batch 24 Loss: 0.000146\n",
      "\tTraining batch 25 Loss: 0.047810\n",
      "\tTraining batch 26 Loss: 0.015683\n",
      "\tTraining batch 27 Loss: 0.323837\n",
      "\tTraining batch 28 Loss: 0.183780\n",
      "\tTraining batch 29 Loss: 0.099443\n",
      "\tTraining batch 30 Loss: 0.006547\n",
      "\tTraining batch 31 Loss: 0.000053\n",
      "\tTraining batch 32 Loss: 0.000003\n",
      "\tTraining batch 33 Loss: 0.097266\n",
      "\tTraining batch 34 Loss: 0.024508\n",
      "\tTraining batch 35 Loss: 0.008264\n",
      "\tTraining batch 36 Loss: 0.109563\n",
      "\tTraining batch 37 Loss: 0.001473\n",
      "\tTraining batch 38 Loss: 0.001293\n",
      "\tTraining batch 39 Loss: 0.000065\n",
      "\tTraining batch 40 Loss: 0.000016\n",
      "\tTraining batch 41 Loss: 0.000012\n",
      "\tTraining batch 42 Loss: 0.012284\n",
      "\tTraining batch 43 Loss: 0.057447\n",
      "\tTraining batch 44 Loss: 0.000650\n",
      "\tTraining batch 45 Loss: 0.073671\n",
      "\tTraining batch 46 Loss: 0.163495\n",
      "Training set: Average loss: 0.060651\n",
      "Validation set: Average loss: 8.384599, Accuracy: 1296/1959 (66.16%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.006826\n",
      "\tTraining batch 2 Loss: 0.000124\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000294\n",
      "\tTraining batch 5 Loss: 0.000501\n",
      "\tTraining batch 6 Loss: 0.000048\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.094308\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000140\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000001\n",
      "\tTraining batch 13 Loss: 0.000011\n",
      "\tTraining batch 14 Loss: 0.015040\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000010\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000099\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000002\n",
      "\tTraining batch 22 Loss: 0.013751\n",
      "\tTraining batch 23 Loss: 0.000030\n",
      "\tTraining batch 24 Loss: 0.000002\n",
      "\tTraining batch 25 Loss: 0.000116\n",
      "\tTraining batch 26 Loss: 0.000085\n",
      "\tTraining batch 27 Loss: 0.000023\n",
      "\tTraining batch 28 Loss: 0.000004\n",
      "\tTraining batch 29 Loss: 0.052915\n",
      "\tTraining batch 30 Loss: 0.026678\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000921\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000553\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000003\n",
      "\tTraining batch 38 Loss: 0.001698\n",
      "\tTraining batch 39 Loss: 0.000089\n",
      "\tTraining batch 40 Loss: 0.008493\n",
      "\tTraining batch 41 Loss: 0.000035\n",
      "\tTraining batch 42 Loss: 0.000042\n",
      "\tTraining batch 43 Loss: 0.019186\n",
      "\tTraining batch 44 Loss: 0.000051\n",
      "\tTraining batch 45 Loss: 0.000232\n",
      "\tTraining batch 46 Loss: 0.001422\n",
      "Training set: Average loss: 0.005299\n",
      "Validation set: Average loss: 7.945698, Accuracy: 1327/1959 (67.74%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.067099\n",
      "\tTraining batch 2 Loss: 0.000002\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000019\n",
      "\tTraining batch 5 Loss: 0.000045\n",
      "\tTraining batch 6 Loss: 0.000058\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.007314\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000047\n",
      "\tTraining batch 14 Loss: 0.024123\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000077\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000036\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.020138\n",
      "\tTraining batch 26 Loss: 0.000157\n",
      "\tTraining batch 27 Loss: 0.000051\n",
      "\tTraining batch 28 Loss: 0.000001\n",
      "\tTraining batch 29 Loss: 0.001056\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000001\n",
      "\tTraining batch 33 Loss: 0.000379\n",
      "\tTraining batch 34 Loss: 0.000001\n",
      "\tTraining batch 35 Loss: 0.000481\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000002\n",
      "\tTraining batch 38 Loss: 0.000061\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000077\n",
      "\tTraining batch 41 Loss: 0.000007\n",
      "\tTraining batch 42 Loss: 0.000001\n",
      "\tTraining batch 43 Loss: 0.010910\n",
      "\tTraining batch 44 Loss: 0.000149\n",
      "\tTraining batch 45 Loss: 0.000001\n",
      "\tTraining batch 46 Loss: 0.043414\n",
      "Training set: Average loss: 0.003820\n",
      "Validation set: Average loss: 7.534954, Accuracy: 1340/1959 (68.40%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000002\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.089781\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000052\n",
      "\tTraining batch 14 Loss: 0.000047\n",
      "\tTraining batch 15 Loss: 0.000002\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000065\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000048\n",
      "\tTraining batch 26 Loss: 0.000076\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000030\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000301\n",
      "\tTraining batch 35 Loss: 0.000174\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.017809\n",
      "\tTraining batch 38 Loss: 0.000040\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.002305\n",
      "\tTraining batch 41 Loss: 0.000124\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.072361\n",
      "\tTraining batch 44 Loss: 0.000011\n",
      "\tTraining batch 45 Loss: 0.000288\n",
      "\tTraining batch 46 Loss: 0.000015\n",
      "Training set: Average loss: 0.003990\n",
      "Validation set: Average loss: 8.396474, Accuracy: 1326/1959 (67.69%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000855\n",
      "\tTraining batch 2 Loss: 0.000005\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000075\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000144\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000002\n",
      "\tTraining batch 14 Loss: 0.000722\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.015339\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.001484\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000574\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000001\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000059\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000008\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000004\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000963\n",
      "\tTraining batch 44 Loss: 0.000507\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000023\n",
      "Training set: Average loss: 0.000451\n",
      "Validation set: Average loss: 8.448922, Accuracy: 1332/1959 (67.99%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000002\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000002\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000030\n",
      "\tTraining batch 14 Loss: 0.000035\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000015\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000084\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000002\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000036\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000003\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000004\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000848\n",
      "\tTraining batch 44 Loss: 0.000001\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000021\n",
      "Training set: Average loss: 0.000024\n",
      "Validation set: Average loss: 8.450371, Accuracy: 1330/1959 (67.89%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000002\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000002\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000029\n",
      "\tTraining batch 14 Loss: 0.000033\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000014\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000081\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000002\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000030\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000002\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000004\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000710\n",
      "\tTraining batch 44 Loss: 0.000001\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000020\n",
      "Training set: Average loss: 0.000020\n",
      "Validation set: Average loss: 8.454905, Accuracy: 1329/1959 (67.84%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000001\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000002\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000028\n",
      "\tTraining batch 14 Loss: 0.000033\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000013\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000080\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000002\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000025\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000002\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000004\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000603\n",
      "\tTraining batch 44 Loss: 0.000001\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000019\n",
      "Training set: Average loss: 0.000018\n",
      "Validation set: Average loss: 8.459558, Accuracy: 1331/1959 (67.94%)\n",
      "\n",
      "Epoch: 11\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000001\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000002\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000027\n",
      "\tTraining batch 14 Loss: 0.000034\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000012\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000080\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000001\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000022\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000002\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000004\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000523\n",
      "\tTraining batch 44 Loss: 0.000001\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000018\n",
      "Training set: Average loss: 0.000016\n",
      "Validation set: Average loss: 8.463637, Accuracy: 1332/1959 (67.99%)\n",
      "\n",
      "Epoch: 12\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000001\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000002\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000026\n",
      "\tTraining batch 14 Loss: 0.000034\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000011\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000079\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000001\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000019\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000002\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000004\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000462\n",
      "\tTraining batch 44 Loss: 0.000001\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000017\n",
      "Training set: Average loss: 0.000014\n",
      "Validation set: Average loss: 8.467105, Accuracy: 1333/1959 (68.04%)\n",
      "\n",
      "Epoch: 13\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000001\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 6 Loss: 0.000002\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000025\n",
      "\tTraining batch 14 Loss: 0.000033\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000011\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000077\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000001\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000017\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000002\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000004\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000414\n",
      "\tTraining batch 44 Loss: 0.000001\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000016\n",
      "Training set: Average loss: 0.000013\n",
      "Validation set: Average loss: 8.470006, Accuracy: 1333/1959 (68.04%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 46.5%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000001\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000002\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000023\n",
      "\tTraining batch 14 Loss: 0.000033\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000010\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000075\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000001\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000015\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000002\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000004\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000374\n",
      "\tTraining batch 44 Loss: 0.000001\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000015\n",
      "\tTraining batch 47 Loss: 8.370539\n",
      "Training set: Average loss: 0.178108\n",
      "Validation set: Average loss: 7.972613, Accuracy: 1359/1959 (69.37%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000002\n",
      "\tTraining batch 2 Loss: 0.000054\n",
      "\tTraining batch 3 Loss: 0.002496\n",
      "\tTraining batch 4 Loss: 0.015048\n",
      "\tTraining batch 5 Loss: 0.211976\n",
      "\tTraining batch 6 Loss: 0.003878\n",
      "\tTraining batch 7 Loss: 0.004764\n",
      "\tTraining batch 8 Loss: 0.311625\n",
      "\tTraining batch 9 Loss: 0.091438\n",
      "\tTraining batch 10 Loss: 0.122782\n",
      "\tTraining batch 11 Loss: 0.284604\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000129\n",
      "\tTraining batch 14 Loss: 0.000226\n",
      "\tTraining batch 15 Loss: 0.009141\n",
      "\tTraining batch 16 Loss: 0.004808\n",
      "\tTraining batch 17 Loss: 0.000024\n",
      "\tTraining batch 18 Loss: 0.631434\n",
      "\tTraining batch 19 Loss: 0.000092\n",
      "\tTraining batch 20 Loss: 0.005299\n",
      "\tTraining batch 21 Loss: 0.000183\n",
      "\tTraining batch 22 Loss: 0.041841\n",
      "\tTraining batch 23 Loss: 0.180919\n",
      "\tTraining batch 24 Loss: 0.004497\n",
      "\tTraining batch 25 Loss: 0.000337\n",
      "\tTraining batch 26 Loss: 0.223080\n",
      "\tTraining batch 27 Loss: 0.006142\n",
      "\tTraining batch 28 Loss: 0.004210\n",
      "\tTraining batch 29 Loss: 0.364003\n",
      "\tTraining batch 30 Loss: 0.332420\n",
      "\tTraining batch 31 Loss: 0.000219\n",
      "\tTraining batch 32 Loss: 0.264250\n",
      "\tTraining batch 33 Loss: 0.083157\n",
      "\tTraining batch 34 Loss: 0.026439\n",
      "\tTraining batch 35 Loss: 0.525915\n",
      "\tTraining batch 36 Loss: 0.001504\n",
      "\tTraining batch 37 Loss: 0.419801\n",
      "\tTraining batch 38 Loss: 0.004411\n",
      "\tTraining batch 39 Loss: 0.006532\n",
      "\tTraining batch 40 Loss: 0.003473\n",
      "\tTraining batch 41 Loss: 0.010921\n",
      "\tTraining batch 42 Loss: 0.000002\n",
      "\tTraining batch 43 Loss: 0.415246\n",
      "\tTraining batch 44 Loss: 0.093478\n",
      "\tTraining batch 45 Loss: 0.000083\n",
      "\tTraining batch 46 Loss: 0.066300\n",
      "\tTraining batch 47 Loss: 0.006839\n",
      "Training set: Average loss: 0.101830\n",
      "Validation set: Average loss: 8.067616, Accuracy: 1350/1959 (68.91%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.006130\n",
      "\tTraining batch 2 Loss: 0.006328\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.043710\n",
      "\tTraining batch 5 Loss: 0.000060\n",
      "\tTraining batch 6 Loss: 0.327936\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000019\n",
      "\tTraining batch 10 Loss: 0.009397\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000135\n",
      "\tTraining batch 14 Loss: 0.104642\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.018613\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000027\n",
      "\tTraining batch 19 Loss: 0.000002\n",
      "\tTraining batch 20 Loss: 0.000014\n",
      "\tTraining batch 21 Loss: 0.000646\n",
      "\tTraining batch 22 Loss: 0.023126\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000003\n",
      "\tTraining batch 25 Loss: 0.411488\n",
      "\tTraining batch 26 Loss: 0.000012\n",
      "\tTraining batch 27 Loss: 0.009701\n",
      "\tTraining batch 28 Loss: 0.000023\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.031101\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000410\n",
      "\tTraining batch 34 Loss: 0.013311\n",
      "\tTraining batch 35 Loss: 0.000003\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000004\n",
      "\tTraining batch 38 Loss: 0.000015\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.014949\n",
      "\tTraining batch 41 Loss: 0.000411\n",
      "\tTraining batch 42 Loss: 0.038313\n",
      "\tTraining batch 43 Loss: 0.017614\n",
      "\tTraining batch 44 Loss: 0.000020\n",
      "\tTraining batch 45 Loss: 0.199896\n",
      "\tTraining batch 46 Loss: 0.000035\n",
      "\tTraining batch 47 Loss: 0.009622\n",
      "Training set: Average loss: 0.027398\n",
      "Validation set: Average loss: 8.432406, Accuracy: 1364/1959 (69.63%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.006218\n",
      "\tTraining batch 2 Loss: 0.188332\n",
      "\tTraining batch 3 Loss: 0.000033\n",
      "\tTraining batch 4 Loss: 0.000217\n",
      "\tTraining batch 5 Loss: 0.012696\n",
      "\tTraining batch 6 Loss: 0.012712\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000001\n",
      "\tTraining batch 9 Loss: 0.010619\n",
      "\tTraining batch 10 Loss: 0.223868\n",
      "\tTraining batch 11 Loss: 0.445391\n",
      "\tTraining batch 12 Loss: 0.002207\n",
      "\tTraining batch 13 Loss: 0.135433\n",
      "\tTraining batch 14 Loss: 0.000565\n",
      "\tTraining batch 15 Loss: 0.000002\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000059\n",
      "\tTraining batch 19 Loss: 0.000131\n",
      "\tTraining batch 20 Loss: 0.000847\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000222\n",
      "\tTraining batch 26 Loss: 0.007809\n",
      "\tTraining batch 27 Loss: 0.000002\n",
      "\tTraining batch 28 Loss: 0.000058\n",
      "\tTraining batch 29 Loss: 0.005672\n",
      "\tTraining batch 30 Loss: 0.035165\n",
      "\tTraining batch 31 Loss: 0.474727\n",
      "\tTraining batch 32 Loss: 0.015049\n",
      "\tTraining batch 33 Loss: 0.013340\n",
      "\tTraining batch 34 Loss: 0.000002\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000086\n",
      "\tTraining batch 38 Loss: 0.000075\n",
      "\tTraining batch 39 Loss: 0.001497\n",
      "\tTraining batch 40 Loss: 0.000003\n",
      "\tTraining batch 41 Loss: 0.000018\n",
      "\tTraining batch 42 Loss: 0.000942\n",
      "\tTraining batch 43 Loss: 0.016289\n",
      "\tTraining batch 44 Loss: 0.784918\n",
      "\tTraining batch 45 Loss: 0.208353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 46 Loss: 0.020368\n",
      "\tTraining batch 47 Loss: 0.619419\n",
      "Training set: Average loss: 0.069007\n",
      "Validation set: Average loss: 9.566557, Accuracy: 1341/1959 (68.45%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000588\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000619\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.003826\n",
      "\tTraining batch 6 Loss: 0.000359\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000008\n",
      "\tTraining batch 10 Loss: 0.000008\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000442\n",
      "\tTraining batch 14 Loss: 0.534561\n",
      "\tTraining batch 15 Loss: 0.008599\n",
      "\tTraining batch 16 Loss: 0.347153\n",
      "\tTraining batch 17 Loss: 0.117076\n",
      "\tTraining batch 18 Loss: 0.000010\n",
      "\tTraining batch 19 Loss: 0.000249\n",
      "\tTraining batch 20 Loss: 0.000013\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000012\n",
      "\tTraining batch 26 Loss: 0.000005\n",
      "\tTraining batch 27 Loss: 0.000001\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000029\n",
      "\tTraining batch 31 Loss: 0.012132\n",
      "\tTraining batch 32 Loss: 0.215544\n",
      "\tTraining batch 33 Loss: 0.002405\n",
      "\tTraining batch 34 Loss: 0.000959\n",
      "\tTraining batch 35 Loss: 0.000422\n",
      "\tTraining batch 36 Loss: 0.031281\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000015\n",
      "\tTraining batch 39 Loss: 0.001032\n",
      "\tTraining batch 40 Loss: 0.000181\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000014\n",
      "\tTraining batch 43 Loss: 0.001649\n",
      "\tTraining batch 44 Loss: 0.005803\n",
      "\tTraining batch 45 Loss: 0.029145\n",
      "\tTraining batch 46 Loss: 0.000172\n",
      "\tTraining batch 47 Loss: 0.001754\n",
      "Training set: Average loss: 0.028001\n",
      "Validation set: Average loss: 8.681759, Accuracy: 1352/1959 (69.01%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000545\n",
      "\tTraining batch 2 Loss: 0.000004\n",
      "\tTraining batch 3 Loss: 0.349017\n",
      "\tTraining batch 4 Loss: 0.000009\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000004\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000003\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000926\n",
      "\tTraining batch 14 Loss: 0.000047\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000001\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.006798\n",
      "\tTraining batch 20 Loss: 0.000110\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000005\n",
      "\tTraining batch 27 Loss: 0.000004\n",
      "\tTraining batch 28 Loss: 0.000046\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000009\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000001\n",
      "\tTraining batch 38 Loss: 0.000015\n",
      "\tTraining batch 39 Loss: 0.000179\n",
      "\tTraining batch 40 Loss: 0.001286\n",
      "\tTraining batch 41 Loss: 0.000005\n",
      "\tTraining batch 42 Loss: 0.000005\n",
      "\tTraining batch 43 Loss: 0.000057\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.049113\n",
      "\tTraining batch 46 Loss: 0.000020\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "Training set: Average loss: 0.008685\n",
      "Validation set: Average loss: 10.111069, Accuracy: 1347/1959 (68.76%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000065\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000002\n",
      "\tTraining batch 14 Loss: 0.000015\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000001\n",
      "\tTraining batch 18 Loss: 0.000738\n",
      "\tTraining batch 19 Loss: 0.060544\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.115370\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000034\n",
      "\tTraining batch 27 Loss: 0.000008\n",
      "\tTraining batch 28 Loss: 0.002921\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.150451\n",
      "\tTraining batch 31 Loss: 0.000108\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.001014\n",
      "\tTraining batch 34 Loss: 0.000002\n",
      "\tTraining batch 35 Loss: 0.000013\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000003\n",
      "\tTraining batch 39 Loss: 0.000010\n",
      "\tTraining batch 40 Loss: 0.000194\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000001\n",
      "\tTraining batch 43 Loss: 0.004193\n",
      "\tTraining batch 44 Loss: 0.000004\n",
      "\tTraining batch 45 Loss: 0.000012\n",
      "\tTraining batch 46 Loss: 0.000046\n",
      "\tTraining batch 47 Loss: 0.000198\n",
      "Training set: Average loss: 0.007148\n",
      "Validation set: Average loss: 8.898889, Accuracy: 1353/1959 (69.07%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000024\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.001040\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000013\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000006\n",
      "\tTraining batch 19 Loss: 0.000090\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000003\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000001\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000006\n",
      "\tTraining batch 27 Loss: 0.000003\n",
      "\tTraining batch 28 Loss: 0.000001\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000004\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000003\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000005\n",
      "\tTraining batch 39 Loss: 0.000104\n",
      "\tTraining batch 40 Loss: 0.000013\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.003351\n",
      "\tTraining batch 44 Loss: 0.000008\n",
      "\tTraining batch 45 Loss: 0.000053\n",
      "\tTraining batch 46 Loss: 0.000096\n",
      "\tTraining batch 47 Loss: 0.000543\n",
      "Training set: Average loss: 0.000114\n",
      "Validation set: Average loss: 8.918213, Accuracy: 1348/1959 (68.81%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000003\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000020\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000012\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000005\n",
      "\tTraining batch 19 Loss: 0.000086\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000003\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000001\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000005\n",
      "\tTraining batch 27 Loss: 0.000003\n",
      "\tTraining batch 28 Loss: 0.000001\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000002\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000003\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000005\n",
      "\tTraining batch 39 Loss: 0.000082\n",
      "\tTraining batch 40 Loss: 0.000010\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.002961\n",
      "\tTraining batch 44 Loss: 0.000006\n",
      "\tTraining batch 45 Loss: 0.000044\n",
      "\tTraining batch 46 Loss: 0.000064\n",
      "\tTraining batch 47 Loss: 0.000293\n",
      "Training set: Average loss: 0.000077\n",
      "Validation set: Average loss: 8.942413, Accuracy: 1347/1959 (68.76%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000003\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000021\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000011\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000004\n",
      "\tTraining batch 19 Loss: 0.000082\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000003\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000001\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000005\n",
      "\tTraining batch 27 Loss: 0.000002\n",
      "\tTraining batch 28 Loss: 0.000001\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000003\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000004\n",
      "\tTraining batch 39 Loss: 0.000068\n",
      "\tTraining batch 40 Loss: 0.000009\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.002630\n",
      "\tTraining batch 44 Loss: 0.000006\n",
      "\tTraining batch 45 Loss: 0.000037\n",
      "\tTraining batch 46 Loss: 0.000055\n",
      "\tTraining batch 47 Loss: 0.000207\n",
      "Training set: Average loss: 0.000067\n",
      "Validation set: Average loss: 8.963594, Accuracy: 1346/1959 (68.71%)\n",
      "\n",
      "Epoch: 11\n",
      "\tTraining batch 1 Loss: 0.000003\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000022\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000010\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000004\n",
      "\tTraining batch 19 Loss: 0.000078\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000003\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000001\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000005\n",
      "\tTraining batch 27 Loss: 0.000002\n",
      "\tTraining batch 28 Loss: 0.000001\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000003\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000004\n",
      "\tTraining batch 39 Loss: 0.000059\n",
      "\tTraining batch 40 Loss: 0.000008\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.002347\n",
      "\tTraining batch 44 Loss: 0.000005\n",
      "\tTraining batch 45 Loss: 0.000032\n",
      "\tTraining batch 46 Loss: 0.000051\n",
      "\tTraining batch 47 Loss: 0.000164\n",
      "Training set: Average loss: 0.000060\n",
      "Validation set: Average loss: 8.982429, Accuracy: 1344/1959 (68.61%)\n",
      "\n",
      "Epoch: 12\n",
      "\tTraining batch 1 Loss: 0.000003\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000023\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000009\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000003\n",
      "\tTraining batch 19 Loss: 0.000075\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000003\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000001\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000005\n",
      "\tTraining batch 27 Loss: 0.000002\n",
      "\tTraining batch 28 Loss: 0.000001\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000002\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000004\n",
      "\tTraining batch 39 Loss: 0.000053\n",
      "\tTraining batch 40 Loss: 0.000007\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.002109\n",
      "\tTraining batch 44 Loss: 0.000005\n",
      "\tTraining batch 45 Loss: 0.000029\n",
      "\tTraining batch 46 Loss: 0.000048\n",
      "\tTraining batch 47 Loss: 0.000138\n",
      "Training set: Average loss: 0.000054\n",
      "Validation set: Average loss: 8.998789, Accuracy: 1345/1959 (68.66%)\n",
      "\n",
      "Epoch: 13\n",
      "\tTraining batch 1 Loss: 0.000003\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000023\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000008\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000003\n",
      "\tTraining batch 19 Loss: 0.000071\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000002\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000001\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000005\n",
      "\tTraining batch 27 Loss: 0.000002\n",
      "\tTraining batch 28 Loss: 0.000001\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000002\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000004\n",
      "\tTraining batch 39 Loss: 0.000048\n",
      "\tTraining batch 40 Loss: 0.000006\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.001903\n",
      "\tTraining batch 44 Loss: 0.000004\n",
      "\tTraining batch 45 Loss: 0.000026\n",
      "\tTraining batch 46 Loss: 0.000045\n",
      "\tTraining batch 47 Loss: 0.000119\n",
      "Training set: Average loss: 0.000048\n",
      "Validation set: Average loss: 9.014175, Accuracy: 1346/1959 (68.71%)\n",
      "\n",
      "Epoch: 14\n",
      "\tTraining batch 1 Loss: 0.000003\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000023\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000008\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000003\n",
      "\tTraining batch 19 Loss: 0.000068\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000002\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000001\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000005\n",
      "\tTraining batch 27 Loss: 0.000002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 28 Loss: 0.000001\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000002\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000004\n",
      "\tTraining batch 39 Loss: 0.000044\n",
      "\tTraining batch 40 Loss: 0.000006\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.001723\n",
      "\tTraining batch 44 Loss: 0.000004\n",
      "\tTraining batch 45 Loss: 0.000023\n",
      "\tTraining batch 46 Loss: 0.000043\n",
      "\tTraining batch 47 Loss: 0.000104\n",
      "Training set: Average loss: 0.000044\n",
      "Validation set: Average loss: 9.029299, Accuracy: 1344/1959 (68.61%)\n",
      "\n",
      "Epoch: 15\n",
      "\tTraining batch 1 Loss: 0.000002\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000023\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000007\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000003\n",
      "\tTraining batch 19 Loss: 0.000065\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000002\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000001\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000005\n",
      "\tTraining batch 27 Loss: 0.000002\n",
      "\tTraining batch 28 Loss: 0.000001\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000002\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000003\n",
      "\tTraining batch 39 Loss: 0.000041\n",
      "\tTraining batch 40 Loss: 0.000005\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.001566\n",
      "\tTraining batch 44 Loss: 0.000004\n",
      "\tTraining batch 45 Loss: 0.000021\n",
      "\tTraining batch 46 Loss: 0.000041\n",
      "\tTraining batch 47 Loss: 0.000093\n",
      "Training set: Average loss: 0.000040\n",
      "Validation set: Average loss: 9.043753, Accuracy: 1344/1959 (68.61%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 46.5%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000002\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000024\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000007\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000002\n",
      "\tTraining batch 19 Loss: 0.000063\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000002\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000001\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000005\n",
      "\tTraining batch 27 Loss: 0.000002\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000002\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000003\n",
      "\tTraining batch 39 Loss: 0.000038\n",
      "\tTraining batch 40 Loss: 0.000005\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.001429\n",
      "\tTraining batch 44 Loss: 0.000004\n",
      "\tTraining batch 45 Loss: 0.000020\n",
      "\tTraining batch 46 Loss: 0.000039\n",
      "\tTraining batch 47 Loss: 0.000083\n",
      "\tTraining batch 48 Loss: 10.361612\n",
      "Training set: Average loss: 0.215903\n",
      "Validation set: Average loss: 8.242752, Accuracy: 1342/1959 (68.50%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.167276\n",
      "\tTraining batch 2 Loss: 0.012815\n",
      "\tTraining batch 3 Loss: 0.207385\n",
      "\tTraining batch 4 Loss: 0.144345\n",
      "\tTraining batch 5 Loss: 0.150022\n",
      "\tTraining batch 6 Loss: 0.050294\n",
      "\tTraining batch 7 Loss: 0.747311\n",
      "\tTraining batch 8 Loss: 0.142204\n",
      "\tTraining batch 9 Loss: 0.309722\n",
      "\tTraining batch 10 Loss: 0.047328\n",
      "\tTraining batch 11 Loss: 0.000326\n",
      "\tTraining batch 12 Loss: 0.704310\n",
      "\tTraining batch 13 Loss: 0.480699\n",
      "\tTraining batch 14 Loss: 1.252238\n",
      "\tTraining batch 15 Loss: 0.200690\n",
      "\tTraining batch 16 Loss: 0.399363\n",
      "\tTraining batch 17 Loss: 0.180910\n",
      "\tTraining batch 18 Loss: 0.749747\n",
      "\tTraining batch 19 Loss: 0.055177\n",
      "\tTraining batch 20 Loss: 0.112167\n",
      "\tTraining batch 21 Loss: 0.208233\n",
      "\tTraining batch 22 Loss: 0.054573\n",
      "\tTraining batch 23 Loss: 0.131813\n",
      "\tTraining batch 24 Loss: 0.305269\n",
      "\tTraining batch 25 Loss: 0.422486\n",
      "\tTraining batch 26 Loss: 0.516161\n",
      "\tTraining batch 27 Loss: 0.098233\n",
      "\tTraining batch 28 Loss: 0.000321\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000197\n",
      "\tTraining batch 31 Loss: 0.214602\n",
      "\tTraining batch 32 Loss: 0.012276\n",
      "\tTraining batch 33 Loss: 0.000009\n",
      "\tTraining batch 34 Loss: 0.001428\n",
      "\tTraining batch 35 Loss: 0.000004\n",
      "\tTraining batch 36 Loss: 0.216648\n",
      "\tTraining batch 37 Loss: 0.000006\n",
      "\tTraining batch 38 Loss: 0.000004\n",
      "\tTraining batch 39 Loss: 0.000013\n",
      "\tTraining batch 40 Loss: 0.025948\n",
      "\tTraining batch 41 Loss: 0.000586\n",
      "\tTraining batch 42 Loss: 0.000010\n",
      "\tTraining batch 43 Loss: 0.000835\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000004\n",
      "\tTraining batch 46 Loss: 0.000020\n",
      "\tTraining batch 47 Loss: 0.000023\n",
      "\tTraining batch 48 Loss: 0.514338\n",
      "Training set: Average loss: 0.184133\n",
      "Validation set: Average loss: 10.890205, Accuracy: 1320/1959 (67.38%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000008\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.036083\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.451927\n",
      "\tTraining batch 6 Loss: 0.153249\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000081\n",
      "\tTraining batch 9 Loss: 0.000576\n",
      "\tTraining batch 10 Loss: 0.000017\n",
      "\tTraining batch 11 Loss: 0.001601\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000206\n",
      "\tTraining batch 14 Loss: 0.228329\n",
      "\tTraining batch 15 Loss: 0.000021\n",
      "\tTraining batch 16 Loss: 0.000031\n",
      "\tTraining batch 17 Loss: 0.006602\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.184469\n",
      "\tTraining batch 20 Loss: 0.477129\n",
      "\tTraining batch 21 Loss: 0.000006\n",
      "\tTraining batch 22 Loss: 0.000001\n",
      "\tTraining batch 23 Loss: 0.000003\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000429\n",
      "\tTraining batch 27 Loss: 0.000050\n",
      "\tTraining batch 28 Loss: 0.054293\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000004\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000004\n",
      "\tTraining batch 33 Loss: 0.000001\n",
      "\tTraining batch 34 Loss: 0.000079\n",
      "\tTraining batch 35 Loss: 0.078803\n",
      "\tTraining batch 36 Loss: 0.006364\n",
      "\tTraining batch 37 Loss: 0.066843\n",
      "\tTraining batch 38 Loss: 0.000001\n",
      "\tTraining batch 39 Loss: 0.000020\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000007\n",
      "\tTraining batch 42 Loss: 0.478525\n",
      "\tTraining batch 43 Loss: 0.006356\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.007422\n",
      "\tTraining batch 46 Loss: 0.001410\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.005453\n",
      "Training set: Average loss: 0.046800\n",
      "Validation set: Average loss: 10.850032, Accuracy: 1341/1959 (68.45%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000001\n",
      "\tTraining batch 2 Loss: 0.000002\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.035133\n",
      "\tTraining batch 5 Loss: 0.000297\n",
      "\tTraining batch 6 Loss: 0.000039\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000002\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.018783\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.001515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.001049\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000001\n",
      "\tTraining batch 21 Loss: 0.000044\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000351\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000023\n",
      "\tTraining batch 26 Loss: 0.000124\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000001\n",
      "\tTraining batch 29 Loss: 0.000003\n",
      "\tTraining batch 30 Loss: 0.000005\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000010\n",
      "\tTraining batch 36 Loss: 0.000150\n",
      "\tTraining batch 37 Loss: 0.000306\n",
      "\tTraining batch 38 Loss: 0.000279\n",
      "\tTraining batch 39 Loss: 0.000384\n",
      "\tTraining batch 40 Loss: 0.000030\n",
      "\tTraining batch 41 Loss: 0.250138\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.003145\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000020\n",
      "\tTraining batch 46 Loss: 0.000045\n",
      "\tTraining batch 47 Loss: 0.000125\n",
      "\tTraining batch 48 Loss: 0.000014\n",
      "Training set: Average loss: 0.006500\n",
      "Validation set: Average loss: 10.523458, Accuracy: 1345/1959 (68.66%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.033199\n",
      "\tTraining batch 2 Loss: 0.003275\n",
      "\tTraining batch 3 Loss: 0.520307\n",
      "\tTraining batch 4 Loss: 0.000006\n",
      "\tTraining batch 5 Loss: 0.000001\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000019\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000009\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000004\n",
      "\tTraining batch 17 Loss: 0.000004\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000001\n",
      "\tTraining batch 21 Loss: 0.001200\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000022\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000098\n",
      "\tTraining batch 26 Loss: 0.000110\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000003\n",
      "\tTraining batch 29 Loss: 0.029894\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.001162\n",
      "\tTraining batch 35 Loss: 0.000034\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000040\n",
      "\tTraining batch 38 Loss: 0.000001\n",
      "\tTraining batch 39 Loss: 0.000015\n",
      "\tTraining batch 40 Loss: 0.000050\n",
      "\tTraining batch 41 Loss: 0.000008\n",
      "\tTraining batch 42 Loss: 0.000007\n",
      "\tTraining batch 43 Loss: 0.000215\n",
      "\tTraining batch 44 Loss: 0.000001\n",
      "\tTraining batch 45 Loss: 0.000011\n",
      "\tTraining batch 46 Loss: 0.000115\n",
      "\tTraining batch 47 Loss: 0.000002\n",
      "\tTraining batch 48 Loss: 0.000325\n",
      "Training set: Average loss: 0.012295\n",
      "Validation set: Average loss: 8.744211, Accuracy: 1322/1959 (67.48%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.125686\n",
      "\tTraining batch 2 Loss: 0.000002\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000388\n",
      "\tTraining batch 10 Loss: 0.000062\n",
      "\tTraining batch 11 Loss: 0.003053\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.009184\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000001\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000001\n",
      "\tTraining batch 20 Loss: 0.020089\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000049\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000006\n",
      "\tTraining batch 26 Loss: 0.000094\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000016\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000007\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.006070\n",
      "\tTraining batch 38 Loss: 0.000127\n",
      "\tTraining batch 39 Loss: 0.000027\n",
      "\tTraining batch 40 Loss: 0.000730\n",
      "\tTraining batch 41 Loss: 0.000008\n",
      "\tTraining batch 42 Loss: 0.000002\n",
      "\tTraining batch 43 Loss: 0.000640\n",
      "\tTraining batch 44 Loss: 0.000034\n",
      "\tTraining batch 45 Loss: 0.000003\n",
      "\tTraining batch 46 Loss: 0.000019\n",
      "\tTraining batch 47 Loss: 0.000001\n",
      "\tTraining batch 48 Loss: 0.000202\n",
      "Training set: Average loss: 0.003469\n",
      "Validation set: Average loss: 9.772133, Accuracy: 1327/1959 (67.74%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000003\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000001\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000001\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000001\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000010\n",
      "\tTraining batch 26 Loss: 0.000070\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000042\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000010\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000013\n",
      "\tTraining batch 40 Loss: 0.000127\n",
      "\tTraining batch 41 Loss: 0.000009\n",
      "\tTraining batch 42 Loss: 0.000001\n",
      "\tTraining batch 43 Loss: 0.000696\n",
      "\tTraining batch 44 Loss: 0.000026\n",
      "\tTraining batch 45 Loss: 0.000002\n",
      "\tTraining batch 46 Loss: 0.000018\n",
      "\tTraining batch 47 Loss: 0.000001\n",
      "\tTraining batch 48 Loss: 0.000153\n",
      "Training set: Average loss: 0.000025\n",
      "Validation set: Average loss: 9.770718, Accuracy: 1329/1959 (67.84%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000003\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000001\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000001\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000001\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000009\n",
      "\tTraining batch 26 Loss: 0.000069\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000030\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000009\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000008\n",
      "\tTraining batch 40 Loss: 0.000107\n",
      "\tTraining batch 41 Loss: 0.000007\n",
      "\tTraining batch 42 Loss: 0.000001\n",
      "\tTraining batch 43 Loss: 0.000657\n",
      "\tTraining batch 44 Loss: 0.000023\n",
      "\tTraining batch 45 Loss: 0.000002\n",
      "\tTraining batch 46 Loss: 0.000016\n",
      "\tTraining batch 47 Loss: 0.000001\n",
      "\tTraining batch 48 Loss: 0.000129\n",
      "Training set: Average loss: 0.000022\n",
      "Validation set: Average loss: 9.778643, Accuracy: 1330/1959 (67.89%)\n",
      "\n",
      "Epoch: 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000003\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000001\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000001\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000001\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000009\n",
      "\tTraining batch 26 Loss: 0.000069\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000024\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000008\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000006\n",
      "\tTraining batch 40 Loss: 0.000093\n",
      "\tTraining batch 41 Loss: 0.000006\n",
      "\tTraining batch 42 Loss: 0.000001\n",
      "\tTraining batch 43 Loss: 0.000620\n",
      "\tTraining batch 44 Loss: 0.000020\n",
      "\tTraining batch 45 Loss: 0.000002\n",
      "\tTraining batch 46 Loss: 0.000015\n",
      "\tTraining batch 47 Loss: 0.000001\n",
      "\tTraining batch 48 Loss: 0.000112\n",
      "Training set: Average loss: 0.000021\n",
      "Validation set: Average loss: 9.786149, Accuracy: 1331/1959 (67.94%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000002\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000001\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000001\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000001\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000008\n",
      "\tTraining batch 26 Loss: 0.000068\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000019\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000007\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000005\n",
      "\tTraining batch 40 Loss: 0.000082\n",
      "\tTraining batch 41 Loss: 0.000005\n",
      "\tTraining batch 42 Loss: 0.000001\n",
      "\tTraining batch 43 Loss: 0.000587\n",
      "\tTraining batch 44 Loss: 0.000018\n",
      "\tTraining batch 45 Loss: 0.000002\n",
      "\tTraining batch 46 Loss: 0.000015\n",
      "\tTraining batch 47 Loss: 0.000001\n",
      "\tTraining batch 48 Loss: 0.000100\n",
      "Training set: Average loss: 0.000019\n",
      "Validation set: Average loss: 9.793309, Accuracy: 1331/1959 (67.94%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 49.25%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000002\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000001\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000001\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000001\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000007\n",
      "\tTraining batch 26 Loss: 0.000067\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000016\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000006\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000004\n",
      "\tTraining batch 40 Loss: 0.000073\n",
      "\tTraining batch 41 Loss: 0.000004\n",
      "\tTraining batch 42 Loss: 0.000001\n",
      "\tTraining batch 43 Loss: 0.000556\n",
      "\tTraining batch 44 Loss: 0.000016\n",
      "\tTraining batch 45 Loss: 0.000002\n",
      "\tTraining batch 46 Loss: 0.000014\n",
      "\tTraining batch 47 Loss: 0.000001\n",
      "\tTraining batch 48 Loss: 0.000090\n",
      "\tTraining batch 49 Loss: 19.875118\n",
      "Training set: Average loss: 0.405632\n",
      "Validation set: Average loss: 9.136563, Accuracy: 1340/1959 (68.40%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000012\n",
      "\tTraining batch 2 Loss: 0.110190\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000005\n",
      "\tTraining batch 5 Loss: 0.000816\n",
      "\tTraining batch 6 Loss: 0.016575\n",
      "\tTraining batch 7 Loss: 0.344645\n",
      "\tTraining batch 8 Loss: 0.194239\n",
      "\tTraining batch 9 Loss: 0.654027\n",
      "\tTraining batch 10 Loss: 0.019999\n",
      "\tTraining batch 11 Loss: 0.404369\n",
      "\tTraining batch 12 Loss: 0.551552\n",
      "\tTraining batch 13 Loss: 0.096436\n",
      "\tTraining batch 14 Loss: 0.287021\n",
      "\tTraining batch 15 Loss: 0.133140\n",
      "\tTraining batch 16 Loss: 0.068805\n",
      "\tTraining batch 17 Loss: 0.000046\n",
      "\tTraining batch 18 Loss: 0.011168\n",
      "\tTraining batch 19 Loss: 0.100143\n",
      "\tTraining batch 20 Loss: 0.077226\n",
      "\tTraining batch 21 Loss: 0.183182\n",
      "\tTraining batch 22 Loss: 0.066626\n",
      "\tTraining batch 23 Loss: 0.196115\n",
      "\tTraining batch 24 Loss: 0.000859\n",
      "\tTraining batch 25 Loss: 0.061128\n",
      "\tTraining batch 26 Loss: 0.008639\n",
      "\tTraining batch 27 Loss: 0.000003\n",
      "\tTraining batch 28 Loss: 0.448572\n",
      "\tTraining batch 29 Loss: 0.001194\n",
      "\tTraining batch 30 Loss: 0.047899\n",
      "\tTraining batch 31 Loss: 0.195981\n",
      "\tTraining batch 32 Loss: 0.000458\n",
      "\tTraining batch 33 Loss: 0.006802\n",
      "\tTraining batch 34 Loss: 0.374651\n",
      "\tTraining batch 35 Loss: 0.474426\n",
      "\tTraining batch 36 Loss: 0.089471\n",
      "\tTraining batch 37 Loss: 0.621120\n",
      "\tTraining batch 38 Loss: 0.116791\n",
      "\tTraining batch 39 Loss: 0.418116\n",
      "\tTraining batch 40 Loss: 0.316698\n",
      "\tTraining batch 41 Loss: 0.264977\n",
      "\tTraining batch 42 Loss: 0.002585\n",
      "\tTraining batch 43 Loss: 0.005617\n",
      "\tTraining batch 44 Loss: 0.087224\n",
      "\tTraining batch 45 Loss: 0.015639\n",
      "\tTraining batch 46 Loss: 0.416715\n",
      "\tTraining batch 47 Loss: 0.007668\n",
      "\tTraining batch 48 Loss: 0.650555\n",
      "\tTraining batch 49 Loss: 0.672272\n",
      "Training set: Average loss: 0.180049\n",
      "Validation set: Average loss: 8.205679, Accuracy: 1335/1959 (68.15%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000150\n",
      "\tTraining batch 2 Loss: 0.002437\n",
      "\tTraining batch 3 Loss: 0.285215\n",
      "\tTraining batch 4 Loss: 0.000890\n",
      "\tTraining batch 5 Loss: 0.313318\n",
      "\tTraining batch 6 Loss: 0.000054\n",
      "\tTraining batch 7 Loss: 0.000001\n",
      "\tTraining batch 8 Loss: 0.000005\n",
      "\tTraining batch 9 Loss: 0.056875\n",
      "\tTraining batch 10 Loss: 0.207623\n",
      "\tTraining batch 11 Loss: 0.000007\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000005\n",
      "\tTraining batch 14 Loss: 0.018355\n",
      "\tTraining batch 15 Loss: 0.203995\n",
      "\tTraining batch 16 Loss: 0.000002\n",
      "\tTraining batch 17 Loss: 0.000085\n",
      "\tTraining batch 18 Loss: 0.000696\n",
      "\tTraining batch 19 Loss: 0.039832\n",
      "\tTraining batch 20 Loss: 0.020424\n",
      "\tTraining batch 21 Loss: 0.410574\n",
      "\tTraining batch 22 Loss: 0.042496\n",
      "\tTraining batch 23 Loss: 0.146585\n",
      "\tTraining batch 24 Loss: 0.000001\n",
      "\tTraining batch 25 Loss: 0.127712\n",
      "\tTraining batch 26 Loss: 0.002126\n",
      "\tTraining batch 27 Loss: 0.113166\n",
      "\tTraining batch 28 Loss: 0.000167\n",
      "\tTraining batch 29 Loss: 0.000226\n",
      "\tTraining batch 30 Loss: 0.000084\n",
      "\tTraining batch 31 Loss: 0.000001\n",
      "\tTraining batch 32 Loss: 0.050244\n",
      "\tTraining batch 33 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 34 Loss: 0.008505\n",
      "\tTraining batch 35 Loss: 0.072592\n",
      "\tTraining batch 36 Loss: 0.000469\n",
      "\tTraining batch 37 Loss: 0.006665\n",
      "\tTraining batch 38 Loss: 0.255110\n",
      "\tTraining batch 39 Loss: 0.367810\n",
      "\tTraining batch 40 Loss: 0.000180\n",
      "\tTraining batch 41 Loss: 0.004571\n",
      "\tTraining batch 42 Loss: 0.013428\n",
      "\tTraining batch 43 Loss: 0.011798\n",
      "\tTraining batch 44 Loss: 0.005893\n",
      "\tTraining batch 45 Loss: 0.000016\n",
      "\tTraining batch 46 Loss: 0.107288\n",
      "\tTraining batch 47 Loss: 0.678771\n",
      "\tTraining batch 48 Loss: 0.000701\n",
      "\tTraining batch 49 Loss: 0.036025\n",
      "Training set: Average loss: 0.073738\n",
      "Validation set: Average loss: 8.175477, Accuracy: 1352/1959 (69.01%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.001883\n",
      "\tTraining batch 2 Loss: 0.035150\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000003\n",
      "\tTraining batch 5 Loss: 0.000027\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000003\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000797\n",
      "\tTraining batch 11 Loss: 0.000010\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.086561\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.053887\n",
      "\tTraining batch 17 Loss: 0.335227\n",
      "\tTraining batch 18 Loss: 0.000060\n",
      "\tTraining batch 19 Loss: 0.009309\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.043912\n",
      "\tTraining batch 24 Loss: 0.001195\n",
      "\tTraining batch 25 Loss: 0.013831\n",
      "\tTraining batch 26 Loss: 0.000999\n",
      "\tTraining batch 27 Loss: 0.000009\n",
      "\tTraining batch 28 Loss: 0.021633\n",
      "\tTraining batch 29 Loss: 0.201654\n",
      "\tTraining batch 30 Loss: 0.000002\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.326866\n",
      "\tTraining batch 33 Loss: 0.000001\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.047962\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000004\n",
      "\tTraining batch 39 Loss: 0.000005\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000004\n",
      "\tTraining batch 43 Loss: 0.004186\n",
      "\tTraining batch 44 Loss: 0.000005\n",
      "\tTraining batch 45 Loss: 0.000890\n",
      "\tTraining batch 46 Loss: 0.000008\n",
      "\tTraining batch 47 Loss: 0.357967\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.005440\n",
      "Training set: Average loss: 0.031622\n",
      "Validation set: Average loss: 9.503316, Accuracy: 1311/1959 (66.92%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000439\n",
      "\tTraining batch 2 Loss: 0.001026\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.108994\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000181\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000740\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000003\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000025\n",
      "\tTraining batch 19 Loss: 0.001167\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000031\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000001\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000003\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.009338\n",
      "\tTraining batch 35 Loss: 0.000006\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000002\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000001\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000004\n",
      "\tTraining batch 43 Loss: 0.000379\n",
      "\tTraining batch 44 Loss: 0.000008\n",
      "\tTraining batch 45 Loss: 0.000113\n",
      "\tTraining batch 46 Loss: 0.000018\n",
      "\tTraining batch 47 Loss: 0.003902\n",
      "\tTraining batch 48 Loss: 0.000003\n",
      "\tTraining batch 49 Loss: 0.011002\n",
      "Training set: Average loss: 0.002804\n",
      "Validation set: Average loss: 9.324707, Accuracy: 1357/1959 (69.27%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000001\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000017\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000005\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000001\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000002\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000044\n",
      "\tTraining batch 19 Loss: 0.000032\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000027\n",
      "\tTraining batch 27 Loss: 0.000003\n",
      "\tTraining batch 28 Loss: 0.000089\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000002\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000030\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000001\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000003\n",
      "\tTraining batch 43 Loss: 0.002034\n",
      "\tTraining batch 44 Loss: 0.000057\n",
      "\tTraining batch 45 Loss: 0.000030\n",
      "\tTraining batch 46 Loss: 0.000014\n",
      "\tTraining batch 47 Loss: 0.000112\n",
      "\tTraining batch 48 Loss: 0.000001\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "Training set: Average loss: 0.000051\n",
      "Validation set: Average loss: 9.740857, Accuracy: 1349/1959 (68.86%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000002\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000018\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000005\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000001\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000001\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000055\n",
      "\tTraining batch 19 Loss: 0.000017\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000027\n",
      "\tTraining batch 27 Loss: 0.000002\n",
      "\tTraining batch 28 Loss: 0.000055\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000002\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000023\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000001\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000003\n",
      "\tTraining batch 43 Loss: 0.000665\n",
      "\tTraining batch 44 Loss: 0.000045\n",
      "\tTraining batch 45 Loss: 0.000026\n",
      "\tTraining batch 46 Loss: 0.000007\n",
      "\tTraining batch 47 Loss: 0.000063\n",
      "\tTraining batch 48 Loss: 0.000001\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "Training set: Average loss: 0.000021\n",
      "Validation set: Average loss: 9.741162, Accuracy: 1349/1959 (68.86%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 47.5%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000002\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000016\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000006\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000001\n",
      "\tTraining batch 12 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 13 Loss: 0.000001\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000039\n",
      "\tTraining batch 19 Loss: 0.000016\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000026\n",
      "\tTraining batch 27 Loss: 0.000002\n",
      "\tTraining batch 28 Loss: 0.000033\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000002\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000019\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000001\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000002\n",
      "\tTraining batch 43 Loss: 0.000447\n",
      "\tTraining batch 44 Loss: 0.000038\n",
      "\tTraining batch 45 Loss: 0.000025\n",
      "\tTraining batch 46 Loss: 0.000005\n",
      "\tTraining batch 47 Loss: 0.000046\n",
      "\tTraining batch 48 Loss: 0.000001\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 10.899565\n",
      "Training set: Average loss: 0.218006\n",
      "Validation set: Average loss: 9.105988, Accuracy: 1346/1959 (68.71%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000002\n",
      "\tTraining batch 2 Loss: 0.000085\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000006\n",
      "\tTraining batch 5 Loss: 0.000002\n",
      "\tTraining batch 6 Loss: 0.102524\n",
      "\tTraining batch 7 Loss: 0.247947\n",
      "\tTraining batch 8 Loss: 0.003307\n",
      "\tTraining batch 9 Loss: 0.016851\n",
      "\tTraining batch 10 Loss: 0.000113\n",
      "\tTraining batch 11 Loss: 0.018521\n",
      "\tTraining batch 12 Loss: 0.002833\n",
      "\tTraining batch 13 Loss: 0.028196\n",
      "\tTraining batch 14 Loss: 0.021303\n",
      "\tTraining batch 15 Loss: 0.079392\n",
      "\tTraining batch 16 Loss: 0.000038\n",
      "\tTraining batch 17 Loss: 0.395604\n",
      "\tTraining batch 18 Loss: 0.873100\n",
      "\tTraining batch 19 Loss: 0.416482\n",
      "\tTraining batch 20 Loss: 0.006268\n",
      "\tTraining batch 21 Loss: 0.506683\n",
      "\tTraining batch 22 Loss: 0.331048\n",
      "\tTraining batch 23 Loss: 0.000293\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000005\n",
      "\tTraining batch 26 Loss: 0.005364\n",
      "\tTraining batch 27 Loss: 0.000036\n",
      "\tTraining batch 28 Loss: 0.109546\n",
      "\tTraining batch 29 Loss: 0.000002\n",
      "\tTraining batch 30 Loss: 0.000750\n",
      "\tTraining batch 31 Loss: 0.000011\n",
      "\tTraining batch 32 Loss: 0.086774\n",
      "\tTraining batch 33 Loss: 0.000083\n",
      "\tTraining batch 34 Loss: 0.000001\n",
      "\tTraining batch 35 Loss: 0.000004\n",
      "\tTraining batch 36 Loss: 0.000177\n",
      "\tTraining batch 37 Loss: 0.000001\n",
      "\tTraining batch 38 Loss: 0.214354\n",
      "\tTraining batch 39 Loss: 0.089523\n",
      "\tTraining batch 40 Loss: 0.000744\n",
      "\tTraining batch 41 Loss: 0.000997\n",
      "\tTraining batch 42 Loss: 0.763221\n",
      "\tTraining batch 43 Loss: 0.142748\n",
      "\tTraining batch 44 Loss: 0.003991\n",
      "\tTraining batch 45 Loss: 0.004758\n",
      "\tTraining batch 46 Loss: 0.387736\n",
      "\tTraining batch 47 Loss: 0.000582\n",
      "\tTraining batch 48 Loss: 0.001589\n",
      "\tTraining batch 49 Loss: 0.010534\n",
      "\tTraining batch 50 Loss: 0.408485\n",
      "Training set: Average loss: 0.105652\n",
      "Validation set: Average loss: 6.211541, Accuracy: 1387/1959 (70.80%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.250490\n",
      "\tTraining batch 2 Loss: 0.265260\n",
      "\tTraining batch 3 Loss: 0.142501\n",
      "\tTraining batch 4 Loss: 0.000404\n",
      "\tTraining batch 5 Loss: 0.000009\n",
      "\tTraining batch 6 Loss: 0.000025\n",
      "\tTraining batch 7 Loss: 0.000009\n",
      "\tTraining batch 8 Loss: 0.243184\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000040\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000002\n",
      "\tTraining batch 13 Loss: 0.045211\n",
      "\tTraining batch 14 Loss: 0.000348\n",
      "\tTraining batch 15 Loss: 0.080112\n",
      "\tTraining batch 16 Loss: 0.001097\n",
      "\tTraining batch 17 Loss: 0.001552\n",
      "\tTraining batch 18 Loss: 0.000033\n",
      "\tTraining batch 19 Loss: 0.033726\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.053902\n",
      "\tTraining batch 22 Loss: 0.000051\n",
      "\tTraining batch 23 Loss: 0.000001\n",
      "\tTraining batch 24 Loss: 0.084087\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.015642\n",
      "\tTraining batch 27 Loss: 0.104817\n",
      "\tTraining batch 28 Loss: 0.000230\n",
      "\tTraining batch 29 Loss: 0.000545\n",
      "\tTraining batch 30 Loss: 0.149580\n",
      "\tTraining batch 31 Loss: 0.000004\n",
      "\tTraining batch 32 Loss: 0.002283\n",
      "\tTraining batch 33 Loss: 0.003578\n",
      "\tTraining batch 34 Loss: 0.000170\n",
      "\tTraining batch 35 Loss: 0.054994\n",
      "\tTraining batch 36 Loss: 0.037787\n",
      "\tTraining batch 37 Loss: 0.000004\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.005297\n",
      "\tTraining batch 40 Loss: 0.044238\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000838\n",
      "\tTraining batch 43 Loss: 0.119198\n",
      "\tTraining batch 44 Loss: 0.000003\n",
      "\tTraining batch 45 Loss: 0.168336\n",
      "\tTraining batch 46 Loss: 0.000082\n",
      "\tTraining batch 47 Loss: 0.001908\n",
      "\tTraining batch 48 Loss: 0.206745\n",
      "\tTraining batch 49 Loss: 0.000124\n",
      "\tTraining batch 50 Loss: 0.000001\n",
      "Training set: Average loss: 0.042369\n",
      "Validation set: Average loss: 9.329225, Accuracy: 1360/1959 (69.42%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000010\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.048409\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000002\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.006064\n",
      "\tTraining batch 19 Loss: 0.021160\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000001\n",
      "\tTraining batch 26 Loss: 0.001194\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000018\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000008\n",
      "\tTraining batch 34 Loss: 0.001090\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000004\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000113\n",
      "\tTraining batch 40 Loss: 0.000004\n",
      "\tTraining batch 41 Loss: 0.000006\n",
      "\tTraining batch 42 Loss: 0.000097\n",
      "\tTraining batch 43 Loss: 0.000010\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000009\n",
      "\tTraining batch 46 Loss: 0.000622\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.001658\n",
      "Training set: Average loss: 0.001610\n",
      "Validation set: Average loss: 9.956373, Accuracy: 1372/1959 (70.04%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000001\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000001\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.001865\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000018\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000004\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000002\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000013\n",
      "\tTraining batch 40 Loss: 0.000002\n",
      "\tTraining batch 41 Loss: 0.000004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 42 Loss: 0.000019\n",
      "\tTraining batch 43 Loss: 0.000011\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000005\n",
      "\tTraining batch 46 Loss: 0.000009\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000037\n",
      "Training set: Average loss: 0.000040\n",
      "Validation set: Average loss: 9.950800, Accuracy: 1372/1959 (70.04%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 49.0%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000001\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000001\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.001328\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000019\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000004\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000002\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000010\n",
      "\tTraining batch 40 Loss: 0.000002\n",
      "\tTraining batch 41 Loss: 0.000004\n",
      "\tTraining batch 42 Loss: 0.000019\n",
      "\tTraining batch 43 Loss: 0.000010\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000008\n",
      "\tTraining batch 46 Loss: 0.000008\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000033\n",
      "\tTraining batch 51 Loss: 16.992254\n",
      "Training set: Average loss: 0.333210\n",
      "Validation set: Average loss: 7.998137, Accuracy: 1368/1959 (69.83%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000400\n",
      "\tTraining batch 5 Loss: 0.084646\n",
      "\tTraining batch 6 Loss: 0.000305\n",
      "\tTraining batch 7 Loss: 0.150028\n",
      "\tTraining batch 8 Loss: 0.086400\n",
      "\tTraining batch 9 Loss: 0.001502\n",
      "\tTraining batch 10 Loss: 0.109654\n",
      "\tTraining batch 11 Loss: 0.056522\n",
      "\tTraining batch 12 Loss: 0.011851\n",
      "\tTraining batch 13 Loss: 0.001224\n",
      "\tTraining batch 14 Loss: 0.005215\n",
      "\tTraining batch 15 Loss: 0.157907\n",
      "\tTraining batch 16 Loss: 0.003691\n",
      "\tTraining batch 17 Loss: 0.015370\n",
      "\tTraining batch 18 Loss: 0.528330\n",
      "\tTraining batch 19 Loss: 0.173571\n",
      "\tTraining batch 20 Loss: 0.004554\n",
      "\tTraining batch 21 Loss: 0.004140\n",
      "\tTraining batch 22 Loss: 0.000678\n",
      "\tTraining batch 23 Loss: 0.030009\n",
      "\tTraining batch 24 Loss: 0.006222\n",
      "\tTraining batch 25 Loss: 0.325569\n",
      "\tTraining batch 26 Loss: 0.287370\n",
      "\tTraining batch 27 Loss: 0.157968\n",
      "\tTraining batch 28 Loss: 0.396699\n",
      "\tTraining batch 29 Loss: 0.132795\n",
      "\tTraining batch 30 Loss: 0.067877\n",
      "\tTraining batch 31 Loss: 0.026173\n",
      "\tTraining batch 32 Loss: 0.485067\n",
      "\tTraining batch 33 Loss: 0.008094\n",
      "\tTraining batch 34 Loss: 0.064376\n",
      "\tTraining batch 35 Loss: 0.102661\n",
      "\tTraining batch 36 Loss: 0.016999\n",
      "\tTraining batch 37 Loss: 0.145920\n",
      "\tTraining batch 38 Loss: 0.000515\n",
      "\tTraining batch 39 Loss: 0.000405\n",
      "\tTraining batch 40 Loss: 0.007486\n",
      "\tTraining batch 41 Loss: 0.000762\n",
      "\tTraining batch 42 Loss: 0.000132\n",
      "\tTraining batch 43 Loss: 0.012979\n",
      "\tTraining batch 44 Loss: 0.044186\n",
      "\tTraining batch 45 Loss: 0.043394\n",
      "\tTraining batch 46 Loss: 0.045177\n",
      "\tTraining batch 47 Loss: 0.056170\n",
      "\tTraining batch 48 Loss: 0.058544\n",
      "\tTraining batch 49 Loss: 0.349452\n",
      "\tTraining batch 50 Loss: 0.019679\n",
      "\tTraining batch 51 Loss: 0.875560\n",
      "Training set: Average loss: 0.101259\n",
      "Validation set: Average loss: 6.601588, Accuracy: 1359/1959 (69.37%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.063598\n",
      "\tTraining batch 2 Loss: 0.000008\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.002894\n",
      "\tTraining batch 5 Loss: 0.006824\n",
      "\tTraining batch 6 Loss: 0.000006\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000046\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000001\n",
      "\tTraining batch 12 Loss: 0.019745\n",
      "\tTraining batch 13 Loss: 0.001035\n",
      "\tTraining batch 14 Loss: 0.000215\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000046\n",
      "\tTraining batch 17 Loss: 0.000002\n",
      "\tTraining batch 18 Loss: 0.007238\n",
      "\tTraining batch 19 Loss: 0.038369\n",
      "\tTraining batch 20 Loss: 0.000004\n",
      "\tTraining batch 21 Loss: 0.008033\n",
      "\tTraining batch 22 Loss: 0.000023\n",
      "\tTraining batch 23 Loss: 0.001969\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.287045\n",
      "\tTraining batch 26 Loss: 0.003120\n",
      "\tTraining batch 27 Loss: 0.000007\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000001\n",
      "\tTraining batch 30 Loss: 0.001970\n",
      "\tTraining batch 31 Loss: 0.000025\n",
      "\tTraining batch 32 Loss: 0.000062\n",
      "\tTraining batch 33 Loss: 0.034648\n",
      "\tTraining batch 34 Loss: 0.000015\n",
      "\tTraining batch 35 Loss: 0.000001\n",
      "\tTraining batch 36 Loss: 0.047027\n",
      "\tTraining batch 37 Loss: 0.000011\n",
      "\tTraining batch 38 Loss: 0.019793\n",
      "\tTraining batch 39 Loss: 0.000133\n",
      "\tTraining batch 40 Loss: 0.000050\n",
      "\tTraining batch 41 Loss: 0.000142\n",
      "\tTraining batch 42 Loss: 0.012046\n",
      "\tTraining batch 43 Loss: 0.021518\n",
      "\tTraining batch 44 Loss: 0.000212\n",
      "\tTraining batch 45 Loss: 0.016817\n",
      "\tTraining batch 46 Loss: 0.106068\n",
      "\tTraining batch 47 Loss: 0.001311\n",
      "\tTraining batch 48 Loss: 0.000001\n",
      "\tTraining batch 49 Loss: 0.000241\n",
      "\tTraining batch 50 Loss: 0.117117\n",
      "\tTraining batch 51 Loss: 0.197287\n",
      "Training set: Average loss: 0.019936\n",
      "Validation set: Average loss: 7.194946, Accuracy: 1373/1959 (70.09%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.001087\n",
      "\tTraining batch 2 Loss: 0.000001\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.001435\n",
      "\tTraining batch 5 Loss: 0.000068\n",
      "\tTraining batch 6 Loss: 0.000010\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000581\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000027\n",
      "\tTraining batch 14 Loss: 0.000013\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000220\n",
      "\tTraining batch 17 Loss: 0.000042\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.140046\n",
      "\tTraining batch 20 Loss: 0.240463\n",
      "\tTraining batch 21 Loss: 0.000235\n",
      "\tTraining batch 22 Loss: 0.000234\n",
      "\tTraining batch 23 Loss: 0.000001\n",
      "\tTraining batch 24 Loss: 0.000004\n",
      "\tTraining batch 25 Loss: 0.000009\n",
      "\tTraining batch 26 Loss: 0.002826\n",
      "\tTraining batch 27 Loss: 0.000134\n",
      "\tTraining batch 28 Loss: 0.000018\n",
      "\tTraining batch 29 Loss: 0.005974\n",
      "\tTraining batch 30 Loss: 0.001857\n",
      "\tTraining batch 31 Loss: 0.000007\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.021239\n",
      "\tTraining batch 34 Loss: 0.000740\n",
      "\tTraining batch 35 Loss: 0.000007\n",
      "\tTraining batch 36 Loss: 0.000101\n",
      "\tTraining batch 37 Loss: 0.000187\n",
      "\tTraining batch 38 Loss: 0.000242\n",
      "\tTraining batch 39 Loss: 0.043937\n",
      "\tTraining batch 40 Loss: 0.008152\n",
      "\tTraining batch 41 Loss: 0.000149\n",
      "\tTraining batch 42 Loss: 0.010669\n",
      "\tTraining batch 43 Loss: 0.021859\n",
      "\tTraining batch 44 Loss: 0.000393\n",
      "\tTraining batch 45 Loss: 0.003457\n",
      "\tTraining batch 46 Loss: 0.000034\n",
      "\tTraining batch 47 Loss: 0.000003\n",
      "\tTraining batch 48 Loss: 0.002168\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.026067\n",
      "\tTraining batch 51 Loss: 0.005402\n",
      "Training set: Average loss: 0.010590\n",
      "Validation set: Average loss: 6.894100, Accuracy: 1371/1959 (69.98%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.050641\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.103853\n",
      "\tTraining batch 5 Loss: 0.002932\n",
      "\tTraining batch 6 Loss: 0.000050\n",
      "\tTraining batch 7 Loss: 0.001158\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.216802\n",
      "\tTraining batch 10 Loss: 0.000016\n",
      "\tTraining batch 11 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000032\n",
      "\tTraining batch 14 Loss: 0.000002\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.001011\n",
      "\tTraining batch 17 Loss: 0.000007\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.016158\n",
      "\tTraining batch 20 Loss: 0.000001\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000002\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000005\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.001156\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000080\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000975\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000015\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.032961\n",
      "\tTraining batch 35 Loss: 0.000912\n",
      "\tTraining batch 36 Loss: 0.000067\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000003\n",
      "\tTraining batch 39 Loss: 0.000001\n",
      "\tTraining batch 40 Loss: 0.000009\n",
      "\tTraining batch 41 Loss: 0.000009\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.002315\n",
      "\tTraining batch 44 Loss: 0.000027\n",
      "\tTraining batch 45 Loss: 0.002746\n",
      "\tTraining batch 46 Loss: 0.000099\n",
      "\tTraining batch 47 Loss: 0.000001\n",
      "\tTraining batch 48 Loss: 0.000012\n",
      "\tTraining batch 49 Loss: 0.000043\n",
      "\tTraining batch 50 Loss: 0.000021\n",
      "\tTraining batch 51 Loss: 0.110765\n",
      "Training set: Average loss: 0.010684\n",
      "Validation set: Average loss: 8.081139, Accuracy: 1368/1959 (69.83%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000359\n",
      "\tTraining batch 3 Loss: 0.000007\n",
      "\tTraining batch 4 Loss: 0.000077\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000652\n",
      "\tTraining batch 14 Loss: 0.000008\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000722\n",
      "\tTraining batch 17 Loss: 0.000016\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.012835\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000001\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.004608\n",
      "\tTraining batch 27 Loss: 0.000092\n",
      "\tTraining batch 28 Loss: 0.000032\n",
      "\tTraining batch 29 Loss: 0.000003\n",
      "\tTraining batch 30 Loss: 0.000536\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000019\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000034\n",
      "\tTraining batch 39 Loss: 0.000674\n",
      "\tTraining batch 40 Loss: 0.000008\n",
      "\tTraining batch 41 Loss: 0.000009\n",
      "\tTraining batch 42 Loss: 0.000001\n",
      "\tTraining batch 43 Loss: 0.003116\n",
      "\tTraining batch 44 Loss: 0.000064\n",
      "\tTraining batch 45 Loss: 0.002031\n",
      "\tTraining batch 46 Loss: 0.000055\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000005\n",
      "\tTraining batch 49 Loss: 0.000113\n",
      "\tTraining batch 50 Loss: 0.000006\n",
      "\tTraining batch 51 Loss: 0.002550\n",
      "Training set: Average loss: 0.000561\n",
      "Validation set: Average loss: 8.081205, Accuracy: 1365/1959 (69.68%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000011\n",
      "\tTraining batch 3 Loss: 0.000013\n",
      "\tTraining batch 4 Loss: 0.000024\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000001\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000371\n",
      "\tTraining batch 14 Loss: 0.000004\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000263\n",
      "\tTraining batch 17 Loss: 0.000014\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.010676\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000001\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.003703\n",
      "\tTraining batch 27 Loss: 0.000003\n",
      "\tTraining batch 28 Loss: 0.000028\n",
      "\tTraining batch 29 Loss: 0.000003\n",
      "\tTraining batch 30 Loss: 0.000481\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000019\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000029\n",
      "\tTraining batch 39 Loss: 0.000011\n",
      "\tTraining batch 40 Loss: 0.000006\n",
      "\tTraining batch 41 Loss: 0.000007\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.002144\n",
      "\tTraining batch 44 Loss: 0.000033\n",
      "\tTraining batch 45 Loss: 0.001574\n",
      "\tTraining batch 46 Loss: 0.000048\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000005\n",
      "\tTraining batch 49 Loss: 0.000028\n",
      "\tTraining batch 50 Loss: 0.000006\n",
      "\tTraining batch 51 Loss: 0.000129\n",
      "Training set: Average loss: 0.000385\n",
      "Validation set: Average loss: 8.158888, Accuracy: 1364/1959 (69.63%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000008\n",
      "\tTraining batch 3 Loss: 0.000006\n",
      "\tTraining batch 4 Loss: 0.000022\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000001\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000274\n",
      "\tTraining batch 14 Loss: 0.000002\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000121\n",
      "\tTraining batch 17 Loss: 0.000012\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.002985\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000001\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.004412\n",
      "\tTraining batch 27 Loss: 0.000001\n",
      "\tTraining batch 28 Loss: 0.000039\n",
      "\tTraining batch 29 Loss: 0.000004\n",
      "\tTraining batch 30 Loss: 0.000451\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000017\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000023\n",
      "\tTraining batch 39 Loss: 0.000008\n",
      "\tTraining batch 40 Loss: 0.000006\n",
      "\tTraining batch 41 Loss: 0.000005\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.001441\n",
      "\tTraining batch 44 Loss: 0.000025\n",
      "\tTraining batch 45 Loss: 0.000333\n",
      "\tTraining batch 46 Loss: 0.000042\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000006\n",
      "\tTraining batch 49 Loss: 0.000016\n",
      "\tTraining batch 50 Loss: 0.000007\n",
      "\tTraining batch 51 Loss: 0.000118\n",
      "Training set: Average loss: 0.000204\n",
      "Validation set: Average loss: 8.204381, Accuracy: 1367/1959 (69.78%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000006\n",
      "\tTraining batch 3 Loss: 0.000008\n",
      "\tTraining batch 4 Loss: 0.000018\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000002\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000206\n",
      "\tTraining batch 14 Loss: 0.000001\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000067\n",
      "\tTraining batch 17 Loss: 0.000012\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000105\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000001\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.003199\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000041\n",
      "\tTraining batch 29 Loss: 0.000004\n",
      "\tTraining batch 30 Loss: 0.000434\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000019\n",
      "\tTraining batch 39 Loss: 0.000006\n",
      "\tTraining batch 40 Loss: 0.000007\n",
      "\tTraining batch 41 Loss: 0.000004\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.001119\n",
      "\tTraining batch 44 Loss: 0.000023\n",
      "\tTraining batch 45 Loss: 0.000317\n",
      "\tTraining batch 46 Loss: 0.000038\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000006\n",
      "\tTraining batch 49 Loss: 0.000010\n",
      "\tTraining batch 50 Loss: 0.000008\n",
      "\tTraining batch 51 Loss: 0.000114\n",
      "Training set: Average loss: 0.000114\n",
      "Validation set: Average loss: 8.201959, Accuracy: 1365/1959 (69.68%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000005\n",
      "\tTraining batch 3 Loss: 0.000007\n",
      "\tTraining batch 4 Loss: 0.000020\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000002\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000211\n",
      "\tTraining batch 14 Loss: 0.000001\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000050\n",
      "\tTraining batch 17 Loss: 0.000011\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000115\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000001\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.002232\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000039\n",
      "\tTraining batch 29 Loss: 0.000004\n",
      "\tTraining batch 30 Loss: 0.000418\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000019\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000017\n",
      "\tTraining batch 39 Loss: 0.000005\n",
      "\tTraining batch 40 Loss: 0.000007\n",
      "\tTraining batch 41 Loss: 0.000004\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000931\n",
      "\tTraining batch 44 Loss: 0.000021\n",
      "\tTraining batch 45 Loss: 0.000313\n",
      "\tTraining batch 46 Loss: 0.000036\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000006\n",
      "\tTraining batch 49 Loss: 0.000008\n",
      "\tTraining batch 50 Loss: 0.000009\n",
      "\tTraining batch 51 Loss: 0.000104\n",
      "Training set: Average loss: 0.000090\n",
      "Validation set: Average loss: 8.203172, Accuracy: 1367/1959 (69.78%)\n",
      "\n",
      "Epoch: 11\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000005\n",
      "\tTraining batch 3 Loss: 0.000005\n",
      "\tTraining batch 4 Loss: 0.000022\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000002\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000203\n",
      "\tTraining batch 14 Loss: 0.000001\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000038\n",
      "\tTraining batch 17 Loss: 0.000010\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000113\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000001\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.001741\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000037\n",
      "\tTraining batch 29 Loss: 0.000004\n",
      "\tTraining batch 30 Loss: 0.000403\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000018\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000016\n",
      "\tTraining batch 39 Loss: 0.000004\n",
      "\tTraining batch 40 Loss: 0.000008\n",
      "\tTraining batch 41 Loss: 0.000004\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000785\n",
      "\tTraining batch 44 Loss: 0.000020\n",
      "\tTraining batch 45 Loss: 0.000295\n",
      "\tTraining batch 46 Loss: 0.000034\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000006\n",
      "\tTraining batch 49 Loss: 0.000006\n",
      "\tTraining batch 50 Loss: 0.000009\n",
      "\tTraining batch 51 Loss: 0.000092\n",
      "Training set: Average loss: 0.000076\n",
      "Validation set: Average loss: 8.207224, Accuracy: 1368/1959 (69.83%)\n",
      "\n",
      "Epoch: 12\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000005\n",
      "\tTraining batch 3 Loss: 0.000005\n",
      "\tTraining batch 4 Loss: 0.000023\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000001\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000193\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000028\n",
      "\tTraining batch 17 Loss: 0.000009\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000105\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000001\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.001453\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000035\n",
      "\tTraining batch 29 Loss: 0.000004\n",
      "\tTraining batch 30 Loss: 0.000388\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000018\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000015\n",
      "\tTraining batch 39 Loss: 0.000004\n",
      "\tTraining batch 40 Loss: 0.000008\n",
      "\tTraining batch 41 Loss: 0.000004\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000692\n",
      "\tTraining batch 44 Loss: 0.000018\n",
      "\tTraining batch 45 Loss: 0.000277\n",
      "\tTraining batch 46 Loss: 0.000032\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000005\n",
      "\tTraining batch 49 Loss: 0.000006\n",
      "\tTraining batch 50 Loss: 0.000009\n",
      "\tTraining batch 51 Loss: 0.000083\n",
      "Training set: Average loss: 0.000067\n",
      "Validation set: Average loss: 8.213573, Accuracy: 1368/1959 (69.83%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 48.75%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000004\n",
      "\tTraining batch 3 Loss: 0.000004\n",
      "\tTraining batch 4 Loss: 0.000023\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000001\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000176\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000024\n",
      "\tTraining batch 17 Loss: 0.000008\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000093\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000001\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.001265\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000031\n",
      "\tTraining batch 29 Loss: 0.000003\n",
      "\tTraining batch 30 Loss: 0.000373\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000017\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000014\n",
      "\tTraining batch 39 Loss: 0.000003\n",
      "\tTraining batch 40 Loss: 0.000007\n",
      "\tTraining batch 41 Loss: 0.000003\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000611\n",
      "\tTraining batch 44 Loss: 0.000017\n",
      "\tTraining batch 45 Loss: 0.000260\n",
      "\tTraining batch 46 Loss: 0.000030\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000005\n",
      "\tTraining batch 49 Loss: 0.000005\n",
      "\tTraining batch 50 Loss: 0.000010\n",
      "\tTraining batch 51 Loss: 0.000075\n",
      "\tTraining batch 52 Loss: 11.219464\n",
      "Training set: Average loss: 0.215818\n",
      "Validation set: Average loss: 7.493743, Accuracy: 1371/1959 (69.98%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000005\n",
      "\tTraining batch 3 Loss: 0.000307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 4 Loss: 0.003328\n",
      "\tTraining batch 5 Loss: 0.003082\n",
      "\tTraining batch 6 Loss: 0.186635\n",
      "\tTraining batch 7 Loss: 0.000114\n",
      "\tTraining batch 8 Loss: 0.230598\n",
      "\tTraining batch 9 Loss: 0.173111\n",
      "\tTraining batch 10 Loss: 0.081945\n",
      "\tTraining batch 11 Loss: 0.327811\n",
      "\tTraining batch 12 Loss: 0.813589\n",
      "\tTraining batch 13 Loss: 0.016860\n",
      "\tTraining batch 14 Loss: 0.149213\n",
      "\tTraining batch 15 Loss: 0.446739\n",
      "\tTraining batch 16 Loss: 0.243165\n",
      "\tTraining batch 17 Loss: 0.107652\n",
      "\tTraining batch 18 Loss: 0.000025\n",
      "\tTraining batch 19 Loss: 0.032409\n",
      "\tTraining batch 20 Loss: 0.210805\n",
      "\tTraining batch 21 Loss: 0.057485\n",
      "\tTraining batch 22 Loss: 0.298914\n",
      "\tTraining batch 23 Loss: 0.024361\n",
      "\tTraining batch 24 Loss: 0.322620\n",
      "\tTraining batch 25 Loss: 0.017341\n",
      "\tTraining batch 26 Loss: 0.250634\n",
      "\tTraining batch 27 Loss: 0.000824\n",
      "\tTraining batch 28 Loss: 0.017025\n",
      "\tTraining batch 29 Loss: 0.041793\n",
      "\tTraining batch 30 Loss: 0.204155\n",
      "\tTraining batch 31 Loss: 0.165870\n",
      "\tTraining batch 32 Loss: 0.000137\n",
      "\tTraining batch 33 Loss: 0.000087\n",
      "\tTraining batch 34 Loss: 0.129219\n",
      "\tTraining batch 35 Loss: 0.052203\n",
      "\tTraining batch 36 Loss: 0.001713\n",
      "\tTraining batch 37 Loss: 0.002775\n",
      "\tTraining batch 38 Loss: 0.003461\n",
      "\tTraining batch 39 Loss: 0.493003\n",
      "\tTraining batch 40 Loss: 0.367991\n",
      "\tTraining batch 41 Loss: 0.000318\n",
      "\tTraining batch 42 Loss: 0.000352\n",
      "\tTraining batch 43 Loss: 0.018464\n",
      "\tTraining batch 44 Loss: 0.000037\n",
      "\tTraining batch 45 Loss: 0.000996\n",
      "\tTraining batch 46 Loss: 0.001236\n",
      "\tTraining batch 47 Loss: 0.107697\n",
      "\tTraining batch 48 Loss: 0.000400\n",
      "\tTraining batch 49 Loss: 0.000992\n",
      "\tTraining batch 50 Loss: 0.000009\n",
      "\tTraining batch 51 Loss: 0.094772\n",
      "\tTraining batch 52 Loss: 0.294490\n",
      "Training set: Average loss: 0.115361\n",
      "Validation set: Average loss: 6.938226, Accuracy: 1335/1959 (68.15%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.001751\n",
      "\tTraining batch 2 Loss: 0.002962\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000202\n",
      "\tTraining batch 5 Loss: 0.000014\n",
      "\tTraining batch 6 Loss: 0.000001\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000010\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000168\n",
      "\tTraining batch 12 Loss: 0.000006\n",
      "\tTraining batch 13 Loss: 0.000136\n",
      "\tTraining batch 14 Loss: 0.239282\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.082817\n",
      "\tTraining batch 17 Loss: 0.000032\n",
      "\tTraining batch 18 Loss: 0.000296\n",
      "\tTraining batch 19 Loss: 0.008806\n",
      "\tTraining batch 20 Loss: 0.000002\n",
      "\tTraining batch 21 Loss: 0.000012\n",
      "\tTraining batch 22 Loss: 0.000001\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.019630\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000772\n",
      "\tTraining batch 27 Loss: 0.000763\n",
      "\tTraining batch 28 Loss: 0.000034\n",
      "\tTraining batch 29 Loss: 0.000032\n",
      "\tTraining batch 30 Loss: 0.000601\n",
      "\tTraining batch 31 Loss: 0.225241\n",
      "\tTraining batch 32 Loss: 0.000026\n",
      "\tTraining batch 33 Loss: 0.012434\n",
      "\tTraining batch 34 Loss: 0.000597\n",
      "\tTraining batch 35 Loss: 0.000001\n",
      "\tTraining batch 36 Loss: 0.003127\n",
      "\tTraining batch 37 Loss: 0.181890\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.012805\n",
      "\tTraining batch 40 Loss: 0.000067\n",
      "\tTraining batch 41 Loss: 0.000541\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.006221\n",
      "\tTraining batch 44 Loss: 0.026673\n",
      "\tTraining batch 45 Loss: 0.003977\n",
      "\tTraining batch 46 Loss: 0.000056\n",
      "\tTraining batch 47 Loss: 0.000032\n",
      "\tTraining batch 48 Loss: 0.001745\n",
      "\tTraining batch 49 Loss: 0.000004\n",
      "\tTraining batch 50 Loss: 0.000001\n",
      "\tTraining batch 51 Loss: 0.002385\n",
      "\tTraining batch 52 Loss: 0.002683\n",
      "Training set: Average loss: 0.016131\n",
      "Validation set: Average loss: 7.087940, Accuracy: 1346/1959 (68.71%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.001016\n",
      "\tTraining batch 2 Loss: 0.000466\n",
      "\tTraining batch 3 Loss: 0.000002\n",
      "\tTraining batch 4 Loss: 0.000005\n",
      "\tTraining batch 5 Loss: 0.142616\n",
      "\tTraining batch 6 Loss: 0.000001\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000005\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000026\n",
      "\tTraining batch 12 Loss: 0.001072\n",
      "\tTraining batch 13 Loss: 0.027994\n",
      "\tTraining batch 14 Loss: 0.000184\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000010\n",
      "\tTraining batch 17 Loss: 0.004898\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.008959\n",
      "\tTraining batch 20 Loss: 0.000001\n",
      "\tTraining batch 21 Loss: 0.001215\n",
      "\tTraining batch 22 Loss: 0.004840\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000782\n",
      "\tTraining batch 27 Loss: 0.000001\n",
      "\tTraining batch 28 Loss: 0.000004\n",
      "\tTraining batch 29 Loss: 0.018152\n",
      "\tTraining batch 30 Loss: 0.000652\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.381428\n",
      "\tTraining batch 33 Loss: 0.000002\n",
      "\tTraining batch 34 Loss: 0.000211\n",
      "\tTraining batch 35 Loss: 0.000014\n",
      "\tTraining batch 36 Loss: 0.000010\n",
      "\tTraining batch 37 Loss: 0.000002\n",
      "\tTraining batch 38 Loss: 0.000003\n",
      "\tTraining batch 39 Loss: 0.000027\n",
      "\tTraining batch 40 Loss: 0.000020\n",
      "\tTraining batch 41 Loss: 0.000043\n",
      "\tTraining batch 42 Loss: 0.000002\n",
      "\tTraining batch 43 Loss: 0.000700\n",
      "\tTraining batch 44 Loss: 0.001783\n",
      "\tTraining batch 45 Loss: 0.004185\n",
      "\tTraining batch 46 Loss: 0.000024\n",
      "\tTraining batch 47 Loss: 0.000002\n",
      "\tTraining batch 48 Loss: 0.024927\n",
      "\tTraining batch 49 Loss: 0.000001\n",
      "\tTraining batch 50 Loss: 0.000030\n",
      "\tTraining batch 51 Loss: 0.004013\n",
      "\tTraining batch 52 Loss: 0.048024\n",
      "Training set: Average loss: 0.013045\n",
      "Validation set: Average loss: 7.270009, Accuracy: 1341/1959 (68.45%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000001\n",
      "\tTraining batch 2 Loss: 0.015686\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000006\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000202\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000225\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000046\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000014\n",
      "\tTraining batch 17 Loss: 0.000017\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.007034\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000112\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000335\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000833\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000012\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000002\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000081\n",
      "\tTraining batch 39 Loss: 0.000011\n",
      "\tTraining batch 40 Loss: 0.000003\n",
      "\tTraining batch 41 Loss: 0.002959\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.009993\n",
      "\tTraining batch 44 Loss: 0.000002\n",
      "\tTraining batch 45 Loss: 0.001136\n",
      "\tTraining batch 46 Loss: 0.000037\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000261\n",
      "\tTraining batch 49 Loss: 0.000079\n",
      "\tTraining batch 50 Loss: 0.000003\n",
      "\tTraining batch 51 Loss: 0.000005\n",
      "\tTraining batch 52 Loss: 0.000139\n",
      "Training set: Average loss: 0.000755\n",
      "Validation set: Average loss: 7.630099, Accuracy: 1353/1959 (69.07%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000931\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000003\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000002\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000121\n",
      "\tTraining batch 14 Loss: 0.000001\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000022\n",
      "\tTraining batch 17 Loss: 0.000011\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.005900\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000015\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000287\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000113\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000008\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000002\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000028\n",
      "\tTraining batch 39 Loss: 0.000002\n",
      "\tTraining batch 40 Loss: 0.000001\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.006836\n",
      "\tTraining batch 44 Loss: 0.000001\n",
      "\tTraining batch 45 Loss: 0.000912\n",
      "\tTraining batch 46 Loss: 0.000020\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000193\n",
      "\tTraining batch 49 Loss: 0.000016\n",
      "\tTraining batch 50 Loss: 0.000002\n",
      "\tTraining batch 51 Loss: 0.000003\n",
      "\tTraining batch 52 Loss: 0.000153\n",
      "Training set: Average loss: 0.000300\n",
      "Validation set: Average loss: 7.714223, Accuracy: 1352/1959 (69.01%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000003\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000002\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000087\n",
      "\tTraining batch 14 Loss: 0.000001\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000019\n",
      "\tTraining batch 17 Loss: 0.000010\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.004829\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000014\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000252\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000105\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000007\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000001\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000023\n",
      "\tTraining batch 39 Loss: 0.000001\n",
      "\tTraining batch 40 Loss: 0.000001\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.005684\n",
      "\tTraining batch 44 Loss: 0.000001\n",
      "\tTraining batch 45 Loss: 0.000756\n",
      "\tTraining batch 46 Loss: 0.000018\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000165\n",
      "\tTraining batch 49 Loss: 0.000009\n",
      "\tTraining batch 50 Loss: 0.000001\n",
      "\tTraining batch 51 Loss: 0.000002\n",
      "\tTraining batch 52 Loss: 0.000123\n",
      "Training set: Average loss: 0.000233\n",
      "Validation set: Average loss: 7.762843, Accuracy: 1352/1959 (69.01%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 47.75%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000003\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000001\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000068\n",
      "\tTraining batch 14 Loss: 0.000001\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000016\n",
      "\tTraining batch 17 Loss: 0.000010\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.003963\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000013\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000231\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000100\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000006\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000001\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000021\n",
      "\tTraining batch 39 Loss: 0.000001\n",
      "\tTraining batch 40 Loss: 0.000001\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.004791\n",
      "\tTraining batch 44 Loss: 0.000001\n",
      "\tTraining batch 45 Loss: 0.000625\n",
      "\tTraining batch 46 Loss: 0.000017\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000145\n",
      "\tTraining batch 49 Loss: 0.000005\n",
      "\tTraining batch 50 Loss: 0.000001\n",
      "\tTraining batch 51 Loss: 0.000002\n",
      "\tTraining batch 52 Loss: 0.000101\n",
      "\tTraining batch 53 Loss: 15.214307\n",
      "Training set: Average loss: 0.287253\n",
      "Validation set: Average loss: 6.755661, Accuracy: 1338/1959 (68.30%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000021\n",
      "\tTraining batch 2 Loss: 0.003267\n",
      "\tTraining batch 3 Loss: 0.096345\n",
      "\tTraining batch 4 Loss: 0.020144\n",
      "\tTraining batch 5 Loss: 0.807239\n",
      "\tTraining batch 6 Loss: 0.171340\n",
      "\tTraining batch 7 Loss: 0.352294\n",
      "\tTraining batch 8 Loss: 1.535026\n",
      "\tTraining batch 9 Loss: 0.437546\n",
      "\tTraining batch 10 Loss: 0.047084\n",
      "\tTraining batch 11 Loss: 0.199725\n",
      "\tTraining batch 12 Loss: 0.014708\n",
      "\tTraining batch 13 Loss: 0.021171\n",
      "\tTraining batch 14 Loss: 0.075709\n",
      "\tTraining batch 15 Loss: 0.681018\n",
      "\tTraining batch 16 Loss: 0.066859\n",
      "\tTraining batch 17 Loss: 0.148369\n",
      "\tTraining batch 18 Loss: 0.297764\n",
      "\tTraining batch 19 Loss: 0.012372\n",
      "\tTraining batch 20 Loss: 0.038362\n",
      "\tTraining batch 21 Loss: 0.102341\n",
      "\tTraining batch 22 Loss: 0.060206\n",
      "\tTraining batch 23 Loss: 0.014913\n",
      "\tTraining batch 24 Loss: 0.000229\n",
      "\tTraining batch 25 Loss: 0.028116\n",
      "\tTraining batch 26 Loss: 0.023735\n",
      "\tTraining batch 27 Loss: 0.136416\n",
      "\tTraining batch 28 Loss: 0.219982\n",
      "\tTraining batch 29 Loss: 0.029618\n",
      "\tTraining batch 30 Loss: 0.028486\n",
      "\tTraining batch 31 Loss: 0.000001\n",
      "\tTraining batch 32 Loss: 0.003749\n",
      "\tTraining batch 33 Loss: 0.348773\n",
      "\tTraining batch 34 Loss: 0.000138\n",
      "\tTraining batch 35 Loss: 0.055929\n",
      "\tTraining batch 36 Loss: 0.000456\n",
      "\tTraining batch 37 Loss: 0.006698\n",
      "\tTraining batch 38 Loss: 0.337700\n",
      "\tTraining batch 39 Loss: 0.000030\n",
      "\tTraining batch 40 Loss: 0.011609\n",
      "\tTraining batch 41 Loss: 0.170720\n",
      "\tTraining batch 42 Loss: 0.001632\n",
      "\tTraining batch 43 Loss: 0.407706\n",
      "\tTraining batch 44 Loss: 0.000344\n",
      "\tTraining batch 45 Loss: 0.000182\n",
      "\tTraining batch 46 Loss: 0.014099\n",
      "\tTraining batch 47 Loss: 0.002300\n",
      "\tTraining batch 48 Loss: 0.000124\n",
      "\tTraining batch 49 Loss: 0.367770\n",
      "\tTraining batch 50 Loss: 0.022979\n",
      "\tTraining batch 51 Loss: 0.065653\n",
      "\tTraining batch 52 Loss: 0.028657\n",
      "\tTraining batch 53 Loss: 0.371165\n",
      "Training set: Average loss: 0.148846\n",
      "Validation set: Average loss: 5.503474, Accuracy: 1362/1959 (69.53%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.116358\n",
      "\tTraining batch 2 Loss: 0.071988\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000003\n",
      "\tTraining batch 5 Loss: 0.000005\n",
      "\tTraining batch 6 Loss: 0.002250\n",
      "\tTraining batch 7 Loss: 0.000211\n",
      "\tTraining batch 8 Loss: 0.000004\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000001\n",
      "\tTraining batch 13 Loss: 0.000083\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000007\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000811\n",
      "\tTraining batch 20 Loss: 0.000058\n",
      "\tTraining batch 21 Loss: 0.000416\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000001\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.003771\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.001141\n",
      "\tTraining batch 29 Loss: 0.000012\n",
      "\tTraining batch 30 Loss: 0.000692\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.022142\n",
      "\tTraining batch 34 Loss: 0.000004\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.003259\n",
      "\tTraining batch 37 Loss: 0.000923\n",
      "\tTraining batch 38 Loss: 0.000070\n",
      "\tTraining batch 39 Loss: 0.000004\n",
      "\tTraining batch 40 Loss: 0.000083\n",
      "\tTraining batch 41 Loss: 0.000240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 42 Loss: 0.055315\n",
      "\tTraining batch 43 Loss: 0.008167\n",
      "\tTraining batch 44 Loss: 0.076118\n",
      "\tTraining batch 45 Loss: 0.000026\n",
      "\tTraining batch 46 Loss: 0.000394\n",
      "\tTraining batch 47 Loss: 0.000057\n",
      "\tTraining batch 48 Loss: 0.090779\n",
      "\tTraining batch 49 Loss: 0.000002\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000003\n",
      "\tTraining batch 52 Loss: 0.000015\n",
      "\tTraining batch 53 Loss: 0.000938\n",
      "Training set: Average loss: 0.008610\n",
      "Validation set: Average loss: 7.296692, Accuracy: 1356/1959 (69.22%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000003\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000048\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000021\n",
      "\tTraining batch 8 Loss: 0.002744\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000133\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000018\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000008\n",
      "\tTraining batch 20 Loss: 0.000016\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000261\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000001\n",
      "\tTraining batch 30 Loss: 0.000536\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000001\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000017\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000004\n",
      "\tTraining batch 37 Loss: 0.001850\n",
      "\tTraining batch 38 Loss: 0.000003\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000014\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000001\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000006\n",
      "\tTraining batch 46 Loss: 0.000097\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000031\n",
      "\tTraining batch 51 Loss: 0.000546\n",
      "\tTraining batch 52 Loss: 0.000031\n",
      "\tTraining batch 53 Loss: 0.015360\n",
      "Training set: Average loss: 0.000410\n",
      "Validation set: Average loss: 7.689405, Accuracy: 1353/1959 (69.07%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000001\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000108\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000009\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.010062\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000001\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000054\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000706\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000205\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000023\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000051\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000005\n",
      "\tTraining batch 42 Loss: 0.000001\n",
      "\tTraining batch 43 Loss: 0.000967\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000030\n",
      "\tTraining batch 46 Loss: 0.001025\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000396\n",
      "\tTraining batch 49 Loss: 0.000001\n",
      "\tTraining batch 50 Loss: 0.000020\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.001011\n",
      "\tTraining batch 53 Loss: 0.089140\n",
      "Training set: Average loss: 0.001959\n",
      "Validation set: Average loss: 7.577967, Accuracy: 1339/1959 (68.35%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000457\n",
      "\tTraining batch 2 Loss: 0.000002\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000005\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.162812\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000039\n",
      "\tTraining batch 10 Loss: 0.007009\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000016\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000001\n",
      "\tTraining batch 20 Loss: 0.012392\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000047\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000305\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000001\n",
      "\tTraining batch 30 Loss: 0.001898\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.005851\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.001835\n",
      "\tTraining batch 35 Loss: 0.000002\n",
      "\tTraining batch 36 Loss: 0.000024\n",
      "\tTraining batch 37 Loss: 0.000013\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000048\n",
      "\tTraining batch 41 Loss: 0.000001\n",
      "\tTraining batch 42 Loss: 0.000006\n",
      "\tTraining batch 43 Loss: 0.000018\n",
      "\tTraining batch 44 Loss: 0.000116\n",
      "\tTraining batch 45 Loss: 0.000019\n",
      "\tTraining batch 46 Loss: 0.174425\n",
      "\tTraining batch 47 Loss: 0.000072\n",
      "\tTraining batch 48 Loss: 0.000006\n",
      "\tTraining batch 49 Loss: 0.004587\n",
      "\tTraining batch 50 Loss: 0.001904\n",
      "\tTraining batch 51 Loss: 0.000076\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.014718\n",
      "Training set: Average loss: 0.007334\n",
      "Validation set: Average loss: 7.226634, Accuracy: 1356/1959 (69.22%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000004\n",
      "\tTraining batch 14 Loss: 0.000001\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000002\n",
      "\tTraining batch 17 Loss: 0.000001\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000003\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000385\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000711\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000007\n",
      "\tTraining batch 30 Loss: 0.000048\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000001\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000626\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.013644\n",
      "\tTraining batch 41 Loss: 0.000763\n",
      "\tTraining batch 42 Loss: 0.000001\n",
      "\tTraining batch 43 Loss: 0.000111\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000097\n",
      "\tTraining batch 46 Loss: 0.000022\n",
      "\tTraining batch 47 Loss: 0.000005\n",
      "\tTraining batch 48 Loss: 0.000004\n",
      "\tTraining batch 49 Loss: 0.000001\n",
      "\tTraining batch 50 Loss: 0.000007\n",
      "\tTraining batch 51 Loss: 0.000007\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.004465\n",
      "Training set: Average loss: 0.000395\n",
      "Validation set: Average loss: 7.924672, Accuracy: 1346/1959 (68.71%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 2 Loss: 0.000001\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000004\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000002\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000001\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000003\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000048\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000890\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000061\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000002\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000087\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000003\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000041\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000067\n",
      "\tTraining batch 46 Loss: 0.000029\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000005\n",
      "\tTraining batch 49 Loss: 0.000001\n",
      "\tTraining batch 50 Loss: 0.000016\n",
      "\tTraining batch 51 Loss: 0.000003\n",
      "\tTraining batch 52 Loss: 0.000001\n",
      "\tTraining batch 53 Loss: 0.000295\n",
      "Training set: Average loss: 0.000030\n",
      "Validation set: Average loss: 8.138464, Accuracy: 1340/1959 (68.40%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000002\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000004\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000002\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000002\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000003\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000033\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000734\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000062\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000001\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000060\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000003\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000039\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000068\n",
      "\tTraining batch 46 Loss: 0.000027\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000005\n",
      "\tTraining batch 49 Loss: 0.000001\n",
      "\tTraining batch 50 Loss: 0.000016\n",
      "\tTraining batch 51 Loss: 0.000003\n",
      "\tTraining batch 52 Loss: 0.000001\n",
      "\tTraining batch 53 Loss: 0.000249\n",
      "Training set: Average loss: 0.000025\n",
      "Validation set: Average loss: 8.143922, Accuracy: 1340/1959 (68.40%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 47.5%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000002\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000004\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000002\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000001\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000003\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000026\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000619\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000062\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000001\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000045\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000003\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000037\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000068\n",
      "\tTraining batch 46 Loss: 0.000025\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000005\n",
      "\tTraining batch 49 Loss: 0.000001\n",
      "\tTraining batch 50 Loss: 0.000016\n",
      "\tTraining batch 51 Loss: 0.000003\n",
      "\tTraining batch 52 Loss: 0.000001\n",
      "\tTraining batch 53 Loss: 0.000217\n",
      "\tTraining batch 54 Loss: 17.108250\n",
      "Training set: Average loss: 0.316841\n",
      "Validation set: Average loss: 6.843442, Accuracy: 1330/1959 (67.89%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000001\n",
      "\tTraining batch 2 Loss: 0.024556\n",
      "\tTraining batch 3 Loss: 0.000001\n",
      "\tTraining batch 4 Loss: 0.016739\n",
      "\tTraining batch 5 Loss: 0.197041\n",
      "\tTraining batch 6 Loss: 0.064984\n",
      "\tTraining batch 7 Loss: 0.099897\n",
      "\tTraining batch 8 Loss: 0.035694\n",
      "\tTraining batch 9 Loss: 0.356368\n",
      "\tTraining batch 10 Loss: 0.126130\n",
      "\tTraining batch 11 Loss: 0.053622\n",
      "\tTraining batch 12 Loss: 0.351523\n",
      "\tTraining batch 13 Loss: 0.011603\n",
      "\tTraining batch 14 Loss: 0.460782\n",
      "\tTraining batch 15 Loss: 0.249668\n",
      "\tTraining batch 16 Loss: 0.042902\n",
      "\tTraining batch 17 Loss: 0.257595\n",
      "\tTraining batch 18 Loss: 0.052494\n",
      "\tTraining batch 19 Loss: 0.835494\n",
      "\tTraining batch 20 Loss: 0.002606\n",
      "\tTraining batch 21 Loss: 0.047494\n",
      "\tTraining batch 22 Loss: 0.006292\n",
      "\tTraining batch 23 Loss: 0.039173\n",
      "\tTraining batch 24 Loss: 0.003482\n",
      "\tTraining batch 25 Loss: 0.075981\n",
      "\tTraining batch 26 Loss: 0.017347\n",
      "\tTraining batch 27 Loss: 0.013044\n",
      "\tTraining batch 28 Loss: 0.114698\n",
      "\tTraining batch 29 Loss: 0.087834\n",
      "\tTraining batch 30 Loss: 0.001860\n",
      "\tTraining batch 31 Loss: 0.037794\n",
      "\tTraining batch 32 Loss: 0.000428\n",
      "\tTraining batch 33 Loss: 0.000249\n",
      "\tTraining batch 34 Loss: 0.006498\n",
      "\tTraining batch 35 Loss: 0.219868\n",
      "\tTraining batch 36 Loss: 0.000142\n",
      "\tTraining batch 37 Loss: 0.335586\n",
      "\tTraining batch 38 Loss: 0.063631\n",
      "\tTraining batch 39 Loss: 0.003582\n",
      "\tTraining batch 40 Loss: 0.424007\n",
      "\tTraining batch 41 Loss: 0.006793\n",
      "\tTraining batch 42 Loss: 0.001381\n",
      "\tTraining batch 43 Loss: 0.224234\n",
      "\tTraining batch 44 Loss: 0.042305\n",
      "\tTraining batch 45 Loss: 0.000022\n",
      "\tTraining batch 46 Loss: 0.125508\n",
      "\tTraining batch 47 Loss: 0.003026\n",
      "\tTraining batch 48 Loss: 0.223990\n",
      "\tTraining batch 49 Loss: 0.175635\n",
      "\tTraining batch 50 Loss: 0.253569\n",
      "\tTraining batch 51 Loss: 0.189757\n",
      "\tTraining batch 52 Loss: 0.001109\n",
      "\tTraining batch 53 Loss: 0.031786\n",
      "\tTraining batch 54 Loss: 0.509573\n",
      "Training set: Average loss: 0.120877\n",
      "Validation set: Average loss: 6.639699, Accuracy: 1352/1959 (69.01%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.021485\n",
      "\tTraining batch 2 Loss: 0.000001\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000751\n",
      "\tTraining batch 5 Loss: 0.000011\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000034\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000338\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000135\n",
      "\tTraining batch 14 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.154383\n",
      "\tTraining batch 17 Loss: 0.000003\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.006654\n",
      "\tTraining batch 20 Loss: 0.000005\n",
      "\tTraining batch 21 Loss: 0.000001\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000831\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.009448\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000092\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000010\n",
      "\tTraining batch 31 Loss: 0.000546\n",
      "\tTraining batch 32 Loss: 0.000185\n",
      "\tTraining batch 33 Loss: 0.000001\n",
      "\tTraining batch 34 Loss: 0.000114\n",
      "\tTraining batch 35 Loss: 0.000001\n",
      "\tTraining batch 36 Loss: 0.005297\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.232723\n",
      "\tTraining batch 39 Loss: 0.000058\n",
      "\tTraining batch 40 Loss: 0.000001\n",
      "\tTraining batch 41 Loss: 0.000001\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.011699\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.002710\n",
      "\tTraining batch 46 Loss: 0.000086\n",
      "\tTraining batch 47 Loss: 0.000033\n",
      "\tTraining batch 48 Loss: 0.073117\n",
      "\tTraining batch 49 Loss: 0.055273\n",
      "\tTraining batch 50 Loss: 0.000024\n",
      "\tTraining batch 51 Loss: 0.000078\n",
      "\tTraining batch 52 Loss: 0.001757\n",
      "\tTraining batch 53 Loss: 0.000099\n",
      "\tTraining batch 54 Loss: 0.199380\n",
      "Training set: Average loss: 0.014396\n",
      "Validation set: Average loss: 7.725457, Accuracy: 1368/1959 (69.83%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000169\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000010\n",
      "\tTraining batch 5 Loss: 0.326403\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000001\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.001413\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000013\n",
      "\tTraining batch 17 Loss: 0.000007\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.009472\n",
      "\tTraining batch 20 Loss: 0.000006\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000002\n",
      "\tTraining batch 23 Loss: 0.019025\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.166745\n",
      "\tTraining batch 27 Loss: 0.000002\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000004\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000315\n",
      "\tTraining batch 32 Loss: 0.000534\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000003\n",
      "\tTraining batch 35 Loss: 0.000001\n",
      "\tTraining batch 36 Loss: 0.002401\n",
      "\tTraining batch 37 Loss: 0.208392\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.030867\n",
      "\tTraining batch 40 Loss: 0.234766\n",
      "\tTraining batch 41 Loss: 0.000005\n",
      "\tTraining batch 42 Loss: 0.561155\n",
      "\tTraining batch 43 Loss: 0.000048\n",
      "\tTraining batch 44 Loss: 0.000062\n",
      "\tTraining batch 45 Loss: 0.001203\n",
      "\tTraining batch 46 Loss: 0.000034\n",
      "\tTraining batch 47 Loss: 0.003834\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000001\n",
      "\tTraining batch 50 Loss: 0.000001\n",
      "\tTraining batch 51 Loss: 0.159648\n",
      "\tTraining batch 52 Loss: 0.059614\n",
      "\tTraining batch 53 Loss: 0.056890\n",
      "\tTraining batch 54 Loss: 0.070165\n",
      "Training set: Average loss: 0.035430\n",
      "Validation set: Average loss: 8.215355, Accuracy: 1364/1959 (69.63%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000008\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000059\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000007\n",
      "\tTraining batch 7 Loss: 0.000001\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000076\n",
      "\tTraining batch 10 Loss: 0.000001\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000096\n",
      "\tTraining batch 17 Loss: 0.024702\n",
      "\tTraining batch 18 Loss: 0.000001\n",
      "\tTraining batch 19 Loss: 0.007943\n",
      "\tTraining batch 20 Loss: 0.000004\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000003\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000807\n",
      "\tTraining batch 27 Loss: 0.000065\n",
      "\tTraining batch 28 Loss: 0.000002\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.100894\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000001\n",
      "\tTraining batch 34 Loss: 0.001064\n",
      "\tTraining batch 35 Loss: 0.000013\n",
      "\tTraining batch 36 Loss: 0.000001\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000003\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000018\n",
      "\tTraining batch 41 Loss: 0.000478\n",
      "\tTraining batch 42 Loss: 0.000010\n",
      "\tTraining batch 43 Loss: 0.067531\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.001207\n",
      "\tTraining batch 46 Loss: 0.000072\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.017156\n",
      "\tTraining batch 49 Loss: 0.091836\n",
      "\tTraining batch 50 Loss: 0.000107\n",
      "\tTraining batch 51 Loss: 0.000871\n",
      "\tTraining batch 52 Loss: 0.001290\n",
      "\tTraining batch 53 Loss: 0.000046\n",
      "\tTraining batch 54 Loss: 0.027185\n",
      "Training set: Average loss: 0.006362\n",
      "Validation set: Average loss: 8.184855, Accuracy: 1316/1959 (67.18%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000131\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.019038\n",
      "\tTraining batch 4 Loss: 0.001529\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000001\n",
      "\tTraining batch 8 Loss: 0.000001\n",
      "\tTraining batch 9 Loss: 0.000103\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000006\n",
      "\tTraining batch 13 Loss: 0.000001\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.001892\n",
      "\tTraining batch 16 Loss: 0.000007\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001648\n",
      "\tTraining batch 20 Loss: 0.002502\n",
      "\tTraining batch 21 Loss: 0.000445\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000008\n",
      "\tTraining batch 24 Loss: 0.021005\n",
      "\tTraining batch 25 Loss: 0.000001\n",
      "\tTraining batch 26 Loss: 0.000112\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000019\n",
      "\tTraining batch 29 Loss: 0.010780\n",
      "\tTraining batch 30 Loss: 0.000011\n",
      "\tTraining batch 31 Loss: 0.000002\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000011\n",
      "\tTraining batch 34 Loss: 0.000150\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000005\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000001\n",
      "\tTraining batch 41 Loss: 0.000022\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.006445\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000280\n",
      "\tTraining batch 46 Loss: 0.000028\n",
      "\tTraining batch 47 Loss: 0.000007\n",
      "\tTraining batch 48 Loss: 0.000012\n",
      "\tTraining batch 49 Loss: 0.000001\n",
      "\tTraining batch 50 Loss: 0.000216\n",
      "\tTraining batch 51 Loss: 0.001301\n",
      "\tTraining batch 52 Loss: 0.000883\n",
      "\tTraining batch 53 Loss: 0.000108\n",
      "\tTraining batch 54 Loss: 0.015173\n",
      "Training set: Average loss: 0.001553\n",
      "Validation set: Average loss: 7.643474, Accuracy: 1376/1959 (70.24%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000028\n",
      "\tTraining batch 2 Loss: 0.000018\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000004\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000001\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000066\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001860\n",
      "\tTraining batch 20 Loss: 0.000052\n",
      "\tTraining batch 21 Loss: 0.000005\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000023\n",
      "\tTraining batch 27 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 28 Loss: 0.000013\n",
      "\tTraining batch 29 Loss: 0.000002\n",
      "\tTraining batch 30 Loss: 0.000004\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000014\n",
      "\tTraining batch 34 Loss: 0.000013\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000003\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000017\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.002103\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000238\n",
      "\tTraining batch 46 Loss: 0.000020\n",
      "\tTraining batch 47 Loss: 0.000006\n",
      "\tTraining batch 48 Loss: 0.000013\n",
      "\tTraining batch 49 Loss: 0.000001\n",
      "\tTraining batch 50 Loss: 0.000029\n",
      "\tTraining batch 51 Loss: 0.000002\n",
      "\tTraining batch 52 Loss: 0.000030\n",
      "\tTraining batch 53 Loss: 0.000087\n",
      "\tTraining batch 54 Loss: 0.011518\n",
      "Training set: Average loss: 0.000299\n",
      "Validation set: Average loss: 7.734483, Accuracy: 1377/1959 (70.29%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000020\n",
      "\tTraining batch 2 Loss: 0.000007\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000003\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000046\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001756\n",
      "\tTraining batch 20 Loss: 0.000051\n",
      "\tTraining batch 21 Loss: 0.000006\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000022\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000012\n",
      "\tTraining batch 29 Loss: 0.000003\n",
      "\tTraining batch 30 Loss: 0.000004\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000019\n",
      "\tTraining batch 34 Loss: 0.000012\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000003\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000012\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.001364\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000212\n",
      "\tTraining batch 46 Loss: 0.000019\n",
      "\tTraining batch 47 Loss: 0.000006\n",
      "\tTraining batch 48 Loss: 0.000012\n",
      "\tTraining batch 49 Loss: 0.000001\n",
      "\tTraining batch 50 Loss: 0.000023\n",
      "\tTraining batch 51 Loss: 0.000002\n",
      "\tTraining batch 52 Loss: 0.000020\n",
      "\tTraining batch 53 Loss: 0.000081\n",
      "\tTraining batch 54 Loss: 0.009520\n",
      "Training set: Average loss: 0.000245\n",
      "Validation set: Average loss: 7.769300, Accuracy: 1378/1959 (70.34%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000016\n",
      "\tTraining batch 2 Loss: 0.000006\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000003\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000036\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001655\n",
      "\tTraining batch 20 Loss: 0.000048\n",
      "\tTraining batch 21 Loss: 0.000007\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000022\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000011\n",
      "\tTraining batch 29 Loss: 0.000003\n",
      "\tTraining batch 30 Loss: 0.000003\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000023\n",
      "\tTraining batch 34 Loss: 0.000012\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000003\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000010\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.001017\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000194\n",
      "\tTraining batch 46 Loss: 0.000018\n",
      "\tTraining batch 47 Loss: 0.000005\n",
      "\tTraining batch 48 Loss: 0.000011\n",
      "\tTraining batch 49 Loss: 0.000002\n",
      "\tTraining batch 50 Loss: 0.000021\n",
      "\tTraining batch 51 Loss: 0.000002\n",
      "\tTraining batch 52 Loss: 0.000016\n",
      "\tTraining batch 53 Loss: 0.000077\n",
      "\tTraining batch 54 Loss: 0.008185\n",
      "Training set: Average loss: 0.000211\n",
      "Validation set: Average loss: 7.796873, Accuracy: 1378/1959 (70.34%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 50.24999999999999%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000014\n",
      "\tTraining batch 2 Loss: 0.000005\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000003\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000023\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001518\n",
      "\tTraining batch 20 Loss: 0.000042\n",
      "\tTraining batch 21 Loss: 0.000007\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000001\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000021\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000009\n",
      "\tTraining batch 29 Loss: 0.000004\n",
      "\tTraining batch 30 Loss: 0.000003\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000021\n",
      "\tTraining batch 34 Loss: 0.000013\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000003\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000007\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000763\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000178\n",
      "\tTraining batch 46 Loss: 0.000018\n",
      "\tTraining batch 47 Loss: 0.000004\n",
      "\tTraining batch 48 Loss: 0.000009\n",
      "\tTraining batch 49 Loss: 0.000002\n",
      "\tTraining batch 50 Loss: 0.000017\n",
      "\tTraining batch 51 Loss: 0.000002\n",
      "\tTraining batch 52 Loss: 0.000015\n",
      "\tTraining batch 53 Loss: 0.000072\n",
      "\tTraining batch 54 Loss: 0.007320\n",
      "\tTraining batch 55 Loss: 11.549535\n",
      "Training set: Average loss: 0.210175\n",
      "Validation set: Average loss: 6.792870, Accuracy: 1369/1959 (69.88%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000020\n",
      "\tTraining batch 2 Loss: 0.434621\n",
      "\tTraining batch 3 Loss: 0.000005\n",
      "\tTraining batch 4 Loss: 0.104212\n",
      "\tTraining batch 5 Loss: 0.072035\n",
      "\tTraining batch 6 Loss: 0.056483\n",
      "\tTraining batch 7 Loss: 0.242395\n",
      "\tTraining batch 8 Loss: 0.105709\n",
      "\tTraining batch 9 Loss: 0.287426\n",
      "\tTraining batch 10 Loss: 0.593017\n",
      "\tTraining batch 11 Loss: 0.251336\n",
      "\tTraining batch 12 Loss: 0.062283\n",
      "\tTraining batch 13 Loss: 0.317712\n",
      "\tTraining batch 14 Loss: 0.272095\n",
      "\tTraining batch 15 Loss: 0.043147\n",
      "\tTraining batch 16 Loss: 0.049844\n",
      "\tTraining batch 17 Loss: 0.084302\n",
      "\tTraining batch 18 Loss: 0.070797\n",
      "\tTraining batch 19 Loss: 0.407125\n",
      "\tTraining batch 20 Loss: 0.049643\n",
      "\tTraining batch 21 Loss: 0.022610\n",
      "\tTraining batch 22 Loss: 0.000619\n",
      "\tTraining batch 23 Loss: 0.023628\n",
      "\tTraining batch 24 Loss: 0.369487\n",
      "\tTraining batch 25 Loss: 0.002295\n",
      "\tTraining batch 26 Loss: 0.179860\n",
      "\tTraining batch 27 Loss: 0.001772\n",
      "\tTraining batch 28 Loss: 0.389841\n",
      "\tTraining batch 29 Loss: 0.070158\n",
      "\tTraining batch 30 Loss: 0.710308\n",
      "\tTraining batch 31 Loss: 0.097987\n",
      "\tTraining batch 32 Loss: 0.288419\n",
      "\tTraining batch 33 Loss: 0.254014\n",
      "\tTraining batch 34 Loss: 0.110755\n",
      "\tTraining batch 35 Loss: 0.069491\n",
      "\tTraining batch 36 Loss: 0.003199\n",
      "\tTraining batch 37 Loss: 0.003196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 38 Loss: 0.003713\n",
      "\tTraining batch 39 Loss: 0.006912\n",
      "\tTraining batch 40 Loss: 0.164119\n",
      "\tTraining batch 41 Loss: 0.030567\n",
      "\tTraining batch 42 Loss: 0.428096\n",
      "\tTraining batch 43 Loss: 0.007647\n",
      "\tTraining batch 44 Loss: 0.506436\n",
      "\tTraining batch 45 Loss: 0.008867\n",
      "\tTraining batch 46 Loss: 0.000394\n",
      "\tTraining batch 47 Loss: 0.003397\n",
      "\tTraining batch 48 Loss: 0.062198\n",
      "\tTraining batch 49 Loss: 0.221328\n",
      "\tTraining batch 50 Loss: 0.119858\n",
      "\tTraining batch 51 Loss: 0.079732\n",
      "\tTraining batch 52 Loss: 0.002269\n",
      "\tTraining batch 53 Loss: 0.171322\n",
      "\tTraining batch 54 Loss: 0.008554\n",
      "\tTraining batch 55 Loss: 0.376473\n",
      "Training set: Average loss: 0.150977\n",
      "Validation set: Average loss: 6.131753, Accuracy: 1349/1959 (68.86%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.004368\n",
      "\tTraining batch 2 Loss: 0.000080\n",
      "\tTraining batch 3 Loss: 0.000002\n",
      "\tTraining batch 4 Loss: 0.000471\n",
      "\tTraining batch 5 Loss: 0.000002\n",
      "\tTraining batch 6 Loss: 0.000002\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000050\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000008\n",
      "\tTraining batch 12 Loss: 0.019530\n",
      "\tTraining batch 13 Loss: 0.000001\n",
      "\tTraining batch 14 Loss: 0.050551\n",
      "\tTraining batch 15 Loss: 0.000320\n",
      "\tTraining batch 16 Loss: 0.029534\n",
      "\tTraining batch 17 Loss: 0.000008\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.002390\n",
      "\tTraining batch 20 Loss: 0.000165\n",
      "\tTraining batch 21 Loss: 0.000001\n",
      "\tTraining batch 22 Loss: 0.000136\n",
      "\tTraining batch 23 Loss: 0.001591\n",
      "\tTraining batch 24 Loss: 0.000006\n",
      "\tTraining batch 25 Loss: 0.076664\n",
      "\tTraining batch 26 Loss: 0.000538\n",
      "\tTraining batch 27 Loss: 0.060119\n",
      "\tTraining batch 28 Loss: 0.000264\n",
      "\tTraining batch 29 Loss: 0.003099\n",
      "\tTraining batch 30 Loss: 0.000033\n",
      "\tTraining batch 31 Loss: 0.017601\n",
      "\tTraining batch 32 Loss: 0.058029\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000927\n",
      "\tTraining batch 35 Loss: 0.000001\n",
      "\tTraining batch 36 Loss: 0.000035\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.486492\n",
      "\tTraining batch 39 Loss: 0.001347\n",
      "\tTraining batch 40 Loss: 0.003549\n",
      "\tTraining batch 41 Loss: 0.033775\n",
      "\tTraining batch 42 Loss: 0.083211\n",
      "\tTraining batch 43 Loss: 0.020277\n",
      "\tTraining batch 44 Loss: 0.000012\n",
      "\tTraining batch 45 Loss: 0.000019\n",
      "\tTraining batch 46 Loss: 0.001978\n",
      "\tTraining batch 47 Loss: 0.001255\n",
      "\tTraining batch 48 Loss: 0.000015\n",
      "\tTraining batch 49 Loss: 0.215480\n",
      "\tTraining batch 50 Loss: 0.251036\n",
      "\tTraining batch 51 Loss: 0.096961\n",
      "\tTraining batch 52 Loss: 0.007519\n",
      "\tTraining batch 53 Loss: 0.021540\n",
      "\tTraining batch 54 Loss: 0.117735\n",
      "\tTraining batch 55 Loss: 0.450301\n",
      "Training set: Average loss: 0.038528\n",
      "Validation set: Average loss: 7.870444, Accuracy: 1330/1959 (67.89%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000022\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.001959\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000001\n",
      "\tTraining batch 10 Loss: 0.000003\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001711\n",
      "\tTraining batch 20 Loss: 0.000084\n",
      "\tTraining batch 21 Loss: 0.394585\n",
      "\tTraining batch 22 Loss: 0.393087\n",
      "\tTraining batch 23 Loss: 0.186302\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000889\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000010\n",
      "\tTraining batch 29 Loss: 0.281067\n",
      "\tTraining batch 30 Loss: 0.000003\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.003562\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000294\n",
      "\tTraining batch 35 Loss: 0.000001\n",
      "\tTraining batch 36 Loss: 0.000074\n",
      "\tTraining batch 37 Loss: 0.000004\n",
      "\tTraining batch 38 Loss: 0.000001\n",
      "\tTraining batch 39 Loss: 0.525358\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000021\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.001120\n",
      "\tTraining batch 44 Loss: 0.001658\n",
      "\tTraining batch 45 Loss: 0.000002\n",
      "\tTraining batch 46 Loss: 0.000029\n",
      "\tTraining batch 47 Loss: 0.004765\n",
      "\tTraining batch 48 Loss: 0.000255\n",
      "\tTraining batch 49 Loss: 0.000002\n",
      "\tTraining batch 50 Loss: 0.000054\n",
      "\tTraining batch 51 Loss: 0.003270\n",
      "\tTraining batch 52 Loss: 0.004199\n",
      "\tTraining batch 53 Loss: 0.000205\n",
      "\tTraining batch 54 Loss: 0.003116\n",
      "\tTraining batch 55 Loss: 0.001861\n",
      "Training set: Average loss: 0.032901\n",
      "Validation set: Average loss: 9.514113, Accuracy: 1308/1959 (66.77%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.002220\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.037792\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000002\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000068\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000001\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000507\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.070129\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000394\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000001\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000042\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000027\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000001\n",
      "\tTraining batch 30 Loss: 0.000015\n",
      "\tTraining batch 31 Loss: 0.000001\n",
      "\tTraining batch 32 Loss: 0.000001\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000488\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.076362\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000014\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000035\n",
      "\tTraining batch 46 Loss: 0.000011\n",
      "\tTraining batch 47 Loss: 0.000003\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000016\n",
      "\tTraining batch 51 Loss: 0.003466\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000001\n",
      "\tTraining batch 54 Loss: 0.005037\n",
      "\tTraining batch 55 Loss: 0.000004\n",
      "Training set: Average loss: 0.003575\n",
      "Validation set: Average loss: 9.097756, Accuracy: 1363/1959 (69.58%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000001\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000423\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000005\n",
      "\tTraining batch 23 Loss: 0.000271\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000018\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000001\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000006\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000027\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000042\n",
      "\tTraining batch 46 Loss: 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000006\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000001\n",
      "\tTraining batch 54 Loss: 0.004597\n",
      "\tTraining batch 55 Loss: 0.000007\n",
      "Training set: Average loss: 0.000098\n",
      "Validation set: Average loss: 9.102348, Accuracy: 1359/1959 (69.37%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000001\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000001\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000001\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000284\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000004\n",
      "\tTraining batch 23 Loss: 0.000104\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000019\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000001\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000006\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000027\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000027\n",
      "\tTraining batch 46 Loss: 0.000001\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000006\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000001\n",
      "\tTraining batch 54 Loss: 0.002477\n",
      "\tTraining batch 55 Loss: 0.000007\n",
      "Training set: Average loss: 0.000054\n",
      "Validation set: Average loss: 9.110744, Accuracy: 1359/1959 (69.37%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 45.5%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000001\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000001\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000028\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000003\n",
      "\tTraining batch 23 Loss: 0.000070\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000023\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000001\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000007\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000026\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000009\n",
      "\tTraining batch 46 Loss: 0.000001\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000006\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000001\n",
      "\tTraining batch 54 Loss: 0.000528\n",
      "\tTraining batch 55 Loss: 0.000009\n",
      "\tTraining batch 56 Loss: 13.360437\n",
      "Training set: Average loss: 0.238592\n",
      "Validation set: Average loss: 8.103612, Accuracy: 1365/1959 (69.68%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000004\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000203\n",
      "\tTraining batch 5 Loss: 0.000001\n",
      "\tTraining batch 6 Loss: 0.000835\n",
      "\tTraining batch 7 Loss: 0.052918\n",
      "\tTraining batch 8 Loss: 0.000003\n",
      "\tTraining batch 9 Loss: 0.149470\n",
      "\tTraining batch 10 Loss: 0.003756\n",
      "\tTraining batch 11 Loss: 0.065798\n",
      "\tTraining batch 12 Loss: 0.017764\n",
      "\tTraining batch 13 Loss: 0.494039\n",
      "\tTraining batch 14 Loss: 0.424500\n",
      "\tTraining batch 15 Loss: 0.196524\n",
      "\tTraining batch 16 Loss: 0.184259\n",
      "\tTraining batch 17 Loss: 0.260025\n",
      "\tTraining batch 18 Loss: 0.062272\n",
      "\tTraining batch 19 Loss: 0.054355\n",
      "\tTraining batch 20 Loss: 0.021549\n",
      "\tTraining batch 21 Loss: 0.027552\n",
      "\tTraining batch 22 Loss: 0.143663\n",
      "\tTraining batch 23 Loss: 0.000999\n",
      "\tTraining batch 24 Loss: 0.067089\n",
      "\tTraining batch 25 Loss: 0.000356\n",
      "\tTraining batch 26 Loss: 0.042065\n",
      "\tTraining batch 27 Loss: 0.129045\n",
      "\tTraining batch 28 Loss: 0.355039\n",
      "\tTraining batch 29 Loss: 0.030985\n",
      "\tTraining batch 30 Loss: 0.000128\n",
      "\tTraining batch 31 Loss: 0.124029\n",
      "\tTraining batch 32 Loss: 0.595669\n",
      "\tTraining batch 33 Loss: 0.000120\n",
      "\tTraining batch 34 Loss: 0.000333\n",
      "\tTraining batch 35 Loss: 0.016160\n",
      "\tTraining batch 36 Loss: 0.321866\n",
      "\tTraining batch 37 Loss: 0.342686\n",
      "\tTraining batch 38 Loss: 0.188456\n",
      "\tTraining batch 39 Loss: 0.001922\n",
      "\tTraining batch 40 Loss: 0.108097\n",
      "\tTraining batch 41 Loss: 0.001028\n",
      "\tTraining batch 42 Loss: 0.066175\n",
      "\tTraining batch 43 Loss: 0.020818\n",
      "\tTraining batch 44 Loss: 0.172599\n",
      "\tTraining batch 45 Loss: 0.009479\n",
      "\tTraining batch 46 Loss: 0.606849\n",
      "\tTraining batch 47 Loss: 0.000005\n",
      "\tTraining batch 48 Loss: 0.085530\n",
      "\tTraining batch 49 Loss: 0.000141\n",
      "\tTraining batch 50 Loss: 0.077242\n",
      "\tTraining batch 51 Loss: 0.224388\n",
      "\tTraining batch 52 Loss: 0.385233\n",
      "\tTraining batch 53 Loss: 0.153942\n",
      "\tTraining batch 54 Loss: 0.024335\n",
      "\tTraining batch 55 Loss: 0.355572\n",
      "\tTraining batch 56 Loss: 0.188846\n",
      "Training set: Average loss: 0.122441\n",
      "Validation set: Average loss: 7.739864, Accuracy: 1340/1959 (68.40%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000001\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.001272\n",
      "\tTraining batch 12 Loss: 0.002797\n",
      "\tTraining batch 13 Loss: 0.000003\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.001584\n",
      "\tTraining batch 16 Loss: 0.000005\n",
      "\tTraining batch 17 Loss: 0.002210\n",
      "\tTraining batch 18 Loss: 0.000289\n",
      "\tTraining batch 19 Loss: 0.004847\n",
      "\tTraining batch 20 Loss: 0.000333\n",
      "\tTraining batch 21 Loss: 0.004117\n",
      "\tTraining batch 22 Loss: 0.045199\n",
      "\tTraining batch 23 Loss: 0.000633\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000004\n",
      "\tTraining batch 26 Loss: 0.008794\n",
      "\tTraining batch 27 Loss: 0.068448\n",
      "\tTraining batch 28 Loss: 0.000217\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000016\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.003297\n",
      "\tTraining batch 34 Loss: 0.000016\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000238\n",
      "\tTraining batch 37 Loss: 0.000043\n",
      "\tTraining batch 38 Loss: 0.000012\n",
      "\tTraining batch 39 Loss: 0.000030\n",
      "\tTraining batch 40 Loss: 0.001508\n",
      "\tTraining batch 41 Loss: 0.000026\n",
      "\tTraining batch 42 Loss: 0.013715\n",
      "\tTraining batch 43 Loss: 0.493133\n",
      "\tTraining batch 44 Loss: 0.000433\n",
      "\tTraining batch 45 Loss: 0.000009\n",
      "\tTraining batch 46 Loss: 0.000020\n",
      "\tTraining batch 47 Loss: 0.000005\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.011071\n",
      "\tTraining batch 50 Loss: 0.000021\n",
      "\tTraining batch 51 Loss: 0.001243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 52 Loss: 0.000001\n",
      "\tTraining batch 53 Loss: 0.000020\n",
      "\tTraining batch 54 Loss: 0.209334\n",
      "\tTraining batch 55 Loss: 0.000077\n",
      "\tTraining batch 56 Loss: 0.630077\n",
      "Training set: Average loss: 0.026877\n",
      "Validation set: Average loss: 8.430130, Accuracy: 1318/1959 (67.28%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000003\n",
      "\tTraining batch 8 Loss: 0.000019\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.001497\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000523\n",
      "\tTraining batch 19 Loss: 0.005836\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000044\n",
      "\tTraining batch 22 Loss: 0.000002\n",
      "\tTraining batch 23 Loss: 0.287186\n",
      "\tTraining batch 24 Loss: 0.000001\n",
      "\tTraining batch 25 Loss: 0.396978\n",
      "\tTraining batch 26 Loss: 0.141003\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.104615\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000002\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000088\n",
      "\tTraining batch 36 Loss: 0.000027\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000042\n",
      "\tTraining batch 39 Loss: 0.000001\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000016\n",
      "\tTraining batch 42 Loss: 0.000015\n",
      "\tTraining batch 43 Loss: 0.000037\n",
      "\tTraining batch 44 Loss: 0.000022\n",
      "\tTraining batch 45 Loss: 0.000002\n",
      "\tTraining batch 46 Loss: 0.071673\n",
      "\tTraining batch 47 Loss: 0.102215\n",
      "\tTraining batch 48 Loss: 0.000010\n",
      "\tTraining batch 49 Loss: 0.000049\n",
      "\tTraining batch 50 Loss: 0.004478\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000014\n",
      "\tTraining batch 53 Loss: 0.000513\n",
      "\tTraining batch 54 Loss: 0.000246\n",
      "\tTraining batch 55 Loss: 0.000355\n",
      "\tTraining batch 56 Loss: 0.034065\n",
      "Training set: Average loss: 0.020564\n",
      "Validation set: Average loss: 9.498904, Accuracy: 1296/1959 (66.16%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.046419\n",
      "\tTraining batch 2 Loss: 0.013069\n",
      "\tTraining batch 3 Loss: 0.000578\n",
      "\tTraining batch 4 Loss: 0.008629\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000100\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000007\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.343931\n",
      "\tTraining batch 14 Loss: 0.000026\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000238\n",
      "\tTraining batch 20 Loss: 0.000002\n",
      "\tTraining batch 21 Loss: 0.000004\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000133\n",
      "\tTraining batch 27 Loss: 0.001905\n",
      "\tTraining batch 28 Loss: 0.151656\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000013\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000001\n",
      "\tTraining batch 34 Loss: 0.000242\n",
      "\tTraining batch 35 Loss: 0.000076\n",
      "\tTraining batch 36 Loss: 0.000008\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000198\n",
      "\tTraining batch 39 Loss: 0.016521\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000124\n",
      "\tTraining batch 42 Loss: 0.000191\n",
      "\tTraining batch 43 Loss: 0.003164\n",
      "\tTraining batch 44 Loss: 0.007016\n",
      "\tTraining batch 45 Loss: 0.278225\n",
      "\tTraining batch 46 Loss: 0.002360\n",
      "\tTraining batch 47 Loss: 0.005418\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000024\n",
      "\tTraining batch 50 Loss: 0.000642\n",
      "\tTraining batch 51 Loss: 0.000003\n",
      "\tTraining batch 52 Loss: 0.001145\n",
      "\tTraining batch 53 Loss: 0.095816\n",
      "\tTraining batch 54 Loss: 0.042359\n",
      "\tTraining batch 55 Loss: 0.000087\n",
      "\tTraining batch 56 Loss: 0.105439\n",
      "Training set: Average loss: 0.020103\n",
      "Validation set: Average loss: 10.012840, Accuracy: 1325/1959 (67.64%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000007\n",
      "\tTraining batch 2 Loss: 0.000143\n",
      "\tTraining batch 3 Loss: 0.003390\n",
      "\tTraining batch 4 Loss: 0.000002\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000005\n",
      "\tTraining batch 7 Loss: 0.347781\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.052048\n",
      "\tTraining batch 10 Loss: 0.001281\n",
      "\tTraining batch 11 Loss: 0.001325\n",
      "\tTraining batch 12 Loss: 0.000005\n",
      "\tTraining batch 13 Loss: 0.000003\n",
      "\tTraining batch 14 Loss: 0.000002\n",
      "\tTraining batch 15 Loss: 0.000016\n",
      "\tTraining batch 16 Loss: 0.000011\n",
      "\tTraining batch 17 Loss: 0.000002\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000019\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000023\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.054568\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000165\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000016\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000008\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000003\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000004\n",
      "\tTraining batch 41 Loss: 0.019929\n",
      "\tTraining batch 42 Loss: 0.000001\n",
      "\tTraining batch 43 Loss: 0.011750\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000010\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000066\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000040\n",
      "\tTraining batch 51 Loss: 0.000009\n",
      "\tTraining batch 52 Loss: 0.000081\n",
      "\tTraining batch 53 Loss: 0.000001\n",
      "\tTraining batch 54 Loss: 0.000048\n",
      "\tTraining batch 55 Loss: 0.110998\n",
      "\tTraining batch 56 Loss: 0.327591\n",
      "Training set: Average loss: 0.016631\n",
      "Validation set: Average loss: 9.479741, Accuracy: 1338/1959 (68.30%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000012\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000001\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000139\n",
      "\tTraining batch 12 Loss: 0.000302\n",
      "\tTraining batch 13 Loss: 0.000001\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000651\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.001245\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000001\n",
      "\tTraining batch 26 Loss: 0.000134\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.026578\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000021\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000001\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000029\n",
      "\tTraining batch 44 Loss: 0.000001\n",
      "\tTraining batch 45 Loss: 0.000011\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.001276\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000466\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000054\n",
      "\tTraining batch 53 Loss: 0.000010\n",
      "\tTraining batch 54 Loss: 0.001011\n",
      "\tTraining batch 55 Loss: 0.000408\n",
      "\tTraining batch 56 Loss: 0.000001\n",
      "Training set: Average loss: 0.000578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set: Average loss: 10.096509, Accuracy: 1352/1959 (69.01%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000077\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000081\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000033\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000001\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000276\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000093\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000006\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000004\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000009\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000007\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000148\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000107\n",
      "\tTraining batch 53 Loss: 0.000006\n",
      "\tTraining batch 54 Loss: 0.000065\n",
      "\tTraining batch 55 Loss: 0.000011\n",
      "\tTraining batch 56 Loss: 0.000001\n",
      "Training set: Average loss: 0.000017\n",
      "Validation set: Average loss: 10.057454, Accuracy: 1354/1959 (69.12%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000024\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000024\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000024\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000001\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000119\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000092\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000006\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000004\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000009\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000007\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000118\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000022\n",
      "\tTraining batch 53 Loss: 0.000006\n",
      "\tTraining batch 54 Loss: 0.000061\n",
      "\tTraining batch 55 Loss: 0.000010\n",
      "\tTraining batch 56 Loss: 0.000001\n",
      "Training set: Average loss: 0.000009\n",
      "Validation set: Average loss: 10.051958, Accuracy: 1354/1959 (69.12%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 48.0%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000019\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000016\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000019\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000001\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000083\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000091\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000006\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000004\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000009\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000007\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000097\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000016\n",
      "\tTraining batch 53 Loss: 0.000006\n",
      "\tTraining batch 54 Loss: 0.000058\n",
      "\tTraining batch 55 Loss: 0.000009\n",
      "\tTraining batch 56 Loss: 0.000001\n",
      "\tTraining batch 57 Loss: 8.372181\n",
      "Training set: Average loss: 0.146888\n",
      "Validation set: Average loss: 9.705568, Accuracy: 1360/1959 (69.42%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000002\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000028\n",
      "\tTraining batch 5 Loss: 0.000002\n",
      "\tTraining batch 6 Loss: 0.000306\n",
      "\tTraining batch 7 Loss: 0.000017\n",
      "\tTraining batch 8 Loss: 0.000034\n",
      "\tTraining batch 9 Loss: 0.005698\n",
      "\tTraining batch 10 Loss: 0.000701\n",
      "\tTraining batch 11 Loss: 0.002808\n",
      "\tTraining batch 12 Loss: 0.001695\n",
      "\tTraining batch 13 Loss: 0.000713\n",
      "\tTraining batch 14 Loss: 0.026569\n",
      "\tTraining batch 15 Loss: 0.031057\n",
      "\tTraining batch 16 Loss: 0.002670\n",
      "\tTraining batch 17 Loss: 0.529338\n",
      "\tTraining batch 18 Loss: 0.291793\n",
      "\tTraining batch 19 Loss: 0.044924\n",
      "\tTraining batch 20 Loss: 0.022162\n",
      "\tTraining batch 21 Loss: 0.011691\n",
      "\tTraining batch 22 Loss: 0.000313\n",
      "\tTraining batch 23 Loss: 0.000005\n",
      "\tTraining batch 24 Loss: 0.000770\n",
      "\tTraining batch 25 Loss: 0.010244\n",
      "\tTraining batch 26 Loss: 0.016798\n",
      "\tTraining batch 27 Loss: 0.000004\n",
      "\tTraining batch 28 Loss: 0.006753\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.010259\n",
      "\tTraining batch 31 Loss: 0.359227\n",
      "\tTraining batch 32 Loss: 0.000152\n",
      "\tTraining batch 33 Loss: 0.000005\n",
      "\tTraining batch 34 Loss: 0.000384\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000003\n",
      "\tTraining batch 37 Loss: 0.148932\n",
      "\tTraining batch 38 Loss: 0.032133\n",
      "\tTraining batch 39 Loss: 0.012341\n",
      "\tTraining batch 40 Loss: 0.000270\n",
      "\tTraining batch 41 Loss: 0.051566\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.328190\n",
      "\tTraining batch 44 Loss: 0.007555\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.325794\n",
      "\tTraining batch 47 Loss: 0.000011\n",
      "\tTraining batch 48 Loss: 0.016187\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.085184\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.005173\n",
      "\tTraining batch 53 Loss: 0.185198\n",
      "\tTraining batch 54 Loss: 0.177420\n",
      "\tTraining batch 55 Loss: 0.000743\n",
      "\tTraining batch 56 Loss: 0.005263\n",
      "\tTraining batch 57 Loss: 0.887641\n",
      "Training set: Average loss: 0.063978\n",
      "Validation set: Average loss: 9.683716, Accuracy: 1348/1959 (68.81%)\n",
      "\n",
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 1 Loss: 0.000005\n",
      "\tTraining batch 2 Loss: 0.000123\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.008785\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000003\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.003202\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000004\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000010\n",
      "\tTraining batch 22 Loss: 0.000163\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000019\n",
      "\tTraining batch 26 Loss: 0.000225\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.194230\n",
      "\tTraining batch 29 Loss: 0.000196\n",
      "\tTraining batch 30 Loss: 0.000979\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000078\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.106476\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.002993\n",
      "\tTraining batch 37 Loss: 0.000115\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000003\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000002\n",
      "\tTraining batch 46 Loss: 0.001584\n",
      "\tTraining batch 47 Loss: 0.000021\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000139\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.004300\n",
      "\tTraining batch 53 Loss: 0.127351\n",
      "\tTraining batch 54 Loss: 0.000002\n",
      "\tTraining batch 55 Loss: 0.001354\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.170137\n",
      "Training set: Average loss: 0.010921\n",
      "Validation set: Average loss: 10.031315, Accuracy: 1335/1959 (68.15%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000124\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000057\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.017832\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000176\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000046\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.016336\n",
      "\tTraining batch 34 Loss: 0.167210\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.242343\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000001\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000018\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000079\n",
      "\tTraining batch 47 Loss: 0.000002\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000027\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.007433\n",
      "\tTraining batch 54 Loss: 0.244835\n",
      "\tTraining batch 55 Loss: 0.551102\n",
      "\tTraining batch 56 Loss: 0.000203\n",
      "\tTraining batch 57 Loss: 0.136669\n",
      "Training set: Average loss: 0.024289\n",
      "Validation set: Average loss: 11.114562, Accuracy: 1345/1959 (68.66%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000008\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000002\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000088\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000724\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.119020\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000011\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000002\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000593\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000062\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000002\n",
      "\tTraining batch 41 Loss: 0.061007\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000002\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000007\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000001\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000148\n",
      "\tTraining batch 51 Loss: 0.002958\n",
      "\tTraining batch 52 Loss: 0.000795\n",
      "\tTraining batch 53 Loss: 0.000063\n",
      "\tTraining batch 54 Loss: 0.000023\n",
      "\tTraining batch 55 Loss: 0.013765\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.355095\n",
      "Training set: Average loss: 0.009726\n",
      "Validation set: Average loss: 11.721281, Accuracy: 1341/1959 (68.45%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000225\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000030\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000001\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000002\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000003\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000002\n",
      "\tTraining batch 26 Loss: 0.000017\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000001\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.001441\n",
      "\tTraining batch 31 Loss: 0.000046\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000004\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000028\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000003\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000001\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000323\n",
      "\tTraining batch 53 Loss: 0.133165\n",
      "\tTraining batch 54 Loss: 0.000004\n",
      "\tTraining batch 55 Loss: 0.000001\n",
      "\tTraining batch 56 Loss: 0.000002\n",
      "\tTraining batch 57 Loss: 0.000437\n",
      "Training set: Average loss: 0.002381\n",
      "Validation set: Average loss: 9.507667, Accuracy: 1341/1959 (68.45%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 48.5%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.196913\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.001158\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000003\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000004\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000008\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000958\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000009\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000005\n",
      "\tTraining batch 26 Loss: 0.000321\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000018\n",
      "\tTraining batch 29 Loss: 0.000001\n",
      "\tTraining batch 30 Loss: 0.000467\n",
      "\tTraining batch 31 Loss: 0.052529\n",
      "\tTraining batch 32 Loss: 0.012630\n",
      "\tTraining batch 33 Loss: 0.387697\n",
      "\tTraining batch 34 Loss: 0.148175\n",
      "\tTraining batch 35 Loss: 0.000863\n",
      "\tTraining batch 36 Loss: 0.000006\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000002\n",
      "\tTraining batch 41 Loss: 0.000130\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000480\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000005\n",
      "\tTraining batch 46 Loss: 0.000014\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000119\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000009\n",
      "\tTraining batch 51 Loss: 0.000001\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000172\n",
      "\tTraining batch 54 Loss: 0.008944\n",
      "\tTraining batch 55 Loss: 0.039702\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.071339\n",
      "\tTraining batch 58 Loss: 18.944891\n",
      "Training set: Average loss: 0.342544\n",
      "Validation set: Average loss: 9.661898, Accuracy: 1345/1959 (68.66%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000025\n",
      "\tTraining batch 2 Loss: 0.028802\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000087\n",
      "\tTraining batch 5 Loss: 0.001512\n",
      "\tTraining batch 6 Loss: 0.147198\n",
      "\tTraining batch 7 Loss: 1.092050\n",
      "\tTraining batch 8 Loss: 0.671980\n",
      "\tTraining batch 9 Loss: 0.121619\n",
      "\tTraining batch 10 Loss: 0.465501\n",
      "\tTraining batch 11 Loss: 0.200154\n",
      "\tTraining batch 12 Loss: 0.198647\n",
      "\tTraining batch 13 Loss: 0.003949\n",
      "\tTraining batch 14 Loss: 0.106717\n",
      "\tTraining batch 15 Loss: 0.375357\n",
      "\tTraining batch 16 Loss: 0.038310\n",
      "\tTraining batch 17 Loss: 0.002183\n",
      "\tTraining batch 18 Loss: 0.746427\n",
      "\tTraining batch 19 Loss: 0.171492\n",
      "\tTraining batch 20 Loss: 0.118517\n",
      "\tTraining batch 21 Loss: 0.994805\n",
      "\tTraining batch 22 Loss: 0.087640\n",
      "\tTraining batch 23 Loss: 0.279134\n",
      "\tTraining batch 24 Loss: 0.994810\n",
      "\tTraining batch 25 Loss: 0.123428\n",
      "\tTraining batch 26 Loss: 0.151910\n",
      "\tTraining batch 27 Loss: 0.139351\n",
      "\tTraining batch 28 Loss: 0.086556\n",
      "\tTraining batch 29 Loss: 0.491096\n",
      "\tTraining batch 30 Loss: 0.186077\n",
      "\tTraining batch 31 Loss: 0.000155\n",
      "\tTraining batch 32 Loss: 0.002599\n",
      "\tTraining batch 33 Loss: 0.136843\n",
      "\tTraining batch 34 Loss: 0.173985\n",
      "\tTraining batch 35 Loss: 0.467318\n",
      "\tTraining batch 36 Loss: 0.039624\n",
      "\tTraining batch 37 Loss: 0.175667\n",
      "\tTraining batch 38 Loss: 0.015303\n",
      "\tTraining batch 39 Loss: 0.045461\n",
      "\tTraining batch 40 Loss: 0.000485\n",
      "\tTraining batch 41 Loss: 0.595601\n",
      "\tTraining batch 42 Loss: 0.173569\n",
      "\tTraining batch 43 Loss: 0.136472\n",
      "\tTraining batch 44 Loss: 0.008092\n",
      "\tTraining batch 45 Loss: 0.111014\n",
      "\tTraining batch 46 Loss: 0.009118\n",
      "\tTraining batch 47 Loss: 0.001900\n",
      "\tTraining batch 48 Loss: 0.000388\n",
      "\tTraining batch 49 Loss: 0.000007\n",
      "\tTraining batch 50 Loss: 0.000001\n",
      "\tTraining batch 51 Loss: 0.021968\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 1.309370\n",
      "\tTraining batch 54 Loss: 0.004945\n",
      "\tTraining batch 55 Loss: 0.047878\n",
      "\tTraining batch 56 Loss: 0.100073\n",
      "\tTraining batch 57 Loss: 0.009631\n",
      "\tTraining batch 58 Loss: 0.248938\n",
      "Training set: Average loss: 0.204513\n",
      "Validation set: Average loss: 7.709365, Accuracy: 1363/1959 (69.58%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.217323\n",
      "\tTraining batch 3 Loss: 0.001755\n",
      "\tTraining batch 4 Loss: 0.116536\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000379\n",
      "\tTraining batch 7 Loss: 0.000109\n",
      "\tTraining batch 8 Loss: 0.000003\n",
      "\tTraining batch 9 Loss: 0.066935\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000127\n",
      "\tTraining batch 13 Loss: 0.000002\n",
      "\tTraining batch 14 Loss: 0.000007\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000304\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000003\n",
      "\tTraining batch 19 Loss: 0.000072\n",
      "\tTraining batch 20 Loss: 0.346655\n",
      "\tTraining batch 21 Loss: 0.000002\n",
      "\tTraining batch 22 Loss: 0.000059\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000104\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000006\n",
      "\tTraining batch 31 Loss: 0.001017\n",
      "\tTraining batch 32 Loss: 0.000001\n",
      "\tTraining batch 33 Loss: 0.000004\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000010\n",
      "\tTraining batch 36 Loss: 0.000004\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000093\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000003\n",
      "\tTraining batch 41 Loss: 0.000007\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.003563\n",
      "\tTraining batch 44 Loss: 0.000002\n",
      "\tTraining batch 45 Loss: 0.000137\n",
      "\tTraining batch 46 Loss: 0.000020\n",
      "\tTraining batch 47 Loss: 0.026560\n",
      "\tTraining batch 48 Loss: 0.000982\n",
      "\tTraining batch 49 Loss: 0.000003\n",
      "\tTraining batch 50 Loss: 0.000004\n",
      "\tTraining batch 51 Loss: 0.000002\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.459556\n",
      "\tTraining batch 54 Loss: 0.026702\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.083514\n",
      "\tTraining batch 58 Loss: 0.125886\n",
      "Training set: Average loss: 0.025490\n",
      "Validation set: Average loss: 8.048010, Accuracy: 1366/1959 (69.73%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000002\n",
      "\tTraining batch 3 Loss: 0.000001\n",
      "\tTraining batch 4 Loss: 0.000003\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000033\n",
      "\tTraining batch 7 Loss: 0.000014\n",
      "\tTraining batch 8 Loss: 0.002788\n",
      "\tTraining batch 9 Loss: 0.000129\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000054\n",
      "\tTraining batch 12 Loss: 0.000005\n",
      "\tTraining batch 13 Loss: 0.122092\n",
      "\tTraining batch 14 Loss: 0.000535\n",
      "\tTraining batch 15 Loss: 0.000248\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000001\n",
      "\tTraining batch 19 Loss: 0.047673\n",
      "\tTraining batch 20 Loss: 0.000007\n",
      "\tTraining batch 21 Loss: 0.002158\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.112426\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.004002\n",
      "\tTraining batch 26 Loss: 0.001347\n",
      "\tTraining batch 27 Loss: 0.000018\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.000002\n",
      "\tTraining batch 32 Loss: 0.002943\n",
      "\tTraining batch 33 Loss: 0.012790\n",
      "\tTraining batch 34 Loss: 0.012712\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000046\n",
      "\tTraining batch 37 Loss: 0.000001\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000001\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000304\n",
      "\tTraining batch 44 Loss: 0.000019\n",
      "\tTraining batch 45 Loss: 0.000373\n",
      "\tTraining batch 46 Loss: 0.000012\n",
      "\tTraining batch 47 Loss: 0.014090\n",
      "\tTraining batch 48 Loss: 0.003510\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000001\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.191508\n",
      "\tTraining batch 53 Loss: 0.013312\n",
      "\tTraining batch 54 Loss: 0.008124\n",
      "\tTraining batch 55 Loss: 0.000019\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.006815\n",
      "\tTraining batch 58 Loss: 0.033685\n",
      "Training set: Average loss: 0.010238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set: Average loss: 10.326198, Accuracy: 1365/1959 (69.68%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000009\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000007\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000001\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000006\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000093\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000001\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000016\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000002\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000049\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000310\n",
      "\tTraining batch 36 Loss: 0.000002\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.041730\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.007711\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000065\n",
      "\tTraining batch 46 Loss: 0.000001\n",
      "\tTraining batch 47 Loss: 0.000057\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.026066\n",
      "\tTraining batch 50 Loss: 0.000390\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.008860\n",
      "\tTraining batch 55 Loss: 0.000008\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.001170\n",
      "\tTraining batch 58 Loss: 0.010596\n",
      "Training set: Average loss: 0.001675\n",
      "Validation set: Average loss: 10.043919, Accuracy: 1371/1959 (69.98%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.004178\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000003\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000038\n",
      "\tTraining batch 17 Loss: 0.000178\n",
      "\tTraining batch 18 Loss: 0.000001\n",
      "\tTraining batch 19 Loss: 0.000090\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000042\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000007\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000011\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000096\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000048\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000080\n",
      "\tTraining batch 50 Loss: 0.000060\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.004293\n",
      "\tTraining batch 55 Loss: 0.000003\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.001217\n",
      "\tTraining batch 58 Loss: 0.000488\n",
      "Training set: Average loss: 0.000187\n",
      "Validation set: Average loss: 10.339825, Accuracy: 1379/1959 (70.39%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000001\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000003\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000040\n",
      "\tTraining batch 17 Loss: 0.000102\n",
      "\tTraining batch 18 Loss: 0.000001\n",
      "\tTraining batch 19 Loss: 0.000095\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000043\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000007\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000011\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000088\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000041\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000055\n",
      "\tTraining batch 50 Loss: 0.000035\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.003466\n",
      "\tTraining batch 55 Loss: 0.000003\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.001045\n",
      "\tTraining batch 58 Loss: 0.000260\n",
      "Training set: Average loss: 0.000091\n",
      "Validation set: Average loss: 10.358472, Accuracy: 1378/1959 (70.34%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000001\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000002\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000031\n",
      "\tTraining batch 17 Loss: 0.000067\n",
      "\tTraining batch 18 Loss: 0.000001\n",
      "\tTraining batch 19 Loss: 0.000090\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000041\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000007\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000011\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000083\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000034\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000041\n",
      "\tTraining batch 50 Loss: 0.000027\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.002765\n",
      "\tTraining batch 55 Loss: 0.000003\n",
      "\tTraining batch 56 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 57 Loss: 0.000901\n",
      "\tTraining batch 58 Loss: 0.000185\n",
      "Training set: Average loss: 0.000074\n",
      "Validation set: Average loss: 10.375634, Accuracy: 1377/1959 (70.29%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000001\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000002\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000025\n",
      "\tTraining batch 17 Loss: 0.000052\n",
      "\tTraining batch 18 Loss: 0.000001\n",
      "\tTraining batch 19 Loss: 0.000087\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000040\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000007\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000010\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000079\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000029\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000033\n",
      "\tTraining batch 50 Loss: 0.000023\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.002290\n",
      "\tTraining batch 55 Loss: 0.000003\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000792\n",
      "\tTraining batch 58 Loss: 0.000144\n",
      "Training set: Average loss: 0.000062\n",
      "Validation set: Average loss: 10.391061, Accuracy: 1377/1959 (70.29%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 47.75%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000001\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000002\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000022\n",
      "\tTraining batch 17 Loss: 0.000042\n",
      "\tTraining batch 18 Loss: 0.000001\n",
      "\tTraining batch 19 Loss: 0.000082\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000038\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000007\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000010\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000075\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000026\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000027\n",
      "\tTraining batch 50 Loss: 0.000021\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.001985\n",
      "\tTraining batch 55 Loss: 0.000003\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000694\n",
      "\tTraining batch 58 Loss: 0.000117\n",
      "\tTraining batch 59 Loss: 10.435904\n",
      "Training set: Average loss: 0.176933\n",
      "Validation set: Average loss: 10.171596, Accuracy: 1362/1959 (69.53%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000155\n",
      "\tTraining batch 5 Loss: 0.000762\n",
      "\tTraining batch 6 Loss: 0.001060\n",
      "\tTraining batch 7 Loss: 0.036862\n",
      "\tTraining batch 8 Loss: 0.000069\n",
      "\tTraining batch 9 Loss: 0.000009\n",
      "\tTraining batch 10 Loss: 0.320058\n",
      "\tTraining batch 11 Loss: 0.000007\n",
      "\tTraining batch 12 Loss: 0.000001\n",
      "\tTraining batch 13 Loss: 0.014766\n",
      "\tTraining batch 14 Loss: 0.000212\n",
      "\tTraining batch 15 Loss: 0.022997\n",
      "\tTraining batch 16 Loss: 0.007792\n",
      "\tTraining batch 17 Loss: 0.010086\n",
      "\tTraining batch 18 Loss: 0.000314\n",
      "\tTraining batch 19 Loss: 0.056962\n",
      "\tTraining batch 20 Loss: 0.441331\n",
      "\tTraining batch 21 Loss: 0.591314\n",
      "\tTraining batch 22 Loss: 0.746814\n",
      "\tTraining batch 23 Loss: 0.010851\n",
      "\tTraining batch 24 Loss: 0.000024\n",
      "\tTraining batch 25 Loss: 0.317549\n",
      "\tTraining batch 26 Loss: 0.246054\n",
      "\tTraining batch 27 Loss: 0.006913\n",
      "\tTraining batch 28 Loss: 0.005082\n",
      "\tTraining batch 29 Loss: 0.000003\n",
      "\tTraining batch 30 Loss: 0.226863\n",
      "\tTraining batch 31 Loss: 0.269844\n",
      "\tTraining batch 32 Loss: 0.089615\n",
      "\tTraining batch 33 Loss: 0.417667\n",
      "\tTraining batch 34 Loss: 0.012832\n",
      "\tTraining batch 35 Loss: 0.005213\n",
      "\tTraining batch 36 Loss: 0.041151\n",
      "\tTraining batch 37 Loss: 0.111348\n",
      "\tTraining batch 38 Loss: 0.000038\n",
      "\tTraining batch 39 Loss: 0.005724\n",
      "\tTraining batch 40 Loss: 0.000248\n",
      "\tTraining batch 41 Loss: 0.000006\n",
      "\tTraining batch 42 Loss: 0.000062\n",
      "\tTraining batch 43 Loss: 0.184396\n",
      "\tTraining batch 44 Loss: 0.000035\n",
      "\tTraining batch 45 Loss: 0.000185\n",
      "\tTraining batch 46 Loss: 0.008053\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000007\n",
      "\tTraining batch 49 Loss: 0.000089\n",
      "\tTraining batch 50 Loss: 0.000031\n",
      "\tTraining batch 51 Loss: 0.050816\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000110\n",
      "\tTraining batch 54 Loss: 0.000020\n",
      "\tTraining batch 55 Loss: 0.000004\n",
      "\tTraining batch 56 Loss: 0.001284\n",
      "\tTraining batch 57 Loss: 0.334752\n",
      "\tTraining batch 58 Loss: 0.003055\n",
      "\tTraining batch 59 Loss: 0.751312\n",
      "Training set: Average loss: 0.090726\n",
      "Validation set: Average loss: 7.799401, Accuracy: 1361/1959 (69.47%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000032\n",
      "\tTraining batch 2 Loss: 0.000003\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000005\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000004\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000128\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000315\n",
      "\tTraining batch 17 Loss: 0.000535\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.007302\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.060714\n",
      "\tTraining batch 23 Loss: 0.000009\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.004175\n",
      "\tTraining batch 26 Loss: 0.042409\n",
      "\tTraining batch 27 Loss: 0.170191\n",
      "\tTraining batch 28 Loss: 0.000185\n",
      "\tTraining batch 29 Loss: 0.048930\n",
      "\tTraining batch 30 Loss: 0.000015\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000005\n",
      "\tTraining batch 33 Loss: 0.000001\n",
      "\tTraining batch 34 Loss: 0.000042\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000008\n",
      "\tTraining batch 37 Loss: 0.000042\n",
      "\tTraining batch 38 Loss: 0.010792\n",
      "\tTraining batch 39 Loss: 0.000064\n",
      "\tTraining batch 40 Loss: 0.010671\n",
      "\tTraining batch 41 Loss: 0.461926\n",
      "\tTraining batch 42 Loss: 0.000254\n",
      "\tTraining batch 43 Loss: 0.033964\n",
      "\tTraining batch 44 Loss: 0.001181\n",
      "\tTraining batch 45 Loss: 0.046365\n",
      "\tTraining batch 46 Loss: 0.000029\n",
      "\tTraining batch 47 Loss: 0.000001\n",
      "\tTraining batch 48 Loss: 0.000002\n",
      "\tTraining batch 49 Loss: 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 50 Loss: 0.000001\n",
      "\tTraining batch 51 Loss: 0.111436\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000135\n",
      "\tTraining batch 54 Loss: 0.031435\n",
      "\tTraining batch 55 Loss: 0.001193\n",
      "\tTraining batch 56 Loss: 0.000002\n",
      "\tTraining batch 57 Loss: 0.000057\n",
      "\tTraining batch 58 Loss: 0.004982\n",
      "\tTraining batch 59 Loss: 0.020386\n",
      "Training set: Average loss: 0.018134\n",
      "Validation set: Average loss: 10.225099, Accuracy: 1346/1959 (68.71%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000001\n",
      "\tTraining batch 2 Loss: 0.000229\n",
      "\tTraining batch 3 Loss: 0.047164\n",
      "\tTraining batch 4 Loss: 0.000012\n",
      "\tTraining batch 5 Loss: 0.003933\n",
      "\tTraining batch 6 Loss: 0.000066\n",
      "\tTraining batch 7 Loss: 0.000046\n",
      "\tTraining batch 8 Loss: 0.000214\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000321\n",
      "\tTraining batch 12 Loss: 0.000003\n",
      "\tTraining batch 13 Loss: 0.000001\n",
      "\tTraining batch 14 Loss: 0.009040\n",
      "\tTraining batch 15 Loss: 0.000819\n",
      "\tTraining batch 16 Loss: 0.012078\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000005\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000001\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000001\n",
      "\tTraining batch 25 Loss: 0.000219\n",
      "\tTraining batch 26 Loss: 0.000253\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000001\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.061406\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000006\n",
      "\tTraining batch 33 Loss: 0.008168\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000002\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000006\n",
      "\tTraining batch 41 Loss: 0.000001\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.011442\n",
      "\tTraining batch 44 Loss: 0.000001\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000005\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000002\n",
      "\tTraining batch 49 Loss: 0.001754\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.085642\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000205\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000002\n",
      "\tTraining batch 57 Loss: 0.066310\n",
      "\tTraining batch 58 Loss: 0.000191\n",
      "\tTraining batch 59 Loss: 0.017590\n",
      "Training set: Average loss: 0.005545\n",
      "Validation set: Average loss: 9.510183, Accuracy: 1357/1959 (69.27%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000001\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.001889\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.048231\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000128\n",
      "\tTraining batch 12 Loss: 0.000004\n",
      "\tTraining batch 13 Loss: 0.000002\n",
      "\tTraining batch 14 Loss: 0.000004\n",
      "\tTraining batch 15 Loss: 0.000003\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000643\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000001\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000004\n",
      "\tTraining batch 23 Loss: 0.020738\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000922\n",
      "\tTraining batch 26 Loss: 0.000530\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000006\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000009\n",
      "\tTraining batch 33 Loss: 0.000014\n",
      "\tTraining batch 34 Loss: 0.000001\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000001\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000369\n",
      "\tTraining batch 44 Loss: 0.000003\n",
      "\tTraining batch 45 Loss: 0.000002\n",
      "\tTraining batch 46 Loss: 0.000017\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000077\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.002876\n",
      "\tTraining batch 55 Loss: 0.000084\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000347\n",
      "\tTraining batch 58 Loss: 0.000028\n",
      "\tTraining batch 59 Loss: 0.001209\n",
      "Training set: Average loss: 0.001324\n",
      "Validation set: Average loss: 9.809942, Accuracy: 1368/1959 (69.83%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000003\n",
      "\tTraining batch 2 Loss: 0.000001\n",
      "\tTraining batch 3 Loss: 0.000001\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000001\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000001\n",
      "\tTraining batch 15 Loss: 0.000001\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000001\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000006\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000002\n",
      "\tTraining batch 26 Loss: 0.001978\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000003\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000069\n",
      "\tTraining batch 33 Loss: 0.000104\n",
      "\tTraining batch 34 Loss: 0.000001\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000001\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000229\n",
      "\tTraining batch 44 Loss: 0.000002\n",
      "\tTraining batch 45 Loss: 0.000002\n",
      "\tTraining batch 46 Loss: 0.000001\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000062\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000013\n",
      "\tTraining batch 55 Loss: 0.000033\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000317\n",
      "\tTraining batch 58 Loss: 0.000016\n",
      "\tTraining batch 59 Loss: 0.000923\n",
      "Training set: Average loss: 0.000064\n",
      "Validation set: Average loss: 9.895038, Accuracy: 1368/1959 (69.83%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 48.5%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000004\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000001\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000001\n",
      "\tTraining batch 15 Loss: 0.000001\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000001\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000004\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000002\n",
      "\tTraining batch 26 Loss: 0.001497\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000003\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000034\n",
      "\tTraining batch 33 Loss: 0.000069\n",
      "\tTraining batch 34 Loss: 0.000001\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000001\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000218\n",
      "\tTraining batch 44 Loss: 0.000002\n",
      "\tTraining batch 45 Loss: 0.000002\n",
      "\tTraining batch 46 Loss: 0.000001\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000052\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000010\n",
      "\tTraining batch 55 Loss: 0.000029\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000271\n",
      "\tTraining batch 58 Loss: 0.000015\n",
      "\tTraining batch 59 Loss: 0.000819\n",
      "\tTraining batch 60 Loss: 9.062967\n",
      "Training set: Average loss: 0.151100\n",
      "Validation set: Average loss: 9.110378, Accuracy: 1371/1959 (69.98%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000016\n",
      "\tTraining batch 2 Loss: 0.000011\n",
      "\tTraining batch 3 Loss: 0.001509\n",
      "\tTraining batch 4 Loss: 0.000018\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000040\n",
      "\tTraining batch 7 Loss: 0.001406\n",
      "\tTraining batch 8 Loss: 0.000559\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.001647\n",
      "\tTraining batch 11 Loss: 0.001030\n",
      "\tTraining batch 12 Loss: 0.128878\n",
      "\tTraining batch 13 Loss: 0.000001\n",
      "\tTraining batch 14 Loss: 0.000017\n",
      "\tTraining batch 15 Loss: 0.000044\n",
      "\tTraining batch 16 Loss: 0.011542\n",
      "\tTraining batch 17 Loss: 0.009594\n",
      "\tTraining batch 18 Loss: 0.000402\n",
      "\tTraining batch 19 Loss: 0.062589\n",
      "\tTraining batch 20 Loss: 0.001869\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000001\n",
      "\tTraining batch 23 Loss: 0.000023\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000054\n",
      "\tTraining batch 26 Loss: 0.066228\n",
      "\tTraining batch 27 Loss: 0.000001\n",
      "\tTraining batch 28 Loss: 0.000705\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000085\n",
      "\tTraining batch 31 Loss: 0.105570\n",
      "\tTraining batch 32 Loss: 0.014770\n",
      "\tTraining batch 33 Loss: 0.000223\n",
      "\tTraining batch 34 Loss: 0.000744\n",
      "\tTraining batch 35 Loss: 0.000006\n",
      "\tTraining batch 36 Loss: 0.001960\n",
      "\tTraining batch 37 Loss: 0.000981\n",
      "\tTraining batch 38 Loss: 0.100648\n",
      "\tTraining batch 39 Loss: 0.005117\n",
      "\tTraining batch 40 Loss: 0.001605\n",
      "\tTraining batch 41 Loss: 0.000013\n",
      "\tTraining batch 42 Loss: 0.000004\n",
      "\tTraining batch 43 Loss: 0.002726\n",
      "\tTraining batch 44 Loss: 0.059209\n",
      "\tTraining batch 45 Loss: 0.004077\n",
      "\tTraining batch 46 Loss: 0.000218\n",
      "\tTraining batch 47 Loss: 0.000005\n",
      "\tTraining batch 48 Loss: 0.000219\n",
      "\tTraining batch 49 Loss: 0.000018\n",
      "\tTraining batch 50 Loss: 0.000085\n",
      "\tTraining batch 51 Loss: 0.003195\n",
      "\tTraining batch 52 Loss: 0.013959\n",
      "\tTraining batch 53 Loss: 0.000022\n",
      "\tTraining batch 54 Loss: 0.004867\n",
      "\tTraining batch 55 Loss: 0.107963\n",
      "\tTraining batch 56 Loss: 0.000757\n",
      "\tTraining batch 57 Loss: 0.005806\n",
      "\tTraining batch 58 Loss: 0.017655\n",
      "\tTraining batch 59 Loss: 0.005771\n",
      "\tTraining batch 60 Loss: 0.592529\n",
      "Training set: Average loss: 0.022317\n",
      "Validation set: Average loss: 7.519275, Accuracy: 1364/1959 (69.63%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.011982\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.026321\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000066\n",
      "\tTraining batch 6 Loss: 0.014688\n",
      "\tTraining batch 7 Loss: 0.000094\n",
      "\tTraining batch 8 Loss: 0.001359\n",
      "\tTraining batch 9 Loss: 0.189913\n",
      "\tTraining batch 10 Loss: 0.000097\n",
      "\tTraining batch 11 Loss: 0.002118\n",
      "\tTraining batch 12 Loss: 0.000058\n",
      "\tTraining batch 13 Loss: 0.000041\n",
      "\tTraining batch 14 Loss: 0.004134\n",
      "\tTraining batch 15 Loss: 0.000005\n",
      "\tTraining batch 16 Loss: 0.000001\n",
      "\tTraining batch 17 Loss: 0.021868\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000130\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.046946\n",
      "\tTraining batch 26 Loss: 0.012742\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000086\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000018\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.064419\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.017760\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000004\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000001\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.001515\n",
      "\tTraining batch 44 Loss: 0.000173\n",
      "\tTraining batch 45 Loss: 0.000152\n",
      "\tTraining batch 46 Loss: 0.000027\n",
      "\tTraining batch 47 Loss: 0.000007\n",
      "\tTraining batch 48 Loss: 0.000002\n",
      "\tTraining batch 49 Loss: 0.000171\n",
      "\tTraining batch 50 Loss: 0.003697\n",
      "\tTraining batch 51 Loss: 0.000001\n",
      "\tTraining batch 52 Loss: 0.000002\n",
      "\tTraining batch 53 Loss: 0.000423\n",
      "\tTraining batch 54 Loss: 0.020623\n",
      "\tTraining batch 55 Loss: 0.000006\n",
      "\tTraining batch 56 Loss: 0.002462\n",
      "\tTraining batch 57 Loss: 0.001112\n",
      "\tTraining batch 58 Loss: 0.024131\n",
      "\tTraining batch 59 Loss: 0.032017\n",
      "\tTraining batch 60 Loss: 0.182932\n",
      "Training set: Average loss: 0.011405\n",
      "Validation set: Average loss: 8.342020, Accuracy: 1361/1959 (69.47%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000075\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000078\n",
      "\tTraining batch 4 Loss: 0.000127\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000032\n",
      "\tTraining batch 7 Loss: 0.000154\n",
      "\tTraining batch 8 Loss: 0.002721\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.001516\n",
      "\tTraining batch 11 Loss: 0.000023\n",
      "\tTraining batch 12 Loss: 0.003177\n",
      "\tTraining batch 13 Loss: 0.000048\n",
      "\tTraining batch 14 Loss: 0.001084\n",
      "\tTraining batch 15 Loss: 0.000432\n",
      "\tTraining batch 16 Loss: 0.236297\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000004\n",
      "\tTraining batch 19 Loss: 0.000052\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000001\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.001152\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000967\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000002\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000060\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.001446\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000169\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000118\n",
      "\tTraining batch 40 Loss: 0.000018\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000805\n",
      "\tTraining batch 44 Loss: 0.002186\n",
      "\tTraining batch 45 Loss: 0.000001\n",
      "\tTraining batch 46 Loss: 0.000021\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000001\n",
      "\tTraining batch 49 Loss: 0.014366\n",
      "\tTraining batch 50 Loss: 0.000001\n",
      "\tTraining batch 51 Loss: 0.000005\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.001253\n",
      "\tTraining batch 54 Loss: 0.000034\n",
      "\tTraining batch 55 Loss: 0.263931\n",
      "\tTraining batch 56 Loss: 0.000136\n",
      "\tTraining batch 57 Loss: 0.000368\n",
      "\tTraining batch 58 Loss: 0.002663\n",
      "\tTraining batch 59 Loss: 0.008318\n",
      "\tTraining batch 60 Loss: 0.008281\n",
      "Training set: Average loss: 0.009202\n",
      "Validation set: Average loss: 9.235601, Accuracy: 1364/1959 (69.63%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000105\n",
      "\tTraining batch 7 Loss: 0.018785\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000003\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000063\n",
      "\tTraining batch 19 Loss: 0.000542\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000040\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000002\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000457\n",
      "\tTraining batch 44 Loss: 0.000176\n",
      "\tTraining batch 45 Loss: 0.000012\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000006\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000001\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.002409\n",
      "\tTraining batch 55 Loss: 0.000001\n",
      "\tTraining batch 56 Loss: 0.000002\n",
      "\tTraining batch 57 Loss: 0.002372\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.051562\n",
      "\tTraining batch 60 Loss: 0.000071\n",
      "Training set: Average loss: 0.001277\n",
      "Validation set: Average loss: 9.863040, Accuracy: 1358/1959 (69.32%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000001\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000008\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000001\n",
      "\tTraining batch 13 Loss: 0.000009\n",
      "\tTraining batch 14 Loss: 0.045981\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000001\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000856\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.005678\n",
      "\tTraining batch 22 Loss: 0.000001\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000047\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000066\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000489\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000152\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.002527\n",
      "\tTraining batch 44 Loss: 0.000044\n",
      "\tTraining batch 45 Loss: 0.000420\n",
      "\tTraining batch 46 Loss: 0.000006\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000006\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000017\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.008417\n",
      "\tTraining batch 55 Loss: 0.000005\n",
      "\tTraining batch 56 Loss: 0.000007\n",
      "\tTraining batch 57 Loss: 0.003560\n",
      "\tTraining batch 58 Loss: 0.017520\n",
      "\tTraining batch 59 Loss: 0.011983\n",
      "\tTraining batch 60 Loss: 0.003340\n",
      "Training set: Average loss: 0.001686\n",
      "Validation set: Average loss: 8.831360, Accuracy: 1359/1959 (69.37%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000010\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000001\n",
      "\tTraining batch 4 Loss: 0.000782\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000002\n",
      "\tTraining batch 7 Loss: 0.000004\n",
      "\tTraining batch 8 Loss: 0.000001\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000848\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000005\n",
      "\tTraining batch 17 Loss: 0.000001\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001023\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000035\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000003\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000001\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000001\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000938\n",
      "\tTraining batch 44 Loss: 0.000022\n",
      "\tTraining batch 45 Loss: 0.000234\n",
      "\tTraining batch 46 Loss: 0.000003\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000004\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000001\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.005686\n",
      "\tTraining batch 55 Loss: 0.000002\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.001807\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.006794\n",
      "\tTraining batch 60 Loss: 0.000002\n",
      "Training set: Average loss: 0.000303\n",
      "Validation set: Average loss: 9.350487, Accuracy: 1372/1959 (70.04%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000009\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000241\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000002\n",
      "\tTraining batch 7 Loss: 0.000001\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000003\n",
      "\tTraining batch 17 Loss: 0.000001\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000913\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000031\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000001\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000001\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000625\n",
      "\tTraining batch 44 Loss: 0.000021\n",
      "\tTraining batch 45 Loss: 0.000197\n",
      "\tTraining batch 46 Loss: 0.000002\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000004\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000001\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.004753\n",
      "\tTraining batch 55 Loss: 0.000002\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.001501\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.005076\n",
      "\tTraining batch 60 Loss: 0.000001\n",
      "Training set: Average loss: 0.000223\n",
      "Validation set: Average loss: 9.428951, Accuracy: 1371/1959 (69.98%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000009\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000128\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000002\n",
      "\tTraining batch 7 Loss: 0.000001\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000003\n",
      "\tTraining batch 17 Loss: 0.000001\n",
      "\tTraining batch 18 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 19 Loss: 0.000868\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000029\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000001\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000001\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000397\n",
      "\tTraining batch 44 Loss: 0.000020\n",
      "\tTraining batch 45 Loss: 0.000183\n",
      "\tTraining batch 46 Loss: 0.000002\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000003\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000001\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.003968\n",
      "\tTraining batch 55 Loss: 0.000001\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.001278\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.003887\n",
      "\tTraining batch 60 Loss: 0.000001\n",
      "Training set: Average loss: 0.000180\n",
      "Validation set: Average loss: 9.494683, Accuracy: 1370/1959 (69.93%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000009\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000075\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000002\n",
      "\tTraining batch 7 Loss: 0.000001\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000003\n",
      "\tTraining batch 17 Loss: 0.000001\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000826\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000027\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000001\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000001\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000264\n",
      "\tTraining batch 44 Loss: 0.000020\n",
      "\tTraining batch 45 Loss: 0.000160\n",
      "\tTraining batch 46 Loss: 0.000001\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000003\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000001\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.003139\n",
      "\tTraining batch 55 Loss: 0.000001\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.001071\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.003027\n",
      "\tTraining batch 60 Loss: 0.000001\n",
      "Training set: Average loss: 0.000144\n",
      "Validation set: Average loss: 9.555907, Accuracy: 1369/1959 (69.88%)\n",
      "\n",
      "Epoch: 11\n",
      "\tTraining batch 1 Loss: 0.000010\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000043\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000002\n",
      "\tTraining batch 7 Loss: 0.000001\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000002\n",
      "\tTraining batch 17 Loss: 0.000001\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000785\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000026\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000001\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000001\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000153\n",
      "\tTraining batch 44 Loss: 0.000019\n",
      "\tTraining batch 45 Loss: 0.000137\n",
      "\tTraining batch 46 Loss: 0.000001\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000002\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000001\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.002417\n",
      "\tTraining batch 55 Loss: 0.000001\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000908\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.002109\n",
      "\tTraining batch 60 Loss: 0.000002\n",
      "Training set: Average loss: 0.000110\n",
      "Validation set: Average loss: 9.621310, Accuracy: 1368/1959 (69.83%)\n",
      "\n",
      "Epoch: 12\n",
      "\tTraining batch 1 Loss: 0.000010\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000025\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000002\n",
      "\tTraining batch 7 Loss: 0.000001\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000002\n",
      "\tTraining batch 17 Loss: 0.000001\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000733\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000024\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000001\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000001\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000104\n",
      "\tTraining batch 44 Loss: 0.000018\n",
      "\tTraining batch 45 Loss: 0.000115\n",
      "\tTraining batch 46 Loss: 0.000001\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000002\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000002\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.001994\n",
      "\tTraining batch 55 Loss: 0.000001\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000795\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.001596\n",
      "\tTraining batch 60 Loss: 0.000002\n",
      "Training set: Average loss: 0.000090\n",
      "Validation set: Average loss: 9.672067, Accuracy: 1370/1959 (69.93%)\n",
      "\n",
      "Epoch: 13\n",
      "\tTraining batch 1 Loss: 0.000011\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000017\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000002\n",
      "\tTraining batch 7 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000002\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000685\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000023\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000001\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000001\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000077\n",
      "\tTraining batch 44 Loss: 0.000017\n",
      "\tTraining batch 45 Loss: 0.000100\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000002\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000002\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.001777\n",
      "\tTraining batch 55 Loss: 0.000001\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000715\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.001268\n",
      "\tTraining batch 60 Loss: 0.000003\n",
      "Training set: Average loss: 0.000078\n",
      "Validation set: Average loss: 9.712715, Accuracy: 1371/1959 (69.98%)\n",
      "\n",
      "Epoch: 14\n",
      "\tTraining batch 1 Loss: 0.000010\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000013\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000002\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000002\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000663\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000023\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000001\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000001\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000060\n",
      "\tTraining batch 44 Loss: 0.000017\n",
      "\tTraining batch 45 Loss: 0.000094\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000002\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000002\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.001583\n",
      "\tTraining batch 55 Loss: 0.000001\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000641\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.001038\n",
      "\tTraining batch 60 Loss: 0.000004\n",
      "Training set: Average loss: 0.000069\n",
      "Validation set: Average loss: 9.749046, Accuracy: 1372/1959 (70.04%)\n",
      "\n",
      "Epoch: 15\n",
      "\tTraining batch 1 Loss: 0.000010\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000010\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000001\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000002\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000627\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000022\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000001\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000001\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000047\n",
      "\tTraining batch 44 Loss: 0.000016\n",
      "\tTraining batch 45 Loss: 0.000084\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000002\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000002\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.001445\n",
      "\tTraining batch 55 Loss: 0.000001\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000582\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000852\n",
      "\tTraining batch 60 Loss: 0.000006\n",
      "Training set: Average loss: 0.000062\n",
      "Validation set: Average loss: 9.783785, Accuracy: 1371/1959 (69.98%)\n",
      "\n",
      "Epoch: 16\n",
      "\tTraining batch 1 Loss: 0.000010\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000008\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000001\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000002\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000604\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000022\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000001\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000001\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000035\n",
      "\tTraining batch 44 Loss: 0.000014\n",
      "\tTraining batch 45 Loss: 0.000080\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000001\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000003\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.001301\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000525\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 60 Loss: 0.000009\n",
      "Training set: Average loss: 0.000055\n",
      "Validation set: Average loss: 9.822119, Accuracy: 1371/1959 (69.98%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 47.75%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000010\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000006\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000001\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000002\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000582\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000022\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000001\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000001\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000031\n",
      "\tTraining batch 44 Loss: 0.000014\n",
      "\tTraining batch 45 Loss: 0.000075\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000001\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000003\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.001186\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000478\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000539\n",
      "\tTraining batch 60 Loss: 0.000011\n",
      "\tTraining batch 61 Loss: 16.482853\n",
      "Training set: Average loss: 0.270259\n",
      "Validation set: Average loss: 10.492531, Accuracy: 1346/1959 (68.71%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000011\n",
      "\tTraining batch 2 Loss: 0.116771\n",
      "\tTraining batch 3 Loss: 0.076315\n",
      "\tTraining batch 4 Loss: 1.596587\n",
      "\tTraining batch 5 Loss: 0.540925\n",
      "\tTraining batch 6 Loss: 0.027443\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.009278\n",
      "\tTraining batch 10 Loss: 0.000868\n",
      "\tTraining batch 11 Loss: 0.073216\n",
      "\tTraining batch 12 Loss: 0.000270\n",
      "\tTraining batch 13 Loss: 0.002463\n",
      "\tTraining batch 14 Loss: 0.000001\n",
      "\tTraining batch 15 Loss: 0.548917\n",
      "\tTraining batch 16 Loss: 0.167851\n",
      "\tTraining batch 17 Loss: 0.013938\n",
      "\tTraining batch 18 Loss: 0.267400\n",
      "\tTraining batch 19 Loss: 0.579426\n",
      "\tTraining batch 20 Loss: 0.065318\n",
      "\tTraining batch 21 Loss: 0.232940\n",
      "\tTraining batch 22 Loss: 0.016098\n",
      "\tTraining batch 23 Loss: 0.048526\n",
      "\tTraining batch 24 Loss: 0.422362\n",
      "\tTraining batch 25 Loss: 0.009979\n",
      "\tTraining batch 26 Loss: 0.032687\n",
      "\tTraining batch 27 Loss: 0.000003\n",
      "\tTraining batch 28 Loss: 0.000726\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.178196\n",
      "\tTraining batch 31 Loss: 0.245357\n",
      "\tTraining batch 32 Loss: 0.004559\n",
      "\tTraining batch 33 Loss: 0.166685\n",
      "\tTraining batch 34 Loss: 0.000151\n",
      "\tTraining batch 35 Loss: 0.229779\n",
      "\tTraining batch 36 Loss: 0.000334\n",
      "\tTraining batch 37 Loss: 0.000077\n",
      "\tTraining batch 38 Loss: 0.016808\n",
      "\tTraining batch 39 Loss: 0.289018\n",
      "\tTraining batch 40 Loss: 0.000548\n",
      "\tTraining batch 41 Loss: 0.000001\n",
      "\tTraining batch 42 Loss: 0.000046\n",
      "\tTraining batch 43 Loss: 0.001063\n",
      "\tTraining batch 44 Loss: 0.438233\n",
      "\tTraining batch 45 Loss: 0.500221\n",
      "\tTraining batch 46 Loss: 0.000557\n",
      "\tTraining batch 47 Loss: 0.000011\n",
      "\tTraining batch 48 Loss: 0.000211\n",
      "\tTraining batch 49 Loss: 0.465539\n",
      "\tTraining batch 50 Loss: 0.000006\n",
      "\tTraining batch 51 Loss: 0.000011\n",
      "\tTraining batch 52 Loss: 0.149603\n",
      "\tTraining batch 53 Loss: 0.000665\n",
      "\tTraining batch 54 Loss: 0.000184\n",
      "\tTraining batch 55 Loss: 0.009818\n",
      "\tTraining batch 56 Loss: 0.000020\n",
      "\tTraining batch 57 Loss: 0.146625\n",
      "\tTraining batch 58 Loss: 0.429074\n",
      "\tTraining batch 59 Loss: 0.001636\n",
      "\tTraining batch 60 Loss: 0.738947\n",
      "\tTraining batch 61 Loss: 1.168407\n",
      "Training set: Average loss: 0.164471\n",
      "Validation set: Average loss: 9.102497, Accuracy: 1340/1959 (68.40%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000027\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000020\n",
      "\tTraining batch 4 Loss: 0.000015\n",
      "\tTraining batch 5 Loss: 0.000022\n",
      "\tTraining batch 6 Loss: 0.001279\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000001\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000002\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.004011\n",
      "\tTraining batch 15 Loss: 0.000002\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000005\n",
      "\tTraining batch 20 Loss: 0.337544\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.138015\n",
      "\tTraining batch 23 Loss: 0.000064\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.001722\n",
      "\tTraining batch 27 Loss: 0.000018\n",
      "\tTraining batch 28 Loss: 0.002126\n",
      "\tTraining batch 29 Loss: 0.000014\n",
      "\tTraining batch 30 Loss: 0.000004\n",
      "\tTraining batch 31 Loss: 0.000336\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000006\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000024\n",
      "\tTraining batch 37 Loss: 0.003941\n",
      "\tTraining batch 38 Loss: 0.000005\n",
      "\tTraining batch 39 Loss: 0.000940\n",
      "\tTraining batch 40 Loss: 0.000100\n",
      "\tTraining batch 41 Loss: 0.007126\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000003\n",
      "\tTraining batch 45 Loss: 0.003611\n",
      "\tTraining batch 46 Loss: 0.005843\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.662755\n",
      "\tTraining batch 49 Loss: 0.002512\n",
      "\tTraining batch 50 Loss: 0.000012\n",
      "\tTraining batch 51 Loss: 0.000019\n",
      "\tTraining batch 52 Loss: 0.025093\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000440\n",
      "\tTraining batch 55 Loss: 0.003513\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.013347\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.271037\n",
      "\tTraining batch 60 Loss: 0.000018\n",
      "\tTraining batch 61 Loss: 0.297062\n",
      "Training set: Average loss: 0.029224\n",
      "Validation set: Average loss: 11.572946, Accuracy: 1366/1959 (69.73%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000016\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000012\n",
      "\tTraining batch 5 Loss: 0.111807\n",
      "\tTraining batch 6 Loss: 0.000035\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000363\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000003\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.235390\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000156\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.248043\n",
      "\tTraining batch 19 Loss: 0.000012\n",
      "\tTraining batch 20 Loss: 0.056638\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.167984\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.003165\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000003\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000062\n",
      "\tTraining batch 31 Loss: 0.000001\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.143628\n",
      "\tTraining batch 34 Loss: 0.000022\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.197587\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.001987\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000002\n",
      "\tTraining batch 43 Loss: 0.002668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000106\n",
      "\tTraining batch 46 Loss: 0.000802\n",
      "\tTraining batch 47 Loss: 0.000004\n",
      "\tTraining batch 48 Loss: 0.000195\n",
      "\tTraining batch 49 Loss: 0.214901\n",
      "\tTraining batch 50 Loss: 0.000054\n",
      "\tTraining batch 51 Loss: 0.000387\n",
      "\tTraining batch 52 Loss: 0.000040\n",
      "\tTraining batch 53 Loss: 0.000045\n",
      "\tTraining batch 54 Loss: 0.081188\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.066655\n",
      "\tTraining batch 57 Loss: 0.130820\n",
      "\tTraining batch 58 Loss: 0.000143\n",
      "\tTraining batch 59 Loss: 0.001045\n",
      "\tTraining batch 60 Loss: 0.000001\n",
      "\tTraining batch 61 Loss: 0.000062\n",
      "Training set: Average loss: 0.027312\n",
      "Validation set: Average loss: 9.740632, Accuracy: 1370/1959 (69.93%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000013\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000003\n",
      "\tTraining batch 5 Loss: 0.000202\n",
      "\tTraining batch 6 Loss: 0.004718\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000009\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000008\n",
      "\tTraining batch 12 Loss: 0.066558\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000008\n",
      "\tTraining batch 15 Loss: 0.000055\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000220\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.003085\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000010\n",
      "\tTraining batch 31 Loss: 0.000001\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000182\n",
      "\tTraining batch 35 Loss: 0.000006\n",
      "\tTraining batch 36 Loss: 0.001167\n",
      "\tTraining batch 37 Loss: 0.000001\n",
      "\tTraining batch 38 Loss: 0.000021\n",
      "\tTraining batch 39 Loss: 0.011457\n",
      "\tTraining batch 40 Loss: 0.000001\n",
      "\tTraining batch 41 Loss: 0.000002\n",
      "\tTraining batch 42 Loss: 0.000001\n",
      "\tTraining batch 43 Loss: 0.000002\n",
      "\tTraining batch 44 Loss: 0.000273\n",
      "\tTraining batch 45 Loss: 0.003200\n",
      "\tTraining batch 46 Loss: 0.000001\n",
      "\tTraining batch 47 Loss: 0.000006\n",
      "\tTraining batch 48 Loss: 0.000003\n",
      "\tTraining batch 49 Loss: 0.087331\n",
      "\tTraining batch 50 Loss: 0.000004\n",
      "\tTraining batch 51 Loss: 0.000001\n",
      "\tTraining batch 52 Loss: 0.001020\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000098\n",
      "\tTraining batch 55 Loss: 0.000049\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000245\n",
      "\tTraining batch 58 Loss: 0.008098\n",
      "\tTraining batch 59 Loss: 0.000004\n",
      "\tTraining batch 60 Loss: 0.000008\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "Training set: Average loss: 0.003083\n",
      "Validation set: Average loss: 10.660610, Accuracy: 1370/1959 (69.93%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 48.25%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000007\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000006\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000104\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000052\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000001\n",
      "\tTraining batch 14 Loss: 0.000018\n",
      "\tTraining batch 15 Loss: 0.000076\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.001193\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000947\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000009\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000003\n",
      "\tTraining batch 35 Loss: 0.000029\n",
      "\tTraining batch 36 Loss: 0.000271\n",
      "\tTraining batch 37 Loss: 0.000021\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000028\n",
      "\tTraining batch 46 Loss: 0.000002\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000002\n",
      "\tTraining batch 49 Loss: 0.000124\n",
      "\tTraining batch 50 Loss: 0.000108\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.008479\n",
      "\tTraining batch 55 Loss: 0.000058\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.006400\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000006\n",
      "\tTraining batch 60 Loss: 0.000011\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 15.927864\n",
      "Training set: Average loss: 0.257191\n",
      "Validation set: Average loss: 10.127678, Accuracy: 1352/1959 (69.01%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000012\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.019196\n",
      "\tTraining batch 4 Loss: 0.000041\n",
      "\tTraining batch 5 Loss: 0.001400\n",
      "\tTraining batch 6 Loss: 0.002544\n",
      "\tTraining batch 7 Loss: 0.067011\n",
      "\tTraining batch 8 Loss: 0.000376\n",
      "\tTraining batch 9 Loss: 0.298061\n",
      "\tTraining batch 10 Loss: 0.367684\n",
      "\tTraining batch 11 Loss: 0.156513\n",
      "\tTraining batch 12 Loss: 0.032300\n",
      "\tTraining batch 13 Loss: 0.396037\n",
      "\tTraining batch 14 Loss: 0.082223\n",
      "\tTraining batch 15 Loss: 0.018720\n",
      "\tTraining batch 16 Loss: 0.025515\n",
      "\tTraining batch 17 Loss: 0.578269\n",
      "\tTraining batch 18 Loss: 0.135504\n",
      "\tTraining batch 19 Loss: 0.200284\n",
      "\tTraining batch 20 Loss: 0.055082\n",
      "\tTraining batch 21 Loss: 1.341828\n",
      "\tTraining batch 22 Loss: 0.510543\n",
      "\tTraining batch 23 Loss: 0.001421\n",
      "\tTraining batch 24 Loss: 0.534976\n",
      "\tTraining batch 25 Loss: 0.217769\n",
      "\tTraining batch 26 Loss: 0.340036\n",
      "\tTraining batch 27 Loss: 0.109469\n",
      "\tTraining batch 28 Loss: 0.026031\n",
      "\tTraining batch 29 Loss: 0.023418\n",
      "\tTraining batch 30 Loss: 0.661078\n",
      "\tTraining batch 31 Loss: 0.550888\n",
      "\tTraining batch 32 Loss: 0.512668\n",
      "\tTraining batch 33 Loss: 0.022528\n",
      "\tTraining batch 34 Loss: 0.289688\n",
      "\tTraining batch 35 Loss: 0.943053\n",
      "\tTraining batch 36 Loss: 0.325914\n",
      "\tTraining batch 37 Loss: 0.718278\n",
      "\tTraining batch 38 Loss: 0.173045\n",
      "\tTraining batch 39 Loss: 0.317931\n",
      "\tTraining batch 40 Loss: 0.428827\n",
      "\tTraining batch 41 Loss: 0.388738\n",
      "\tTraining batch 42 Loss: 0.000047\n",
      "\tTraining batch 43 Loss: 0.027322\n",
      "\tTraining batch 44 Loss: 0.080154\n",
      "\tTraining batch 45 Loss: 0.857441\n",
      "\tTraining batch 46 Loss: 0.643269\n",
      "\tTraining batch 47 Loss: 0.030279\n",
      "\tTraining batch 48 Loss: 0.057156\n",
      "\tTraining batch 49 Loss: 0.274677\n",
      "\tTraining batch 50 Loss: 0.058610\n",
      "\tTraining batch 51 Loss: 0.345470\n",
      "\tTraining batch 52 Loss: 0.322703\n",
      "\tTraining batch 53 Loss: 0.091082\n",
      "\tTraining batch 54 Loss: 0.411594\n",
      "\tTraining batch 55 Loss: 0.000017\n",
      "\tTraining batch 56 Loss: 0.311264\n",
      "\tTraining batch 57 Loss: 0.806946\n",
      "\tTraining batch 58 Loss: 0.008996\n",
      "\tTraining batch 59 Loss: 0.199870\n",
      "\tTraining batch 60 Loss: 0.248291\n",
      "\tTraining batch 61 Loss: 0.338497\n",
      "\tTraining batch 62 Loss: 0.710586\n",
      "Training set: Average loss: 0.269341\n",
      "Validation set: Average loss: 8.737740, Accuracy: 1375/1959 (70.19%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000540\n",
      "\tTraining batch 2 Loss: 0.386057\n",
      "\tTraining batch 3 Loss: 0.000002\n",
      "\tTraining batch 4 Loss: 0.000369\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000035\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000260\n",
      "\tTraining batch 9 Loss: 0.000102\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.091337\n",
      "\tTraining batch 13 Loss: 0.041990\n",
      "\tTraining batch 14 Loss: 0.000009\n",
      "\tTraining batch 15 Loss: 0.000001\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000084\n",
      "\tTraining batch 18 Loss: 0.017061\n",
      "\tTraining batch 19 Loss: 0.001489\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.019133\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.002459\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.001219\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.001532\n",
      "\tTraining batch 34 Loss: 0.084758\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.070611\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.018664\n",
      "\tTraining batch 39 Loss: 0.000020\n",
      "\tTraining batch 40 Loss: 0.000001\n",
      "\tTraining batch 41 Loss: 0.167047\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000057\n",
      "\tTraining batch 44 Loss: 0.000006\n",
      "\tTraining batch 45 Loss: 0.255920\n",
      "\tTraining batch 46 Loss: 0.000001\n",
      "\tTraining batch 47 Loss: 0.000696\n",
      "\tTraining batch 48 Loss: 0.000013\n",
      "\tTraining batch 49 Loss: 0.013490\n",
      "\tTraining batch 50 Loss: 0.162419\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.026361\n",
      "\tTraining batch 54 Loss: 0.000041\n",
      "\tTraining batch 55 Loss: 0.000002\n",
      "\tTraining batch 56 Loss: 0.129508\n",
      "\tTraining batch 57 Loss: 0.004802\n",
      "\tTraining batch 58 Loss: 0.000007\n",
      "\tTraining batch 59 Loss: 0.135020\n",
      "\tTraining batch 60 Loss: 0.003281\n",
      "\tTraining batch 61 Loss: 0.000003\n",
      "\tTraining batch 62 Loss: 0.222540\n",
      "Training set: Average loss: 0.029983\n",
      "Validation set: Average loss: 12.806082, Accuracy: 1364/1959 (69.63%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000004\n",
      "\tTraining batch 2 Loss: 0.000150\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.192900\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000006\n",
      "\tTraining batch 12 Loss: 0.000042\n",
      "\tTraining batch 13 Loss: 0.000002\n",
      "\tTraining batch 14 Loss: 0.000100\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000020\n",
      "\tTraining batch 18 Loss: 0.000362\n",
      "\tTraining batch 19 Loss: 0.001051\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000086\n",
      "\tTraining batch 26 Loss: 0.002400\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.274817\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.060803\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000475\n",
      "\tTraining batch 37 Loss: 0.274268\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000275\n",
      "\tTraining batch 40 Loss: 0.001302\n",
      "\tTraining batch 41 Loss: 0.000026\n",
      "\tTraining batch 42 Loss: 0.000062\n",
      "\tTraining batch 43 Loss: 0.000051\n",
      "\tTraining batch 44 Loss: 0.003314\n",
      "\tTraining batch 45 Loss: 0.000161\n",
      "\tTraining batch 46 Loss: 0.000048\n",
      "\tTraining batch 47 Loss: 0.000012\n",
      "\tTraining batch 48 Loss: 0.000080\n",
      "\tTraining batch 49 Loss: 0.000017\n",
      "\tTraining batch 50 Loss: 0.119286\n",
      "\tTraining batch 51 Loss: 0.031745\n",
      "\tTraining batch 52 Loss: 0.875832\n",
      "\tTraining batch 53 Loss: 0.168518\n",
      "\tTraining batch 54 Loss: 0.129451\n",
      "\tTraining batch 55 Loss: 0.000179\n",
      "\tTraining batch 56 Loss: 0.000175\n",
      "\tTraining batch 57 Loss: 0.223298\n",
      "\tTraining batch 58 Loss: 0.006522\n",
      "\tTraining batch 59 Loss: 0.230618\n",
      "\tTraining batch 60 Loss: 0.000104\n",
      "\tTraining batch 61 Loss: 0.010854\n",
      "\tTraining batch 62 Loss: 0.489741\n",
      "Training set: Average loss: 0.049986\n",
      "Validation set: Average loss: 11.773437, Accuracy: 1363/1959 (69.58%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000302\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.001587\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000067\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000003\n",
      "\tTraining batch 15 Loss: 0.000014\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.021944\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.006611\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.008988\n",
      "\tTraining batch 27 Loss: 0.059955\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000001\n",
      "\tTraining batch 30 Loss: 0.000043\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000725\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000395\n",
      "\tTraining batch 35 Loss: 0.000002\n",
      "\tTraining batch 36 Loss: 0.000001\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000120\n",
      "\tTraining batch 40 Loss: 0.000033\n",
      "\tTraining batch 41 Loss: 0.000001\n",
      "\tTraining batch 42 Loss: 0.000002\n",
      "\tTraining batch 43 Loss: 0.000025\n",
      "\tTraining batch 44 Loss: 0.000004\n",
      "\tTraining batch 45 Loss: 0.005649\n",
      "\tTraining batch 46 Loss: 0.000001\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.001059\n",
      "\tTraining batch 49 Loss: 0.344207\n",
      "\tTraining batch 50 Loss: 0.083143\n",
      "\tTraining batch 51 Loss: 0.001037\n",
      "\tTraining batch 52 Loss: 0.001053\n",
      "\tTraining batch 53 Loss: 0.000006\n",
      "\tTraining batch 54 Loss: 0.000335\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000001\n",
      "\tTraining batch 57 Loss: 0.002750\n",
      "\tTraining batch 58 Loss: 0.140689\n",
      "\tTraining batch 59 Loss: 0.000007\n",
      "\tTraining batch 60 Loss: 0.001732\n",
      "\tTraining batch 61 Loss: 0.003007\n",
      "\tTraining batch 62 Loss: 0.000080\n",
      "Training set: Average loss: 0.011058\n",
      "Validation set: Average loss: 11.571124, Accuracy: 1347/1959 (68.76%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.003349\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000002\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.215569\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.003654\n",
      "\tTraining batch 13 Loss: 0.018608\n",
      "\tTraining batch 14 Loss: 0.000014\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000236\n",
      "\tTraining batch 17 Loss: 0.000014\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000050\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.001281\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000491\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.153706\n",
      "\tTraining batch 36 Loss: 0.099873\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000541\n",
      "\tTraining batch 40 Loss: 0.000016\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000114\n",
      "\tTraining batch 45 Loss: 0.000004\n",
      "\tTraining batch 46 Loss: 0.000001\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000005\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000001\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000059\n",
      "\tTraining batch 58 Loss: 0.000798\n",
      "\tTraining batch 59 Loss: 0.000001\n",
      "\tTraining batch 60 Loss: 0.000421\n",
      "\tTraining batch 61 Loss: 0.000010\n",
      "\tTraining batch 62 Loss: 0.000010\n",
      "Training set: Average loss: 0.008046\n",
      "Validation set: Average loss: 11.125715, Accuracy: 1367/1959 (69.78%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000003\n",
      "\tTraining batch 5 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000001\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000071\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.001111\n",
      "\tTraining batch 27 Loss: 0.000074\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000001\n",
      "\tTraining batch 34 Loss: 0.000002\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000001\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000004\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.002452\n",
      "\tTraining batch 45 Loss: 0.000003\n",
      "\tTraining batch 46 Loss: 0.000003\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000009\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000010\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000001\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000001\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000052\n",
      "\tTraining batch 58 Loss: 0.000125\n",
      "\tTraining batch 59 Loss: 0.000001\n",
      "\tTraining batch 60 Loss: 0.000080\n",
      "\tTraining batch 61 Loss: 0.000008\n",
      "\tTraining batch 62 Loss: 0.000010\n",
      "Training set: Average loss: 0.000065\n",
      "Validation set: Average loss: 11.110517, Accuracy: 1365/1959 (69.68%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000003\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000001\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000065\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000922\n",
      "\tTraining batch 27 Loss: 0.000028\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000002\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000001\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000004\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000001\n",
      "\tTraining batch 44 Loss: 0.000012\n",
      "\tTraining batch 45 Loss: 0.000003\n",
      "\tTraining batch 46 Loss: 0.000003\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000005\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000005\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000001\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000044\n",
      "\tTraining batch 58 Loss: 0.000077\n",
      "\tTraining batch 59 Loss: 0.000001\n",
      "\tTraining batch 60 Loss: 0.000051\n",
      "\tTraining batch 61 Loss: 0.000006\n",
      "\tTraining batch 62 Loss: 0.000009\n",
      "Training set: Average loss: 0.000020\n",
      "Validation set: Average loss: 11.125868, Accuracy: 1365/1959 (69.68%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 49.75%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000003\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000001\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000058\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000824\n",
      "\tTraining batch 27 Loss: 0.000015\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000002\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000001\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000004\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000011\n",
      "\tTraining batch 45 Loss: 0.000003\n",
      "\tTraining batch 46 Loss: 0.000002\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000004\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000004\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000001\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000040\n",
      "\tTraining batch 58 Loss: 0.000058\n",
      "\tTraining batch 59 Loss: 0.000001\n",
      "\tTraining batch 60 Loss: 0.000040\n",
      "\tTraining batch 61 Loss: 0.000006\n",
      "\tTraining batch 62 Loss: 0.000009\n",
      "\tTraining batch 63 Loss: 18.535995\n",
      "Training set: Average loss: 0.294239\n",
      "Validation set: Average loss: 9.218671, Accuracy: 1362/1959 (69.53%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000002\n",
      "\tTraining batch 2 Loss: 0.000049\n",
      "\tTraining batch 3 Loss: 0.000043\n",
      "\tTraining batch 4 Loss: 0.120445\n",
      "\tTraining batch 5 Loss: 0.000411\n",
      "\tTraining batch 6 Loss: 0.095337\n",
      "\tTraining batch 7 Loss: 0.033676\n",
      "\tTraining batch 8 Loss: 1.347004\n",
      "\tTraining batch 9 Loss: 0.070897\n",
      "\tTraining batch 10 Loss: 0.050394\n",
      "\tTraining batch 11 Loss: 0.345626\n",
      "\tTraining batch 12 Loss: 0.295689\n",
      "\tTraining batch 13 Loss: 0.004635\n",
      "\tTraining batch 14 Loss: 0.006199\n",
      "\tTraining batch 15 Loss: 0.307857\n",
      "\tTraining batch 16 Loss: 0.252712\n",
      "\tTraining batch 17 Loss: 0.467146\n",
      "\tTraining batch 18 Loss: 0.097739\n",
      "\tTraining batch 19 Loss: 0.111906\n",
      "\tTraining batch 20 Loss: 0.088174\n",
      "\tTraining batch 21 Loss: 0.000705\n",
      "\tTraining batch 22 Loss: 0.000046\n",
      "\tTraining batch 23 Loss: 0.022780\n",
      "\tTraining batch 24 Loss: 0.008043\n",
      "\tTraining batch 25 Loss: 0.000005\n",
      "\tTraining batch 26 Loss: 0.044244\n",
      "\tTraining batch 27 Loss: 0.097181\n",
      "\tTraining batch 28 Loss: 0.066348\n",
      "\tTraining batch 29 Loss: 0.227245\n",
      "\tTraining batch 30 Loss: 0.227499\n",
      "\tTraining batch 31 Loss: 0.036225\n",
      "\tTraining batch 32 Loss: 0.001676\n",
      "\tTraining batch 33 Loss: 0.000001\n",
      "\tTraining batch 34 Loss: 0.004456\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.074522\n",
      "\tTraining batch 37 Loss: 0.100605\n",
      "\tTraining batch 38 Loss: 0.000014\n",
      "\tTraining batch 39 Loss: 0.001303\n",
      "\tTraining batch 40 Loss: 0.178484\n",
      "\tTraining batch 41 Loss: 0.055802\n",
      "\tTraining batch 42 Loss: 0.324385\n",
      "\tTraining batch 43 Loss: 0.465918\n",
      "\tTraining batch 44 Loss: 0.277168\n",
      "\tTraining batch 45 Loss: 0.121603\n",
      "\tTraining batch 46 Loss: 0.156972\n",
      "\tTraining batch 47 Loss: 0.980531\n",
      "\tTraining batch 48 Loss: 0.103587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000002\n",
      "\tTraining batch 51 Loss: 0.000117\n",
      "\tTraining batch 52 Loss: 0.006178\n",
      "\tTraining batch 53 Loss: 0.003371\n",
      "\tTraining batch 54 Loss: 0.000025\n",
      "\tTraining batch 55 Loss: 0.000055\n",
      "\tTraining batch 56 Loss: 0.024785\n",
      "\tTraining batch 57 Loss: 0.293952\n",
      "\tTraining batch 58 Loss: 0.000545\n",
      "\tTraining batch 59 Loss: 0.062491\n",
      "\tTraining batch 60 Loss: 0.001173\n",
      "\tTraining batch 61 Loss: 0.928534\n",
      "\tTraining batch 62 Loss: 0.278318\n",
      "\tTraining batch 63 Loss: 0.871722\n",
      "Training set: Average loss: 0.154676\n",
      "Validation set: Average loss: 7.856201, Accuracy: 1381/1959 (70.50%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000080\n",
      "\tTraining batch 2 Loss: 0.000002\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000006\n",
      "\tTraining batch 5 Loss: 0.022839\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000592\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.152727\n",
      "\tTraining batch 11 Loss: 0.000014\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000002\n",
      "\tTraining batch 14 Loss: 0.000010\n",
      "\tTraining batch 15 Loss: 0.003375\n",
      "\tTraining batch 16 Loss: 0.000097\n",
      "\tTraining batch 17 Loss: 0.004185\n",
      "\tTraining batch 18 Loss: 0.000187\n",
      "\tTraining batch 19 Loss: 0.018265\n",
      "\tTraining batch 20 Loss: 0.000002\n",
      "\tTraining batch 21 Loss: 0.000126\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.106895\n",
      "\tTraining batch 24 Loss: 0.000021\n",
      "\tTraining batch 25 Loss: 0.000084\n",
      "\tTraining batch 26 Loss: 0.018954\n",
      "\tTraining batch 27 Loss: 0.173852\n",
      "\tTraining batch 28 Loss: 0.001105\n",
      "\tTraining batch 29 Loss: 0.000012\n",
      "\tTraining batch 30 Loss: 0.000005\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000130\n",
      "\tTraining batch 33 Loss: 0.001018\n",
      "\tTraining batch 34 Loss: 0.044142\n",
      "\tTraining batch 35 Loss: 0.000003\n",
      "\tTraining batch 36 Loss: 0.148869\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.217781\n",
      "\tTraining batch 39 Loss: 0.050903\n",
      "\tTraining batch 40 Loss: 0.021800\n",
      "\tTraining batch 41 Loss: 0.000306\n",
      "\tTraining batch 42 Loss: 0.296915\n",
      "\tTraining batch 43 Loss: 0.213110\n",
      "\tTraining batch 44 Loss: 0.000288\n",
      "\tTraining batch 45 Loss: 0.000857\n",
      "\tTraining batch 46 Loss: 0.030460\n",
      "\tTraining batch 47 Loss: 0.019864\n",
      "\tTraining batch 48 Loss: 0.071733\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000008\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000022\n",
      "\tTraining batch 54 Loss: 0.023472\n",
      "\tTraining batch 55 Loss: 0.000010\n",
      "\tTraining batch 56 Loss: 0.132389\n",
      "\tTraining batch 57 Loss: 0.007612\n",
      "\tTraining batch 58 Loss: 0.024050\n",
      "\tTraining batch 59 Loss: 0.000356\n",
      "\tTraining batch 60 Loss: 0.001529\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000242\n",
      "\tTraining batch 63 Loss: 0.017544\n",
      "Training set: Average loss: 0.029029\n",
      "Validation set: Average loss: 10.272250, Accuracy: 1359/1959 (69.37%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.178495\n",
      "\tTraining batch 2 Loss: 0.141788\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.001210\n",
      "\tTraining batch 11 Loss: 0.000001\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000001\n",
      "\tTraining batch 15 Loss: 0.000017\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000002\n",
      "\tTraining batch 19 Loss: 0.008372\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.018056\n",
      "\tTraining batch 26 Loss: 0.013577\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.003180\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000002\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.002137\n",
      "\tTraining batch 33 Loss: 0.000028\n",
      "\tTraining batch 34 Loss: 0.000020\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000001\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000001\n",
      "\tTraining batch 41 Loss: 0.000005\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000501\n",
      "\tTraining batch 44 Loss: 0.000010\n",
      "\tTraining batch 45 Loss: 0.000167\n",
      "\tTraining batch 46 Loss: 0.000003\n",
      "\tTraining batch 47 Loss: 0.000629\n",
      "\tTraining batch 48 Loss: 0.000021\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000122\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000001\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000604\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.073170\n",
      "\tTraining batch 57 Loss: 0.003067\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000626\n",
      "\tTraining batch 60 Loss: 0.061394\n",
      "\tTraining batch 61 Loss: 0.000015\n",
      "\tTraining batch 62 Loss: 0.002178\n",
      "\tTraining batch 63 Loss: 0.000390\n",
      "Training set: Average loss: 0.008092\n",
      "Validation set: Average loss: 10.655919, Accuracy: 1386/1959 (70.75%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000048\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000082\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000018\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000216\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000001\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000494\n",
      "\tTraining batch 27 Loss: 0.004023\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000010\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000028\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000066\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000001\n",
      "\tTraining batch 41 Loss: 0.000005\n",
      "\tTraining batch 42 Loss: 0.000001\n",
      "\tTraining batch 43 Loss: 0.000015\n",
      "\tTraining batch 44 Loss: 0.000073\n",
      "\tTraining batch 45 Loss: 0.000012\n",
      "\tTraining batch 46 Loss: 0.000002\n",
      "\tTraining batch 47 Loss: 0.000074\n",
      "\tTraining batch 48 Loss: 0.000070\n",
      "\tTraining batch 49 Loss: 0.000023\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000001\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000016\n",
      "\tTraining batch 55 Loss: 0.000002\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000214\n",
      "\tTraining batch 58 Loss: 0.000001\n",
      "\tTraining batch 59 Loss: 0.000012\n",
      "\tTraining batch 60 Loss: 0.000001\n",
      "\tTraining batch 61 Loss: 0.000001\n",
      "\tTraining batch 62 Loss: 0.054150\n",
      "\tTraining batch 63 Loss: 0.000323\n",
      "Training set: Average loss: 0.000952\n",
      "Validation set: Average loss: 11.248766, Accuracy: 1382/1959 (70.55%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000336\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000007\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.005851\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000023\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.004877\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 26 Loss: 0.000204\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000005\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000013\n",
      "\tTraining batch 33 Loss: 0.000029\n",
      "\tTraining batch 34 Loss: 0.000061\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000014\n",
      "\tTraining batch 41 Loss: 0.000018\n",
      "\tTraining batch 42 Loss: 0.000001\n",
      "\tTraining batch 43 Loss: 0.000003\n",
      "\tTraining batch 44 Loss: 0.000013\n",
      "\tTraining batch 45 Loss: 0.000007\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000060\n",
      "\tTraining batch 48 Loss: 0.000003\n",
      "\tTraining batch 49 Loss: 0.000028\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000001\n",
      "\tTraining batch 55 Loss: 0.000018\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000043\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000016\n",
      "\tTraining batch 60 Loss: 0.000006\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000001\n",
      "\tTraining batch 63 Loss: 0.000096\n",
      "Training set: Average loss: 0.000186\n",
      "Validation set: Average loss: 11.629508, Accuracy: 1377/1959 (70.29%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000023\n",
      "\tTraining batch 15 Loss: 0.000001\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000003\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000167\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000005\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000015\n",
      "\tTraining batch 33 Loss: 0.000032\n",
      "\tTraining batch 34 Loss: 0.000058\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000013\n",
      "\tTraining batch 41 Loss: 0.000013\n",
      "\tTraining batch 42 Loss: 0.000001\n",
      "\tTraining batch 43 Loss: 0.000003\n",
      "\tTraining batch 44 Loss: 0.000012\n",
      "\tTraining batch 45 Loss: 0.000006\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000051\n",
      "\tTraining batch 48 Loss: 0.000003\n",
      "\tTraining batch 49 Loss: 0.000007\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000001\n",
      "\tTraining batch 55 Loss: 0.000014\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000040\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000015\n",
      "\tTraining batch 60 Loss: 0.000006\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000001\n",
      "\tTraining batch 63 Loss: 0.000044\n",
      "Training set: Average loss: 0.000008\n",
      "Validation set: Average loss: 11.631230, Accuracy: 1377/1959 (70.29%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 47.25%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000022\n",
      "\tTraining batch 15 Loss: 0.000001\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000003\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000161\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000005\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000013\n",
      "\tTraining batch 33 Loss: 0.000028\n",
      "\tTraining batch 34 Loss: 0.000056\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000012\n",
      "\tTraining batch 41 Loss: 0.000010\n",
      "\tTraining batch 42 Loss: 0.000001\n",
      "\tTraining batch 43 Loss: 0.000003\n",
      "\tTraining batch 44 Loss: 0.000011\n",
      "\tTraining batch 45 Loss: 0.000006\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000044\n",
      "\tTraining batch 48 Loss: 0.000003\n",
      "\tTraining batch 49 Loss: 0.000005\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000001\n",
      "\tTraining batch 55 Loss: 0.000011\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000039\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000014\n",
      "\tTraining batch 60 Loss: 0.000005\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000001\n",
      "\tTraining batch 63 Loss: 0.000029\n",
      "\tTraining batch 64 Loss: 15.656349\n",
      "Training set: Average loss: 0.244638\n",
      "Validation set: Average loss: 9.710578, Accuracy: 1396/1959 (71.26%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000003\n",
      "\tTraining batch 3 Loss: 0.000013\n",
      "\tTraining batch 4 Loss: 0.001415\n",
      "\tTraining batch 5 Loss: 0.000002\n",
      "\tTraining batch 6 Loss: 0.395685\n",
      "\tTraining batch 7 Loss: 0.000045\n",
      "\tTraining batch 8 Loss: 0.012967\n",
      "\tTraining batch 9 Loss: 0.246342\n",
      "\tTraining batch 10 Loss: 0.082596\n",
      "\tTraining batch 11 Loss: 0.399368\n",
      "\tTraining batch 12 Loss: 0.084113\n",
      "\tTraining batch 13 Loss: 0.145693\n",
      "\tTraining batch 14 Loss: 0.007195\n",
      "\tTraining batch 15 Loss: 0.137003\n",
      "\tTraining batch 16 Loss: 0.033340\n",
      "\tTraining batch 17 Loss: 0.056022\n",
      "\tTraining batch 18 Loss: 0.064514\n",
      "\tTraining batch 19 Loss: 0.050296\n",
      "\tTraining batch 20 Loss: 0.025116\n",
      "\tTraining batch 21 Loss: 0.011030\n",
      "\tTraining batch 22 Loss: 0.338274\n",
      "\tTraining batch 23 Loss: 0.150928\n",
      "\tTraining batch 24 Loss: 0.478362\n",
      "\tTraining batch 25 Loss: 0.180098\n",
      "\tTraining batch 26 Loss: 0.146032\n",
      "\tTraining batch 27 Loss: 0.012641\n",
      "\tTraining batch 28 Loss: 0.063840\n",
      "\tTraining batch 29 Loss: 0.016539\n",
      "\tTraining batch 30 Loss: 0.017718\n",
      "\tTraining batch 31 Loss: 0.000046\n",
      "\tTraining batch 32 Loss: 0.001214\n",
      "\tTraining batch 33 Loss: 0.180522\n",
      "\tTraining batch 34 Loss: 0.000116\n",
      "\tTraining batch 35 Loss: 0.000024\n",
      "\tTraining batch 36 Loss: 0.001314\n",
      "\tTraining batch 37 Loss: 0.000010\n",
      "\tTraining batch 38 Loss: 0.203036\n",
      "\tTraining batch 39 Loss: 0.018037\n",
      "\tTraining batch 40 Loss: 0.000455\n",
      "\tTraining batch 41 Loss: 0.055463\n",
      "\tTraining batch 42 Loss: 0.006950\n",
      "\tTraining batch 43 Loss: 0.072371\n",
      "\tTraining batch 44 Loss: 0.136563\n",
      "\tTraining batch 45 Loss: 0.002042\n",
      "\tTraining batch 46 Loss: 0.198178\n",
      "\tTraining batch 47 Loss: 0.000340\n",
      "\tTraining batch 48 Loss: 0.004274\n",
      "\tTraining batch 49 Loss: 0.000027\n",
      "\tTraining batch 50 Loss: 0.000030\n",
      "\tTraining batch 51 Loss: 0.000029\n",
      "\tTraining batch 52 Loss: 0.000037\n",
      "\tTraining batch 53 Loss: 0.001509\n",
      "\tTraining batch 54 Loss: 0.031426\n",
      "\tTraining batch 55 Loss: 0.001112\n",
      "\tTraining batch 56 Loss: 0.000707\n",
      "\tTraining batch 57 Loss: 0.020527\n",
      "\tTraining batch 58 Loss: 0.000884\n",
      "\tTraining batch 59 Loss: 0.000478\n",
      "\tTraining batch 60 Loss: 0.000001\n",
      "\tTraining batch 61 Loss: 0.000214\n",
      "\tTraining batch 62 Loss: 0.015631\n",
      "\tTraining batch 63 Loss: 0.276097\n",
      "\tTraining batch 64 Loss: 0.184184\n",
      "Training set: Average loss: 0.071423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set: Average loss: 7.359694, Accuracy: 1384/1959 (70.65%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000001\n",
      "\tTraining batch 2 Loss: 0.000010\n",
      "\tTraining batch 3 Loss: 0.099278\n",
      "\tTraining batch 4 Loss: 0.062542\n",
      "\tTraining batch 5 Loss: 0.000055\n",
      "\tTraining batch 6 Loss: 0.000002\n",
      "\tTraining batch 7 Loss: 0.000061\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000002\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000001\n",
      "\tTraining batch 15 Loss: 0.000008\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.011387\n",
      "\tTraining batch 19 Loss: 0.012196\n",
      "\tTraining batch 20 Loss: 0.000069\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000143\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.004179\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000206\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.095281\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000433\n",
      "\tTraining batch 33 Loss: 0.009365\n",
      "\tTraining batch 34 Loss: 0.000025\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000014\n",
      "\tTraining batch 38 Loss: 0.000271\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000015\n",
      "\tTraining batch 41 Loss: 0.043006\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000024\n",
      "\tTraining batch 44 Loss: 0.359377\n",
      "\tTraining batch 45 Loss: 0.000474\n",
      "\tTraining batch 46 Loss: 0.029676\n",
      "\tTraining batch 47 Loss: 0.000011\n",
      "\tTraining batch 48 Loss: 0.000175\n",
      "\tTraining batch 49 Loss: 0.000026\n",
      "\tTraining batch 50 Loss: 0.013294\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000086\n",
      "\tTraining batch 53 Loss: 0.002070\n",
      "\tTraining batch 54 Loss: 0.001086\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.004006\n",
      "\tTraining batch 58 Loss: 0.006508\n",
      "\tTraining batch 59 Loss: 0.000040\n",
      "\tTraining batch 60 Loss: 0.000116\n",
      "\tTraining batch 61 Loss: 0.000016\n",
      "\tTraining batch 62 Loss: 0.010732\n",
      "\tTraining batch 63 Loss: 0.000175\n",
      "\tTraining batch 64 Loss: 0.071113\n",
      "Training set: Average loss: 0.013087\n",
      "Validation set: Average loss: 8.587055, Accuracy: 1384/1959 (70.65%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 46.25%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000013\n",
      "\tTraining batch 2 Loss: 0.000003\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000148\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000001\n",
      "\tTraining batch 7 Loss: 0.000461\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.004273\n",
      "\tTraining batch 16 Loss: 0.000001\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.019377\n",
      "\tTraining batch 20 Loss: 0.000035\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000001\n",
      "\tTraining batch 24 Loss: 0.153240\n",
      "\tTraining batch 25 Loss: 0.000342\n",
      "\tTraining batch 26 Loss: 0.161510\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000364\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000032\n",
      "\tTraining batch 31 Loss: 0.000161\n",
      "\tTraining batch 32 Loss: 0.001338\n",
      "\tTraining batch 33 Loss: 0.031318\n",
      "\tTraining batch 34 Loss: 0.000096\n",
      "\tTraining batch 35 Loss: 0.000011\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000446\n",
      "\tTraining batch 38 Loss: 0.000295\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000749\n",
      "\tTraining batch 41 Loss: 0.000337\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.153010\n",
      "\tTraining batch 44 Loss: 0.000008\n",
      "\tTraining batch 45 Loss: 0.000930\n",
      "\tTraining batch 46 Loss: 0.000001\n",
      "\tTraining batch 47 Loss: 0.000431\n",
      "\tTraining batch 48 Loss: 0.000003\n",
      "\tTraining batch 49 Loss: 0.003218\n",
      "\tTraining batch 50 Loss: 0.000127\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000252\n",
      "\tTraining batch 53 Loss: 0.000081\n",
      "\tTraining batch 54 Loss: 0.000655\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.034948\n",
      "\tTraining batch 57 Loss: 0.005022\n",
      "\tTraining batch 58 Loss: 0.000900\n",
      "\tTraining batch 59 Loss: 0.000579\n",
      "\tTraining batch 60 Loss: 0.027798\n",
      "\tTraining batch 61 Loss: 0.000004\n",
      "\tTraining batch 62 Loss: 0.000001\n",
      "\tTraining batch 63 Loss: 0.045633\n",
      "\tTraining batch 64 Loss: 0.001329\n",
      "\tTraining batch 65 Loss: 16.062601\n",
      "Training set: Average loss: 0.257109\n",
      "Validation set: Average loss: 8.489959, Accuracy: 1352/1959 (69.01%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000006\n",
      "\tTraining batch 2 Loss: 0.000019\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.018416\n",
      "\tTraining batch 5 Loss: 0.060850\n",
      "\tTraining batch 6 Loss: 0.000092\n",
      "\tTraining batch 7 Loss: 0.070329\n",
      "\tTraining batch 8 Loss: 0.413052\n",
      "\tTraining batch 9 Loss: 0.223103\n",
      "\tTraining batch 10 Loss: 0.000042\n",
      "\tTraining batch 11 Loss: 0.096861\n",
      "\tTraining batch 12 Loss: 0.020843\n",
      "\tTraining batch 13 Loss: 0.219251\n",
      "\tTraining batch 14 Loss: 0.008328\n",
      "\tTraining batch 15 Loss: 0.379841\n",
      "\tTraining batch 16 Loss: 0.046137\n",
      "\tTraining batch 17 Loss: 0.000703\n",
      "\tTraining batch 18 Loss: 0.059308\n",
      "\tTraining batch 19 Loss: 0.000111\n",
      "\tTraining batch 20 Loss: 0.003989\n",
      "\tTraining batch 21 Loss: 0.000010\n",
      "\tTraining batch 22 Loss: 0.000201\n",
      "\tTraining batch 23 Loss: 0.006573\n",
      "\tTraining batch 24 Loss: 0.007910\n",
      "\tTraining batch 25 Loss: 0.185877\n",
      "\tTraining batch 26 Loss: 0.205624\n",
      "\tTraining batch 27 Loss: 0.000006\n",
      "\tTraining batch 28 Loss: 0.252614\n",
      "\tTraining batch 29 Loss: 0.009319\n",
      "\tTraining batch 30 Loss: 0.000789\n",
      "\tTraining batch 31 Loss: 0.180279\n",
      "\tTraining batch 32 Loss: 0.053768\n",
      "\tTraining batch 33 Loss: 0.000016\n",
      "\tTraining batch 34 Loss: 0.016784\n",
      "\tTraining batch 35 Loss: 0.000151\n",
      "\tTraining batch 36 Loss: 0.000218\n",
      "\tTraining batch 37 Loss: 0.001394\n",
      "\tTraining batch 38 Loss: 0.291496\n",
      "\tTraining batch 39 Loss: 0.041839\n",
      "\tTraining batch 40 Loss: 0.132119\n",
      "\tTraining batch 41 Loss: 0.000367\n",
      "\tTraining batch 42 Loss: 0.580890\n",
      "\tTraining batch 43 Loss: 0.251872\n",
      "\tTraining batch 44 Loss: 0.000078\n",
      "\tTraining batch 45 Loss: 0.094533\n",
      "\tTraining batch 46 Loss: 0.204406\n",
      "\tTraining batch 47 Loss: 0.084535\n",
      "\tTraining batch 48 Loss: 0.000433\n",
      "\tTraining batch 49 Loss: 0.082640\n",
      "\tTraining batch 50 Loss: 0.093954\n",
      "\tTraining batch 51 Loss: 0.000963\n",
      "\tTraining batch 52 Loss: 0.077008\n",
      "\tTraining batch 53 Loss: 0.175335\n",
      "\tTraining batch 54 Loss: 0.369151\n",
      "\tTraining batch 55 Loss: 0.222198\n",
      "\tTraining batch 56 Loss: 0.231163\n",
      "\tTraining batch 57 Loss: 0.091961\n",
      "\tTraining batch 58 Loss: 0.003559\n",
      "\tTraining batch 59 Loss: 0.016052\n",
      "\tTraining batch 60 Loss: 0.065427\n",
      "\tTraining batch 61 Loss: 0.003511\n",
      "\tTraining batch 62 Loss: 0.000070\n",
      "\tTraining batch 63 Loss: 0.094478\n",
      "\tTraining batch 64 Loss: 0.118779\n",
      "\tTraining batch 65 Loss: 0.907893\n",
      "Training set: Average loss: 0.104300\n",
      "Validation set: Average loss: 8.618265, Accuracy: 1362/1959 (69.53%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000004\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000290\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000001\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.001207\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.191088\n",
      "\tTraining batch 11 Loss: 0.000001\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000189\n",
      "\tTraining batch 14 Loss: 0.003875\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.412221\n",
      "\tTraining batch 17 Loss: 0.001086\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.007369\n",
      "\tTraining batch 20 Loss: 0.000990\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.002480\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000001\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000058\n",
      "\tTraining batch 31 Loss: 0.000010\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000031\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000041\n",
      "\tTraining batch 43 Loss: 0.000002\n",
      "\tTraining batch 44 Loss: 0.000013\n",
      "\tTraining batch 45 Loss: 0.002131\n",
      "\tTraining batch 46 Loss: 0.000016\n",
      "\tTraining batch 47 Loss: 0.258736\n",
      "\tTraining batch 48 Loss: 0.000042\n",
      "\tTraining batch 49 Loss: 0.205592\n",
      "\tTraining batch 50 Loss: 0.000073\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.027390\n",
      "\tTraining batch 53 Loss: 0.000002\n",
      "\tTraining batch 54 Loss: 0.010085\n",
      "\tTraining batch 55 Loss: 0.000016\n",
      "\tTraining batch 56 Loss: 0.065258\n",
      "\tTraining batch 57 Loss: 0.004792\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.008338\n",
      "\tTraining batch 60 Loss: 0.000062\n",
      "\tTraining batch 61 Loss: 0.244550\n",
      "\tTraining batch 62 Loss: 0.000187\n",
      "\tTraining batch 63 Loss: 0.078110\n",
      "\tTraining batch 64 Loss: 0.168561\n",
      "\tTraining batch 65 Loss: 0.063462\n",
      "Training set: Average loss: 0.027056\n",
      "Validation set: Average loss: 9.097512, Accuracy: 1368/1959 (69.83%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000006\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.000007\n",
      "\tTraining batch 6 Loss: 0.000001\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.004378\n",
      "\tTraining batch 9 Loss: 0.112823\n",
      "\tTraining batch 10 Loss: 0.000023\n",
      "\tTraining batch 11 Loss: 0.000004\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.002177\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000005\n",
      "\tTraining batch 17 Loss: 0.545818\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.010725\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000011\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.001009\n",
      "\tTraining batch 26 Loss: 0.008071\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.092484\n",
      "\tTraining batch 29 Loss: 0.000008\n",
      "\tTraining batch 30 Loss: 0.002243\n",
      "\tTraining batch 31 Loss: 0.000012\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000001\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.114477\n",
      "\tTraining batch 37 Loss: 0.000002\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000337\n",
      "\tTraining batch 40 Loss: 0.000001\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000070\n",
      "\tTraining batch 43 Loss: 0.120446\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.001365\n",
      "\tTraining batch 46 Loss: 0.000013\n",
      "\tTraining batch 47 Loss: 0.000053\n",
      "\tTraining batch 48 Loss: 0.000008\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.010006\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000381\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.002893\n",
      "\tTraining batch 55 Loss: 0.000297\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000979\n",
      "\tTraining batch 58 Loss: 0.160345\n",
      "\tTraining batch 59 Loss: 0.001581\n",
      "\tTraining batch 60 Loss: 0.000016\n",
      "\tTraining batch 61 Loss: 0.000019\n",
      "\tTraining batch 62 Loss: 0.003465\n",
      "\tTraining batch 63 Loss: 0.000007\n",
      "\tTraining batch 64 Loss: 0.058825\n",
      "\tTraining batch 65 Loss: 0.007030\n",
      "Training set: Average loss: 0.019422\n",
      "Validation set: Average loss: 9.148105, Accuracy: 1370/1959 (69.93%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000083\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.003711\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000001\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000003\n",
      "\tTraining batch 15 Loss: 0.000048\n",
      "\tTraining batch 16 Loss: 0.002666\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000984\n",
      "\tTraining batch 19 Loss: 0.010838\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.226945\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.021092\n",
      "\tTraining batch 26 Loss: 0.012721\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000063\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000137\n",
      "\tTraining batch 31 Loss: 0.228334\n",
      "\tTraining batch 32 Loss: 0.056873\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000003\n",
      "\tTraining batch 35 Loss: 0.000011\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000002\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000035\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000175\n",
      "\tTraining batch 46 Loss: 0.000004\n",
      "\tTraining batch 47 Loss: 0.000001\n",
      "\tTraining batch 48 Loss: 0.000176\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.001855\n",
      "\tTraining batch 55 Loss: 0.000114\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000179\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000689\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000006\n",
      "\tTraining batch 62 Loss: 0.000133\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000077\n",
      "\tTraining batch 65 Loss: 0.000293\n",
      "Training set: Average loss: 0.008742\n",
      "Validation set: Average loss: 10.033161, Accuracy: 1362/1959 (69.53%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000001\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000082\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000001\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000002\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000020\n",
      "\tTraining batch 17 Loss: 0.001112\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.003668\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000001\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000148\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000032\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000071\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000001\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000061\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000296\n",
      "\tTraining batch 46 Loss: 0.000002\n",
      "\tTraining batch 47 Loss: 0.000004\n",
      "\tTraining batch 48 Loss: 0.000011\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000962\n",
      "\tTraining batch 55 Loss: 0.000058\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000148\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000190\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000007\n",
      "\tTraining batch 62 Loss: 0.000081\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000062\n",
      "\tTraining batch 65 Loss: 0.000067\n",
      "Training set: Average loss: 0.000109\n",
      "Validation set: Average loss: 10.102748, Accuracy: 1366/1959 (69.73%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000001\n",
      "\tTraining batch 3 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 4 Loss: 0.000041\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000002\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000002\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000011\n",
      "\tTraining batch 17 Loss: 0.000194\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.003200\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000001\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000145\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000034\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000069\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000054\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000075\n",
      "\tTraining batch 46 Loss: 0.000002\n",
      "\tTraining batch 47 Loss: 0.000006\n",
      "\tTraining batch 48 Loss: 0.000004\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000030\n",
      "\tTraining batch 55 Loss: 0.000014\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000048\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000162\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000004\n",
      "\tTraining batch 62 Loss: 0.000044\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000051\n",
      "\tTraining batch 65 Loss: 0.000049\n",
      "Training set: Average loss: 0.000065\n",
      "Validation set: Average loss: 10.173126, Accuracy: 1365/1959 (69.68%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000001\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000025\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000003\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000002\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000007\n",
      "\tTraining batch 17 Loss: 0.000136\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000024\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000001\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000149\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000034\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000066\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000049\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000067\n",
      "\tTraining batch 46 Loss: 0.000002\n",
      "\tTraining batch 47 Loss: 0.000006\n",
      "\tTraining batch 48 Loss: 0.000003\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000027\n",
      "\tTraining batch 55 Loss: 0.000008\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000045\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000142\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000004\n",
      "\tTraining batch 62 Loss: 0.000032\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000038\n",
      "\tTraining batch 65 Loss: 0.000039\n",
      "Training set: Average loss: 0.000014\n",
      "Validation set: Average loss: 10.179858, Accuracy: 1365/1959 (69.68%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 50.0%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000001\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000018\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000003\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000002\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000005\n",
      "\tTraining batch 17 Loss: 0.000105\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000023\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000001\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000148\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000034\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000063\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000046\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000066\n",
      "\tTraining batch 46 Loss: 0.000002\n",
      "\tTraining batch 47 Loss: 0.000006\n",
      "\tTraining batch 48 Loss: 0.000003\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000026\n",
      "\tTraining batch 55 Loss: 0.000006\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000042\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000128\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000004\n",
      "\tTraining batch 62 Loss: 0.000025\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000031\n",
      "\tTraining batch 65 Loss: 0.000032\n",
      "\tTraining batch 66 Loss: 7.627782\n",
      "Training set: Average loss: 0.115585\n",
      "Validation set: Average loss: 9.369327, Accuracy: 1380/1959 (70.44%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000129\n",
      "\tTraining batch 3 Loss: 0.035052\n",
      "\tTraining batch 4 Loss: 0.130274\n",
      "\tTraining batch 5 Loss: 0.000003\n",
      "\tTraining batch 6 Loss: 0.556319\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.437133\n",
      "\tTraining batch 9 Loss: 0.005062\n",
      "\tTraining batch 10 Loss: 0.138372\n",
      "\tTraining batch 11 Loss: 0.000104\n",
      "\tTraining batch 12 Loss: 0.000087\n",
      "\tTraining batch 13 Loss: 0.015843\n",
      "\tTraining batch 14 Loss: 0.001057\n",
      "\tTraining batch 15 Loss: 0.007698\n",
      "\tTraining batch 16 Loss: 0.517693\n",
      "\tTraining batch 17 Loss: 0.100386\n",
      "\tTraining batch 18 Loss: 0.050401\n",
      "\tTraining batch 19 Loss: 0.241743\n",
      "\tTraining batch 20 Loss: 0.036564\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000098\n",
      "\tTraining batch 23 Loss: 0.005760\n",
      "\tTraining batch 24 Loss: 0.369905\n",
      "\tTraining batch 25 Loss: 0.000001\n",
      "\tTraining batch 26 Loss: 0.311900\n",
      "\tTraining batch 27 Loss: 0.000127\n",
      "\tTraining batch 28 Loss: 0.000078\n",
      "\tTraining batch 29 Loss: 0.000022\n",
      "\tTraining batch 30 Loss: 0.235777\n",
      "\tTraining batch 31 Loss: 0.000038\n",
      "\tTraining batch 32 Loss: 0.214069\n",
      "\tTraining batch 33 Loss: 0.000076\n",
      "\tTraining batch 34 Loss: 0.050836\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.369200\n",
      "\tTraining batch 37 Loss: 0.006445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 38 Loss: 0.007753\n",
      "\tTraining batch 39 Loss: 0.000001\n",
      "\tTraining batch 40 Loss: 0.240424\n",
      "\tTraining batch 41 Loss: 0.000001\n",
      "\tTraining batch 42 Loss: 0.106969\n",
      "\tTraining batch 43 Loss: 0.135117\n",
      "\tTraining batch 44 Loss: 0.000505\n",
      "\tTraining batch 45 Loss: 0.000050\n",
      "\tTraining batch 46 Loss: 0.241157\n",
      "\tTraining batch 47 Loss: 0.167922\n",
      "\tTraining batch 48 Loss: 0.249868\n",
      "\tTraining batch 49 Loss: 0.114543\n",
      "\tTraining batch 50 Loss: 0.076297\n",
      "\tTraining batch 51 Loss: 0.000721\n",
      "\tTraining batch 52 Loss: 0.318602\n",
      "\tTraining batch 53 Loss: 0.003627\n",
      "\tTraining batch 54 Loss: 0.006323\n",
      "\tTraining batch 55 Loss: 0.060752\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.001700\n",
      "\tTraining batch 58 Loss: 0.171980\n",
      "\tTraining batch 59 Loss: 0.014034\n",
      "\tTraining batch 60 Loss: 0.058774\n",
      "\tTraining batch 61 Loss: 0.167637\n",
      "\tTraining batch 62 Loss: 0.100618\n",
      "\tTraining batch 63 Loss: 0.644600\n",
      "\tTraining batch 64 Loss: 0.059399\n",
      "\tTraining batch 65 Loss: 0.003731\n",
      "\tTraining batch 66 Loss: 0.032198\n",
      "Training set: Average loss: 0.103387\n",
      "Validation set: Average loss: 9.240155, Accuracy: 1377/1959 (70.29%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.034906\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000012\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000039\n",
      "\tTraining batch 7 Loss: 0.000066\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000004\n",
      "\tTraining batch 10 Loss: 0.048112\n",
      "\tTraining batch 11 Loss: 0.001890\n",
      "\tTraining batch 12 Loss: 0.260780\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000403\n",
      "\tTraining batch 15 Loss: 0.163346\n",
      "\tTraining batch 16 Loss: 0.000038\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000001\n",
      "\tTraining batch 19 Loss: 0.008231\n",
      "\tTraining batch 20 Loss: 0.000020\n",
      "\tTraining batch 21 Loss: 0.000003\n",
      "\tTraining batch 22 Loss: 0.000001\n",
      "\tTraining batch 23 Loss: 0.001258\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.002791\n",
      "\tTraining batch 27 Loss: 0.666005\n",
      "\tTraining batch 28 Loss: 0.176794\n",
      "\tTraining batch 29 Loss: 0.151708\n",
      "\tTraining batch 30 Loss: 0.000016\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.216471\n",
      "\tTraining batch 33 Loss: 0.000012\n",
      "\tTraining batch 34 Loss: 0.000041\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.001772\n",
      "\tTraining batch 38 Loss: 0.000487\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000001\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000005\n",
      "\tTraining batch 43 Loss: 0.010677\n",
      "\tTraining batch 44 Loss: 0.233641\n",
      "\tTraining batch 45 Loss: 0.000142\n",
      "\tTraining batch 46 Loss: 0.017444\n",
      "\tTraining batch 47 Loss: 0.000344\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.039498\n",
      "\tTraining batch 51 Loss: 0.000004\n",
      "\tTraining batch 52 Loss: 0.000467\n",
      "\tTraining batch 53 Loss: 0.000105\n",
      "\tTraining batch 54 Loss: 0.000910\n",
      "\tTraining batch 55 Loss: 0.011768\n",
      "\tTraining batch 56 Loss: 0.000001\n",
      "\tTraining batch 57 Loss: 0.346911\n",
      "\tTraining batch 58 Loss: 0.011601\n",
      "\tTraining batch 59 Loss: 0.074562\n",
      "\tTraining batch 60 Loss: 0.000010\n",
      "\tTraining batch 61 Loss: 0.000139\n",
      "\tTraining batch 62 Loss: 0.004475\n",
      "\tTraining batch 63 Loss: 0.023805\n",
      "\tTraining batch 64 Loss: 0.023105\n",
      "\tTraining batch 65 Loss: 0.130610\n",
      "\tTraining batch 66 Loss: 0.635248\n",
      "Training set: Average loss: 0.050010\n",
      "Validation set: Average loss: 9.438988, Accuracy: 1359/1959 (69.37%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.004870\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000020\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000001\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.038114\n",
      "\tTraining batch 11 Loss: 0.000012\n",
      "\tTraining batch 12 Loss: 0.000003\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000001\n",
      "\tTraining batch 15 Loss: 0.000479\n",
      "\tTraining batch 16 Loss: 0.107937\n",
      "\tTraining batch 17 Loss: 0.000454\n",
      "\tTraining batch 18 Loss: 0.000003\n",
      "\tTraining batch 19 Loss: 0.005832\n",
      "\tTraining batch 20 Loss: 0.000030\n",
      "\tTraining batch 21 Loss: 0.000007\n",
      "\tTraining batch 22 Loss: 0.000003\n",
      "\tTraining batch 23 Loss: 0.000029\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.001404\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000004\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000014\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000001\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.135657\n",
      "\tTraining batch 39 Loss: 0.003623\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000100\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000095\n",
      "\tTraining batch 44 Loss: 0.000011\n",
      "\tTraining batch 45 Loss: 0.000182\n",
      "\tTraining batch 46 Loss: 0.000287\n",
      "\tTraining batch 47 Loss: 0.000097\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000011\n",
      "\tTraining batch 51 Loss: 0.000001\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.087225\n",
      "\tTraining batch 54 Loss: 0.014160\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000754\n",
      "\tTraining batch 58 Loss: 0.000001\n",
      "\tTraining batch 59 Loss: 0.000213\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000297\n",
      "\tTraining batch 62 Loss: 0.000001\n",
      "\tTraining batch 63 Loss: 0.212365\n",
      "\tTraining batch 64 Loss: 0.205719\n",
      "\tTraining batch 65 Loss: 0.423842\n",
      "\tTraining batch 66 Loss: 0.253202\n",
      "Training set: Average loss: 0.022683\n",
      "Validation set: Average loss: 11.693420, Accuracy: 1386/1959 (70.75%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000328\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000431\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.002991\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000001\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.007704\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000014\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000634\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000001\n",
      "\tTraining batch 29 Loss: 0.000002\n",
      "\tTraining batch 30 Loss: 0.000004\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000585\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000342\n",
      "\tTraining batch 46 Loss: 0.000013\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.146212\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.080023\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.002129\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000210\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000044\n",
      "\tTraining batch 62 Loss: 0.000002\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000091\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000110\n",
      "Training set: Average loss: 0.003665\n",
      "Validation set: Average loss: 10.309391, Accuracy: 1395/1959 (71.21%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000003\n",
      "\tTraining batch 6 Loss: 0.000266\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000426\n",
      "\tTraining batch 17 Loss: 0.000006\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.004947\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.001205\n",
      "\tTraining batch 22 Loss: 0.000253\n",
      "\tTraining batch 23 Loss: 0.000003\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000979\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000002\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000028\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000001\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.169860\n",
      "\tTraining batch 40 Loss: 0.001478\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.015418\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000002\n",
      "\tTraining batch 47 Loss: 0.074902\n",
      "\tTraining batch 48 Loss: 0.000017\n",
      "\tTraining batch 49 Loss: 0.000042\n",
      "\tTraining batch 50 Loss: 0.097237\n",
      "\tTraining batch 51 Loss: 0.000011\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000075\n",
      "\tTraining batch 54 Loss: 0.395268\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000156\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.001388\n",
      "\tTraining batch 60 Loss: 0.000223\n",
      "\tTraining batch 61 Loss: 0.000613\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.047822\n",
      "\tTraining batch 65 Loss: 0.013630\n",
      "\tTraining batch 66 Loss: 0.000761\n",
      "Training set: Average loss: 0.012542\n",
      "Validation set: Average loss: 12.764081, Accuracy: 1369/1959 (69.88%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000001\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.049024\n",
      "\tTraining batch 4 Loss: 0.000087\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000004\n",
      "\tTraining batch 11 Loss: 0.115411\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000010\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000002\n",
      "\tTraining batch 17 Loss: 0.000001\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.015843\n",
      "\tTraining batch 20 Loss: 0.000008\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000011\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.001666\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.004891\n",
      "\tTraining batch 29 Loss: 0.209886\n",
      "\tTraining batch 30 Loss: 0.000005\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000001\n",
      "\tTraining batch 34 Loss: 0.000091\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000003\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000137\n",
      "\tTraining batch 41 Loss: 0.594405\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.055629\n",
      "\tTraining batch 44 Loss: 0.000069\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000005\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000002\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000004\n",
      "\tTraining batch 53 Loss: 0.000001\n",
      "\tTraining batch 54 Loss: 0.000046\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.001055\n",
      "\tTraining batch 58 Loss: 0.000007\n",
      "\tTraining batch 59 Loss: 0.000235\n",
      "\tTraining batch 60 Loss: 0.000001\n",
      "\tTraining batch 61 Loss: 0.000180\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.442960\n",
      "\tTraining batch 65 Loss: 0.000001\n",
      "\tTraining batch 66 Loss: 0.004886\n",
      "Training set: Average loss: 0.022675\n",
      "Validation set: Average loss: 12.059404, Accuracy: 1388/1959 (70.85%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000001\n",
      "\tTraining batch 2 Loss: 0.000048\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000004\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000001\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000001\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000061\n",
      "\tTraining batch 17 Loss: 0.000003\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000536\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000001\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000729\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000001\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000006\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.001681\n",
      "\tTraining batch 44 Loss: 0.069116\n",
      "\tTraining batch 45 Loss: 0.000004\n",
      "\tTraining batch 46 Loss: 0.000011\n",
      "\tTraining batch 47 Loss: 0.000001\n",
      "\tTraining batch 48 Loss: 0.008257\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000002\n",
      "\tTraining batch 54 Loss: 0.002462\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000349\n",
      "\tTraining batch 58 Loss: 0.000033\n",
      "\tTraining batch 59 Loss: 0.000339\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000182\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000130\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000384\n",
      "Training set: Average loss: 0.001278\n",
      "Validation set: Average loss: 12.111144, Accuracy: 1388/1959 (70.85%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 49.25%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000062\n",
      "\tTraining batch 2 Loss: 0.191656\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000024\n",
      "\tTraining batch 11 Loss: 0.000002\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000014\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000455\n",
      "\tTraining batch 17 Loss: 0.000002\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.003762\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000624\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.278319\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000114\n",
      "\tTraining batch 45 Loss: 0.000010\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.078829\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000008\n",
      "\tTraining batch 54 Loss: 0.000450\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000208\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.023252\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.001853\n",
      "\tTraining batch 63 Loss: 0.000001\n",
      "\tTraining batch 64 Loss: 0.055257\n",
      "\tTraining batch 65 Loss: 0.000005\n",
      "\tTraining batch 66 Loss: 0.243718\n",
      "\tTraining batch 67 Loss: 15.403165\n",
      "Training set: Average loss: 0.243012\n",
      "Validation set: Average loss: 12.192608, Accuracy: 1360/1959 (69.42%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.519151\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.005664\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000001\n",
      "\tTraining batch 11 Loss: 0.019495\n",
      "\tTraining batch 12 Loss: 0.830282\n",
      "\tTraining batch 13 Loss: 0.284205\n",
      "\tTraining batch 14 Loss: 0.129238\n",
      "\tTraining batch 15 Loss: 0.003415\n",
      "\tTraining batch 16 Loss: 0.781434\n",
      "\tTraining batch 17 Loss: 0.000025\n",
      "\tTraining batch 18 Loss: 0.001336\n",
      "\tTraining batch 19 Loss: 0.000016\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000034\n",
      "\tTraining batch 24 Loss: 0.000003\n",
      "\tTraining batch 25 Loss: 0.265864\n",
      "\tTraining batch 26 Loss: 0.113211\n",
      "\tTraining batch 27 Loss: 0.175488\n",
      "\tTraining batch 28 Loss: 0.010250\n",
      "\tTraining batch 29 Loss: 0.075675\n",
      "\tTraining batch 30 Loss: 0.002664\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.203420\n",
      "\tTraining batch 33 Loss: 0.005023\n",
      "\tTraining batch 34 Loss: 0.000004\n",
      "\tTraining batch 35 Loss: 0.028178\n",
      "\tTraining batch 36 Loss: 0.000065\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000259\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000050\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.003934\n",
      "\tTraining batch 44 Loss: 0.000002\n",
      "\tTraining batch 45 Loss: 0.000002\n",
      "\tTraining batch 46 Loss: 0.005428\n",
      "\tTraining batch 47 Loss: 0.000575\n",
      "\tTraining batch 48 Loss: 0.000039\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000046\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000001\n",
      "\tTraining batch 53 Loss: 0.053059\n",
      "\tTraining batch 54 Loss: 0.000024\n",
      "\tTraining batch 55 Loss: 0.000126\n",
      "\tTraining batch 56 Loss: 0.000004\n",
      "\tTraining batch 57 Loss: 0.029736\n",
      "\tTraining batch 58 Loss: 0.119790\n",
      "\tTraining batch 59 Loss: 0.088827\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000061\n",
      "\tTraining batch 62 Loss: 0.000004\n",
      "\tTraining batch 63 Loss: 0.000002\n",
      "\tTraining batch 64 Loss: 0.000001\n",
      "\tTraining batch 65 Loss: 0.000002\n",
      "\tTraining batch 66 Loss: 0.000134\n",
      "\tTraining batch 67 Loss: 0.388828\n",
      "Training set: Average loss: 0.061867\n",
      "Validation set: Average loss: 8.799450, Accuracy: 1374/1959 (70.14%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.082995\n",
      "\tTraining batch 3 Loss: 0.001306\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000001\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000219\n",
      "\tTraining batch 18 Loss: 0.000030\n",
      "\tTraining batch 19 Loss: 0.000013\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.003685\n",
      "\tTraining batch 22 Loss: 0.000005\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.009050\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000001\n",
      "\tTraining batch 30 Loss: 0.000013\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000003\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.004515\n",
      "\tTraining batch 37 Loss: 0.000002\n",
      "\tTraining batch 38 Loss: 0.000003\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000898\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000020\n",
      "\tTraining batch 44 Loss: 0.002124\n",
      "\tTraining batch 45 Loss: 0.000088\n",
      "\tTraining batch 46 Loss: 0.000688\n",
      "\tTraining batch 47 Loss: 0.000017\n",
      "\tTraining batch 48 Loss: 0.000008\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000016\n",
      "\tTraining batch 51 Loss: 0.001590\n",
      "\tTraining batch 52 Loss: 0.000007\n",
      "\tTraining batch 53 Loss: 0.000005\n",
      "\tTraining batch 54 Loss: 0.103967\n",
      "\tTraining batch 55 Loss: 0.000004\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.004934\n",
      "\tTraining batch 58 Loss: 0.000005\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000017\n",
      "\tTraining batch 61 Loss: 0.000015\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.001353\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.127209\n",
      "\tTraining batch 66 Loss: 0.001343\n",
      "\tTraining batch 67 Loss: 0.106042\n",
      "Training set: Average loss: 0.006749\n",
      "Validation set: Average loss: 9.562110, Accuracy: 1388/1959 (70.85%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000285\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.007514\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.045080\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.007728\n",
      "\tTraining batch 14 Loss: 0.000027\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000217\n",
      "\tTraining batch 18 Loss: 0.002177\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000069\n",
      "\tTraining batch 21 Loss: 0.080528\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000058\n",
      "\tTraining batch 26 Loss: 0.000111\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.014457\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000008\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000011\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000021\n",
      "\tTraining batch 47 Loss: 0.000007\n",
      "\tTraining batch 48 Loss: 0.000004\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000001\n",
      "\tTraining batch 52 Loss: 0.000001\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000007\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000016\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000007\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000521\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000006\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 66 Loss: 0.000112\n",
      "\tTraining batch 67 Loss: 0.205185\n",
      "Training set: Average loss: 0.005435\n",
      "Validation set: Average loss: 10.451724, Accuracy: 1382/1959 (70.55%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.019932\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000001\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000278\n",
      "\tTraining batch 20 Loss: 0.000273\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000609\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000714\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000004\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000002\n",
      "\tTraining batch 34 Loss: 0.000004\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000001\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000003\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000021\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.007468\n",
      "\tTraining batch 47 Loss: 0.000001\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000018\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000001\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000006\n",
      "\tTraining batch 58 Loss: 0.000278\n",
      "\tTraining batch 59 Loss: 0.000006\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.038410\n",
      "\tTraining batch 62 Loss: 0.009223\n",
      "\tTraining batch 63 Loss: 0.269812\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000373\n",
      "\tTraining batch 66 Loss: 0.008725\n",
      "\tTraining batch 67 Loss: 0.053425\n",
      "Training set: Average loss: 0.006113\n",
      "Validation set: Average loss: 10.417981, Accuracy: 1381/1959 (70.50%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000016\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.061550\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.169496\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000101\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000003\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000001\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.006814\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000022\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000002\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.001305\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000001\n",
      "\tTraining batch 43 Loss: 0.000816\n",
      "\tTraining batch 44 Loss: 0.000062\n",
      "\tTraining batch 45 Loss: 0.000002\n",
      "\tTraining batch 46 Loss: 0.000120\n",
      "\tTraining batch 47 Loss: 0.000999\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000001\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000001\n",
      "\tTraining batch 52 Loss: 0.000005\n",
      "\tTraining batch 53 Loss: 0.180207\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000103\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000005\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.001723\n",
      "\tTraining batch 62 Loss: 0.000050\n",
      "\tTraining batch 63 Loss: 0.000007\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000206\n",
      "\tTraining batch 66 Loss: 0.000134\n",
      "\tTraining batch 67 Loss: 0.000026\n",
      "Training set: Average loss: 0.006325\n",
      "Validation set: Average loss: 11.240974, Accuracy: 1363/1959 (69.58%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.001300\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.344686\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000001\n",
      "\tTraining batch 12 Loss: 0.000002\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.012236\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000435\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000011\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000121\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.006709\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000022\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.123105\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000003\n",
      "\tTraining batch 47 Loss: 0.000212\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.051610\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000025\n",
      "\tTraining batch 61 Loss: 0.000877\n",
      "\tTraining batch 62 Loss: 0.000001\n",
      "\tTraining batch 63 Loss: 0.000138\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000045\n",
      "\tTraining batch 66 Loss: 0.300910\n",
      "\tTraining batch 67 Loss: 0.177615\n",
      "Training set: Average loss: 0.015225\n",
      "Validation set: Average loss: 13.864972, Accuracy: 1379/1959 (70.39%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000969\n",
      "\tTraining batch 10 Loss: 0.000010\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 27 Loss: 0.001074\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000001\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000164\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000001\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000006\n",
      "\tTraining batch 47 Loss: 0.000042\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.086341\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000002\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000004\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000017\n",
      "\tTraining batch 61 Loss: 0.000002\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000005\n",
      "\tTraining batch 64 Loss: 0.000018\n",
      "\tTraining batch 65 Loss: 0.029058\n",
      "\tTraining batch 66 Loss: 0.000022\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "Training set: Average loss: 0.001757\n",
      "Validation set: Average loss: 13.583019, Accuracy: 1375/1959 (70.19%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.002235\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000010\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000021\n",
      "\tTraining batch 14 Loss: 0.000918\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000801\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000091\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.063669\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000007\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000001\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.003872\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000745\n",
      "\tTraining batch 61 Loss: 0.000053\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000020\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000009\n",
      "\tTraining batch 66 Loss: 0.000201\n",
      "\tTraining batch 67 Loss: 0.000037\n",
      "Training set: Average loss: 0.001085\n",
      "Validation set: Average loss: 13.937784, Accuracy: 1389/1959 (70.90%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.004329\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000002\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000001\n",
      "\tTraining batch 41 Loss: 0.000047\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000002\n",
      "\tTraining batch 47 Loss: 0.000001\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000001\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000005\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000004\n",
      "\tTraining batch 61 Loss: 0.000064\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000009\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000021\n",
      "\tTraining batch 66 Loss: 0.000009\n",
      "\tTraining batch 67 Loss: 0.000025\n",
      "Training set: Average loss: 0.000067\n",
      "Validation set: Average loss: 13.827886, Accuracy: 1393/1959 (71.11%)\n",
      "\n",
      "Epoch: 11\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000001\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000002\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000001\n",
      "\tTraining batch 41 Loss: 0.000026\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000002\n",
      "\tTraining batch 47 Loss: 0.000001\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000001\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 58 Loss: 0.000004\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000004\n",
      "\tTraining batch 61 Loss: 0.000045\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000008\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000015\n",
      "\tTraining batch 66 Loss: 0.000009\n",
      "\tTraining batch 67 Loss: 0.000017\n",
      "Training set: Average loss: 0.000002\n",
      "Validation set: Average loss: 13.832173, Accuracy: 1393/1959 (71.11%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 50.5%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000001\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000002\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000001\n",
      "\tTraining batch 41 Loss: 0.000017\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000002\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000001\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000004\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000003\n",
      "\tTraining batch 61 Loss: 0.000035\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000007\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000012\n",
      "\tTraining batch 66 Loss: 0.000009\n",
      "\tTraining batch 67 Loss: 0.000013\n",
      "\tTraining batch 68 Loss: 19.127628\n",
      "Training set: Average loss: 0.281290\n",
      "Validation set: Average loss: 12.248041, Accuracy: 1385/1959 (70.70%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000044\n",
      "\tTraining batch 3 Loss: 0.014232\n",
      "\tTraining batch 4 Loss: 0.000086\n",
      "\tTraining batch 5 Loss: 0.000022\n",
      "\tTraining batch 6 Loss: 0.146584\n",
      "\tTraining batch 7 Loss: 0.304893\n",
      "\tTraining batch 8 Loss: 0.516888\n",
      "\tTraining batch 9 Loss: 0.090240\n",
      "\tTraining batch 10 Loss: 0.413880\n",
      "\tTraining batch 11 Loss: 0.001773\n",
      "\tTraining batch 12 Loss: 0.012632\n",
      "\tTraining batch 13 Loss: 0.081593\n",
      "\tTraining batch 14 Loss: 0.065808\n",
      "\tTraining batch 15 Loss: 0.106626\n",
      "\tTraining batch 16 Loss: 0.000374\n",
      "\tTraining batch 17 Loss: 0.269937\n",
      "\tTraining batch 18 Loss: 0.000007\n",
      "\tTraining batch 19 Loss: 0.005890\n",
      "\tTraining batch 20 Loss: 0.094025\n",
      "\tTraining batch 21 Loss: 0.022035\n",
      "\tTraining batch 22 Loss: 0.110949\n",
      "\tTraining batch 23 Loss: 0.002493\n",
      "\tTraining batch 24 Loss: 0.004237\n",
      "\tTraining batch 25 Loss: 0.067673\n",
      "\tTraining batch 26 Loss: 0.002287\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000009\n",
      "\tTraining batch 29 Loss: 0.000001\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.000082\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000001\n",
      "\tTraining batch 34 Loss: 0.000227\n",
      "\tTraining batch 35 Loss: 0.000243\n",
      "\tTraining batch 36 Loss: 0.000101\n",
      "\tTraining batch 37 Loss: 0.000018\n",
      "\tTraining batch 38 Loss: 0.000005\n",
      "\tTraining batch 39 Loss: 0.006578\n",
      "\tTraining batch 40 Loss: 0.018082\n",
      "\tTraining batch 41 Loss: 0.027230\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.001153\n",
      "\tTraining batch 44 Loss: 0.001108\n",
      "\tTraining batch 45 Loss: 0.087479\n",
      "\tTraining batch 46 Loss: 0.085949\n",
      "\tTraining batch 47 Loss: 0.001747\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.395018\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000001\n",
      "\tTraining batch 52 Loss: 0.000262\n",
      "\tTraining batch 53 Loss: 0.002174\n",
      "\tTraining batch 54 Loss: 0.004809\n",
      "\tTraining batch 55 Loss: 0.000031\n",
      "\tTraining batch 56 Loss: 0.000001\n",
      "\tTraining batch 57 Loss: 0.129034\n",
      "\tTraining batch 58 Loss: 0.000091\n",
      "\tTraining batch 59 Loss: 0.382324\n",
      "\tTraining batch 60 Loss: 0.000020\n",
      "\tTraining batch 61 Loss: 0.014924\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.049696\n",
      "\tTraining batch 64 Loss: 0.001550\n",
      "\tTraining batch 65 Loss: 0.007191\n",
      "\tTraining batch 66 Loss: 0.000399\n",
      "\tTraining batch 67 Loss: 0.153626\n",
      "\tTraining batch 68 Loss: 0.121306\n",
      "Training set: Average loss: 0.056289\n",
      "Validation set: Average loss: 11.397247, Accuracy: 1343/1959 (68.56%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.031189\n",
      "\tTraining batch 5 Loss: 0.000001\n",
      "\tTraining batch 6 Loss: 0.000442\n",
      "\tTraining batch 7 Loss: 0.118151\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000236\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000001\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000488\n",
      "\tTraining batch 19 Loss: 0.000456\n",
      "\tTraining batch 20 Loss: 0.280801\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000004\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000001\n",
      "\tTraining batch 38 Loss: 0.000008\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.257028\n",
      "\tTraining batch 41 Loss: 0.000006\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000011\n",
      "\tTraining batch 46 Loss: 0.109687\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000010\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000473\n",
      "\tTraining batch 53 Loss: 0.067710\n",
      "\tTraining batch 54 Loss: 0.000228\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.348586\n",
      "\tTraining batch 57 Loss: 0.000603\n",
      "\tTraining batch 58 Loss: 0.000029\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000002\n",
      "\tTraining batch 61 Loss: 0.009835\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000129\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000135\n",
      "Training set: Average loss: 0.018033\n",
      "Validation set: Average loss: 12.058538, Accuracy: 1380/1959 (70.44%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000012\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.451593\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001229\n",
      "\tTraining batch 20 Loss: 0.000187\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000012\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000110\n",
      "\tTraining batch 34 Loss: 0.008888\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000003\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000027\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000248\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000412\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000807\n",
      "\tTraining batch 58 Loss: 0.003053\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000242\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000002\n",
      "Training set: Average loss: 0.006865\n",
      "Validation set: Average loss: 11.901257, Accuracy: 1389/1959 (70.90%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000001\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000007\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000015\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000002\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000994\n",
      "\tTraining batch 20 Loss: 0.000002\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000033\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000002\n",
      "\tTraining batch 46 Loss: 0.000001\n",
      "\tTraining batch 47 Loss: 0.000002\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000003\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000032\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000308\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000590\n",
      "\tTraining batch 58 Loss: 0.000012\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000116\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000002\n",
      "Training set: Average loss: 0.000031\n",
      "Validation set: Average loss: 11.913852, Accuracy: 1390/1959 (70.95%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000005\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000015\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000002\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000938\n",
      "\tTraining batch 20 Loss: 0.000002\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000030\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000002\n",
      "\tTraining batch 46 Loss: 0.000002\n",
      "\tTraining batch 47 Loss: 0.000002\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000003\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000025\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000289\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000549\n",
      "\tTraining batch 58 Loss: 0.000012\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000101\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000002\n",
      "Training set: Average loss: 0.000029\n",
      "Validation set: Average loss: 11.925632, Accuracy: 1390/1959 (70.95%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 50.74999999999999%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000005\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000012\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000002\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000893\n",
      "\tTraining batch 20 Loss: 0.000002\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000028\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000002\n",
      "\tTraining batch 46 Loss: 0.000002\n",
      "\tTraining batch 47 Loss: 0.000002\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000003\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000021\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000274\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000513\n",
      "\tTraining batch 58 Loss: 0.000011\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000089\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000002\n",
      "\tTraining batch 69 Loss: 20.615978\n",
      "Training set: Average loss: 0.298809\n",
      "Validation set: Average loss: 10.579778, Accuracy: 1372/1959 (70.04%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000013\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000002\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000351\n",
      "\tTraining batch 6 Loss: 0.000746\n",
      "\tTraining batch 7 Loss: 0.000287\n",
      "\tTraining batch 8 Loss: 0.000110\n",
      "\tTraining batch 9 Loss: 0.037618\n",
      "\tTraining batch 10 Loss: 0.000137\n",
      "\tTraining batch 11 Loss: 0.016223\n",
      "\tTraining batch 12 Loss: 0.107666\n",
      "\tTraining batch 13 Loss: 0.001310\n",
      "\tTraining batch 14 Loss: 0.058840\n",
      "\tTraining batch 15 Loss: 0.519895\n",
      "\tTraining batch 16 Loss: 0.008603\n",
      "\tTraining batch 17 Loss: 0.129108\n",
      "\tTraining batch 18 Loss: 0.184300\n",
      "\tTraining batch 19 Loss: 0.246570\n",
      "\tTraining batch 20 Loss: 0.028504\n",
      "\tTraining batch 21 Loss: 0.005783\n",
      "\tTraining batch 22 Loss: 0.016625\n",
      "\tTraining batch 23 Loss: 0.285214\n",
      "\tTraining batch 24 Loss: 0.041611\n",
      "\tTraining batch 25 Loss: 0.000356\n",
      "\tTraining batch 26 Loss: 0.034219\n",
      "\tTraining batch 27 Loss: 0.250936\n",
      "\tTraining batch 28 Loss: 0.477322\n",
      "\tTraining batch 29 Loss: 1.139727\n",
      "\tTraining batch 30 Loss: 0.002546\n",
      "\tTraining batch 31 Loss: 0.001429\n",
      "\tTraining batch 32 Loss: 0.213371\n",
      "\tTraining batch 33 Loss: 0.217163\n",
      "\tTraining batch 34 Loss: 0.235483\n",
      "\tTraining batch 35 Loss: 0.155791\n",
      "\tTraining batch 36 Loss: 0.000752\n",
      "\tTraining batch 37 Loss: 0.289368\n",
      "\tTraining batch 38 Loss: 0.003250\n",
      "\tTraining batch 39 Loss: 0.082806\n",
      "\tTraining batch 40 Loss: 0.001741\n",
      "\tTraining batch 41 Loss: 0.152186\n",
      "\tTraining batch 42 Loss: 0.004770\n",
      "\tTraining batch 43 Loss: 0.011950\n",
      "\tTraining batch 44 Loss: 0.099949\n",
      "\tTraining batch 45 Loss: 0.002536\n",
      "\tTraining batch 46 Loss: 0.044576\n",
      "\tTraining batch 47 Loss: 0.010369\n",
      "\tTraining batch 48 Loss: 0.002995\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.005388\n",
      "\tTraining batch 51 Loss: 0.081553\n",
      "\tTraining batch 52 Loss: 0.316491\n",
      "\tTraining batch 53 Loss: 0.085517\n",
      "\tTraining batch 54 Loss: 0.018346\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000026\n",
      "\tTraining batch 57 Loss: 0.024976\n",
      "\tTraining batch 58 Loss: 0.017281\n",
      "\tTraining batch 59 Loss: 0.001844\n",
      "\tTraining batch 60 Loss: 0.000240\n",
      "\tTraining batch 61 Loss: 0.084560\n",
      "\tTraining batch 62 Loss: 0.000007\n",
      "\tTraining batch 63 Loss: 0.000086\n",
      "\tTraining batch 64 Loss: 0.037680\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.001527\n",
      "\tTraining batch 67 Loss: 0.007301\n",
      "\tTraining batch 68 Loss: 0.339987\n",
      "\tTraining batch 69 Loss: 0.829675\n",
      "Training set: Average loss: 0.101125\n",
      "Validation set: Average loss: 10.207456, Accuracy: 1374/1959 (70.14%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.010990\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000071\n",
      "\tTraining batch 5 Loss: 0.000015\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000165\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000222\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000001\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000011\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000019\n",
      "\tTraining batch 17 Loss: 0.000001\n",
      "\tTraining batch 18 Loss: 0.000001\n",
      "\tTraining batch 19 Loss: 0.011855\n",
      "\tTraining batch 20 Loss: 0.000817\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000586\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000052\n",
      "\tTraining batch 26 Loss: 0.268704\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000007\n",
      "\tTraining batch 29 Loss: 0.002296\n",
      "\tTraining batch 30 Loss: 0.181562\n",
      "\tTraining batch 31 Loss: 0.255340\n",
      "\tTraining batch 32 Loss: 0.000035\n",
      "\tTraining batch 33 Loss: 0.005484\n",
      "\tTraining batch 34 Loss: 0.181946\n",
      "\tTraining batch 35 Loss: 0.068159\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000117\n",
      "\tTraining batch 38 Loss: 0.000022\n",
      "\tTraining batch 39 Loss: 0.000003\n",
      "\tTraining batch 40 Loss: 0.000039\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000970\n",
      "\tTraining batch 43 Loss: 0.000165\n",
      "\tTraining batch 44 Loss: 0.000002\n",
      "\tTraining batch 45 Loss: 0.001101\n",
      "\tTraining batch 46 Loss: 0.003973\n",
      "\tTraining batch 47 Loss: 0.000371\n",
      "\tTraining batch 48 Loss: 0.000602\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.398970\n",
      "\tTraining batch 52 Loss: 0.015612\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.005962\n",
      "\tTraining batch 55 Loss: 0.009809\n",
      "\tTraining batch 56 Loss: 0.000034\n",
      "\tTraining batch 57 Loss: 0.000973\n",
      "\tTraining batch 58 Loss: 0.121584\n",
      "\tTraining batch 59 Loss: 0.000001\n",
      "\tTraining batch 60 Loss: 0.099800\n",
      "\tTraining batch 61 Loss: 0.000076\n",
      "\tTraining batch 62 Loss: 0.048576\n",
      "\tTraining batch 63 Loss: 0.287880\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000007\n",
      "\tTraining batch 66 Loss: 0.621908\n",
      "\tTraining batch 67 Loss: 0.002842\n",
      "\tTraining batch 68 Loss: 0.017640\n",
      "\tTraining batch 69 Loss: 1.627482\n",
      "Training set: Average loss: 0.061665\n",
      "Validation set: Average loss: 12.420432, Accuracy: 1356/1959 (69.22%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.412651\n",
      "\tTraining batch 2 Loss: 0.000019\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.006034\n",
      "\tTraining batch 5 Loss: 0.173647\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000287\n",
      "\tTraining batch 8 Loss: 0.005633\n",
      "\tTraining batch 9 Loss: 0.000005\n",
      "\tTraining batch 10 Loss: 0.456233\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000386\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000038\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000017\n",
      "\tTraining batch 17 Loss: 0.000022\n",
      "\tTraining batch 18 Loss: 0.029283\n",
      "\tTraining batch 19 Loss: 0.386797\n",
      "\tTraining batch 20 Loss: 0.000012\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000013\n",
      "\tTraining batch 23 Loss: 0.000001\n",
      "\tTraining batch 24 Loss: 0.000018\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.037792\n",
      "\tTraining batch 27 Loss: 0.000006\n",
      "\tTraining batch 28 Loss: 0.084111\n",
      "\tTraining batch 29 Loss: 0.000004\n",
      "\tTraining batch 30 Loss: 0.089887\n",
      "\tTraining batch 31 Loss: 0.000022\n",
      "\tTraining batch 32 Loss: 0.000001\n",
      "\tTraining batch 33 Loss: 0.000013\n",
      "\tTraining batch 34 Loss: 0.001372\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000002\n",
      "\tTraining batch 37 Loss: 0.289647\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000023\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000380\n",
      "\tTraining batch 44 Loss: 0.002549\n",
      "\tTraining batch 45 Loss: 0.000002\n",
      "\tTraining batch 46 Loss: 0.247255\n",
      "\tTraining batch 47 Loss: 0.003607\n",
      "\tTraining batch 48 Loss: 0.000028\n",
      "\tTraining batch 49 Loss: 1.753901\n",
      "\tTraining batch 50 Loss: 0.028826\n",
      "\tTraining batch 51 Loss: 0.000041\n",
      "\tTraining batch 52 Loss: 0.000456\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.004785\n",
      "\tTraining batch 55 Loss: 0.004829\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.001033\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000001\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.003363\n",
      "\tTraining batch 62 Loss: 0.256747\n",
      "\tTraining batch 63 Loss: 0.118749\n",
      "\tTraining batch 64 Loss: 0.173016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 65 Loss: 0.179606\n",
      "\tTraining batch 66 Loss: 0.000117\n",
      "\tTraining batch 67 Loss: 1.033389\n",
      "\tTraining batch 68 Loss: 0.678995\n",
      "\tTraining batch 69 Loss: 0.000002\n",
      "Training set: Average loss: 0.093705\n",
      "Validation set: Average loss: 12.132113, Accuracy: 1370/1959 (69.93%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.039500\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000035\n",
      "\tTraining batch 5 Loss: 0.577059\n",
      "\tTraining batch 6 Loss: 0.388291\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.040339\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.315290\n",
      "\tTraining batch 11 Loss: 0.000001\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000152\n",
      "\tTraining batch 14 Loss: 0.192649\n",
      "\tTraining batch 15 Loss: 0.000002\n",
      "\tTraining batch 16 Loss: 0.000017\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.003027\n",
      "\tTraining batch 19 Loss: 0.007161\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000049\n",
      "\tTraining batch 22 Loss: 0.000002\n",
      "\tTraining batch 23 Loss: 1.059380\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.005228\n",
      "\tTraining batch 27 Loss: 0.000001\n",
      "\tTraining batch 28 Loss: 0.000190\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000021\n",
      "\tTraining batch 33 Loss: 0.000025\n",
      "\tTraining batch 34 Loss: 0.033249\n",
      "\tTraining batch 35 Loss: 0.104315\n",
      "\tTraining batch 36 Loss: 0.000034\n",
      "\tTraining batch 37 Loss: 0.000072\n",
      "\tTraining batch 38 Loss: 0.002735\n",
      "\tTraining batch 39 Loss: 0.000014\n",
      "\tTraining batch 40 Loss: 0.294038\n",
      "\tTraining batch 41 Loss: 0.000005\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.008724\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.051411\n",
      "\tTraining batch 46 Loss: 0.000004\n",
      "\tTraining batch 47 Loss: 0.000186\n",
      "\tTraining batch 48 Loss: 0.407655\n",
      "\tTraining batch 49 Loss: 0.000354\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000023\n",
      "\tTraining batch 52 Loss: 0.010917\n",
      "\tTraining batch 53 Loss: 0.000002\n",
      "\tTraining batch 54 Loss: 0.005309\n",
      "\tTraining batch 55 Loss: 0.000006\n",
      "\tTraining batch 56 Loss: 0.000100\n",
      "\tTraining batch 57 Loss: 0.002741\n",
      "\tTraining batch 58 Loss: 0.000049\n",
      "\tTraining batch 59 Loss: 0.013367\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.417000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000138\n",
      "\tTraining batch 65 Loss: 0.000025\n",
      "\tTraining batch 66 Loss: 0.000045\n",
      "\tTraining batch 67 Loss: 0.000001\n",
      "\tTraining batch 68 Loss: 0.000123\n",
      "\tTraining batch 69 Loss: 1.107598\n",
      "Training set: Average loss: 0.073749\n",
      "Validation set: Average loss: 11.257961, Accuracy: 1356/1959 (69.22%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000020\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.002498\n",
      "\tTraining batch 4 Loss: 0.054756\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000001\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000010\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000002\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000019\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000053\n",
      "\tTraining batch 16 Loss: 0.000001\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.011214\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000001\n",
      "\tTraining batch 22 Loss: 0.000030\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000758\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000003\n",
      "\tTraining batch 29 Loss: 0.000001\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.000025\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000042\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.046215\n",
      "\tTraining batch 38 Loss: 0.000071\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.141256\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000525\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000438\n",
      "\tTraining batch 46 Loss: 0.078257\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000003\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000001\n",
      "\tTraining batch 52 Loss: 0.037700\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.003418\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000287\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000092\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.001765\n",
      "\tTraining batch 66 Loss: 0.000020\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000063\n",
      "\tTraining batch 69 Loss: 0.000905\n",
      "Training set: Average loss: 0.005514\n",
      "Validation set: Average loss: 11.760387, Accuracy: 1349/1959 (68.86%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000062\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.069637\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000024\n",
      "\tTraining batch 23 Loss: 0.098472\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000002\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000012\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.001007\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000002\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000019\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000295\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.008891\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000076\n",
      "\tTraining batch 60 Loss: 0.000004\n",
      "\tTraining batch 61 Loss: 0.000001\n",
      "\tTraining batch 62 Loss: 0.000080\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000001\n",
      "\tTraining batch 65 Loss: 0.000248\n",
      "\tTraining batch 66 Loss: 0.000134\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000004\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "Training set: Average loss: 0.002594\n",
      "Validation set: Average loss: 11.540879, Accuracy: 1360/1959 (69.42%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000001\n",
      "\tTraining batch 4 Loss: 0.008692\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000005\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000014\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000333\n",
      "\tTraining batch 17 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000008\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000006\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000004\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000007\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000013\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000369\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000001\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000002\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000005\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000001\n",
      "\tTraining batch 57 Loss: 0.000072\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000015\n",
      "\tTraining batch 60 Loss: 0.000001\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000010\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000009\n",
      "\tTraining batch 66 Loss: 0.000050\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000005\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "Training set: Average loss: 0.000139\n",
      "Validation set: Average loss: 11.583176, Accuracy: 1364/1959 (69.63%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000068\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000502\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000006\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000004\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000004\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000007\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000012\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000347\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000001\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000002\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000004\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000001\n",
      "\tTraining batch 57 Loss: 0.000064\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000015\n",
      "\tTraining batch 60 Loss: 0.000001\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000006\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000008\n",
      "\tTraining batch 66 Loss: 0.000048\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000005\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "Training set: Average loss: 0.000016\n",
      "Validation set: Average loss: 11.592711, Accuracy: 1363/1959 (69.58%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000047\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000166\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000005\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000003\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000004\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000006\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000012\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000327\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000001\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000002\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000004\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000001\n",
      "\tTraining batch 57 Loss: 0.000059\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000014\n",
      "\tTraining batch 60 Loss: 0.000001\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000004\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000008\n",
      "\tTraining batch 66 Loss: 0.000046\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000005\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "Training set: Average loss: 0.000010\n",
      "Validation set: Average loss: 11.597048, Accuracy: 1364/1959 (69.63%)\n",
      "\n",
      "Epoch: 11\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000037\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000113\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000004\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000003\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000003\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000006\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000012\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 43 Loss: 0.000308\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000001\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000002\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000003\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000001\n",
      "\tTraining batch 57 Loss: 0.000055\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000014\n",
      "\tTraining batch 60 Loss: 0.000001\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000004\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000008\n",
      "\tTraining batch 66 Loss: 0.000045\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000004\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "Training set: Average loss: 0.000009\n",
      "Validation set: Average loss: 11.600847, Accuracy: 1363/1959 (69.58%)\n",
      "\n",
      "Epoch: 12\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000030\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000086\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000004\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000003\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000003\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000005\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000011\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000290\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000001\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000002\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000003\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000001\n",
      "\tTraining batch 57 Loss: 0.000051\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000013\n",
      "\tTraining batch 60 Loss: 0.000001\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000003\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000007\n",
      "\tTraining batch 66 Loss: 0.000043\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000004\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "Training set: Average loss: 0.000008\n",
      "Validation set: Average loss: 11.604384, Accuracy: 1363/1959 (69.58%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 47.75%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000026\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000069\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000004\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000002\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000003\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000005\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000011\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000274\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000001\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000002\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000003\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000001\n",
      "\tTraining batch 57 Loss: 0.000048\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000013\n",
      "\tTraining batch 60 Loss: 0.000001\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000003\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000007\n",
      "\tTraining batch 66 Loss: 0.000042\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000004\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 14.723409\n",
      "Training set: Average loss: 0.210342\n",
      "Validation set: Average loss: 10.564864, Accuracy: 1371/1959 (69.98%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.004344\n",
      "\tTraining batch 3 Loss: 0.000932\n",
      "\tTraining batch 4 Loss: 0.000607\n",
      "\tTraining batch 5 Loss: 0.000230\n",
      "\tTraining batch 6 Loss: 0.001937\n",
      "\tTraining batch 7 Loss: 0.000616\n",
      "\tTraining batch 8 Loss: 0.000157\n",
      "\tTraining batch 9 Loss: 0.002620\n",
      "\tTraining batch 10 Loss: 0.419951\n",
      "\tTraining batch 11 Loss: 0.290027\n",
      "\tTraining batch 12 Loss: 0.197817\n",
      "\tTraining batch 13 Loss: 0.155945\n",
      "\tTraining batch 14 Loss: 0.001133\n",
      "\tTraining batch 15 Loss: 0.287443\n",
      "\tTraining batch 16 Loss: 0.157699\n",
      "\tTraining batch 17 Loss: 0.545353\n",
      "\tTraining batch 18 Loss: 1.041396\n",
      "\tTraining batch 19 Loss: 0.201833\n",
      "\tTraining batch 20 Loss: 0.409800\n",
      "\tTraining batch 21 Loss: 0.005383\n",
      "\tTraining batch 22 Loss: 1.210508\n",
      "\tTraining batch 23 Loss: 0.688335\n",
      "\tTraining batch 24 Loss: 0.288792\n",
      "\tTraining batch 25 Loss: 0.058208\n",
      "\tTraining batch 26 Loss: 0.063260\n",
      "\tTraining batch 27 Loss: 0.634941\n",
      "\tTraining batch 28 Loss: 0.119085\n",
      "\tTraining batch 29 Loss: 0.104092\n",
      "\tTraining batch 30 Loss: 0.099391\n",
      "\tTraining batch 31 Loss: 0.027371\n",
      "\tTraining batch 32 Loss: 0.222425\n",
      "\tTraining batch 33 Loss: 0.000003\n",
      "\tTraining batch 34 Loss: 0.006105\n",
      "\tTraining batch 35 Loss: 0.453095\n",
      "\tTraining batch 36 Loss: 0.209406\n",
      "\tTraining batch 37 Loss: 0.000012\n",
      "\tTraining batch 38 Loss: 0.000032\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.468645\n",
      "\tTraining batch 41 Loss: 0.000164\n",
      "\tTraining batch 42 Loss: 0.015901\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.308168\n",
      "\tTraining batch 45 Loss: 0.000004\n",
      "\tTraining batch 46 Loss: 0.267015\n",
      "\tTraining batch 47 Loss: 0.000003\n",
      "\tTraining batch 48 Loss: 0.072865\n",
      "\tTraining batch 49 Loss: 0.131572\n",
      "\tTraining batch 50 Loss: 0.191224\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000014\n",
      "\tTraining batch 54 Loss: 0.000030\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.068350\n",
      "\tTraining batch 57 Loss: 0.005179\n",
      "\tTraining batch 58 Loss: 0.000056\n",
      "\tTraining batch 59 Loss: 0.029527\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000016\n",
      "\tTraining batch 62 Loss: 0.344124\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.156946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 65 Loss: 0.749446\n",
      "\tTraining batch 66 Loss: 0.261052\n",
      "\tTraining batch 67 Loss: 0.000019\n",
      "\tTraining batch 68 Loss: 0.001307\n",
      "\tTraining batch 69 Loss: 0.222945\n",
      "\tTraining batch 70 Loss: 3.103668\n",
      "Training set: Average loss: 0.204408\n",
      "Validation set: Average loss: 13.412454, Accuracy: 1348/1959 (68.81%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000211\n",
      "\tTraining batch 5 Loss: 0.035497\n",
      "\tTraining batch 6 Loss: 0.342173\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000005\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000014\n",
      "\tTraining batch 19 Loss: 0.000209\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000001\n",
      "\tTraining batch 23 Loss: 0.005394\n",
      "\tTraining batch 24 Loss: 0.003939\n",
      "\tTraining batch 25 Loss: 0.497832\n",
      "\tTraining batch 26 Loss: 0.043605\n",
      "\tTraining batch 27 Loss: 0.000195\n",
      "\tTraining batch 28 Loss: 0.000002\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000209\n",
      "\tTraining batch 31 Loss: 0.000074\n",
      "\tTraining batch 32 Loss: 1.487524\n",
      "\tTraining batch 33 Loss: 0.044912\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000005\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000001\n",
      "\tTraining batch 39 Loss: 0.337748\n",
      "\tTraining batch 40 Loss: 0.000133\n",
      "\tTraining batch 41 Loss: 0.000386\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.351637\n",
      "\tTraining batch 46 Loss: 0.052862\n",
      "\tTraining batch 47 Loss: 0.088072\n",
      "\tTraining batch 48 Loss: 0.000002\n",
      "\tTraining batch 49 Loss: 0.000007\n",
      "\tTraining batch 50 Loss: 0.000017\n",
      "\tTraining batch 51 Loss: 0.117740\n",
      "\tTraining batch 52 Loss: 0.000018\n",
      "\tTraining batch 53 Loss: 0.000002\n",
      "\tTraining batch 54 Loss: 0.000718\n",
      "\tTraining batch 55 Loss: 0.000014\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.004210\n",
      "\tTraining batch 58 Loss: 0.001469\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.153856\n",
      "\tTraining batch 62 Loss: 0.000006\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000003\n",
      "\tTraining batch 66 Loss: 0.000107\n",
      "\tTraining batch 67 Loss: 0.007731\n",
      "\tTraining batch 68 Loss: 0.192220\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "Training set: Average loss: 0.053868\n",
      "Validation set: Average loss: 12.919818, Accuracy: 1351/1959 (68.96%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.018645\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000005\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000672\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000001\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000072\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.002321\n",
      "\tTraining batch 19 Loss: 0.180278\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.294249\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000005\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000012\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000419\n",
      "\tTraining batch 29 Loss: 0.002902\n",
      "\tTraining batch 30 Loss: 0.317054\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000002\n",
      "\tTraining batch 33 Loss: 0.163770\n",
      "\tTraining batch 34 Loss: 0.000040\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.005535\n",
      "\tTraining batch 37 Loss: 0.000031\n",
      "\tTraining batch 38 Loss: 0.000006\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000012\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.004578\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000057\n",
      "\tTraining batch 48 Loss: 0.001014\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.203628\n",
      "\tTraining batch 52 Loss: 0.295762\n",
      "\tTraining batch 53 Loss: 0.113079\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000021\n",
      "\tTraining batch 56 Loss: 0.000195\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.017266\n",
      "\tTraining batch 61 Loss: 0.009959\n",
      "\tTraining batch 62 Loss: 0.000261\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.056204\n",
      "\tTraining batch 65 Loss: 0.111075\n",
      "\tTraining batch 66 Loss: 0.000831\n",
      "\tTraining batch 67 Loss: 0.043223\n",
      "\tTraining batch 68 Loss: 0.001059\n",
      "\tTraining batch 69 Loss: 0.008464\n",
      "\tTraining batch 70 Loss: 0.008821\n",
      "Training set: Average loss: 0.026593\n",
      "Validation set: Average loss: 11.605669, Accuracy: 1358/1959 (69.32%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.063510\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.077758\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000141\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000013\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000005\n",
      "\tTraining batch 18 Loss: 0.000001\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000002\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000521\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.136579\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.260243\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000070\n",
      "\tTraining batch 31 Loss: 0.000123\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.005869\n",
      "\tTraining batch 34 Loss: 0.000039\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000020\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000027\n",
      "\tTraining batch 46 Loss: 0.000649\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000154\n",
      "\tTraining batch 51 Loss: 0.146819\n",
      "\tTraining batch 52 Loss: 0.000005\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000179\n",
      "\tTraining batch 56 Loss: 0.004375\n",
      "\tTraining batch 57 Loss: 0.004402\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000202\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.107095\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000065\n",
      "\tTraining batch 66 Loss: 0.004556\n",
      "\tTraining batch 67 Loss: 0.107705\n",
      "\tTraining batch 68 Loss: 0.000060\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "Training set: Average loss: 0.013160\n",
      "Validation set: Average loss: 13.689506, Accuracy: 1370/1959 (69.93%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.014866\n",
      "\tTraining batch 2 Loss: 0.000003\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000465\n",
      "\tTraining batch 11 Loss: 0.126046\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000383\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000027\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000011\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.001052\n",
      "\tTraining batch 35 Loss: 0.000823\n",
      "\tTraining batch 36 Loss: 0.000001\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000001\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000040\n",
      "\tTraining batch 44 Loss: 0.011293\n",
      "\tTraining batch 45 Loss: 0.000007\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.002322\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000663\n",
      "\tTraining batch 57 Loss: 0.000031\n",
      "\tTraining batch 58 Loss: 0.187350\n",
      "\tTraining batch 59 Loss: 0.029801\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000006\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000001\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000316\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.001230\n",
      "Training set: Average loss: 0.005382\n",
      "Validation set: Average loss: 15.585629, Accuracy: 1357/1959 (69.27%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000012\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000626\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000038\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.084724\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000033\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000124\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000006\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000029\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000027\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000001\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000005\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000150\n",
      "\tTraining batch 56 Loss: 0.002241\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.331935\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000035\n",
      "\tTraining batch 66 Loss: 0.000001\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.002117\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "Training set: Average loss: 0.006030\n",
      "Validation set: Average loss: 15.520784, Accuracy: 1375/1959 (70.19%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000010\n",
      "\tTraining batch 2 Loss: 0.000001\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000043\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.122726\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000001\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.001756\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000201\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000018\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000007\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000165\n",
      "\tTraining batch 46 Loss: 0.000835\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.002371\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000049\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000105\n",
      "\tTraining batch 53 Loss: 0.000001\n",
      "\tTraining batch 54 Loss: 0.000004\n",
      "\tTraining batch 55 Loss: 0.000003\n",
      "\tTraining batch 56 Loss: 0.000018\n",
      "\tTraining batch 57 Loss: 0.000001\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000001\n",
      "\tTraining batch 60 Loss: 0.000033\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000031\n",
      "\tTraining batch 66 Loss: 0.000008\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000163\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000001\n",
      "Training set: Average loss: 0.001836\n",
      "Validation set: Average loss: 13.851341, Accuracy: 1390/1959 (70.95%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.006546\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.003270\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000013\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000071\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000004\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000001\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000015\n",
      "\tTraining batch 51 Loss: 0.000004\n",
      "\tTraining batch 52 Loss: 0.000002\n",
      "\tTraining batch 53 Loss: 0.000001\n",
      "\tTraining batch 54 Loss: 0.000004\n",
      "\tTraining batch 55 Loss: 0.000007\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000001\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000002\n",
      "\tTraining batch 60 Loss: 0.000004\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000002\n",
      "\tTraining batch 66 Loss: 0.000003\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000001\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "Training set: Average loss: 0.000142\n",
      "Validation set: Average loss: 13.998976, Accuracy: 1373/1959 (70.09%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000008\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000011\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000063\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000004\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000001\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000014\n",
      "\tTraining batch 51 Loss: 0.000003\n",
      "\tTraining batch 52 Loss: 0.000001\n",
      "\tTraining batch 53 Loss: 0.000001\n",
      "\tTraining batch 54 Loss: 0.000003\n",
      "\tTraining batch 55 Loss: 0.000006\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000001\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000002\n",
      "\tTraining batch 60 Loss: 0.000004\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000002\n",
      "\tTraining batch 66 Loss: 0.000003\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000001\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "Training set: Average loss: 0.000002\n",
      "Validation set: Average loss: 14.000445, Accuracy: 1374/1959 (70.14%)\n",
      "\n",
      "Epoch: 11\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000006\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000011\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000058\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000004\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000001\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000013\n",
      "\tTraining batch 51 Loss: 0.000003\n",
      "\tTraining batch 52 Loss: 0.000001\n",
      "\tTraining batch 53 Loss: 0.000001\n",
      "\tTraining batch 54 Loss: 0.000003\n",
      "\tTraining batch 55 Loss: 0.000006\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000001\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000002\n",
      "\tTraining batch 60 Loss: 0.000004\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000002\n",
      "\tTraining batch 66 Loss: 0.000003\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000001\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "Training set: Average loss: 0.000002\n",
      "Validation set: Average loss: 14.001694, Accuracy: 1374/1959 (70.14%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 46.5%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000005\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000011\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000053\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000004\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000001\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000013\n",
      "\tTraining batch 51 Loss: 0.000003\n",
      "\tTraining batch 52 Loss: 0.000001\n",
      "\tTraining batch 53 Loss: 0.000001\n",
      "\tTraining batch 54 Loss: 0.000003\n",
      "\tTraining batch 55 Loss: 0.000005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000001\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000002\n",
      "\tTraining batch 60 Loss: 0.000004\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000002\n",
      "\tTraining batch 66 Loss: 0.000003\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000001\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 24.323198\n",
      "Training set: Average loss: 0.342582\n",
      "Validation set: Average loss: 11.934908, Accuracy: 1397/1959 (71.31%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.170623\n",
      "\tTraining batch 3 Loss: 0.045299\n",
      "\tTraining batch 4 Loss: 0.000822\n",
      "\tTraining batch 5 Loss: 0.037489\n",
      "\tTraining batch 6 Loss: 0.000065\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000032\n",
      "\tTraining batch 9 Loss: 0.151626\n",
      "\tTraining batch 10 Loss: 0.014779\n",
      "\tTraining batch 11 Loss: 0.001911\n",
      "\tTraining batch 12 Loss: 0.012444\n",
      "\tTraining batch 13 Loss: 0.000992\n",
      "\tTraining batch 14 Loss: 0.000029\n",
      "\tTraining batch 15 Loss: 0.458716\n",
      "\tTraining batch 16 Loss: 0.136271\n",
      "\tTraining batch 17 Loss: 0.047121\n",
      "\tTraining batch 18 Loss: 0.010517\n",
      "\tTraining batch 19 Loss: 0.055406\n",
      "\tTraining batch 20 Loss: 0.078509\n",
      "\tTraining batch 21 Loss: 0.000547\n",
      "\tTraining batch 22 Loss: 0.000480\n",
      "\tTraining batch 23 Loss: 0.184575\n",
      "\tTraining batch 24 Loss: 0.179421\n",
      "\tTraining batch 25 Loss: 0.242255\n",
      "\tTraining batch 26 Loss: 0.192919\n",
      "\tTraining batch 27 Loss: 0.000472\n",
      "\tTraining batch 28 Loss: 0.041969\n",
      "\tTraining batch 29 Loss: 0.009872\n",
      "\tTraining batch 30 Loss: 0.003083\n",
      "\tTraining batch 31 Loss: 0.342824\n",
      "\tTraining batch 32 Loss: 0.499401\n",
      "\tTraining batch 33 Loss: 0.056521\n",
      "\tTraining batch 34 Loss: 0.001784\n",
      "\tTraining batch 35 Loss: 0.176914\n",
      "\tTraining batch 36 Loss: 0.444695\n",
      "\tTraining batch 37 Loss: 0.051441\n",
      "\tTraining batch 38 Loss: 0.517878\n",
      "\tTraining batch 39 Loss: 0.508423\n",
      "\tTraining batch 40 Loss: 0.014164\n",
      "\tTraining batch 41 Loss: 0.004265\n",
      "\tTraining batch 42 Loss: 0.000399\n",
      "\tTraining batch 43 Loss: 0.835694\n",
      "\tTraining batch 44 Loss: 0.000095\n",
      "\tTraining batch 45 Loss: 1.003974\n",
      "\tTraining batch 46 Loss: 0.137134\n",
      "\tTraining batch 47 Loss: 0.000101\n",
      "\tTraining batch 48 Loss: 0.361019\n",
      "\tTraining batch 49 Loss: 0.065987\n",
      "\tTraining batch 50 Loss: 0.000364\n",
      "\tTraining batch 51 Loss: 0.011286\n",
      "\tTraining batch 52 Loss: 0.000236\n",
      "\tTraining batch 53 Loss: 0.000417\n",
      "\tTraining batch 54 Loss: 0.001594\n",
      "\tTraining batch 55 Loss: 0.386699\n",
      "\tTraining batch 56 Loss: 0.244997\n",
      "\tTraining batch 57 Loss: 0.000736\n",
      "\tTraining batch 58 Loss: 0.683102\n",
      "\tTraining batch 59 Loss: 0.000290\n",
      "\tTraining batch 60 Loss: 0.001334\n",
      "\tTraining batch 61 Loss: 0.000775\n",
      "\tTraining batch 62 Loss: 0.000071\n",
      "\tTraining batch 63 Loss: 0.905727\n",
      "\tTraining batch 64 Loss: 0.145648\n",
      "\tTraining batch 65 Loss: 0.039416\n",
      "\tTraining batch 66 Loss: 0.093110\n",
      "\tTraining batch 67 Loss: 0.000007\n",
      "\tTraining batch 68 Loss: 0.017811\n",
      "\tTraining batch 69 Loss: 0.122232\n",
      "\tTraining batch 70 Loss: 0.162088\n",
      "\tTraining batch 71 Loss: 3.524606\n",
      "Training set: Average loss: 0.189345\n",
      "Validation set: Average loss: 9.044171, Accuracy: 1357/1959 (69.27%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.270018\n",
      "\tTraining batch 2 Loss: 0.010861\n",
      "\tTraining batch 3 Loss: 0.340911\n",
      "\tTraining batch 4 Loss: 0.508039\n",
      "\tTraining batch 5 Loss: 0.187857\n",
      "\tTraining batch 6 Loss: 0.210695\n",
      "\tTraining batch 7 Loss: 0.322966\n",
      "\tTraining batch 8 Loss: 0.000825\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000001\n",
      "\tTraining batch 11 Loss: 0.531929\n",
      "\tTraining batch 12 Loss: 0.000351\n",
      "\tTraining batch 13 Loss: 0.020504\n",
      "\tTraining batch 14 Loss: 0.000013\n",
      "\tTraining batch 15 Loss: 0.000007\n",
      "\tTraining batch 16 Loss: 0.000334\n",
      "\tTraining batch 17 Loss: 0.000242\n",
      "\tTraining batch 18 Loss: 0.000515\n",
      "\tTraining batch 19 Loss: 0.047338\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000001\n",
      "\tTraining batch 26 Loss: 0.000106\n",
      "\tTraining batch 27 Loss: 0.003073\n",
      "\tTraining batch 28 Loss: 0.393872\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.099280\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.010300\n",
      "\tTraining batch 34 Loss: 0.161794\n",
      "\tTraining batch 35 Loss: 0.000001\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000537\n",
      "\tTraining batch 39 Loss: 0.000057\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.022241\n",
      "\tTraining batch 43 Loss: 0.219206\n",
      "\tTraining batch 44 Loss: 0.036841\n",
      "\tTraining batch 45 Loss: 0.101842\n",
      "\tTraining batch 46 Loss: 0.000009\n",
      "\tTraining batch 47 Loss: 0.000007\n",
      "\tTraining batch 48 Loss: 0.135094\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.185068\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000055\n",
      "\tTraining batch 53 Loss: 0.000120\n",
      "\tTraining batch 54 Loss: 0.038661\n",
      "\tTraining batch 55 Loss: 0.000109\n",
      "\tTraining batch 56 Loss: 0.329877\n",
      "\tTraining batch 57 Loss: 0.000452\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.009838\n",
      "\tTraining batch 60 Loss: 0.020946\n",
      "\tTraining batch 61 Loss: 0.002135\n",
      "\tTraining batch 62 Loss: 0.000104\n",
      "\tTraining batch 63 Loss: 0.004798\n",
      "\tTraining batch 64 Loss: 0.254482\n",
      "\tTraining batch 65 Loss: 0.000191\n",
      "\tTraining batch 66 Loss: 0.782214\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000001\n",
      "\tTraining batch 71 Loss: 1.891133\n",
      "Training set: Average loss: 0.100815\n",
      "Validation set: Average loss: 14.987330, Accuracy: 1373/1959 (70.09%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.457749\n",
      "\tTraining batch 2 Loss: 0.000002\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000048\n",
      "\tTraining batch 5 Loss: 0.000002\n",
      "\tTraining batch 6 Loss: 0.000008\n",
      "\tTraining batch 7 Loss: 0.091569\n",
      "\tTraining batch 8 Loss: 0.760616\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000010\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000005\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.001655\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000904\n",
      "\tTraining batch 19 Loss: 0.026178\n",
      "\tTraining batch 20 Loss: 0.000029\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000009\n",
      "\tTraining batch 23 Loss: 0.002048\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.126093\n",
      "\tTraining batch 26 Loss: 0.589836\n",
      "\tTraining batch 27 Loss: 0.020798\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.751119\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000008\n",
      "\tTraining batch 35 Loss: 0.131026\n",
      "\tTraining batch 36 Loss: 0.000001\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000006\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.412146\n",
      "\tTraining batch 42 Loss: 0.000004\n",
      "\tTraining batch 43 Loss: 0.002628\n",
      "\tTraining batch 44 Loss: 0.000013\n",
      "\tTraining batch 45 Loss: 0.000002\n",
      "\tTraining batch 46 Loss: 0.005892\n",
      "\tTraining batch 47 Loss: 0.614842\n",
      "\tTraining batch 48 Loss: 0.386184\n",
      "\tTraining batch 49 Loss: 0.078814\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000728\n",
      "\tTraining batch 55 Loss: 0.000018\n",
      "\tTraining batch 56 Loss: 0.000005\n",
      "\tTraining batch 57 Loss: 0.000202\n",
      "\tTraining batch 58 Loss: 0.000039\n",
      "\tTraining batch 59 Loss: 0.000310\n",
      "\tTraining batch 60 Loss: 0.218802\n",
      "\tTraining batch 61 Loss: 0.001681\n",
      "\tTraining batch 62 Loss: 0.215918\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.004835\n",
      "\tTraining batch 66 Loss: 0.000019\n",
      "\tTraining batch 67 Loss: 0.209453\n",
      "\tTraining batch 68 Loss: 0.000004\n",
      "\tTraining batch 69 Loss: 0.002924\n",
      "\tTraining batch 70 Loss: 0.000018\n",
      "\tTraining batch 71 Loss: 2.382949\n",
      "Training set: Average loss: 0.105608\n",
      "Validation set: Average loss: 13.902773, Accuracy: 1375/1959 (70.19%)\n",
      "\n",
      "Epoch: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.018410\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.109025\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000005\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000001\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000217\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001201\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000002\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.389236\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000007\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.398646\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000007\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000001\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.001074\n",
      "\tTraining batch 46 Loss: 0.061530\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000003\n",
      "\tTraining batch 52 Loss: 0.000037\n",
      "\tTraining batch 53 Loss: 0.001244\n",
      "\tTraining batch 54 Loss: 0.265804\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.022495\n",
      "\tTraining batch 57 Loss: 0.000049\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000011\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.722824\n",
      "\tTraining batch 62 Loss: 0.000028\n",
      "\tTraining batch 63 Loss: 0.000001\n",
      "\tTraining batch 64 Loss: 0.000080\n",
      "\tTraining batch 65 Loss: 0.000145\n",
      "\tTraining batch 66 Loss: 0.000054\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000085\n",
      "\tTraining batch 71 Loss: 0.342620\n",
      "Training set: Average loss: 0.032885\n",
      "Validation set: Average loss: 13.644336, Accuracy: 1344/1959 (68.61%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000001\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000001\n",
      "\tTraining batch 6 Loss: 0.000227\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000994\n",
      "\tTraining batch 15 Loss: 0.003998\n",
      "\tTraining batch 16 Loss: 0.000038\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000001\n",
      "\tTraining batch 19 Loss: 0.005020\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000001\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000038\n",
      "\tTraining batch 27 Loss: 0.000002\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000007\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000001\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000012\n",
      "\tTraining batch 39 Loss: 0.000097\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000025\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000516\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000007\n",
      "\tTraining batch 46 Loss: 0.000002\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000002\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000007\n",
      "\tTraining batch 54 Loss: 0.093691\n",
      "\tTraining batch 55 Loss: 0.002590\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.002401\n",
      "\tTraining batch 58 Loss: 0.001307\n",
      "\tTraining batch 59 Loss: 0.000029\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000003\n",
      "\tTraining batch 66 Loss: 0.000058\n",
      "\tTraining batch 67 Loss: 0.000015\n",
      "\tTraining batch 68 Loss: 0.000001\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000001\n",
      "\tTraining batch 71 Loss: 0.392182\n",
      "Training set: Average loss: 0.007088\n",
      "Validation set: Average loss: 12.358454, Accuracy: 1370/1959 (69.93%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000061\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000011\n",
      "\tTraining batch 5 Loss: 0.000010\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000054\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000230\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.009092\n",
      "\tTraining batch 20 Loss: 0.000097\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.002184\n",
      "\tTraining batch 26 Loss: 0.012720\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.010023\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.034041\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000015\n",
      "\tTraining batch 37 Loss: 0.000048\n",
      "\tTraining batch 38 Loss: 0.000804\n",
      "\tTraining batch 39 Loss: 0.178449\n",
      "\tTraining batch 40 Loss: 0.000002\n",
      "\tTraining batch 41 Loss: 0.000413\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.014057\n",
      "\tTraining batch 44 Loss: 0.000019\n",
      "\tTraining batch 45 Loss: 0.001168\n",
      "\tTraining batch 46 Loss: 0.000010\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000002\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.001061\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000223\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000746\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000015\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000014\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.002987\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000005\n",
      "Training set: Average loss: 0.003783\n",
      "Validation set: Average loss: 13.010678, Accuracy: 1364/1959 (69.63%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000003\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000012\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000001\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000486\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.003223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 20 Loss: 0.000002\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000648\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000030\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000002\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.002524\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000025\n",
      "\tTraining batch 46 Loss: 0.000004\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.001595\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000150\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000163\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000022\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000028\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "Training set: Average loss: 0.000126\n",
      "Validation set: Average loss: 13.275158, Accuracy: 1370/1959 (69.93%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000001\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000177\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.003019\n",
      "\tTraining batch 20 Loss: 0.000006\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000428\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000015\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000002\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000606\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000022\n",
      "\tTraining batch 46 Loss: 0.000002\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.001239\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000146\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000117\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000012\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000029\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "Training set: Average loss: 0.000082\n",
      "Validation set: Average loss: 13.311200, Accuracy: 1370/1959 (69.93%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 49.75%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000001\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000107\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.002872\n",
      "\tTraining batch 20 Loss: 0.000008\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000365\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000010\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000002\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000422\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000019\n",
      "\tTraining batch 46 Loss: 0.000002\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.001038\n",
      "\tTraining batch 55 Loss: 0.000001\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000142\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000099\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000008\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000029\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 23.840559\n",
      "Training set: Average loss: 0.331190\n",
      "Validation set: Average loss: 12.017627, Accuracy: 1354/1959 (69.12%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.017506\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000303\n",
      "\tTraining batch 5 Loss: 0.000886\n",
      "\tTraining batch 6 Loss: 0.001988\n",
      "\tTraining batch 7 Loss: 0.263509\n",
      "\tTraining batch 8 Loss: 0.000845\n",
      "\tTraining batch 9 Loss: 0.038296\n",
      "\tTraining batch 10 Loss: 0.006125\n",
      "\tTraining batch 11 Loss: 0.000008\n",
      "\tTraining batch 12 Loss: 0.000006\n",
      "\tTraining batch 13 Loss: 0.104182\n",
      "\tTraining batch 14 Loss: 0.327617\n",
      "\tTraining batch 15 Loss: 0.055446\n",
      "\tTraining batch 16 Loss: 0.007405\n",
      "\tTraining batch 17 Loss: 0.000014\n",
      "\tTraining batch 18 Loss: 0.181092\n",
      "\tTraining batch 19 Loss: 0.696614\n",
      "\tTraining batch 20 Loss: 0.004252\n",
      "\tTraining batch 21 Loss: 0.621710\n",
      "\tTraining batch 22 Loss: 0.364114\n",
      "\tTraining batch 23 Loss: 0.470105\n",
      "\tTraining batch 24 Loss: 0.861719\n",
      "\tTraining batch 25 Loss: 1.065962\n",
      "\tTraining batch 26 Loss: 0.144882\n",
      "\tTraining batch 27 Loss: 0.267568\n",
      "\tTraining batch 28 Loss: 0.275276\n",
      "\tTraining batch 29 Loss: 0.121841\n",
      "\tTraining batch 30 Loss: 0.916290\n",
      "\tTraining batch 31 Loss: 0.029468\n",
      "\tTraining batch 32 Loss: 0.000068\n",
      "\tTraining batch 33 Loss: 0.206579\n",
      "\tTraining batch 34 Loss: 0.073399\n",
      "\tTraining batch 35 Loss: 0.128450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 36 Loss: 0.023931\n",
      "\tTraining batch 37 Loss: 0.073240\n",
      "\tTraining batch 38 Loss: 0.111659\n",
      "\tTraining batch 39 Loss: 0.114758\n",
      "\tTraining batch 40 Loss: 0.000149\n",
      "\tTraining batch 41 Loss: 0.002958\n",
      "\tTraining batch 42 Loss: 0.000394\n",
      "\tTraining batch 43 Loss: 0.336974\n",
      "\tTraining batch 44 Loss: 0.000017\n",
      "\tTraining batch 45 Loss: 0.003278\n",
      "\tTraining batch 46 Loss: 0.223274\n",
      "\tTraining batch 47 Loss: 0.011835\n",
      "\tTraining batch 48 Loss: 0.001748\n",
      "\tTraining batch 49 Loss: 0.000429\n",
      "\tTraining batch 50 Loss: 0.116873\n",
      "\tTraining batch 51 Loss: 0.000070\n",
      "\tTraining batch 52 Loss: 0.000009\n",
      "\tTraining batch 53 Loss: 0.047233\n",
      "\tTraining batch 54 Loss: 0.016346\n",
      "\tTraining batch 55 Loss: 0.334635\n",
      "\tTraining batch 56 Loss: 0.000039\n",
      "\tTraining batch 57 Loss: 0.130261\n",
      "\tTraining batch 58 Loss: 0.298083\n",
      "\tTraining batch 59 Loss: 0.007147\n",
      "\tTraining batch 60 Loss: 0.041833\n",
      "\tTraining batch 61 Loss: 0.307358\n",
      "\tTraining batch 62 Loss: 0.354586\n",
      "\tTraining batch 63 Loss: 0.056012\n",
      "\tTraining batch 64 Loss: 0.000015\n",
      "\tTraining batch 65 Loss: 0.323867\n",
      "\tTraining batch 66 Loss: 0.000834\n",
      "\tTraining batch 67 Loss: 0.000001\n",
      "\tTraining batch 68 Loss: 0.052413\n",
      "\tTraining batch 69 Loss: 0.031036\n",
      "\tTraining batch 70 Loss: 0.312539\n",
      "\tTraining batch 71 Loss: 0.118896\n",
      "\tTraining batch 72 Loss: 5.077450\n",
      "Training set: Average loss: 0.219246\n",
      "Validation set: Average loss: 12.120769, Accuracy: 1377/1959 (70.29%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000095\n",
      "\tTraining batch 2 Loss: 0.000006\n",
      "\tTraining batch 3 Loss: 0.000865\n",
      "\tTraining batch 4 Loss: 0.003943\n",
      "\tTraining batch 5 Loss: 0.001804\n",
      "\tTraining batch 6 Loss: 0.000057\n",
      "\tTraining batch 7 Loss: 0.000167\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.003556\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000003\n",
      "\tTraining batch 14 Loss: 0.000009\n",
      "\tTraining batch 15 Loss: 0.003784\n",
      "\tTraining batch 16 Loss: 0.002176\n",
      "\tTraining batch 17 Loss: 0.000013\n",
      "\tTraining batch 18 Loss: 0.000055\n",
      "\tTraining batch 19 Loss: 0.009336\n",
      "\tTraining batch 20 Loss: 0.064344\n",
      "\tTraining batch 21 Loss: 0.346912\n",
      "\tTraining batch 22 Loss: 0.000198\n",
      "\tTraining batch 23 Loss: 0.000001\n",
      "\tTraining batch 24 Loss: 0.018660\n",
      "\tTraining batch 25 Loss: 0.000001\n",
      "\tTraining batch 26 Loss: 0.044610\n",
      "\tTraining batch 27 Loss: 0.002551\n",
      "\tTraining batch 28 Loss: 0.000352\n",
      "\tTraining batch 29 Loss: 0.009137\n",
      "\tTraining batch 30 Loss: 0.000209\n",
      "\tTraining batch 31 Loss: 0.000015\n",
      "\tTraining batch 32 Loss: 0.075611\n",
      "\tTraining batch 33 Loss: 0.088943\n",
      "\tTraining batch 34 Loss: 0.001644\n",
      "\tTraining batch 35 Loss: 0.000018\n",
      "\tTraining batch 36 Loss: 0.545503\n",
      "\tTraining batch 37 Loss: 0.214883\n",
      "\tTraining batch 38 Loss: 0.001050\n",
      "\tTraining batch 39 Loss: 0.082976\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.199906\n",
      "\tTraining batch 42 Loss: 0.000012\n",
      "\tTraining batch 43 Loss: 0.000401\n",
      "\tTraining batch 44 Loss: 0.000002\n",
      "\tTraining batch 45 Loss: 0.434504\n",
      "\tTraining batch 46 Loss: 0.000836\n",
      "\tTraining batch 47 Loss: 0.652289\n",
      "\tTraining batch 48 Loss: 0.021623\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000032\n",
      "\tTraining batch 51 Loss: 0.388836\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.542807\n",
      "\tTraining batch 54 Loss: 0.001123\n",
      "\tTraining batch 55 Loss: 0.000053\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.003780\n",
      "\tTraining batch 58 Loss: 0.005417\n",
      "\tTraining batch 59 Loss: 0.000159\n",
      "\tTraining batch 60 Loss: 0.006325\n",
      "\tTraining batch 61 Loss: 0.000870\n",
      "\tTraining batch 62 Loss: 0.000854\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000090\n",
      "\tTraining batch 65 Loss: 0.000026\n",
      "\tTraining batch 66 Loss: 0.000184\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000001\n",
      "\tTraining batch 69 Loss: 0.272606\n",
      "\tTraining batch 70 Loss: 0.035116\n",
      "\tTraining batch 71 Loss: 0.004505\n",
      "\tTraining batch 72 Loss: 0.319284\n",
      "Training set: Average loss: 0.061321\n",
      "Validation set: Average loss: 12.170487, Accuracy: 1362/1959 (69.53%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000013\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.359386\n",
      "\tTraining batch 4 Loss: 0.000036\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000035\n",
      "\tTraining batch 7 Loss: 0.000168\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.107519\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000004\n",
      "\tTraining batch 14 Loss: 0.000017\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000220\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.007215\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000001\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.044592\n",
      "\tTraining batch 27 Loss: 0.000402\n",
      "\tTraining batch 28 Loss: 0.000023\n",
      "\tTraining batch 29 Loss: 0.000001\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.003037\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.058350\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000002\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000008\n",
      "\tTraining batch 40 Loss: 0.000001\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.004396\n",
      "\tTraining batch 43 Loss: 0.000340\n",
      "\tTraining batch 44 Loss: 0.320822\n",
      "\tTraining batch 45 Loss: 0.000198\n",
      "\tTraining batch 46 Loss: 0.000072\n",
      "\tTraining batch 47 Loss: 0.000004\n",
      "\tTraining batch 48 Loss: 0.000004\n",
      "\tTraining batch 49 Loss: 0.000349\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000085\n",
      "\tTraining batch 55 Loss: 0.006077\n",
      "\tTraining batch 56 Loss: 0.000020\n",
      "\tTraining batch 57 Loss: 0.000032\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000255\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000005\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000034\n",
      "\tTraining batch 64 Loss: 0.282413\n",
      "\tTraining batch 65 Loss: 0.304943\n",
      "\tTraining batch 66 Loss: 0.000009\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.030201\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000142\n",
      "\tTraining batch 71 Loss: 0.115703\n",
      "\tTraining batch 72 Loss: 0.359941\n",
      "Training set: Average loss: 0.027876\n",
      "Validation set: Average loss: 13.933069, Accuracy: 1340/1959 (68.40%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.012878\n",
      "\tTraining batch 2 Loss: 0.000001\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000041\n",
      "\tTraining batch 5 Loss: 0.000003\n",
      "\tTraining batch 6 Loss: 0.144007\n",
      "\tTraining batch 7 Loss: 0.289494\n",
      "\tTraining batch 8 Loss: 0.000253\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000858\n",
      "\tTraining batch 11 Loss: 0.000001\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.004242\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.039164\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.004224\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000044\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000011\n",
      "\tTraining batch 26 Loss: 0.012176\n",
      "\tTraining batch 27 Loss: 0.000001\n",
      "\tTraining batch 28 Loss: 0.093150\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.004842\n",
      "\tTraining batch 32 Loss: 0.003332\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.008086\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000096\n",
      "\tTraining batch 37 Loss: 0.005822\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000569\n",
      "\tTraining batch 40 Loss: 0.000001\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.003633\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.002585\n",
      "\tTraining batch 46 Loss: 0.000992\n",
      "\tTraining batch 47 Loss: 0.000016\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.001266\n",
      "\tTraining batch 51 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 52 Loss: 0.000005\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.007129\n",
      "\tTraining batch 55 Loss: 0.181884\n",
      "\tTraining batch 56 Loss: 0.000289\n",
      "\tTraining batch 57 Loss: 0.000049\n",
      "\tTraining batch 58 Loss: 0.003628\n",
      "\tTraining batch 59 Loss: 0.000030\n",
      "\tTraining batch 60 Loss: 0.000001\n",
      "\tTraining batch 61 Loss: 0.000005\n",
      "\tTraining batch 62 Loss: 0.000001\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000008\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000001\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "Training set: Average loss: 0.011456\n",
      "Validation set: Average loss: 13.129481, Accuracy: 1355/1959 (69.17%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000234\n",
      "\tTraining batch 5 Loss: 0.001214\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.020119\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000011\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.004277\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000990\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000015\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000001\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000003\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.002567\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000028\n",
      "\tTraining batch 45 Loss: 0.004983\n",
      "\tTraining batch 46 Loss: 0.003768\n",
      "\tTraining batch 47 Loss: 0.173631\n",
      "\tTraining batch 48 Loss: 0.000004\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000006\n",
      "\tTraining batch 54 Loss: 0.000008\n",
      "\tTraining batch 55 Loss: 0.044412\n",
      "\tTraining batch 56 Loss: 0.000211\n",
      "\tTraining batch 57 Loss: 0.006472\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000001\n",
      "\tTraining batch 60 Loss: 0.000013\n",
      "\tTraining batch 61 Loss: 0.245797\n",
      "\tTraining batch 62 Loss: 0.144362\n",
      "\tTraining batch 63 Loss: 0.238615\n",
      "\tTraining batch 64 Loss: 0.000001\n",
      "\tTraining batch 65 Loss: 0.000014\n",
      "\tTraining batch 66 Loss: 0.000010\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000001\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000004\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "Training set: Average loss: 0.012386\n",
      "Validation set: Average loss: 12.867465, Accuracy: 1329/1959 (67.84%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000008\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000020\n",
      "\tTraining batch 4 Loss: 0.000145\n",
      "\tTraining batch 5 Loss: 0.146253\n",
      "\tTraining batch 6 Loss: 0.031567\n",
      "\tTraining batch 7 Loss: 0.000257\n",
      "\tTraining batch 8 Loss: 0.000002\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000210\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000004\n",
      "\tTraining batch 16 Loss: 0.000003\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.003939\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000003\n",
      "\tTraining batch 22 Loss: 0.000191\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.021974\n",
      "\tTraining batch 27 Loss: 0.000001\n",
      "\tTraining batch 28 Loss: 0.000073\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000001\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000028\n",
      "\tTraining batch 34 Loss: 0.000142\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000228\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000079\n",
      "\tTraining batch 39 Loss: 0.000129\n",
      "\tTraining batch 40 Loss: 0.004805\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.434799\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000003\n",
      "\tTraining batch 46 Loss: 0.000035\n",
      "\tTraining batch 47 Loss: 0.000001\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000011\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000436\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000097\n",
      "\tTraining batch 57 Loss: 0.000043\n",
      "\tTraining batch 58 Loss: 0.000359\n",
      "\tTraining batch 59 Loss: 0.054610\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000015\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000015\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000002\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000009\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "Training set: Average loss: 0.009729\n",
      "Validation set: Average loss: 13.793216, Accuracy: 1351/1959 (68.96%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000001\n",
      "\tTraining batch 4 Loss: 0.000005\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000001\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.004816\n",
      "\tTraining batch 15 Loss: 0.000003\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001416\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000020\n",
      "\tTraining batch 23 Loss: 0.020057\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000004\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000002\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.080933\n",
      "\tTraining batch 48 Loss: 0.000001\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000365\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000036\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.004720\n",
      "\tTraining batch 66 Loss: 0.000025\n",
      "\tTraining batch 67 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 68 Loss: 0.000025\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000116\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "Training set: Average loss: 0.001563\n",
      "Validation set: Average loss: 13.782836, Accuracy: 1358/1959 (69.32%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.003199\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000003\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000428\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000001\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000291\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000031\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000027\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000019\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000069\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "Training set: Average loss: 0.000057\n",
      "Validation set: Average loss: 13.697342, Accuracy: 1360/1959 (69.42%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.002964\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000003\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000344\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000001\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000269\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000030\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000026\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000017\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000048\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "Training set: Average loss: 0.000051\n",
      "Validation set: Average loss: 13.718683, Accuracy: 1360/1959 (69.42%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 48.5%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.002775\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000003\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000287\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000001\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000251\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000029\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000025\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000015\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000037\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 24.032969\n",
      "Training set: Average loss: 0.329266\n",
      "Validation set: Average loss: 12.020272, Accuracy: 1370/1959 (69.93%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000001\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 6 Loss: 0.025767\n",
      "\tTraining batch 7 Loss: 0.000007\n",
      "\tTraining batch 8 Loss: 0.006782\n",
      "\tTraining batch 9 Loss: 0.000038\n",
      "\tTraining batch 10 Loss: 0.000005\n",
      "\tTraining batch 11 Loss: 0.000009\n",
      "\tTraining batch 12 Loss: 0.012674\n",
      "\tTraining batch 13 Loss: 0.001598\n",
      "\tTraining batch 14 Loss: 0.000032\n",
      "\tTraining batch 15 Loss: 0.519443\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000002\n",
      "\tTraining batch 18 Loss: 0.413536\n",
      "\tTraining batch 19 Loss: 0.009778\n",
      "\tTraining batch 20 Loss: 0.000013\n",
      "\tTraining batch 21 Loss: 0.038332\n",
      "\tTraining batch 22 Loss: 0.013030\n",
      "\tTraining batch 23 Loss: 0.000003\n",
      "\tTraining batch 24 Loss: 0.081497\n",
      "\tTraining batch 25 Loss: 0.147995\n",
      "\tTraining batch 26 Loss: 0.067538\n",
      "\tTraining batch 27 Loss: 0.010486\n",
      "\tTraining batch 28 Loss: 0.218962\n",
      "\tTraining batch 29 Loss: 0.000002\n",
      "\tTraining batch 30 Loss: 0.244895\n",
      "\tTraining batch 31 Loss: 0.000015\n",
      "\tTraining batch 32 Loss: 0.178260\n",
      "\tTraining batch 33 Loss: 0.006893\n",
      "\tTraining batch 34 Loss: 0.048969\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000352\n",
      "\tTraining batch 37 Loss: 0.007666\n",
      "\tTraining batch 38 Loss: 0.000030\n",
      "\tTraining batch 39 Loss: 0.000001\n",
      "\tTraining batch 40 Loss: 0.000217\n",
      "\tTraining batch 41 Loss: 0.000004\n",
      "\tTraining batch 42 Loss: 0.312934\n",
      "\tTraining batch 43 Loss: 0.109432\n",
      "\tTraining batch 44 Loss: 0.011559\n",
      "\tTraining batch 45 Loss: 0.062442\n",
      "\tTraining batch 46 Loss: 0.002371\n",
      "\tTraining batch 47 Loss: 0.000613\n",
      "\tTraining batch 48 Loss: 0.173179\n",
      "\tTraining batch 49 Loss: 0.005286\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000231\n",
      "\tTraining batch 52 Loss: 0.000001\n",
      "\tTraining batch 53 Loss: 0.000001\n",
      "\tTraining batch 54 Loss: 0.123109\n",
      "\tTraining batch 55 Loss: 0.156246\n",
      "\tTraining batch 56 Loss: 0.332282\n",
      "\tTraining batch 57 Loss: 0.000942\n",
      "\tTraining batch 58 Loss: 0.316621\n",
      "\tTraining batch 59 Loss: 0.042119\n",
      "\tTraining batch 60 Loss: 0.000007\n",
      "\tTraining batch 61 Loss: 0.000008\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.025277\n",
      "\tTraining batch 65 Loss: 0.154130\n",
      "\tTraining batch 66 Loss: 0.000038\n",
      "\tTraining batch 67 Loss: 0.110206\n",
      "\tTraining batch 68 Loss: 0.000330\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.200030\n",
      "\tTraining batch 71 Loss: 0.454854\n",
      "\tTraining batch 72 Loss: 0.232821\n",
      "\tTraining batch 73 Loss: 0.360560\n",
      "Training set: Average loss: 0.071815\n",
      "Validation set: Average loss: 12.422039, Accuracy: 1383/1959 (70.60%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000002\n",
      "\tTraining batch 5 Loss: 0.052564\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.283444\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000003\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.001014\n",
      "\tTraining batch 16 Loss: 0.194240\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001498\n",
      "\tTraining batch 20 Loss: 0.015565\n",
      "\tTraining batch 21 Loss: 0.383677\n",
      "\tTraining batch 22 Loss: 0.000002\n",
      "\tTraining batch 23 Loss: 0.000036\n",
      "\tTraining batch 24 Loss: 0.001439\n",
      "\tTraining batch 25 Loss: 0.000303\n",
      "\tTraining batch 26 Loss: 0.001125\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.162965\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000001\n",
      "\tTraining batch 42 Loss: 0.000001\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000024\n",
      "\tTraining batch 45 Loss: 0.000096\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000176\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000003\n",
      "\tTraining batch 51 Loss: 0.000001\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.002392\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.088063\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000406\n",
      "\tTraining batch 62 Loss: 0.000016\n",
      "\tTraining batch 63 Loss: 0.000022\n",
      "\tTraining batch 64 Loss: 0.000416\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000015\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000098\n",
      "\tTraining batch 69 Loss: 0.000004\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.021631\n",
      "\tTraining batch 72 Loss: 0.003412\n",
      "\tTraining batch 73 Loss: 0.509325\n",
      "Training set: Average loss: 0.023616\n",
      "Validation set: Average loss: 12.320452, Accuracy: 1392/1959 (71.06%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000020\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000010\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.197380\n",
      "\tTraining batch 10 Loss: 0.000428\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000054\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000029\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001457\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000050\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000096\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.002706\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.175141\n",
      "\tTraining batch 42 Loss: 0.000007\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000005\n",
      "\tTraining batch 46 Loss: 0.000020\n",
      "\tTraining batch 47 Loss: 0.000004\n",
      "\tTraining batch 48 Loss: 0.000047\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000503\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000072\n",
      "\tTraining batch 58 Loss: 0.000001\n",
      "\tTraining batch 59 Loss: 0.000001\n",
      "\tTraining batch 60 Loss: 0.000002\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.448625\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000009\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.112756\n",
      "Training set: Average loss: 0.012869\n",
      "Validation set: Average loss: 13.131218, Accuracy: 1394/1959 (71.16%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000330\n",
      "\tTraining batch 4 Loss: 0.000002\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000017\n",
      "\tTraining batch 7 Loss: 0.007303\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 19 Loss: 0.001800\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000816\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.003301\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.464938\n",
      "\tTraining batch 34 Loss: 0.000003\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000001\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.059979\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000039\n",
      "\tTraining batch 46 Loss: 0.050341\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000009\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.282371\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000001\n",
      "\tTraining batch 54 Loss: 0.000910\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000003\n",
      "\tTraining batch 57 Loss: 0.000492\n",
      "\tTraining batch 58 Loss: 0.000030\n",
      "\tTraining batch 59 Loss: 0.000041\n",
      "\tTraining batch 60 Loss: 0.000026\n",
      "\tTraining batch 61 Loss: 0.175854\n",
      "\tTraining batch 62 Loss: 0.000045\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000002\n",
      "\tTraining batch 66 Loss: 0.000036\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.017199\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.424309\n",
      "\tTraining batch 71 Loss: 0.001370\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.419904\n",
      "Training set: Average loss: 0.026185\n",
      "Validation set: Average loss: 12.797366, Accuracy: 1389/1959 (70.90%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000001\n",
      "\tTraining batch 4 Loss: 0.000013\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000330\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000411\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.002516\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.002521\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.004617\n",
      "\tTraining batch 25 Loss: 0.049340\n",
      "\tTraining batch 26 Loss: 0.001888\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000004\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.020431\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000001\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.305294\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000078\n",
      "\tTraining batch 41 Loss: 0.000001\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000001\n",
      "\tTraining batch 45 Loss: 0.000010\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.008659\n",
      "\tTraining batch 48 Loss: 0.819795\n",
      "\tTraining batch 49 Loss: 0.000089\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000045\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000116\n",
      "\tTraining batch 55 Loss: 0.071214\n",
      "\tTraining batch 56 Loss: 0.000006\n",
      "\tTraining batch 57 Loss: 0.000039\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000004\n",
      "\tTraining batch 60 Loss: 0.000026\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.082018\n",
      "\tTraining batch 65 Loss: 0.000003\n",
      "\tTraining batch 66 Loss: 0.000006\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.112126\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.091260\n",
      "Training set: Average loss: 0.021546\n",
      "Validation set: Average loss: 15.254196, Accuracy: 1365/1959 (69.68%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000066\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.037343\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.338175\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000001\n",
      "\tTraining batch 19 Loss: 0.000001\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000039\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000217\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000019\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000041\n",
      "\tTraining batch 39 Loss: 0.027339\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000003\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000001\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.011506\n",
      "\tTraining batch 61 Loss: 0.000076\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000009\n",
      "\tTraining batch 66 Loss: 0.000001\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.053688\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "Training set: Average loss: 0.006418\n",
      "Validation set: Average loss: 14.460426, Accuracy: 1394/1959 (71.16%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000001\n",
      "\tTraining batch 4 Loss: 0.000080\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000009\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000332\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000115\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.194907\n",
      "\tTraining batch 26 Loss: 0.000375\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000001\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000009\n",
      "\tTraining batch 31 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 32 Loss: 0.000023\n",
      "\tTraining batch 33 Loss: 0.131434\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000010\n",
      "\tTraining batch 36 Loss: 0.001619\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.250690\n",
      "\tTraining batch 39 Loss: 0.028104\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000058\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000004\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.068514\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000002\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000281\n",
      "\tTraining batch 57 Loss: 0.002516\n",
      "\tTraining batch 58 Loss: 0.001550\n",
      "\tTraining batch 59 Loss: 0.000005\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000055\n",
      "\tTraining batch 62 Loss: 0.057355\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000507\n",
      "\tTraining batch 65 Loss: 0.241314\n",
      "\tTraining batch 66 Loss: 0.000001\n",
      "\tTraining batch 67 Loss: 0.004377\n",
      "\tTraining batch 68 Loss: 0.000002\n",
      "\tTraining batch 69 Loss: 0.000059\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.079967\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.004425\n",
      "Training set: Average loss: 0.014640\n",
      "Validation set: Average loss: 12.846390, Accuracy: 1361/1959 (69.47%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000057\n",
      "\tTraining batch 16 Loss: 0.000002\n",
      "\tTraining batch 17 Loss: 0.000001\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.006683\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000291\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000947\n",
      "\tTraining batch 26 Loss: 0.000025\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000001\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000131\n",
      "\tTraining batch 31 Loss: 0.468208\n",
      "\tTraining batch 32 Loss: 0.000001\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000520\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000001\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000012\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000002\n",
      "\tTraining batch 49 Loss: 0.000001\n",
      "\tTraining batch 50 Loss: 0.000090\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.005649\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.004699\n",
      "\tTraining batch 57 Loss: 0.000239\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.146516\n",
      "\tTraining batch 61 Loss: 0.000010\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000012\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000001\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000062\n",
      "\tTraining batch 69 Loss: 0.000011\n",
      "\tTraining batch 70 Loss: 0.813124\n",
      "\tTraining batch 71 Loss: 0.001422\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.001937\n",
      "Training set: Average loss: 0.019872\n",
      "Validation set: Average loss: 12.676730, Accuracy: 1376/1959 (70.24%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000011\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.002206\n",
      "\tTraining batch 8 Loss: 0.004396\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000001\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000127\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.004394\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000182\n",
      "\tTraining batch 26 Loss: 0.004894\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000005\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000002\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000021\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000001\n",
      "\tTraining batch 38 Loss: 0.000015\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000043\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.004911\n",
      "\tTraining batch 44 Loss: 0.360652\n",
      "\tTraining batch 45 Loss: 0.000012\n",
      "\tTraining batch 46 Loss: 0.000001\n",
      "\tTraining batch 47 Loss: 0.038023\n",
      "\tTraining batch 48 Loss: 0.000002\n",
      "\tTraining batch 49 Loss: 0.000006\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.869226\n",
      "\tTraining batch 53 Loss: 0.000002\n",
      "\tTraining batch 54 Loss: 0.000009\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000181\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.332134\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000003\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.116729\n",
      "\tTraining batch 73 Loss: 0.047940\n",
      "Training set: Average loss: 0.024468\n",
      "Validation set: Average loss: 17.721913, Accuracy: 1375/1959 (70.19%)\n",
      "\n",
      "Epoch: 11\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000002\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000002\n",
      "\tTraining batch 11 Loss: 0.000042\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000007\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.079846\n",
      "\tTraining batch 20 Loss: 0.072714\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000001\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.217531\n",
      "\tTraining batch 43 Loss: 0.009842\n",
      "\tTraining batch 44 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.015869\n",
      "\tTraining batch 49 Loss: 0.004942\n",
      "\tTraining batch 50 Loss: 0.000009\n",
      "\tTraining batch 51 Loss: 0.001333\n",
      "\tTraining batch 52 Loss: 0.953604\n",
      "\tTraining batch 53 Loss: 0.000001\n",
      "\tTraining batch 54 Loss: 0.000002\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.004055\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000010\n",
      "\tTraining batch 60 Loss: 0.119773\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.090106\n",
      "\tTraining batch 65 Loss: 0.000003\n",
      "\tTraining batch 66 Loss: 0.001074\n",
      "\tTraining batch 67 Loss: 0.034735\n",
      "\tTraining batch 68 Loss: 0.000363\n",
      "\tTraining batch 69 Loss: 0.000010\n",
      "\tTraining batch 70 Loss: 0.098095\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000007\n",
      "Training set: Average loss: 0.023342\n",
      "Validation set: Average loss: 13.781246, Accuracy: 1391/1959 (71.01%)\n",
      "\n",
      "Epoch: 12\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000041\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000019\n",
      "\tTraining batch 7 Loss: 0.000761\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000020\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.308174\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000592\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000008\n",
      "\tTraining batch 17 Loss: 0.000017\n",
      "\tTraining batch 18 Loss: 0.212556\n",
      "\tTraining batch 19 Loss: 0.002613\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000017\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.008032\n",
      "\tTraining batch 27 Loss: 0.001310\n",
      "\tTraining batch 28 Loss: 0.282156\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000002\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000925\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000674\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000012\n",
      "\tTraining batch 47 Loss: 0.466170\n",
      "\tTraining batch 48 Loss: 0.000013\n",
      "\tTraining batch 49 Loss: 0.275443\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.508372\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.001312\n",
      "\tTraining batch 57 Loss: 0.000025\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000001\n",
      "\tTraining batch 62 Loss: 0.000057\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000005\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000471\n",
      "\tTraining batch 73 Loss: 0.168816\n",
      "Training set: Average loss: 0.030666\n",
      "Validation set: Average loss: 17.083989, Accuracy: 1393/1959 (71.11%)\n",
      "\n",
      "Epoch: 13\n",
      "\tTraining batch 1 Loss: 0.000001\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.237406\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.002227\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000001\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 1.127679\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000302\n",
      "\tTraining batch 27 Loss: 0.000001\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000020\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000189\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000047\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000148\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000443\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.031956\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000122\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "Training set: Average loss: 0.019185\n",
      "Validation set: Average loss: 17.263139, Accuracy: 1407/1959 (71.82%)\n",
      "\n",
      "Epoch: 14\n",
      "\tTraining batch 1 Loss: 0.000097\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.202414\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000012\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000208\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000001\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.009574\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.059071\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000153\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 58 Loss: 0.000321\n",
      "\tTraining batch 59 Loss: 0.000983\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000003\n",
      "\tTraining batch 62 Loss: 0.102555\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.553568\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.174647\n",
      "\tTraining batch 73 Loss: 0.000001\n",
      "Training set: Average loss: 0.015118\n",
      "Validation set: Average loss: 17.393253, Accuracy: 1408/1959 (71.87%)\n",
      "\n",
      "Epoch: 15\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000004\n",
      "\tTraining batch 4 Loss: 0.000015\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.002525\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000015\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001005\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.230039\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000848\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.004064\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000001\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000016\n",
      "\tTraining batch 51 Loss: 0.000002\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.392118\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000001\n",
      "\tTraining batch 57 Loss: 0.000001\n",
      "\tTraining batch 58 Loss: 0.040261\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.559756\n",
      "\tTraining batch 61 Loss: 0.000035\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000019\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000008\n",
      "\tTraining batch 69 Loss: 0.473269\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000021\n",
      "Training set: Average loss: 0.023343\n",
      "Validation set: Average loss: 16.399949, Accuracy: 1397/1959 (71.31%)\n",
      "\n",
      "Epoch: 16\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000001\n",
      "\tTraining batch 3 Loss: 0.241483\n",
      "\tTraining batch 4 Loss: 0.000008\n",
      "\tTraining batch 5 Loss: 0.000002\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000001\n",
      "\tTraining batch 10 Loss: 0.324163\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.059456\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000006\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000004\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.002872\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.014228\n",
      "\tTraining batch 22 Loss: 0.006138\n",
      "\tTraining batch 23 Loss: 0.000002\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.010983\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.003955\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.066018\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000292\n",
      "\tTraining batch 45 Loss: 0.027340\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000096\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.409777\n",
      "\tTraining batch 57 Loss: 0.000034\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000014\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000039\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "Training set: Average loss: 0.015985\n",
      "Validation set: Average loss: 16.740743, Accuracy: 1386/1959 (70.75%)\n",
      "\n",
      "Epoch: 17\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001289\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000002\n",
      "\tTraining batch 41 Loss: 0.000006\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000066\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000001\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000094\n",
      "\tTraining batch 55 Loss: 0.000001\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.001511\n",
      "\tTraining batch 58 Loss: 0.002812\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000017\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 71 Loss: 0.000017\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "Training set: Average loss: 0.000080\n",
      "Validation set: Average loss: 17.012522, Accuracy: 1382/1959 (70.55%)\n",
      "\n",
      "Epoch: 18\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001296\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000076\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000001\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000110\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000007\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000013\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000007\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "Training set: Average loss: 0.000021\n",
      "Validation set: Average loss: 17.031668, Accuracy: 1381/1959 (70.50%)\n",
      "\n",
      "Epoch: 19\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001247\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000059\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000001\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000105\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000007\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000012\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000005\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "Training set: Average loss: 0.000020\n",
      "Validation set: Average loss: 17.037536, Accuracy: 1381/1959 (70.50%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 48.75%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001199\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000047\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000001\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000100\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000006\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000010\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000004\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 23.006765\n",
      "Training set: Average loss: 0.310921\n",
      "Validation set: Average loss: 16.304311, Accuracy: 1388/1959 (70.85%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000002\n",
      "\tTraining batch 4 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.003152\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.548222\n",
      "\tTraining batch 9 Loss: 0.000560\n",
      "\tTraining batch 10 Loss: 0.470765\n",
      "\tTraining batch 11 Loss: 0.006546\n",
      "\tTraining batch 12 Loss: 0.216899\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000016\n",
      "\tTraining batch 15 Loss: 0.000011\n",
      "\tTraining batch 16 Loss: 0.789800\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.050608\n",
      "\tTraining batch 19 Loss: 0.173254\n",
      "\tTraining batch 20 Loss: 0.003202\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.350064\n",
      "\tTraining batch 23 Loss: 0.431906\n",
      "\tTraining batch 24 Loss: 0.506309\n",
      "\tTraining batch 25 Loss: 0.949103\n",
      "\tTraining batch 26 Loss: 0.095734\n",
      "\tTraining batch 27 Loss: 0.000054\n",
      "\tTraining batch 28 Loss: 0.000099\n",
      "\tTraining batch 29 Loss: 0.000004\n",
      "\tTraining batch 30 Loss: 0.001145\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.001710\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000226\n",
      "\tTraining batch 35 Loss: 0.154801\n",
      "\tTraining batch 36 Loss: 0.192111\n",
      "\tTraining batch 37 Loss: 0.069926\n",
      "\tTraining batch 38 Loss: 0.040025\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.046399\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.515272\n",
      "\tTraining batch 45 Loss: 0.059063\n",
      "\tTraining batch 46 Loss: 0.000011\n",
      "\tTraining batch 47 Loss: 0.015111\n",
      "\tTraining batch 48 Loss: 0.000077\n",
      "\tTraining batch 49 Loss: 0.202916\n",
      "\tTraining batch 50 Loss: 0.000076\n",
      "\tTraining batch 51 Loss: 0.005525\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000005\n",
      "\tTraining batch 55 Loss: 0.000001\n",
      "\tTraining batch 56 Loss: 0.000001\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.484968\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.007589\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 1.357437\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000015\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.048356\n",
      "\tTraining batch 69 Loss: 0.489082\n",
      "\tTraining batch 70 Loss: 0.010092\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.009448\n",
      "\tTraining batch 74 Loss: 0.819965\n",
      "Training set: Average loss: 0.123347\n",
      "Validation set: Average loss: 14.549814, Accuracy: 1382/1959 (70.55%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.073090\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000005\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.001233\n",
      "\tTraining batch 11 Loss: 0.000240\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.011178\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000001\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000004\n",
      "\tTraining batch 27 Loss: 0.378120\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000106\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.187556\n",
      "\tTraining batch 45 Loss: 0.001109\n",
      "\tTraining batch 46 Loss: 0.000058\n",
      "\tTraining batch 47 Loss: 0.315373\n",
      "\tTraining batch 48 Loss: 0.000009\n",
      "\tTraining batch 49 Loss: 0.000045\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000095\n",
      "\tTraining batch 54 Loss: 0.000501\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000003\n",
      "\tTraining batch 57 Loss: 0.000034\n",
      "\tTraining batch 58 Loss: 0.000426\n",
      "\tTraining batch 59 Loss: 0.000807\n",
      "\tTraining batch 60 Loss: 0.071562\n",
      "\tTraining batch 61 Loss: 0.112419\n",
      "\tTraining batch 62 Loss: 0.000001\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000192\n",
      "\tTraining batch 66 Loss: 0.000001\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000054\n",
      "\tTraining batch 69 Loss: 0.082950\n",
      "\tTraining batch 70 Loss: 0.000047\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.081039\n",
      "Training set: Average loss: 0.017814\n",
      "Validation set: Average loss: 15.673016, Accuracy: 1391/1959 (71.01%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000029\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000002\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000138\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000002\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000001\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000001\n",
      "\tTraining batch 47 Loss: 0.000003\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.013804\n",
      "\tTraining batch 50 Loss: 0.000236\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.166768\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.061589\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000038\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.003706\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000002\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.077327\n",
      "\tTraining batch 70 Loss: 0.000011\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.073600\n",
      "\tTraining batch 74 Loss: 0.031293\n",
      "Training set: Average loss: 0.005791\n",
      "Validation set: Average loss: 16.243776, Accuracy: 1385/1959 (70.70%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000001\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000006\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 15 Loss: 0.000068\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000531\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.003389\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.270348\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000001\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000136\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000070\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000032\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000001\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000045\n",
      "Training set: Average loss: 0.003711\n",
      "Validation set: Average loss: 16.707057, Accuracy: 1395/1959 (71.21%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000015\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000001\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000057\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000093\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000001\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000049\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000026\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000010\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000001\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000041\n",
      "Training set: Average loss: 0.000004\n",
      "Validation set: Average loss: 16.715460, Accuracy: 1394/1959 (71.16%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000010\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000001\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000033\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000086\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000001\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000034\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000020\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000008\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000001\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000039\n",
      "Training set: Average loss: 0.000003\n",
      "Validation set: Average loss: 16.717374, Accuracy: 1394/1959 (71.16%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 50.24999999999999%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000008\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000001\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000022\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000081\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000001\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000025\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000017\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000007\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000001\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000036\n",
      "\tTraining batch 75 Loss: 29.827301\n",
      "Training set: Average loss: 0.397700\n",
      "Validation set: Average loss: 14.828503, Accuracy: 1381/1959 (70.50%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000002\n",
      "\tTraining batch 3 Loss: 0.000381\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.000001\n",
      "\tTraining batch 6 Loss: 0.001990\n",
      "\tTraining batch 7 Loss: 0.505896\n",
      "\tTraining batch 8 Loss: 0.057023\n",
      "\tTraining batch 9 Loss: 0.018883\n",
      "\tTraining batch 10 Loss: 0.073295\n",
      "\tTraining batch 11 Loss: 0.266109\n",
      "\tTraining batch 12 Loss: 0.637349\n",
      "\tTraining batch 13 Loss: 0.037103\n",
      "\tTraining batch 14 Loss: 2.371078\n",
      "\tTraining batch 15 Loss: 0.000001\n",
      "\tTraining batch 16 Loss: 0.704081\n",
      "\tTraining batch 17 Loss: 0.042051\n",
      "\tTraining batch 18 Loss: 0.007397\n",
      "\tTraining batch 19 Loss: 0.008613\n",
      "\tTraining batch 20 Loss: 0.000471\n",
      "\tTraining batch 21 Loss: 0.005332\n",
      "\tTraining batch 22 Loss: 0.326287\n",
      "\tTraining batch 23 Loss: 0.000138\n",
      "\tTraining batch 24 Loss: 0.481218\n",
      "\tTraining batch 25 Loss: 0.454521\n",
      "\tTraining batch 26 Loss: 0.069329\n",
      "\tTraining batch 27 Loss: 0.000005\n",
      "\tTraining batch 28 Loss: 0.452403\n",
      "\tTraining batch 29 Loss: 0.301774\n",
      "\tTraining batch 30 Loss: 0.007953\n",
      "\tTraining batch 31 Loss: 0.000056\n",
      "\tTraining batch 32 Loss: 0.134710\n",
      "\tTraining batch 33 Loss: 0.128403\n",
      "\tTraining batch 34 Loss: 0.268582\n",
      "\tTraining batch 35 Loss: 0.000003\n",
      "\tTraining batch 36 Loss: 0.196058\n",
      "\tTraining batch 37 Loss: 0.292720\n",
      "\tTraining batch 38 Loss: 0.000096\n",
      "\tTraining batch 39 Loss: 0.325507\n",
      "\tTraining batch 40 Loss: 0.000494\n",
      "\tTraining batch 41 Loss: 0.002042\n",
      "\tTraining batch 42 Loss: 0.417911\n",
      "\tTraining batch 43 Loss: 1.198815\n",
      "\tTraining batch 44 Loss: 0.000036\n",
      "\tTraining batch 45 Loss: 0.137569\n",
      "\tTraining batch 46 Loss: 0.317443\n",
      "\tTraining batch 47 Loss: 0.002019\n",
      "\tTraining batch 48 Loss: 0.000567\n",
      "\tTraining batch 49 Loss: 0.000161\n",
      "\tTraining batch 50 Loss: 0.000216\n",
      "\tTraining batch 51 Loss: 0.000492\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000001\n",
      "\tTraining batch 54 Loss: 0.349477\n",
      "\tTraining batch 55 Loss: 0.016967\n",
      "\tTraining batch 56 Loss: 0.033510\n",
      "\tTraining batch 57 Loss: 0.003539\n",
      "\tTraining batch 58 Loss: 0.000006\n",
      "\tTraining batch 59 Loss: 0.111070\n",
      "\tTraining batch 60 Loss: 0.007037\n",
      "\tTraining batch 61 Loss: 0.000043\n",
      "\tTraining batch 62 Loss: 0.000002\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.465515\n",
      "\tTraining batch 66 Loss: 0.000383\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.530575\n",
      "\tTraining batch 69 Loss: 0.000104\n",
      "\tTraining batch 70 Loss: 0.000091\n",
      "\tTraining batch 71 Loss: 0.008708\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.005071\n",
      "\tTraining batch 74 Loss: 0.119641\n",
      "\tTraining batch 75 Loss: 1.276001\n",
      "Training set: Average loss: 0.175764\n",
      "Validation set: Average loss: 11.224111, Accuracy: 1380/1959 (70.44%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000001\n",
      "\tTraining batch 2 Loss: 0.000046\n",
      "\tTraining batch 3 Loss: 0.000012\n",
      "\tTraining batch 4 Loss: 0.011257\n",
      "\tTraining batch 5 Loss: 0.000274\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000027\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000329\n",
      "\tTraining batch 11 Loss: 0.000021\n",
      "\tTraining batch 12 Loss: 0.112257\n",
      "\tTraining batch 13 Loss: 0.000603\n",
      "\tTraining batch 14 Loss: 0.000148\n",
      "\tTraining batch 15 Loss: 0.185179\n",
      "\tTraining batch 16 Loss: 0.000028\n",
      "\tTraining batch 17 Loss: 0.000007\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.002083\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.115464\n",
      "\tTraining batch 23 Loss: 0.000744\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.052906\n",
      "\tTraining batch 27 Loss: 0.000010\n",
      "\tTraining batch 28 Loss: 0.000031\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000033\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.334700\n",
      "\tTraining batch 35 Loss: 0.000829\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000054\n",
      "\tTraining batch 38 Loss: 0.000016\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000542\n",
      "\tTraining batch 41 Loss: 0.000054\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.015163\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.132322\n",
      "\tTraining batch 46 Loss: 0.005536\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.010320\n",
      "\tTraining batch 49 Loss: 0.826236\n",
      "\tTraining batch 50 Loss: 0.000002\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000499\n",
      "\tTraining batch 53 Loss: 0.013576\n",
      "\tTraining batch 54 Loss: 0.002359\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.005613\n",
      "\tTraining batch 57 Loss: 0.005878\n",
      "\tTraining batch 58 Loss: 0.001168\n",
      "\tTraining batch 59 Loss: 0.000833\n",
      "\tTraining batch 60 Loss: 0.000001\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000058\n",
      "\tTraining batch 66 Loss: 0.000045\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000096\n",
      "\tTraining batch 69 Loss: 0.000016\n",
      "\tTraining batch 70 Loss: 0.101782\n",
      "\tTraining batch 71 Loss: 0.000373\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.130004\n",
      "\tTraining batch 74 Loss: 0.002035\n",
      "\tTraining batch 75 Loss: 0.073189\n",
      "Training set: Average loss: 0.028597\n",
      "Validation set: Average loss: 11.972304, Accuracy: 1384/1959 (70.65%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.001534\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000003\n",
      "\tTraining batch 4 Loss: 0.000118\n",
      "\tTraining batch 5 Loss: 0.001701\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000005\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.001032\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.035377\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000007\n",
      "\tTraining batch 18 Loss: 0.022675\n",
      "\tTraining batch 19 Loss: 0.003568\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.005333\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000001\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.041966\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000050\n",
      "\tTraining batch 43 Loss: 0.000001\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000206\n",
      "\tTraining batch 46 Loss: 0.007052\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000342\n",
      "\tTraining batch 55 Loss: 0.000024\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.004144\n",
      "\tTraining batch 58 Loss: 0.000004\n",
      "\tTraining batch 59 Loss: 0.000009\n",
      "\tTraining batch 60 Loss: 0.001123\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000004\n",
      "\tTraining batch 66 Loss: 0.000043\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.001836\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.004077\n",
      "Training set: Average loss: 0.001763\n",
      "Validation set: Average loss: 12.927555, Accuracy: 1381/1959 (70.50%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000301\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000001\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001446\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000325\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000002\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.001376\n",
      "\tTraining batch 43 Loss: 0.000003\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000026\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000114\n",
      "\tTraining batch 55 Loss: 0.000016\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.002627\n",
      "\tTraining batch 58 Loss: 0.000001\n",
      "\tTraining batch 59 Loss: 0.000016\n",
      "\tTraining batch 60 Loss: 0.000001\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000006\n",
      "\tTraining batch 66 Loss: 0.000045\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000011\n",
      "\tTraining batch 69 Loss: 0.000001\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000027\n",
      "Training set: Average loss: 0.000085\n",
      "Validation set: Average loss: 12.905965, Accuracy: 1383/1959 (70.60%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000001\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001391\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000317\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000002\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000024\n",
      "\tTraining batch 43 Loss: 0.000003\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000024\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000102\n",
      "\tTraining batch 55 Loss: 0.000014\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.002205\n",
      "\tTraining batch 58 Loss: 0.000001\n",
      "\tTraining batch 59 Loss: 0.000015\n",
      "\tTraining batch 60 Loss: 0.000001\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000006\n",
      "\tTraining batch 66 Loss: 0.000044\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000011\n",
      "\tTraining batch 69 Loss: 0.000001\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000019\n",
      "Training set: Average loss: 0.000056\n",
      "Validation set: Average loss: 12.918004, Accuracy: 1383/1959 (70.60%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 49.0%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000001\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001321\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000289\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000002\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000023\n",
      "\tTraining batch 43 Loss: 0.000003\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000022\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000092\n",
      "\tTraining batch 55 Loss: 0.000014\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.001885\n",
      "\tTraining batch 58 Loss: 0.000001\n",
      "\tTraining batch 59 Loss: 0.000015\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000006\n",
      "\tTraining batch 66 Loss: 0.000043\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000011\n",
      "\tTraining batch 69 Loss: 0.000001\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000015\n",
      "\tTraining batch 76 Loss: 20.996412\n",
      "Training set: Average loss: 0.276318\n",
      "Validation set: Average loss: 11.813150, Accuracy: 1362/1959 (69.53%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000021\n",
      "\tTraining batch 2 Loss: 0.000125\n",
      "\tTraining batch 3 Loss: 0.000001\n",
      "\tTraining batch 4 Loss: 0.002121\n",
      "\tTraining batch 5 Loss: 0.000003\n",
      "\tTraining batch 6 Loss: 0.000345\n",
      "\tTraining batch 7 Loss: 0.001812\n",
      "\tTraining batch 8 Loss: 0.010003\n",
      "\tTraining batch 9 Loss: 0.013957\n",
      "\tTraining batch 10 Loss: 0.070289\n",
      "\tTraining batch 11 Loss: 0.008176\n",
      "\tTraining batch 12 Loss: 0.000648\n",
      "\tTraining batch 13 Loss: 0.121329\n",
      "\tTraining batch 14 Loss: 0.000568\n",
      "\tTraining batch 15 Loss: 0.000477\n",
      "\tTraining batch 16 Loss: 0.000643\n",
      "\tTraining batch 17 Loss: 0.004947\n",
      "\tTraining batch 18 Loss: 0.222904\n",
      "\tTraining batch 19 Loss: 0.384370\n",
      "\tTraining batch 20 Loss: 0.006799\n",
      "\tTraining batch 21 Loss: 0.054566\n",
      "\tTraining batch 22 Loss: 0.116215\n",
      "\tTraining batch 23 Loss: 0.038664\n",
      "\tTraining batch 24 Loss: 0.001513\n",
      "\tTraining batch 25 Loss: 0.256844\n",
      "\tTraining batch 26 Loss: 0.061302\n",
      "\tTraining batch 27 Loss: 0.004619\n",
      "\tTraining batch 28 Loss: 0.227804\n",
      "\tTraining batch 29 Loss: 0.019804\n",
      "\tTraining batch 30 Loss: 0.008294\n",
      "\tTraining batch 31 Loss: 0.252157\n",
      "\tTraining batch 32 Loss: 0.000445\n",
      "\tTraining batch 33 Loss: 0.000093\n",
      "\tTraining batch 34 Loss: 0.000202\n",
      "\tTraining batch 35 Loss: 0.000003\n",
      "\tTraining batch 36 Loss: 0.002532\n",
      "\tTraining batch 37 Loss: 0.263037\n",
      "\tTraining batch 38 Loss: 0.041462\n",
      "\tTraining batch 39 Loss: 0.128680\n",
      "\tTraining batch 40 Loss: 0.195031\n",
      "\tTraining batch 41 Loss: 0.000156\n",
      "\tTraining batch 42 Loss: 0.000225\n",
      "\tTraining batch 43 Loss: 0.032258\n",
      "\tTraining batch 44 Loss: 0.012086\n",
      "\tTraining batch 45 Loss: 0.001202\n",
      "\tTraining batch 46 Loss: 0.097407\n",
      "\tTraining batch 47 Loss: 0.008777\n",
      "\tTraining batch 48 Loss: 0.003006\n",
      "\tTraining batch 49 Loss: 0.000776\n",
      "\tTraining batch 50 Loss: 0.000065\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000002\n",
      "\tTraining batch 53 Loss: 0.709266\n",
      "\tTraining batch 54 Loss: 0.006886\n",
      "\tTraining batch 55 Loss: 0.037490\n",
      "\tTraining batch 56 Loss: 0.032197\n",
      "\tTraining batch 57 Loss: 0.007367\n",
      "\tTraining batch 58 Loss: 0.000002\n",
      "\tTraining batch 59 Loss: 0.003590\n",
      "\tTraining batch 60 Loss: 0.000028\n",
      "\tTraining batch 61 Loss: 0.051982\n",
      "\tTraining batch 62 Loss: 0.021232\n",
      "\tTraining batch 63 Loss: 0.000008\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000817\n",
      "\tTraining batch 66 Loss: 0.147424\n",
      "\tTraining batch 67 Loss: 0.000044\n",
      "\tTraining batch 68 Loss: 0.008268\n",
      "\tTraining batch 69 Loss: 0.050738\n",
      "\tTraining batch 70 Loss: 0.001660\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.010045\n",
      "\tTraining batch 73 Loss: 0.000257\n",
      "\tTraining batch 74 Loss: 0.001385\n",
      "\tTraining batch 75 Loss: 0.000691\n",
      "\tTraining batch 76 Loss: 0.645484\n",
      "Training set: Average loss: 0.058100\n",
      "Validation set: Average loss: 9.118443, Accuracy: 1393/1959 (71.11%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.130871\n",
      "\tTraining batch 2 Loss: 0.000002\n",
      "\tTraining batch 3 Loss: 0.000678\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.000127\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000001\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000029\n",
      "\tTraining batch 11 Loss: 0.000332\n",
      "\tTraining batch 12 Loss: 0.048164\n",
      "\tTraining batch 13 Loss: 0.000004\n",
      "\tTraining batch 14 Loss: 0.475933\n",
      "\tTraining batch 15 Loss: 0.069170\n",
      "\tTraining batch 16 Loss: 0.000036\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.002329\n",
      "\tTraining batch 20 Loss: 0.000040\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000072\n",
      "\tTraining batch 23 Loss: 0.000032\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.046918\n",
      "\tTraining batch 27 Loss: 0.000001\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000153\n",
      "\tTraining batch 30 Loss: 0.000011\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000007\n",
      "\tTraining batch 33 Loss: 0.536507\n",
      "\tTraining batch 34 Loss: 0.000271\n",
      "\tTraining batch 35 Loss: 0.218888\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.092010\n",
      "\tTraining batch 38 Loss: 0.231871\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.223160\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000002\n",
      "\tTraining batch 43 Loss: 0.000001\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000629\n",
      "\tTraining batch 46 Loss: 0.000072\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000121\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.058605\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.010423\n",
      "\tTraining batch 57 Loss: 0.000487\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000014\n",
      "\tTraining batch 60 Loss: 0.300931\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000015\n",
      "\tTraining batch 66 Loss: 0.005280\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000002\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.017876\n",
      "\tTraining batch 73 Loss: 0.001289\n",
      "\tTraining batch 74 Loss: 0.027390\n",
      "\tTraining batch 75 Loss: 0.022737\n",
      "\tTraining batch 76 Loss: 0.952277\n",
      "Training set: Average loss: 0.045734\n",
      "Validation set: Average loss: 11.463594, Accuracy: 1354/1959 (69.12%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000057\n",
      "\tTraining batch 3 Loss: 0.002629\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.003891\n",
      "\tTraining batch 6 Loss: 0.000030\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000011\n",
      "\tTraining batch 11 Loss: 0.059340\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.003515\n",
      "\tTraining batch 15 Loss: 0.008356\n",
      "\tTraining batch 16 Loss: 0.003440\n",
      "\tTraining batch 17 Loss: 0.005646\n",
      "\tTraining batch 18 Loss: 0.007951\n",
      "\tTraining batch 19 Loss: 0.175396\n",
      "\tTraining batch 20 Loss: 0.000499\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.021312\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000254\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.090718\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000021\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.269343\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000003\n",
      "\tTraining batch 37 Loss: 0.001487\n",
      "\tTraining batch 38 Loss: 0.000003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 39 Loss: 0.310851\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.021692\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000124\n",
      "\tTraining batch 44 Loss: 0.000024\n",
      "\tTraining batch 45 Loss: 0.127629\n",
      "\tTraining batch 46 Loss: 0.000002\n",
      "\tTraining batch 47 Loss: 0.006399\n",
      "\tTraining batch 48 Loss: 0.000582\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000160\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.002476\n",
      "\tTraining batch 53 Loss: 0.083503\n",
      "\tTraining batch 54 Loss: 0.003682\n",
      "\tTraining batch 55 Loss: 0.014366\n",
      "\tTraining batch 56 Loss: 0.241442\n",
      "\tTraining batch 57 Loss: 0.008714\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000078\n",
      "\tTraining batch 60 Loss: 0.000066\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.001090\n",
      "\tTraining batch 66 Loss: 0.000287\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000001\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000008\n",
      "\tTraining batch 73 Loss: 0.001522\n",
      "\tTraining batch 74 Loss: 0.285297\n",
      "\tTraining batch 75 Loss: 0.698354\n",
      "\tTraining batch 76 Loss: 0.738891\n",
      "Training set: Average loss: 0.042120\n",
      "Validation set: Average loss: 13.696683, Accuracy: 1338/1959 (68.30%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000004\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000008\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000164\n",
      "\tTraining batch 17 Loss: 0.000001\n",
      "\tTraining batch 18 Loss: 0.001563\n",
      "\tTraining batch 19 Loss: 0.003807\n",
      "\tTraining batch 20 Loss: 0.431438\n",
      "\tTraining batch 21 Loss: 0.000001\n",
      "\tTraining batch 22 Loss: 0.006707\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.016259\n",
      "\tTraining batch 26 Loss: 0.006788\n",
      "\tTraining batch 27 Loss: 0.202430\n",
      "\tTraining batch 28 Loss: 0.000035\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.444111\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.032156\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000002\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000072\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.502061\n",
      "\tTraining batch 42 Loss: 0.000153\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.240283\n",
      "\tTraining batch 46 Loss: 0.000050\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000018\n",
      "\tTraining batch 51 Loss: 0.000001\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000123\n",
      "\tTraining batch 55 Loss: 0.054263\n",
      "\tTraining batch 56 Loss: 0.207112\n",
      "\tTraining batch 57 Loss: 0.421450\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000054\n",
      "\tTraining batch 60 Loss: 0.000001\n",
      "\tTraining batch 61 Loss: 0.000022\n",
      "\tTraining batch 62 Loss: 0.266941\n",
      "\tTraining batch 63 Loss: 0.039256\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.376841\n",
      "\tTraining batch 66 Loss: 0.004052\n",
      "\tTraining batch 67 Loss: 0.183734\n",
      "\tTraining batch 68 Loss: 0.000101\n",
      "\tTraining batch 69 Loss: 0.214389\n",
      "\tTraining batch 70 Loss: 0.004939\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.272735\n",
      "\tTraining batch 73 Loss: 0.139799\n",
      "\tTraining batch 74 Loss: 0.002576\n",
      "\tTraining batch 75 Loss: 0.022222\n",
      "\tTraining batch 76 Loss: 0.068900\n",
      "Training set: Average loss: 0.054837\n",
      "Validation set: Average loss: 19.282256, Accuracy: 1323/1959 (67.53%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000020\n",
      "\tTraining batch 5 Loss: 0.035706\n",
      "\tTraining batch 6 Loss: 0.000001\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.297507\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000009\n",
      "\tTraining batch 12 Loss: 0.000008\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.007967\n",
      "\tTraining batch 18 Loss: 0.535408\n",
      "\tTraining batch 19 Loss: 0.000358\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000018\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000050\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000001\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.001111\n",
      "\tTraining batch 43 Loss: 0.000219\n",
      "\tTraining batch 44 Loss: 0.000001\n",
      "\tTraining batch 45 Loss: 0.000074\n",
      "\tTraining batch 46 Loss: 0.002189\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000020\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000028\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000123\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000476\n",
      "\tTraining batch 58 Loss: 0.191709\n",
      "\tTraining batch 59 Loss: 0.000568\n",
      "\tTraining batch 60 Loss: 0.005259\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000017\n",
      "\tTraining batch 68 Loss: 0.000082\n",
      "\tTraining batch 69 Loss: 0.000002\n",
      "\tTraining batch 70 Loss: 0.209411\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.163645\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.032840\n",
      "Training set: Average loss: 0.019537\n",
      "Validation set: Average loss: 17.022876, Accuracy: 1358/1959 (69.32%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000096\n",
      "\tTraining batch 5 Loss: 0.000046\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.629982\n",
      "\tTraining batch 11 Loss: 0.000003\n",
      "\tTraining batch 12 Loss: 0.072362\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000009\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000767\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000001\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000003\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000002\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000006\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 43 Loss: 0.000003\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.011508\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.113710\n",
      "\tTraining batch 52 Loss: 0.000001\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000623\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000396\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000001\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000001\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.002485\n",
      "Training set: Average loss: 0.010947\n",
      "Validation set: Average loss: 15.586072, Accuracy: 1360/1959 (69.42%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.035762\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000202\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000453\n",
      "\tTraining batch 20 Loss: 0.000002\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000001\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000001\n",
      "\tTraining batch 29 Loss: 0.000044\n",
      "\tTraining batch 30 Loss: 0.024171\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000003\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000560\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000020\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000006\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000639\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000507\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000927\n",
      "\tTraining batch 57 Loss: 0.000072\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000008\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.119072\n",
      "\tTraining batch 66 Loss: 0.000008\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.004078\n",
      "Training set: Average loss: 0.002454\n",
      "Validation set: Average loss: 15.775637, Accuracy: 1356/1959 (69.22%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000889\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001355\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000011\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000006\n",
      "\tTraining batch 32 Loss: 0.000033\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000001\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000018\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.001009\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000460\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.002037\n",
      "Training set: Average loss: 0.000077\n",
      "Validation set: Average loss: 15.799895, Accuracy: 1361/1959 (69.47%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000001\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000024\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001362\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000011\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000007\n",
      "\tTraining batch 32 Loss: 0.000026\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000001\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000017\n",
      "\tTraining batch 46 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000972\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000424\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000968\n",
      "Training set: Average loss: 0.000050\n",
      "Validation set: Average loss: 15.802623, Accuracy: 1362/1959 (69.53%)\n",
      "\n",
      "Epoch: 11\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000022\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001327\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000010\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000006\n",
      "\tTraining batch 32 Loss: 0.000020\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000001\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000017\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000939\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000394\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000650\n",
      "Training set: Average loss: 0.000045\n",
      "Validation set: Average loss: 15.806730, Accuracy: 1363/1959 (69.58%)\n",
      "\n",
      "Epoch: 12\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000020\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001290\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000009\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000005\n",
      "\tTraining batch 32 Loss: 0.000016\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000001\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000017\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000903\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000367\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000487\n",
      "Training set: Average loss: 0.000041\n",
      "Validation set: Average loss: 15.811039, Accuracy: 1365/1959 (69.68%)\n",
      "\n",
      "Epoch: 13\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000018\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001251\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000009\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000004\n",
      "\tTraining batch 32 Loss: 0.000013\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000001\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000016\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000867\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000342\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000392\n",
      "Training set: Average loss: 0.000038\n",
      "Validation set: Average loss: 15.814635, Accuracy: 1365/1959 (69.68%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 49.0%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000017\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001213\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000008\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000003\n",
      "\tTraining batch 32 Loss: 0.000011\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000001\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000016\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000830\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000320\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000326\n",
      "\tTraining batch 77 Loss: 11.804631\n",
      "Training set: Average loss: 0.153343\n",
      "Validation set: Average loss: 13.620779, Accuracy: 1390/1959 (70.95%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000043\n",
      "\tTraining batch 5 Loss: 0.018693\n",
      "\tTraining batch 6 Loss: 0.105162\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.518305\n",
      "\tTraining batch 11 Loss: 0.743903\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000004\n",
      "\tTraining batch 14 Loss: 0.000004\n",
      "\tTraining batch 15 Loss: 0.047471\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001828\n",
      "\tTraining batch 20 Loss: 0.058084\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000001\n",
      "\tTraining batch 23 Loss: 0.000014\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.089400\n",
      "\tTraining batch 27 Loss: 0.000001\n",
      "\tTraining batch 28 Loss: 0.000003\n",
      "\tTraining batch 29 Loss: 0.072776\n",
      "\tTraining batch 30 Loss: 0.226884\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.162445\n",
      "\tTraining batch 33 Loss: 0.000001\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000008\n",
      "\tTraining batch 37 Loss: 0.000026\n",
      "\tTraining batch 38 Loss: 0.000258\n",
      "\tTraining batch 39 Loss: 0.059382\n",
      "\tTraining batch 40 Loss: 0.000193\n",
      "\tTraining batch 41 Loss: 0.094568\n",
      "\tTraining batch 42 Loss: 0.000513\n",
      "\tTraining batch 43 Loss: 0.004304\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000077\n",
      "\tTraining batch 46 Loss: 0.000008\n",
      "\tTraining batch 47 Loss: 0.000101\n",
      "\tTraining batch 48 Loss: 0.000011\n",
      "\tTraining batch 49 Loss: 0.101381\n",
      "\tTraining batch 50 Loss: 0.000001\n",
      "\tTraining batch 51 Loss: 0.000001\n",
      "\tTraining batch 52 Loss: 0.036178\n",
      "\tTraining batch 53 Loss: 0.000019\n",
      "\tTraining batch 54 Loss: 0.000058\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000106\n",
      "\tTraining batch 58 Loss: 0.000035\n",
      "\tTraining batch 59 Loss: 0.000002\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.027274\n",
      "\tTraining batch 64 Loss: 0.050678\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.002493\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.264376\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.001731\n",
      "\tTraining batch 75 Loss: 0.173622\n",
      "\tTraining batch 76 Loss: 0.053334\n",
      "\tTraining batch 77 Loss: 1.124158\n",
      "Training set: Average loss: 0.052466\n",
      "Validation set: Average loss: 12.894197, Accuracy: 1393/1959 (71.11%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000705\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.002930\n",
      "\tTraining batch 4 Loss: 0.009412\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000003\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000153\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.089364\n",
      "\tTraining batch 11 Loss: 0.000020\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.066732\n",
      "\tTraining batch 15 Loss: 0.000146\n",
      "\tTraining batch 16 Loss: 0.000023\n",
      "\tTraining batch 17 Loss: 0.000004\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.194696\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000010\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000001\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000001\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000002\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000005\n",
      "\tTraining batch 55 Loss: 0.000001\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000056\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000014\n",
      "\tTraining batch 66 Loss: 0.000013\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000639\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000008\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.213487\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000580\n",
      "\tTraining batch 77 Loss: 0.000329\n",
      "Training set: Average loss: 0.007524\n",
      "Validation set: Average loss: 14.936925, Accuracy: 1371/1959 (69.98%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000003\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000004\n",
      "\tTraining batch 6 Loss: 0.294281\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000464\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000047\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000014\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.041725\n",
      "\tTraining batch 19 Loss: 0.000911\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000001\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000054\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.852929\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000047\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.001702\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000055\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.379353\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000002\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000006\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000024\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000007\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.221132\n",
      "\tTraining batch 66 Loss: 0.000001\n",
      "\tTraining batch 67 Loss: 0.000028\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000001\n",
      "\tTraining batch 70 Loss: 0.439232\n",
      "\tTraining batch 71 Loss: 0.000022\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000108\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000922\n",
      "\tTraining batch 77 Loss: 0.214688\n",
      "Training set: Average loss: 0.031789\n",
      "Validation set: Average loss: 14.737892, Accuracy: 1377/1959 (70.29%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.006240\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.000184\n",
      "\tTraining batch 6 Loss: 0.101785\n",
      "\tTraining batch 7 Loss: 0.398526\n",
      "\tTraining batch 8 Loss: 0.054620\n",
      "\tTraining batch 9 Loss: 0.017455\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000001\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.043975\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001303\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000065\n",
      "\tTraining batch 27 Loss: 0.000320\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000002\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000001\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.018572\n",
      "\tTraining batch 41 Loss: 0.000003\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000002\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000002\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000001\n",
      "\tTraining batch 51 Loss: 0.009169\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000064\n",
      "\tTraining batch 55 Loss: 0.000001\n",
      "\tTraining batch 56 Loss: 0.000006\n",
      "\tTraining batch 57 Loss: 0.196201\n",
      "\tTraining batch 58 Loss: 0.000039\n",
      "\tTraining batch 59 Loss: 0.000422\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000001\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.058715\n",
      "\tTraining batch 67 Loss: 0.000001\n",
      "\tTraining batch 68 Loss: 0.000125\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000001\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000077\n",
      "\tTraining batch 75 Loss: 0.000209\n",
      "\tTraining batch 76 Loss: 0.000038\n",
      "\tTraining batch 77 Loss: 0.095432\n",
      "Training set: Average loss: 0.013033\n",
      "Validation set: Average loss: 13.271659, Accuracy: 1409/1959 (71.92%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000006\n",
      "\tTraining batch 3 Loss: 0.000002\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000001\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000705\n",
      "\tTraining batch 19 Loss: 0.000491\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000005\n",
      "\tTraining batch 25 Loss: 0.000002\n",
      "\tTraining batch 26 Loss: 0.000004\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000036\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000006\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000001\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000010\n",
      "\tTraining batch 51 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000003\n",
      "\tTraining batch 54 Loss: 0.000006\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000012\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000047\n",
      "\tTraining batch 62 Loss: 0.040947\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000002\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.304352\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.006266\n",
      "\tTraining batch 76 Loss: 0.240366\n",
      "\tTraining batch 77 Loss: 1.216367\n",
      "Training set: Average loss: 0.023502\n",
      "Validation set: Average loss: 16.775492, Accuracy: 1348/1959 (68.81%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.039193\n",
      "\tTraining batch 3 Loss: 0.000963\n",
      "\tTraining batch 4 Loss: 0.064711\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000010\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000001\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000001\n",
      "\tTraining batch 17 Loss: 0.007917\n",
      "\tTraining batch 18 Loss: 0.000001\n",
      "\tTraining batch 19 Loss: 0.000684\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000016\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000002\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000039\n",
      "\tTraining batch 27 Loss: 0.006150\n",
      "\tTraining batch 28 Loss: 0.195472\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000014\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000022\n",
      "\tTraining batch 46 Loss: 0.000002\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.082417\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000014\n",
      "\tTraining batch 54 Loss: 0.002236\n",
      "\tTraining batch 55 Loss: 0.000004\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.001486\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000372\n",
      "\tTraining batch 65 Loss: 0.000001\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000005\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.721652\n",
      "\tTraining batch 74 Loss: 1.844819\n",
      "\tTraining batch 75 Loss: 0.047995\n",
      "\tTraining batch 76 Loss: 0.082864\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "Training set: Average loss: 0.040248\n",
      "Validation set: Average loss: 17.434012, Accuracy: 1398/1959 (71.36%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.013473\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.770476\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000007\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000764\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000001\n",
      "\tTraining batch 16 Loss: 0.000047\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000612\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.354360\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.004623\n",
      "\tTraining batch 26 Loss: 0.001372\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000001\n",
      "\tTraining batch 29 Loss: 0.006286\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000003\n",
      "\tTraining batch 32 Loss: 0.604555\n",
      "\tTraining batch 33 Loss: 0.001950\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000138\n",
      "\tTraining batch 47 Loss: 0.222207\n",
      "\tTraining batch 48 Loss: 0.000002\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000685\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.089589\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000196\n",
      "\tTraining batch 55 Loss: 0.016742\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000315\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000002\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.240484\n",
      "\tTraining batch 64 Loss: 0.539302\n",
      "\tTraining batch 65 Loss: 0.000460\n",
      "\tTraining batch 66 Loss: 0.079802\n",
      "\tTraining batch 67 Loss: 0.000047\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000014\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.149346\n",
      "\tTraining batch 72 Loss: 0.000028\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000001\n",
      "\tTraining batch 75 Loss: 0.000002\n",
      "\tTraining batch 76 Loss: 0.691221\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "Training set: Average loss: 0.049209\n",
      "Validation set: Average loss: 15.825267, Accuracy: 1389/1959 (70.90%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000666\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.002056\n",
      "\tTraining batch 19 Loss: 0.001358\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000238\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.054469\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.119814\n",
      "\tTraining batch 43 Loss: 0.000008\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000003\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.017198\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 53 Loss: 0.170576\n",
      "\tTraining batch 54 Loss: 0.000802\n",
      "\tTraining batch 55 Loss: 0.124752\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000037\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000008\n",
      "\tTraining batch 60 Loss: 0.000004\n",
      "\tTraining batch 61 Loss: 0.000002\n",
      "\tTraining batch 62 Loss: 0.000129\n",
      "\tTraining batch 63 Loss: 0.084676\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.039142\n",
      "\tTraining batch 68 Loss: 0.127406\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.566896\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000004\n",
      "\tTraining batch 75 Loss: 0.071433\n",
      "\tTraining batch 76 Loss: 0.003665\n",
      "\tTraining batch 77 Loss: 0.919900\n",
      "Training set: Average loss: 0.029938\n",
      "Validation set: Average loss: 18.089304, Accuracy: 1397/1959 (71.31%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.310294\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000002\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.001110\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.003442\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000227\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000250\n",
      "\tTraining batch 26 Loss: 0.059109\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.195635\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.194584\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000085\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000001\n",
      "\tTraining batch 47 Loss: 0.000017\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000224\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.001333\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000001\n",
      "\tTraining batch 57 Loss: 0.000001\n",
      "\tTraining batch 58 Loss: 0.299229\n",
      "\tTraining batch 59 Loss: 0.000575\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.770635\n",
      "\tTraining batch 63 Loss: 0.001594\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.001086\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.176665\n",
      "\tTraining batch 69 Loss: 0.000024\n",
      "\tTraining batch 70 Loss: 0.003559\n",
      "\tTraining batch 71 Loss: 0.655464\n",
      "\tTraining batch 72 Loss: 0.191960\n",
      "\tTraining batch 73 Loss: 1.653264\n",
      "\tTraining batch 74 Loss: 0.104182\n",
      "\tTraining batch 75 Loss: 0.000004\n",
      "\tTraining batch 76 Loss: 0.000002\n",
      "\tTraining batch 77 Loss: 0.000001\n",
      "Training set: Average loss: 0.060059\n",
      "Validation set: Average loss: 16.258606, Accuracy: 1403/1959 (71.62%)\n",
      "\n",
      "Epoch: 11\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.041600\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.178624\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000001\n",
      "\tTraining batch 18 Loss: 0.093147\n",
      "\tTraining batch 19 Loss: 0.001439\n",
      "\tTraining batch 20 Loss: 0.205401\n",
      "\tTraining batch 21 Loss: 0.000007\n",
      "\tTraining batch 22 Loss: 0.000002\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.157030\n",
      "\tTraining batch 26 Loss: 0.187394\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.347652\n",
      "\tTraining batch 30 Loss: 0.216788\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.117598\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.785613\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000009\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000001\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000843\n",
      "\tTraining batch 51 Loss: 0.000001\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.618832\n",
      "\tTraining batch 54 Loss: 0.000001\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.285908\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.252739\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.023795\n",
      "\tTraining batch 68 Loss: 0.000303\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000154\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000044\n",
      "\tTraining batch 77 Loss: 0.164021\n",
      "Training set: Average loss: 0.047779\n",
      "Validation set: Average loss: 19.632350, Accuracy: 1377/1959 (70.29%)\n",
      "\n",
      "Epoch: 12\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.148168\n",
      "\tTraining batch 3 Loss: 0.059497\n",
      "\tTraining batch 4 Loss: 0.000030\n",
      "\tTraining batch 5 Loss: 0.221301\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.002470\n",
      "\tTraining batch 11 Loss: 0.165401\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.328920\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.403326\n",
      "\tTraining batch 18 Loss: 0.000024\n",
      "\tTraining batch 19 Loss: 0.001098\n",
      "\tTraining batch 20 Loss: 0.341263\n",
      "\tTraining batch 21 Loss: 0.005579\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.323899\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 1.841467\n",
      "\tTraining batch 27 Loss: 0.249220\n",
      "\tTraining batch 28 Loss: 0.000008\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000001\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.287589\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000004\n",
      "\tTraining batch 42 Loss: 0.000067\n",
      "\tTraining batch 43 Loss: 0.000052\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.055109\n",
      "\tTraining batch 46 Loss: 0.000144\n",
      "\tTraining batch 47 Loss: 0.000022\n",
      "\tTraining batch 48 Loss: 0.002056\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.016375\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 54 Loss: 0.001382\n",
      "\tTraining batch 55 Loss: 1.277307\n",
      "\tTraining batch 56 Loss: 0.000003\n",
      "\tTraining batch 57 Loss: 0.000124\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 1.022619\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.023167\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.011834\n",
      "\tTraining batch 69 Loss: 0.068679\n",
      "\tTraining batch 70 Loss: 0.000003\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.069752\n",
      "\tTraining batch 74 Loss: 0.000063\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "Training set: Average loss: 0.089974\n",
      "Validation set: Average loss: 15.963940, Accuracy: 1392/1959 (71.06%)\n",
      "\n",
      "Epoch: 13\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.217490\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.371425\n",
      "\tTraining batch 5 Loss: 0.008557\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.011527\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.004028\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.005750\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.002783\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000001\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.028899\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000023\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.299676\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000056\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.032349\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.064193\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000903\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000975\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000003\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.220536\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000498\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.023901\n",
      "\tTraining batch 75 Loss: 0.000001\n",
      "\tTraining batch 76 Loss: 0.000097\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "Training set: Average loss: 0.016801\n",
      "Validation set: Average loss: 17.517739, Accuracy: 1402/1959 (71.57%)\n",
      "\n",
      "Epoch: 14\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000307\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.012403\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000018\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000769\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.159326\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000001\n",
      "\tTraining batch 34 Loss: 0.000019\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000003\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000001\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000004\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000002\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000047\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000101\n",
      "\tTraining batch 57 Loss: 0.000084\n",
      "\tTraining batch 58 Loss: 0.008051\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.410966\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000326\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000011\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "Training set: Average loss: 0.007694\n",
      "Validation set: Average loss: 17.346285, Accuracy: 1405/1959 (71.72%)\n",
      "\n",
      "Epoch: 15\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.004775\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000004\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000003\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.090936\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000067\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.084983\n",
      "\tTraining batch 71 Loss: 0.017478\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000022\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "Training set: Average loss: 0.002578\n",
      "Validation set: Average loss: 15.221079, Accuracy: 1401/1959 (71.52%)\n",
      "\n",
      "Epoch: 16\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.009438\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000399\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000011\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.015991\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000007\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000005\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000189\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000010\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.052654\n",
      "\tTraining batch 52 Loss: 0.183328\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.017879\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.001081\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000002\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000002\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000066\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.023606\n",
      "\tTraining batch 74 Loss: 0.045410\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000316\n",
      "Training set: Average loss: 0.004551\n",
      "Validation set: Average loss: 17.655326, Accuracy: 1389/1959 (70.90%)\n",
      "\n",
      "Epoch: 17\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.656310\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.002767\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000014\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000002\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000001\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.147693\n",
      "\tTraining batch 51 Loss: 0.000006\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000056\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000027\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000007\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000002\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000007\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000113\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "Training set: Average loss: 0.010481\n",
      "Validation set: Average loss: 19.052061, Accuracy: 1394/1959 (71.16%)\n",
      "\n",
      "Epoch: 18\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000285\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000001\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000018\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000028\n",
      "\tTraining batch 55 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000008\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000026\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000001\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000013\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000112\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "Training set: Average loss: 0.000006\n",
      "Validation set: Average loss: 19.210512, Accuracy: 1390/1959 (70.95%)\n",
      "\n",
      "Epoch: 19\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000276\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000001\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000017\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000028\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000008\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000026\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000001\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000010\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000108\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "Training set: Average loss: 0.000006\n",
      "Validation set: Average loss: 19.216045, Accuracy: 1390/1959 (70.95%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 48.25%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000272\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000001\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000017\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000027\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000008\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000025\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000001\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000008\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000104\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 22.279623\n",
      "Training set: Average loss: 0.285642\n",
      "Validation set: Average loss: 18.447653, Accuracy: 1388/1959 (70.85%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.006752\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000003\n",
      "\tTraining batch 13 Loss: 0.012079\n",
      "\tTraining batch 14 Loss: 0.180313\n",
      "\tTraining batch 15 Loss: 0.259706\n",
      "\tTraining batch 16 Loss: 0.000003\n",
      "\tTraining batch 17 Loss: 0.000869\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.014190\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.540570\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.003429\n",
      "\tTraining batch 25 Loss: 0.000118\n",
      "\tTraining batch 26 Loss: 0.151715\n",
      "\tTraining batch 27 Loss: 0.001957\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.231208\n",
      "\tTraining batch 30 Loss: 0.000268\n",
      "\tTraining batch 31 Loss: 0.099128\n",
      "\tTraining batch 32 Loss: 0.295527\n",
      "\tTraining batch 33 Loss: 0.276654\n",
      "\tTraining batch 34 Loss: 0.874900\n",
      "\tTraining batch 35 Loss: 0.362345\n",
      "\tTraining batch 36 Loss: 0.114316\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 1.146806\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000197\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.001164\n",
      "\tTraining batch 46 Loss: 0.035310\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000007\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000003\n",
      "\tTraining batch 53 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 54 Loss: 0.001570\n",
      "\tTraining batch 55 Loss: 0.000225\n",
      "\tTraining batch 56 Loss: 0.273472\n",
      "\tTraining batch 57 Loss: 0.001643\n",
      "\tTraining batch 58 Loss: 0.000383\n",
      "\tTraining batch 59 Loss: 0.000024\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.002519\n",
      "\tTraining batch 62 Loss: 0.000020\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000098\n",
      "\tTraining batch 65 Loss: 0.000012\n",
      "\tTraining batch 66 Loss: 0.000747\n",
      "\tTraining batch 67 Loss: 0.000001\n",
      "\tTraining batch 68 Loss: 0.056820\n",
      "\tTraining batch 69 Loss: 0.001826\n",
      "\tTraining batch 70 Loss: 0.097601\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.002641\n",
      "\tTraining batch 74 Loss: 0.563879\n",
      "\tTraining batch 75 Loss: 0.000001\n",
      "\tTraining batch 76 Loss: 0.062841\n",
      "\tTraining batch 77 Loss: 0.000006\n",
      "\tTraining batch 78 Loss: 1.882858\n",
      "Training set: Average loss: 0.096907\n",
      "Validation set: Average loss: 18.237575, Accuracy: 1376/1959 (70.24%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000020\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.487806\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.033151\n",
      "\tTraining batch 18 Loss: 0.001929\n",
      "\tTraining batch 19 Loss: 0.000056\n",
      "\tTraining batch 20 Loss: 0.000003\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000742\n",
      "\tTraining batch 26 Loss: 0.050735\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.005651\n",
      "\tTraining batch 29 Loss: 0.775454\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.155697\n",
      "\tTraining batch 33 Loss: 0.100274\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.003726\n",
      "\tTraining batch 37 Loss: 0.098626\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.190499\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.668107\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.001454\n",
      "\tTraining batch 55 Loss: 0.317031\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000258\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.001401\n",
      "\tTraining batch 65 Loss: 0.496507\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.016337\n",
      "\tTraining batch 72 Loss: 0.000001\n",
      "\tTraining batch 73 Loss: 0.000008\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000067\n",
      "\tTraining batch 76 Loss: 2.517980\n",
      "\tTraining batch 77 Loss: 0.000003\n",
      "\tTraining batch 78 Loss: 0.705201\n",
      "Training set: Average loss: 0.084984\n",
      "Validation set: Average loss: 19.256894, Accuracy: 1380/1959 (70.44%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000001\n",
      "\tTraining batch 11 Loss: 0.000002\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.007724\n",
      "\tTraining batch 16 Loss: 0.000001\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.140957\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000001\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000001\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000001\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000015\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.014258\n",
      "\tTraining batch 54 Loss: 0.002493\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000003\n",
      "\tTraining batch 57 Loss: 0.000148\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.002615\n",
      "\tTraining batch 65 Loss: 0.274594\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000017\n",
      "\tTraining batch 76 Loss: 0.000596\n",
      "\tTraining batch 77 Loss: 0.000002\n",
      "\tTraining batch 78 Loss: 2.122786\n",
      "Training set: Average loss: 0.032900\n",
      "Validation set: Average loss: 20.765974, Accuracy: 1391/1959 (71.01%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.457257\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000978\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000019\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.088688\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.364784\n",
      "\tTraining batch 43 Loss: 0.000265\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000222\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 1.198808\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.224291\n",
      "\tTraining batch 57 Loss: 0.000093\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000349\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000001\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000067\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.197014\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.004863\n",
      "\tTraining batch 76 Loss: 0.019945\n",
      "\tTraining batch 77 Loss: 0.376397\n",
      "\tTraining batch 78 Loss: 0.408254\n",
      "Training set: Average loss: 0.042850\n",
      "Validation set: Average loss: 22.111763, Accuracy: 1379/1959 (70.39%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.530462\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.333566\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.020067\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.002231\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000019\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000036\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.188210\n",
      "\tTraining batch 32 Loss: 0.097419\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000180\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.073934\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000037\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.599194\n",
      "\tTraining batch 57 Loss: 0.023406\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.131278\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.099508\n",
      "\tTraining batch 62 Loss: 0.334270\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000001\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.040958\n",
      "\tTraining batch 75 Loss: 0.000184\n",
      "\tTraining batch 76 Loss: 0.000009\n",
      "\tTraining batch 77 Loss: 0.148862\n",
      "\tTraining batch 78 Loss: 0.000018\n",
      "Training set: Average loss: 0.033639\n",
      "Validation set: Average loss: 23.181071, Accuracy: 1382/1959 (70.55%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.002132\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000837\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.022756\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000002\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000259\n",
      "\tTraining batch 43 Loss: 0.000004\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000180\n",
      "\tTraining batch 46 Loss: 0.000001\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.003266\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.001766\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.426586\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.020589\n",
      "\tTraining batch 76 Loss: 0.697745\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "Training set: Average loss: 0.015078\n",
      "Validation set: Average loss: 24.496787, Accuracy: 1373/1959 (70.09%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.021516\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000001\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000004\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000046\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.006689\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.005901\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000366\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000006\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000039\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000698\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.048384\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.081101\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.052198\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "Training set: Average loss: 0.002781\n",
      "Validation set: Average loss: 24.452702, Accuracy: 1372/1959 (70.04%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.035430\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000390\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.014844\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.078349\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000009\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.242871\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.001017\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000078\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.163519\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "Training set: Average loss: 0.006878\n",
      "Validation set: Average loss: 24.246310, Accuracy: 1377/1959 (70.29%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001411\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.012038\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000943\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000001\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000001\n",
      "\tTraining batch 43 Loss: 0.000350\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000336\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000004\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000391\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.001206\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.022480\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000002\n",
      "\tTraining batch 69 Loss: 0.000037\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000245\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.002248\n",
      "Training set: Average loss: 0.000535\n",
      "Validation set: Average loss: 24.101918, Accuracy: 1375/1959 (70.19%)\n",
      "\n",
      "Epoch: 11\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000002\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000828\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000030\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000235\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000016\n",
      "\tTraining batch 47 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000327\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000349\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000004\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000005\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000001\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000213\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "Training set: Average loss: 0.000026\n",
      "Validation set: Average loss: 24.197185, Accuracy: 1374/1959 (70.14%)\n",
      "\n",
      "Epoch: 12\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000003\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000808\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000029\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000224\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000015\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000305\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000295\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000004\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000005\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000001\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000187\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "Training set: Average loss: 0.000024\n",
      "Validation set: Average loss: 24.208480, Accuracy: 1374/1959 (70.14%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 48.75%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000003\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000774\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000029\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000214\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000013\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000286\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000255\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000004\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000004\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000001\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000166\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 47.409035\n",
      "Training set: Average loss: 0.600137\n",
      "Validation set: Average loss: 21.467544, Accuracy: 1381/1959 (70.50%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.001735\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.223636\n",
      "\tTraining batch 4 Loss: 0.000169\n",
      "\tTraining batch 5 Loss: 0.690419\n",
      "\tTraining batch 6 Loss: 0.917987\n",
      "\tTraining batch 7 Loss: 0.673347\n",
      "\tTraining batch 8 Loss: 1.954747\n",
      "\tTraining batch 9 Loss: 0.308145\n",
      "\tTraining batch 10 Loss: 1.182600\n",
      "\tTraining batch 11 Loss: 0.000076\n",
      "\tTraining batch 12 Loss: 0.893613\n",
      "\tTraining batch 13 Loss: 0.513796\n",
      "\tTraining batch 14 Loss: 1.156345\n",
      "\tTraining batch 15 Loss: 0.579082\n",
      "\tTraining batch 16 Loss: 0.835282\n",
      "\tTraining batch 17 Loss: 3.400065\n",
      "\tTraining batch 18 Loss: 0.086873\n",
      "\tTraining batch 19 Loss: 0.341404\n",
      "\tTraining batch 20 Loss: 0.283192\n",
      "\tTraining batch 21 Loss: 0.968171\n",
      "\tTraining batch 22 Loss: 2.225776\n",
      "\tTraining batch 23 Loss: 0.000006\n",
      "\tTraining batch 24 Loss: 0.024460\n",
      "\tTraining batch 25 Loss: 0.409568\n",
      "\tTraining batch 26 Loss: 0.076225\n",
      "\tTraining batch 27 Loss: 0.099158\n",
      "\tTraining batch 28 Loss: 0.195502\n",
      "\tTraining batch 29 Loss: 0.431998\n",
      "\tTraining batch 30 Loss: 0.635001\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.111931\n",
      "\tTraining batch 33 Loss: 0.000002\n",
      "\tTraining batch 34 Loss: 0.000006\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000322\n",
      "\tTraining batch 37 Loss: 0.749469\n",
      "\tTraining batch 38 Loss: 0.156188\n",
      "\tTraining batch 39 Loss: 0.000116\n",
      "\tTraining batch 40 Loss: 0.004860\n",
      "\tTraining batch 41 Loss: 0.000001\n",
      "\tTraining batch 42 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 43 Loss: 0.006221\n",
      "\tTraining batch 44 Loss: 0.000037\n",
      "\tTraining batch 45 Loss: 0.000003\n",
      "\tTraining batch 46 Loss: 0.001915\n",
      "\tTraining batch 47 Loss: 0.000002\n",
      "\tTraining batch 48 Loss: 0.466145\n",
      "\tTraining batch 49 Loss: 0.701431\n",
      "\tTraining batch 50 Loss: 0.131556\n",
      "\tTraining batch 51 Loss: 0.000223\n",
      "\tTraining batch 52 Loss: 0.439247\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000213\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.007531\n",
      "\tTraining batch 57 Loss: 0.005567\n",
      "\tTraining batch 58 Loss: 0.007501\n",
      "\tTraining batch 59 Loss: 0.000131\n",
      "\tTraining batch 60 Loss: 0.001845\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.817955\n",
      "\tTraining batch 63 Loss: 0.006808\n",
      "\tTraining batch 64 Loss: 0.343314\n",
      "\tTraining batch 65 Loss: 0.344575\n",
      "\tTraining batch 66 Loss: 0.000005\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000197\n",
      "\tTraining batch 69 Loss: 0.001379\n",
      "\tTraining batch 70 Loss: 0.000363\n",
      "\tTraining batch 71 Loss: 0.000003\n",
      "\tTraining batch 72 Loss: 0.594571\n",
      "\tTraining batch 73 Loss: 0.493513\n",
      "\tTraining batch 74 Loss: 0.000003\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.205920\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.281445\n",
      "\tTraining batch 79 Loss: 4.904122\n",
      "Training set: Average loss: 0.378418\n",
      "Validation set: Average loss: 15.659320, Accuracy: 1363/1959 (69.58%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000048\n",
      "\tTraining batch 4 Loss: 0.000897\n",
      "\tTraining batch 5 Loss: 0.000033\n",
      "\tTraining batch 6 Loss: 0.105863\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000001\n",
      "\tTraining batch 11 Loss: 0.494893\n",
      "\tTraining batch 12 Loss: 0.250024\n",
      "\tTraining batch 13 Loss: 0.000261\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000063\n",
      "\tTraining batch 16 Loss: 0.000149\n",
      "\tTraining batch 17 Loss: 0.000001\n",
      "\tTraining batch 18 Loss: 0.000258\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.579148\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.002778\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.126022\n",
      "\tTraining batch 27 Loss: 0.000509\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.143595\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.001965\n",
      "\tTraining batch 33 Loss: 0.109665\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.184157\n",
      "\tTraining batch 36 Loss: 0.379263\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.285576\n",
      "\tTraining batch 39 Loss: 0.000465\n",
      "\tTraining batch 40 Loss: 0.273977\n",
      "\tTraining batch 41 Loss: 0.148801\n",
      "\tTraining batch 42 Loss: 0.024357\n",
      "\tTraining batch 43 Loss: 0.003971\n",
      "\tTraining batch 44 Loss: 0.595883\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000001\n",
      "\tTraining batch 47 Loss: 0.147675\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.459589\n",
      "\tTraining batch 50 Loss: 0.082309\n",
      "\tTraining batch 51 Loss: 0.109211\n",
      "\tTraining batch 52 Loss: 0.000009\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.119964\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.198266\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000032\n",
      "\tTraining batch 60 Loss: 0.471754\n",
      "\tTraining batch 61 Loss: 0.018231\n",
      "\tTraining batch 62 Loss: 0.000012\n",
      "\tTraining batch 63 Loss: 0.000173\n",
      "\tTraining batch 64 Loss: 0.000001\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000183\n",
      "\tTraining batch 68 Loss: 0.191854\n",
      "\tTraining batch 69 Loss: 0.000123\n",
      "\tTraining batch 70 Loss: 0.000843\n",
      "\tTraining batch 71 Loss: 0.241059\n",
      "\tTraining batch 72 Loss: 0.300658\n",
      "\tTraining batch 73 Loss: 0.321902\n",
      "\tTraining batch 74 Loss: 0.000003\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000061\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.036014\n",
      "\tTraining batch 79 Loss: 0.974314\n",
      "Training set: Average loss: 0.093505\n",
      "Validation set: Average loss: 19.320727, Accuracy: 1354/1959 (69.12%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.229916\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.421797\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.260179\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.018117\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.034668\n",
      "\tTraining batch 43 Loss: 0.000004\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.298858\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000014\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.290022\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000114\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.127307\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000002\n",
      "\tTraining batch 79 Loss: 0.013015\n",
      "Training set: Average loss: 0.021443\n",
      "Validation set: Average loss: 20.374803, Accuracy: 1363/1959 (69.58%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000066\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000005\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000005\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.001441\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000006\n",
      "\tTraining batch 37 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000156\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000030\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000062\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000003\n",
      "\tTraining batch 58 Loss: 0.004794\n",
      "\tTraining batch 59 Loss: 0.000010\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000004\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000004\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.031488\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.001086\n",
      "\tTraining batch 77 Loss: 0.602689\n",
      "\tTraining batch 78 Loss: 0.016134\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "Training set: Average loss: 0.008329\n",
      "Validation set: Average loss: 21.184631, Accuracy: 1380/1959 (70.44%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000503\n",
      "\tTraining batch 21 Loss: 0.000015\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.007363\n",
      "\tTraining batch 35 Loss: 0.321685\n",
      "\tTraining batch 36 Loss: 0.047103\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.142539\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000002\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.231287\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000003\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000003\n",
      "\tTraining batch 71 Loss: 0.006530\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000129\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "Training set: Average loss: 0.009584\n",
      "Validation set: Average loss: 22.940916, Accuracy: 1364/1959 (69.63%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000175\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000016\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000001\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000003\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.034440\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000003\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000012\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000001\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000003\n",
      "Training set: Average loss: 0.000439\n",
      "Validation set: Average loss: 22.581389, Accuracy: 1358/1959 (69.32%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000001\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000009\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000022\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000016\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000001\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000005\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000013\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000001\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000003\n",
      "Training set: Average loss: 0.000001\n",
      "Validation set: Average loss: 22.564709, Accuracy: 1359/1959 (69.37%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000001\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000008\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000009\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000016\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000001\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000004\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000013\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000001\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000003\n",
      "Training set: Average loss: 0.000001\n",
      "Validation set: Average loss: 22.565420, Accuracy: 1359/1959 (69.37%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 48.5%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000001\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000008\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000006\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000016\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000001\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000004\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000013\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000001\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000003\n",
      "\tTraining batch 80 Loss: 23.581451\n",
      "Training set: Average loss: 0.294769\n",
      "Validation set: Average loss: 19.801398, Accuracy: 1369/1959 (69.88%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.016434\n",
      "\tTraining batch 7 Loss: 0.000149\n",
      "\tTraining batch 8 Loss: 0.000002\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.009207\n",
      "\tTraining batch 12 Loss: 0.000004\n",
      "\tTraining batch 13 Loss: 0.717642\n",
      "\tTraining batch 14 Loss: 0.001759\n",
      "\tTraining batch 15 Loss: 0.000007\n",
      "\tTraining batch 16 Loss: 0.013149\n",
      "\tTraining batch 17 Loss: 0.474523\n",
      "\tTraining batch 18 Loss: 0.032063\n",
      "\tTraining batch 19 Loss: 0.000175\n",
      "\tTraining batch 20 Loss: 0.006593\n",
      "\tTraining batch 21 Loss: 0.133814\n",
      "\tTraining batch 22 Loss: 0.568372\n",
      "\tTraining batch 23 Loss: 0.000003\n",
      "\tTraining batch 24 Loss: 0.092331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 25 Loss: 0.011394\n",
      "\tTraining batch 26 Loss: 0.850142\n",
      "\tTraining batch 27 Loss: 0.019230\n",
      "\tTraining batch 28 Loss: 1.590934\n",
      "\tTraining batch 29 Loss: 0.000010\n",
      "\tTraining batch 30 Loss: 0.000098\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.209690\n",
      "\tTraining batch 33 Loss: 0.064966\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000457\n",
      "\tTraining batch 38 Loss: 0.000001\n",
      "\tTraining batch 39 Loss: 0.335764\n",
      "\tTraining batch 40 Loss: 0.214078\n",
      "\tTraining batch 41 Loss: 0.108835\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.036110\n",
      "\tTraining batch 44 Loss: 0.060290\n",
      "\tTraining batch 45 Loss: 0.686403\n",
      "\tTraining batch 46 Loss: 0.013396\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000001\n",
      "\tTraining batch 49 Loss: 0.083611\n",
      "\tTraining batch 50 Loss: 0.000221\n",
      "\tTraining batch 51 Loss: 0.056724\n",
      "\tTraining batch 52 Loss: 0.205536\n",
      "\tTraining batch 53 Loss: 0.039806\n",
      "\tTraining batch 54 Loss: 0.000398\n",
      "\tTraining batch 55 Loss: 0.000592\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.001213\n",
      "\tTraining batch 58 Loss: 0.245472\n",
      "\tTraining batch 59 Loss: 0.005677\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.379184\n",
      "\tTraining batch 64 Loss: 0.219769\n",
      "\tTraining batch 65 Loss: 0.012041\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000906\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.005889\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000728\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.090717\n",
      "\tTraining batch 79 Loss: 0.000103\n",
      "\tTraining batch 80 Loss: 0.162365\n",
      "Training set: Average loss: 0.097237\n",
      "Validation set: Average loss: 18.271056, Accuracy: 1378/1959 (70.34%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000001\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000002\n",
      "\tTraining batch 19 Loss: 0.127464\n",
      "\tTraining batch 20 Loss: 0.000001\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.156962\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000005\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000016\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000002\n",
      "\tTraining batch 39 Loss: 0.000275\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000568\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.002143\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.008867\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000021\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000027\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.388136\n",
      "\tTraining batch 79 Loss: 0.057818\n",
      "\tTraining batch 80 Loss: 0.139814\n",
      "Training set: Average loss: 0.011027\n",
      "Validation set: Average loss: 17.920639, Accuracy: 1396/1959 (71.26%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000031\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.172293\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000179\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.727283\n",
      "\tTraining batch 26 Loss: 0.010669\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.010330\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000001\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000001\n",
      "\tTraining batch 46 Loss: 0.000001\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000011\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000312\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000834\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000008\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000006\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000001\n",
      "\tTraining batch 72 Loss: 0.001291\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000003\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000001\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.534523\n",
      "\tTraining batch 80 Loss: 0.173433\n",
      "Training set: Average loss: 0.020390\n",
      "Validation set: Average loss: 17.577092, Accuracy: 1390/1959 (70.95%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000007\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000475\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.232337\n",
      "\tTraining batch 19 Loss: 0.000182\n",
      "\tTraining batch 20 Loss: 0.142596\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000171\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000056\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000025\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000141\n",
      "\tTraining batch 43 Loss: 0.000046\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000003\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000257\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000672\n",
      "\tTraining batch 55 Loss: 0.000430\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.010068\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000388\n",
      "\tTraining batch 60 Loss: 0.000011\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 1.176579\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000001\n",
      "\tTraining batch 67 Loss: 0.191142\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000268\n",
      "\tTraining batch 72 Loss: 0.338660\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.120294\n",
      "\tTraining batch 75 Loss: 0.000025\n",
      "\tTraining batch 76 Loss: 0.017595\n",
      "\tTraining batch 77 Loss: 0.000003\n",
      "\tTraining batch 78 Loss: 0.038958\n",
      "\tTraining batch 79 Loss: 0.008705\n",
      "\tTraining batch 80 Loss: 0.271237\n",
      "Training set: Average loss: 0.031892\n",
      "Validation set: Average loss: 15.938078, Accuracy: 1389/1959 (70.90%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000476\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000294\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000001\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.027831\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000157\n",
      "\tTraining batch 43 Loss: 0.000002\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000002\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000931\n",
      "\tTraining batch 49 Loss: 0.051656\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.006151\n",
      "\tTraining batch 55 Loss: 0.000001\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.005621\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000001\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000001\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000003\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.098507\n",
      "Training set: Average loss: 0.002395\n",
      "Validation set: Average loss: 19.085694, Accuracy: 1400/1959 (71.47%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000155\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000426\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000001\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000002\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000020\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000006\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000001\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000001\n",
      "\tTraining batch 46 Loss: 0.023658\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000938\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.004508\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000002\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000002\n",
      "\tTraining batch 76 Loss: 0.000086\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.034524\n",
      "\tTraining batch 80 Loss: 0.000001\n",
      "Training set: Average loss: 0.000804\n",
      "Validation set: Average loss: 18.146519, Accuracy: 1379/1959 (70.39%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000711\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000046\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000067\n",
      "\tTraining batch 30 Loss: 0.000002\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000001\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000001\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000001\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000693\n",
      "\tTraining batch 55 Loss: 0.000004\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.003911\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000455\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000011\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "Training set: Average loss: 0.000074\n",
      "Validation set: Average loss: 17.377566, Accuracy: 1388/1959 (70.85%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000691\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000038\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000010\n",
      "\tTraining batch 30 Loss: 0.000002\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000001\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000001\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000582\n",
      "\tTraining batch 55 Loss: 0.000003\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.002171\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000010\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "Training set: Average loss: 0.000044\n",
      "Validation set: Average loss: 17.406749, Accuracy: 1387/1959 (70.80%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000655\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000034\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000007\n",
      "\tTraining batch 30 Loss: 0.000002\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000001\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000001\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000500\n",
      "\tTraining batch 55 Loss: 0.000003\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.001596\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000009\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "Training set: Average loss: 0.000035\n",
      "Validation set: Average loss: 17.426362, Accuracy: 1388/1959 (70.85%)\n",
      "\n",
      "Epoch: 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000618\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000031\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000005\n",
      "\tTraining batch 30 Loss: 0.000002\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000001\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000001\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000434\n",
      "\tTraining batch 55 Loss: 0.000003\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.001262\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000009\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "Training set: Average loss: 0.000030\n",
      "Validation set: Average loss: 17.444887, Accuracy: 1388/1959 (70.85%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 48.75%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000583\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000028\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000004\n",
      "\tTraining batch 30 Loss: 0.000002\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000001\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000001\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000380\n",
      "\tTraining batch 55 Loss: 0.000003\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.001047\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000008\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 27.432230\n",
      "Training set: Average loss: 0.338695\n",
      "Validation set: Average loss: 15.929138, Accuracy: 1367/1959 (69.78%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.041569\n",
      "\tTraining batch 4 Loss: 0.005101\n",
      "\tTraining batch 5 Loss: 0.088462\n",
      "\tTraining batch 6 Loss: 0.812046\n",
      "\tTraining batch 7 Loss: 0.000221\n",
      "\tTraining batch 8 Loss: 0.195455\n",
      "\tTraining batch 9 Loss: 0.707576\n",
      "\tTraining batch 10 Loss: 0.190092\n",
      "\tTraining batch 11 Loss: 0.929510\n",
      "\tTraining batch 12 Loss: 0.010378\n",
      "\tTraining batch 13 Loss: 0.000002\n",
      "\tTraining batch 14 Loss: 1.159651\n",
      "\tTraining batch 15 Loss: 0.102511\n",
      "\tTraining batch 16 Loss: 0.505358\n",
      "\tTraining batch 17 Loss: 0.051750\n",
      "\tTraining batch 18 Loss: 0.000503\n",
      "\tTraining batch 19 Loss: 1.009813\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.002894\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.011532\n",
      "\tTraining batch 27 Loss: 0.218371\n",
      "\tTraining batch 28 Loss: 0.204180\n",
      "\tTraining batch 29 Loss: 0.147499\n",
      "\tTraining batch 30 Loss: 0.432546\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000060\n",
      "\tTraining batch 33 Loss: 0.005002\n",
      "\tTraining batch 34 Loss: 0.657577\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.559489\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.092960\n",
      "\tTraining batch 39 Loss: 0.070174\n",
      "\tTraining batch 40 Loss: 0.928105\n",
      "\tTraining batch 41 Loss: 0.231501\n",
      "\tTraining batch 42 Loss: 0.271411\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000618\n",
      "\tTraining batch 46 Loss: 0.000113\n",
      "\tTraining batch 47 Loss: 0.301159\n",
      "\tTraining batch 48 Loss: 0.000005\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.225689\n",
      "\tTraining batch 51 Loss: 0.000266\n",
      "\tTraining batch 52 Loss: 0.193373\n",
      "\tTraining batch 53 Loss: 0.104095\n",
      "\tTraining batch 54 Loss: 0.003852\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.007152\n",
      "\tTraining batch 57 Loss: 0.001311\n",
      "\tTraining batch 58 Loss: 0.000255\n",
      "\tTraining batch 59 Loss: 0.046344\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.271571\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000026\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000001\n",
      "\tTraining batch 66 Loss: 1.460779\n",
      "\tTraining batch 67 Loss: 0.555235\n",
      "\tTraining batch 68 Loss: 0.031357\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.486350\n",
      "\tTraining batch 71 Loss: 0.000025\n",
      "\tTraining batch 72 Loss: 0.316647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.003194\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.240818\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.154238\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.532661\n",
      "\tTraining batch 81 Loss: 0.756724\n",
      "Training set: Average loss: 0.189348\n",
      "Validation set: Average loss: 16.992097, Accuracy: 1405/1959 (71.72%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.057138\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001955\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.818059\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.001124\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.056716\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000002\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000010\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000001\n",
      "\tTraining batch 48 Loss: 0.000007\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000044\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.001906\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.001009\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000001\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.006932\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.098732\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000001\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.863060\n",
      "Training set: Average loss: 0.023539\n",
      "Validation set: Average loss: 16.914985, Accuracy: 1398/1959 (71.36%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.114039\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000002\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.006313\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000002\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.063249\n",
      "\tTraining batch 27 Loss: 0.000409\n",
      "\tTraining batch 28 Loss: 1.420081\n",
      "\tTraining batch 29 Loss: 1.901078\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.343669\n",
      "\tTraining batch 38 Loss: 0.000001\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000001\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.017051\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000002\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.012617\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.894979\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.561057\n",
      "\tTraining batch 53 Loss: 0.055617\n",
      "\tTraining batch 54 Loss: 0.000607\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.043377\n",
      "\tTraining batch 57 Loss: 0.005308\n",
      "\tTraining batch 58 Loss: 0.373398\n",
      "\tTraining batch 59 Loss: 0.009290\n",
      "\tTraining batch 60 Loss: 0.000002\n",
      "\tTraining batch 61 Loss: 0.595042\n",
      "\tTraining batch 62 Loss: 0.105038\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000634\n",
      "\tTraining batch 65 Loss: 0.000095\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.011783\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000003\n",
      "\tTraining batch 74 Loss: 0.214002\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000001\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000003\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000008\n",
      "Training set: Average loss: 0.083318\n",
      "Validation set: Average loss: 18.172926, Accuracy: 1403/1959 (71.62%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000593\n",
      "\tTraining batch 6 Loss: 0.000041\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000002\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000302\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000014\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000024\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.188932\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000001\n",
      "\tTraining batch 46 Loss: 0.000032\n",
      "\tTraining batch 47 Loss: 0.007572\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.001449\n",
      "\tTraining batch 55 Loss: 0.000001\n",
      "\tTraining batch 56 Loss: 0.003953\n",
      "\tTraining batch 57 Loss: 0.002488\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.231745\n",
      "\tTraining batch 61 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000001\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.296861\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.318716\n",
      "Training set: Average loss: 0.012997\n",
      "Validation set: Average loss: 17.588767, Accuracy: 1398/1959 (71.36%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000122\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000001\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000003\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000569\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000008\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000036\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000216\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000367\n",
      "\tTraining batch 55 Loss: 0.000003\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000071\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000096\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000171\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.287475\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000002\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.001010\n",
      "\tTraining batch 76 Loss: 0.054853\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "Training set: Average loss: 0.004259\n",
      "Validation set: Average loss: 18.233120, Accuracy: 1380/1959 (70.44%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.008345\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.056680\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000001\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000242\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000017\n",
      "\tTraining batch 46 Loss: 0.000007\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.001172\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000416\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000001\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.156194\n",
      "\tTraining batch 76 Loss: 0.000251\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "Training set: Average loss: 0.002757\n",
      "Validation set: Average loss: 17.300920, Accuracy: 1405/1959 (71.72%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.003526\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.003615\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000004\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000155\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000037\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "Training set: Average loss: 0.000091\n",
      "Validation set: Average loss: 17.794878, Accuracy: 1395/1959 (71.21%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.002525\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.002045\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000003\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000131\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000035\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "Training set: Average loss: 0.000059\n",
      "Validation set: Average loss: 17.854252, Accuracy: 1395/1959 (71.21%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 49.25%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.002189\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.001527\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000003\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000118\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000033\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 22.558241\n",
      "Training set: Average loss: 0.275148\n",
      "Validation set: Average loss: 18.858532, Accuracy: 1378/1959 (70.34%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000030\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 1.031567\n",
      "\tTraining batch 7 Loss: 0.139699\n",
      "\tTraining batch 8 Loss: 1.729892\n",
      "\tTraining batch 9 Loss: 0.382506\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.020749\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000981\n",
      "\tTraining batch 14 Loss: 0.141964\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000002\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.005396\n",
      "\tTraining batch 20 Loss: 0.080158\n",
      "\tTraining batch 21 Loss: 0.002146\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.023829\n",
      "\tTraining batch 25 Loss: 0.002056\n",
      "\tTraining batch 26 Loss: 0.084612\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.537337\n",
      "\tTraining batch 29 Loss: 0.069864\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.273696\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000185\n",
      "\tTraining batch 35 Loss: 0.143396\n",
      "\tTraining batch 36 Loss: 0.025795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 37 Loss: 0.037441\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.295611\n",
      "\tTraining batch 40 Loss: 0.015053\n",
      "\tTraining batch 41 Loss: 0.070377\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.002974\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000085\n",
      "\tTraining batch 46 Loss: 0.051648\n",
      "\tTraining batch 47 Loss: 0.000649\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.474725\n",
      "\tTraining batch 54 Loss: 0.003388\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000167\n",
      "\tTraining batch 57 Loss: 0.087554\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.002433\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.126528\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.007687\n",
      "\tTraining batch 66 Loss: 0.000005\n",
      "\tTraining batch 67 Loss: 0.000828\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.559775\n",
      "\tTraining batch 70 Loss: 0.000001\n",
      "\tTraining batch 71 Loss: 0.000007\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.270546\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000205\n",
      "\tTraining batch 76 Loss: 0.000017\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.356917\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.090596\n",
      "\tTraining batch 81 Loss: 0.000001\n",
      "\tTraining batch 82 Loss: 3.408739\n",
      "Training set: Average loss: 0.128778\n",
      "Validation set: Average loss: 19.299253, Accuracy: 1365/1959 (69.68%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.400706\n",
      "\tTraining batch 14 Loss: 0.000002\n",
      "\tTraining batch 15 Loss: 0.000006\n",
      "\tTraining batch 16 Loss: 0.000124\n",
      "\tTraining batch 17 Loss: 0.661241\n",
      "\tTraining batch 18 Loss: 0.084648\n",
      "\tTraining batch 19 Loss: 0.028718\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000002\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000010\n",
      "\tTraining batch 26 Loss: 0.079262\n",
      "\tTraining batch 27 Loss: 0.073189\n",
      "\tTraining batch 28 Loss: 0.000230\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.029984\n",
      "\tTraining batch 31 Loss: 1.014528\n",
      "\tTraining batch 32 Loss: 0.026935\n",
      "\tTraining batch 33 Loss: 0.631501\n",
      "\tTraining batch 34 Loss: 0.397034\n",
      "\tTraining batch 35 Loss: 0.021598\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000919\n",
      "\tTraining batch 44 Loss: 0.000446\n",
      "\tTraining batch 45 Loss: 0.000004\n",
      "\tTraining batch 46 Loss: 0.245096\n",
      "\tTraining batch 47 Loss: 0.000080\n",
      "\tTraining batch 48 Loss: 0.345803\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.001192\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000571\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000102\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.125319\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.243852\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.032951\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000210\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000031\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.505735\n",
      "\tTraining batch 82 Loss: 0.133881\n",
      "Training set: Average loss: 0.062023\n",
      "Validation set: Average loss: 22.622635, Accuracy: 1364/1959 (69.63%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000079\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000005\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001173\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.626574\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000010\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.050475\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000004\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000157\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000936\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.029819\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000641\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000001\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000004\n",
      "\tTraining batch 77 Loss: 0.000001\n",
      "\tTraining batch 78 Loss: 0.000004\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "Training set: Average loss: 0.008657\n",
      "Validation set: Average loss: 20.402965, Accuracy: 1368/1959 (69.83%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000001\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000866\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000388\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000002\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.001395\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000517\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000001\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000005\n",
      "\tTraining batch 77 Loss: 0.000003\n",
      "\tTraining batch 78 Loss: 0.000003\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "Training set: Average loss: 0.000039\n",
      "Validation set: Average loss: 20.431608, Accuracy: 1368/1959 (69.83%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 48.5%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000822\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000181\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000002\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.001327\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000486\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000001\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000005\n",
      "\tTraining batch 77 Loss: 0.000003\n",
      "\tTraining batch 78 Loss: 0.000003\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 24.439697\n",
      "Training set: Average loss: 0.294488\n",
      "Validation set: Average loss: 18.512857, Accuracy: 1375/1959 (70.19%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.264345\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.707790\n",
      "\tTraining batch 10 Loss: 0.000021\n",
      "\tTraining batch 11 Loss: 0.113868\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000001\n",
      "\tTraining batch 15 Loss: 0.000009\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000003\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.003735\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.455134\n",
      "\tTraining batch 22 Loss: 0.058589\n",
      "\tTraining batch 23 Loss: 0.000693\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.437955\n",
      "\tTraining batch 26 Loss: 0.380636\n",
      "\tTraining batch 27 Loss: 0.265532\n",
      "\tTraining batch 28 Loss: 0.039060\n",
      "\tTraining batch 29 Loss: 0.005946\n",
      "\tTraining batch 30 Loss: 0.002292\n",
      "\tTraining batch 31 Loss: 0.000041\n",
      "\tTraining batch 32 Loss: 0.140716\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.025608\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000412\n",
      "\tTraining batch 38 Loss: 0.244369\n",
      "\tTraining batch 39 Loss: 0.003069\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.076042\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.918385\n",
      "\tTraining batch 46 Loss: 0.000045\n",
      "\tTraining batch 47 Loss: 0.228152\n",
      "\tTraining batch 48 Loss: 0.417653\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.001982\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000869\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000003\n",
      "\tTraining batch 60 Loss: 0.000003\n",
      "\tTraining batch 61 Loss: 0.079525\n",
      "\tTraining batch 62 Loss: 0.000002\n",
      "\tTraining batch 63 Loss: 0.000020\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000005\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.244654\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000003\n",
      "\tTraining batch 71 Loss: 0.070309\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000018\n",
      "\tTraining batch 74 Loss: 0.000110\n",
      "\tTraining batch 75 Loss: 0.283235\n",
      "\tTraining batch 76 Loss: 0.002729\n",
      "\tTraining batch 77 Loss: 0.001158\n",
      "\tTraining batch 78 Loss: 0.000102\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000016\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.462780\n",
      "\tTraining batch 83 Loss: 2.282513\n",
      "Training set: Average loss: 0.099038\n",
      "Validation set: Average loss: 16.820471, Accuracy: 1358/1959 (69.32%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.688622\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 5 Loss: 1.269078\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000017\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000001\n",
      "\tTraining batch 11 Loss: 0.000034\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000004\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000002\n",
      "\tTraining batch 19 Loss: 0.003492\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000001\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.320204\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000028\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000018\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000005\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.235660\n",
      "\tTraining batch 38 Loss: 0.000279\n",
      "\tTraining batch 39 Loss: 0.000002\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000006\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000031\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000002\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.269906\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000189\n",
      "\tTraining batch 58 Loss: 0.001883\n",
      "\tTraining batch 59 Loss: 0.000026\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000016\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.341312\n",
      "\tTraining batch 64 Loss: 0.008549\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000004\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000001\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000467\n",
      "\tTraining batch 77 Loss: 0.001639\n",
      "\tTraining batch 78 Loss: 0.065075\n",
      "\tTraining batch 79 Loss: 0.430545\n",
      "\tTraining batch 80 Loss: 0.147130\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.129446\n",
      "\tTraining batch 83 Loss: 2.816677\n",
      "Training set: Average loss: 0.081089\n",
      "Validation set: Average loss: 20.619804, Accuracy: 1359/1959 (69.37%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.352730\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000770\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.183277\n",
      "\tTraining batch 10 Loss: 1.048640\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.160351\n",
      "\tTraining batch 13 Loss: 0.000001\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000049\n",
      "\tTraining batch 16 Loss: 0.000001\n",
      "\tTraining batch 17 Loss: 0.000001\n",
      "\tTraining batch 18 Loss: 0.002603\n",
      "\tTraining batch 19 Loss: 0.001573\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000016\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.051206\n",
      "\tTraining batch 27 Loss: 0.000158\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.436309\n",
      "\tTraining batch 36 Loss: 0.003019\n",
      "\tTraining batch 37 Loss: 0.189870\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000002\n",
      "\tTraining batch 40 Loss: 0.304913\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.001919\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000483\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000021\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000001\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.005144\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000001\n",
      "\tTraining batch 57 Loss: 0.010691\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000174\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000042\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000039\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.115934\n",
      "\tTraining batch 68 Loss: 1.438715\n",
      "\tTraining batch 69 Loss: 0.618078\n",
      "\tTraining batch 70 Loss: 0.005224\n",
      "\tTraining batch 71 Loss: 0.000028\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.721300\n",
      "\tTraining batch 75 Loss: 0.003075\n",
      "\tTraining batch 76 Loss: 0.053759\n",
      "\tTraining batch 77 Loss: 0.000017\n",
      "\tTraining batch 78 Loss: 0.000002\n",
      "\tTraining batch 79 Loss: 0.000002\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.243366\n",
      "\tTraining batch 82 Loss: 0.005634\n",
      "\tTraining batch 83 Loss: 0.891625\n",
      "Training set: Average loss: 0.082539\n",
      "Validation set: Average loss: 19.137035, Accuracy: 1333/1959 (68.04%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000029\n",
      "\tTraining batch 2 Loss: 0.000001\n",
      "\tTraining batch 3 Loss: 0.000036\n",
      "\tTraining batch 4 Loss: 0.000160\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.419823\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000821\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000815\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.032569\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.002662\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000010\n",
      "\tTraining batch 35 Loss: 0.000008\n",
      "\tTraining batch 36 Loss: 0.214717\n",
      "\tTraining batch 37 Loss: 0.000001\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.023931\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000002\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000036\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000002\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.685716\n",
      "\tTraining batch 51 Loss: 0.000004\n",
      "\tTraining batch 52 Loss: 0.000089\n",
      "\tTraining batch 53 Loss: 0.276051\n",
      "\tTraining batch 54 Loss: 0.053536\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.001876\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.079173\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.032464\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.009230\n",
      "\tTraining batch 73 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.010639\n",
      "\tTraining batch 77 Loss: 0.009519\n",
      "\tTraining batch 78 Loss: 0.285844\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "Training set: Average loss: 0.025780\n",
      "Validation set: Average loss: 18.688655, Accuracy: 1367/1959 (69.78%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.005788\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000005\n",
      "\tTraining batch 25 Loss: 0.000124\n",
      "\tTraining batch 26 Loss: 0.021223\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.130952\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000010\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000013\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000048\n",
      "\tTraining batch 54 Loss: 0.002783\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.001538\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.001996\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000009\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000006\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000243\n",
      "\tTraining batch 83 Loss: 0.000078\n",
      "Training set: Average loss: 0.001986\n",
      "Validation set: Average loss: 18.503402, Accuracy: 1370/1959 (69.93%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001497\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.001257\n",
      "\tTraining batch 26 Loss: 0.003646\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000004\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.001745\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000710\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000001\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000006\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000006\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000132\n",
      "\tTraining batch 83 Loss: 0.000005\n",
      "Training set: Average loss: 0.000109\n",
      "Validation set: Average loss: 18.580959, Accuracy: 1366/1959 (69.73%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001283\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.001710\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000004\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.001350\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 57 Loss: 0.000539\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000001\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000005\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000006\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000099\n",
      "\tTraining batch 83 Loss: 0.000002\n",
      "Training set: Average loss: 0.000060\n",
      "Validation set: Average loss: 18.620645, Accuracy: 1366/1959 (69.73%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 48.25%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001131\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000988\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000004\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.001142\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000439\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000001\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000004\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000006\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000081\n",
      "\tTraining batch 83 Loss: 0.000001\n",
      "\tTraining batch 84 Loss: 27.846333\n",
      "Training set: Average loss: 0.331549\n",
      "Validation set: Average loss: 16.610407, Accuracy: 1385/1959 (70.70%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.043775\n",
      "\tTraining batch 4 Loss: 0.000305\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.176601\n",
      "\tTraining batch 10 Loss: 1.029008\n",
      "\tTraining batch 11 Loss: 0.172704\n",
      "\tTraining batch 12 Loss: 0.713111\n",
      "\tTraining batch 13 Loss: 0.000049\n",
      "\tTraining batch 14 Loss: 0.972468\n",
      "\tTraining batch 15 Loss: 0.558429\n",
      "\tTraining batch 16 Loss: 0.048696\n",
      "\tTraining batch 17 Loss: 0.130174\n",
      "\tTraining batch 18 Loss: 0.000437\n",
      "\tTraining batch 19 Loss: 0.008813\n",
      "\tTraining batch 20 Loss: 0.000113\n",
      "\tTraining batch 21 Loss: 0.479980\n",
      "\tTraining batch 22 Loss: 0.335967\n",
      "\tTraining batch 23 Loss: 0.672234\n",
      "\tTraining batch 24 Loss: 0.555576\n",
      "\tTraining batch 25 Loss: 1.368996\n",
      "\tTraining batch 26 Loss: 0.440584\n",
      "\tTraining batch 27 Loss: 0.078879\n",
      "\tTraining batch 28 Loss: 0.512959\n",
      "\tTraining batch 29 Loss: 0.193318\n",
      "\tTraining batch 30 Loss: 0.218014\n",
      "\tTraining batch 31 Loss: 0.197061\n",
      "\tTraining batch 32 Loss: 0.046860\n",
      "\tTraining batch 33 Loss: 0.570950\n",
      "\tTraining batch 34 Loss: 0.000020\n",
      "\tTraining batch 35 Loss: 0.067447\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.525769\n",
      "\tTraining batch 38 Loss: 0.119839\n",
      "\tTraining batch 39 Loss: 0.000985\n",
      "\tTraining batch 40 Loss: 0.000009\n",
      "\tTraining batch 41 Loss: 0.165614\n",
      "\tTraining batch 42 Loss: 0.000337\n",
      "\tTraining batch 43 Loss: 0.048997\n",
      "\tTraining batch 44 Loss: 0.898254\n",
      "\tTraining batch 45 Loss: 1.313132\n",
      "\tTraining batch 46 Loss: 0.525393\n",
      "\tTraining batch 47 Loss: 0.373590\n",
      "\tTraining batch 48 Loss: 1.215677\n",
      "\tTraining batch 49 Loss: 0.243151\n",
      "\tTraining batch 50 Loss: 0.152192\n",
      "\tTraining batch 51 Loss: 2.599951\n",
      "\tTraining batch 52 Loss: 1.939479\n",
      "\tTraining batch 53 Loss: 1.537243\n",
      "\tTraining batch 54 Loss: 0.744020\n",
      "\tTraining batch 55 Loss: 0.563478\n",
      "\tTraining batch 56 Loss: 0.186435\n",
      "\tTraining batch 57 Loss: 0.000577\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.184638\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.771872\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000014\n",
      "\tTraining batch 67 Loss: 0.094602\n",
      "\tTraining batch 68 Loss: 0.542590\n",
      "\tTraining batch 69 Loss: 0.000001\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000001\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.219713\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.002020\n",
      "\tTraining batch 83 Loss: 0.163872\n",
      "\tTraining batch 84 Loss: 7.403787\n",
      "Training set: Average loss: 0.382509\n",
      "Validation set: Average loss: 19.272961, Accuracy: 1374/1959 (70.14%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.001834\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.042806\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.462037\n",
      "\tTraining batch 15 Loss: 0.013173\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.003441\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000248\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 1.005743\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 1.038567\n",
      "\tTraining batch 27 Loss: 0.000001\n",
      "\tTraining batch 28 Loss: 0.000001\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000020\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 36 Loss: 0.012190\n",
      "\tTraining batch 37 Loss: 0.178531\n",
      "\tTraining batch 38 Loss: 0.253089\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000001\n",
      "\tTraining batch 43 Loss: 0.000587\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000002\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.040732\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.002588\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.007267\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.001963\n",
      "\tTraining batch 57 Loss: 0.000012\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000030\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.641926\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.176256\n",
      "\tTraining batch 66 Loss: 0.000002\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000002\n",
      "\tTraining batch 69 Loss: 0.572674\n",
      "\tTraining batch 70 Loss: 0.000001\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.002433\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000781\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000001\n",
      "\tTraining batch 81 Loss: 0.605485\n",
      "\tTraining batch 82 Loss: 0.000243\n",
      "\tTraining batch 83 Loss: 0.040577\n",
      "\tTraining batch 84 Loss: 0.007752\n",
      "Training set: Average loss: 0.060869\n",
      "Validation set: Average loss: 16.722510, Accuracy: 1373/1959 (70.09%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000001\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000018\n",
      "\tTraining batch 15 Loss: 0.000179\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000669\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.014801\n",
      "\tTraining batch 26 Loss: 0.000105\n",
      "\tTraining batch 27 Loss: 0.220426\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000074\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000025\n",
      "\tTraining batch 44 Loss: 0.028979\n",
      "\tTraining batch 45 Loss: 0.000001\n",
      "\tTraining batch 46 Loss: 0.000003\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.230632\n",
      "\tTraining batch 54 Loss: 0.001129\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000087\n",
      "\tTraining batch 57 Loss: 0.000059\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000002\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000015\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000004\n",
      "\tTraining batch 82 Loss: 0.469879\n",
      "\tTraining batch 83 Loss: 0.352203\n",
      "\tTraining batch 84 Loss: 0.037947\n",
      "Training set: Average loss: 0.016158\n",
      "Validation set: Average loss: 16.927864, Accuracy: 1376/1959 (70.24%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000002\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000424\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.006125\n",
      "\tTraining batch 14 Loss: 0.000537\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000053\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000268\n",
      "\tTraining batch 26 Loss: 0.000003\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000001\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000801\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.002777\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000248\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000182\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.016279\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000002\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.023556\n",
      "\tTraining batch 71 Loss: 0.000010\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.161067\n",
      "\tTraining batch 76 Loss: 0.000008\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.957478\n",
      "\tTraining batch 82 Loss: 0.000002\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.196745\n",
      "Training set: Average loss: 0.016269\n",
      "Validation set: Average loss: 20.587867, Accuracy: 1400/1959 (71.47%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000001\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000033\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000237\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.021235\n",
      "\tTraining batch 12 Loss: 0.036536\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.011056\n",
      "\tTraining batch 15 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.037356\n",
      "\tTraining batch 19 Loss: 0.000937\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000051\n",
      "\tTraining batch 23 Loss: 0.000319\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.724357\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.013526\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000002\n",
      "\tTraining batch 38 Loss: 0.534770\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.002915\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000123\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.132414\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.001281\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000100\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000001\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000008\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.827301\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000011\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000001\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000003\n",
      "\tTraining batch 82 Loss: 0.000024\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.209801\n",
      "Training set: Average loss: 0.030410\n",
      "Validation set: Average loss: 18.138581, Accuracy: 1381/1959 (70.50%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.043189\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000045\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000096\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.002086\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.025736\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000005\n",
      "\tTraining batch 23 Loss: 0.026681\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000002\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000004\n",
      "\tTraining batch 33 Loss: 0.000004\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.055348\n",
      "\tTraining batch 37 Loss: 0.000004\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000026\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000008\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 1.403292\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000206\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.013390\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.263027\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.001298\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000163\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.022410\n",
      "\tTraining batch 76 Loss: 0.000001\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000001\n",
      "\tTraining batch 82 Loss: 0.267722\n",
      "\tTraining batch 83 Loss: 0.002700\n",
      "\tTraining batch 84 Loss: 0.000013\n",
      "Training set: Average loss: 0.025327\n",
      "Validation set: Average loss: 20.702751, Accuracy: 1383/1959 (70.60%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000005\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000039\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000277\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.004339\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000005\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000002\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000035\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000004\n",
      "\tTraining batch 58 Loss: 0.139456\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000005\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000001\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.172102\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000014\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000001\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.017680\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.036267\n",
      "\tTraining batch 81 Loss: 0.000635\n",
      "\tTraining batch 82 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000034\n",
      "Training set: Average loss: 0.004415\n",
      "Validation set: Average loss: 20.201257, Accuracy: 1391/1959 (71.01%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000001\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000126\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000990\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000071\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000005\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000730\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.107297\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000009\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000001\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.054177\n",
      "Training set: Average loss: 0.001945\n",
      "Validation set: Average loss: 22.247309, Accuracy: 1396/1959 (71.26%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000003\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000001\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000057\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000020\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.002016\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000051\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000002\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.080570\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000001\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.135057\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000001\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.211032\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "Training set: Average loss: 0.005105\n",
      "Validation set: Average loss: 22.098392, Accuracy: 1389/1959 (70.90%)\n",
      "\n",
      "Epoch: 11\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.001323\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000157\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.034218\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000007\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000005\n",
      "\tTraining batch 20 Loss: 0.000502\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000034\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.434198\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000018\n",
      "\tTraining batch 55 Loss: 0.000008\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000003\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.175208\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000001\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.700427\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.017088\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.001847\n",
      "Training set: Average loss: 0.016251\n",
      "Validation set: Average loss: 24.394944, Accuracy: 1379/1959 (70.39%)\n",
      "\n",
      "Epoch: 12\n",
      "\tTraining batch 1 Loss: 0.000002\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000031\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000015\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.025792\n",
      "\tTraining batch 26 Loss: 0.000084\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000001\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.244042\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000022\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000001\n",
      "\tTraining batch 68 Loss: 0.000001\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000001\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "Training set: Average loss: 0.003214\n",
      "Validation set: Average loss: 24.042536, Accuracy: 1392/1959 (71.06%)\n",
      "\n",
      "Epoch: 13\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000066\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000002\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000017\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000004\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000009\n",
      "\tTraining batch 55 Loss: 0.000001\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000002\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000002\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "Training set: Average loss: 0.000001\n",
      "Validation set: Average loss: 24.054301, Accuracy: 1391/1959 (71.01%)\n",
      "\n",
      "Epoch: 14\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000002\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000002\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000017\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 43 Loss: 0.000004\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000009\n",
      "\tTraining batch 55 Loss: 0.000001\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000002\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000002\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "Training set: Average loss: 0.000000\n",
      "Validation set: Average loss: 24.054309, Accuracy: 1391/1959 (71.01%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 50.0%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000002\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000002\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000017\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000004\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000009\n",
      "\tTraining batch 55 Loss: 0.000001\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000002\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000002\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 33.198700\n",
      "Training set: Average loss: 0.390573\n",
      "Validation set: Average loss: 22.201280, Accuracy: 1373/1959 (70.09%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000571\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000001\n",
      "\tTraining batch 9 Loss: 0.696098\n",
      "\tTraining batch 10 Loss: 0.004324\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000008\n",
      "\tTraining batch 13 Loss: 0.191983\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000008\n",
      "\tTraining batch 16 Loss: 0.168360\n",
      "\tTraining batch 17 Loss: 0.000064\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.002153\n",
      "\tTraining batch 20 Loss: 0.000001\n",
      "\tTraining batch 21 Loss: 0.000001\n",
      "\tTraining batch 22 Loss: 0.001196\n",
      "\tTraining batch 23 Loss: 0.057448\n",
      "\tTraining batch 24 Loss: 0.000186\n",
      "\tTraining batch 25 Loss: 0.069368\n",
      "\tTraining batch 26 Loss: 0.416392\n",
      "\tTraining batch 27 Loss: 0.000005\n",
      "\tTraining batch 28 Loss: 0.308216\n",
      "\tTraining batch 29 Loss: 0.084835\n",
      "\tTraining batch 30 Loss: 0.000100\n",
      "\tTraining batch 31 Loss: 0.008318\n",
      "\tTraining batch 32 Loss: 0.280692\n",
      "\tTraining batch 33 Loss: 0.293048\n",
      "\tTraining batch 34 Loss: 1.316131\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000001\n",
      "\tTraining batch 37 Loss: 0.766729\n",
      "\tTraining batch 38 Loss: 0.340731\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.516518\n",
      "\tTraining batch 42 Loss: 0.305058\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000005\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.001432\n",
      "\tTraining batch 48 Loss: 0.002331\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.159232\n",
      "\tTraining batch 51 Loss: 0.000111\n",
      "\tTraining batch 52 Loss: 0.000862\n",
      "\tTraining batch 53 Loss: 0.349863\n",
      "\tTraining batch 54 Loss: 0.078009\n",
      "\tTraining batch 55 Loss: 0.202826\n",
      "\tTraining batch 56 Loss: 0.446097\n",
      "\tTraining batch 57 Loss: 0.863562\n",
      "\tTraining batch 58 Loss: 0.000260\n",
      "\tTraining batch 59 Loss: 0.222296\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000004\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 1.082735\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.096175\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000001\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.100231\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000065\n",
      "\tTraining batch 74 Loss: 0.000012\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.982980\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000121\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.546627\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.516290\n",
      "\tTraining batch 83 Loss: 0.000143\n",
      "\tTraining batch 84 Loss: 0.082884\n",
      "\tTraining batch 85 Loss: 5.690143\n",
      "Training set: Average loss: 0.202986\n",
      "Validation set: Average loss: 19.445595, Accuracy: 1341/1959 (68.45%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000171\n",
      "\tTraining batch 2 Loss: 0.178524\n",
      "\tTraining batch 3 Loss: 0.068413\n",
      "\tTraining batch 4 Loss: 0.087639\n",
      "\tTraining batch 5 Loss: 0.839125\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.032795\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.216944\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.002101\n",
      "\tTraining batch 18 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 19 Loss: 0.003347\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.186135\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.037494\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000115\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000014\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000001\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000293\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.005039\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000022\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000001\n",
      "\tTraining batch 54 Loss: 0.005329\n",
      "\tTraining batch 55 Loss: 0.000015\n",
      "\tTraining batch 56 Loss: 0.083481\n",
      "\tTraining batch 57 Loss: 0.003449\n",
      "\tTraining batch 58 Loss: 0.023859\n",
      "\tTraining batch 59 Loss: 0.000029\n",
      "\tTraining batch 60 Loss: 0.000132\n",
      "\tTraining batch 61 Loss: 0.025711\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 1.404297\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.433721\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000124\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000566\n",
      "\tTraining batch 73 Loss: 0.000001\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.011064\n",
      "\tTraining batch 77 Loss: 0.000048\n",
      "\tTraining batch 78 Loss: 0.200554\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.134256\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000028\n",
      "\tTraining batch 85 Loss: 0.212759\n",
      "Training set: Average loss: 0.049383\n",
      "Validation set: Average loss: 16.089497, Accuracy: 1344/1959 (68.61%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.019481\n",
      "\tTraining batch 2 Loss: 0.000001\n",
      "\tTraining batch 3 Loss: 0.000001\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.001680\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.130035\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000093\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000024\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000884\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.002678\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.008932\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000002\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000014\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.047900\n",
      "\tTraining batch 42 Loss: 0.000001\n",
      "\tTraining batch 43 Loss: 0.000107\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.113369\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.001070\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000532\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000011\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000024\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000001\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.077987\n",
      "\tTraining batch 84 Loss: 0.378483\n",
      "\tTraining batch 85 Loss: 0.000674\n",
      "Training set: Average loss: 0.009223\n",
      "Validation set: Average loss: 21.511319, Accuracy: 1357/1959 (69.27%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000125\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000142\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000031\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000002\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000046\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000072\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.016159\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000707\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000002\n",
      "\tTraining batch 57 Loss: 0.000044\n",
      "\tTraining batch 58 Loss: 0.249015\n",
      "\tTraining batch 59 Loss: 0.000017\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000001\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.012468\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000001\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "Training set: Average loss: 0.003280\n",
      "Validation set: Average loss: 22.517790, Accuracy: 1371/1959 (69.98%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000260\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.012941\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000001\n",
      "\tTraining batch 27 Loss: 0.310270\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000327\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000013\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.001072\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000009\n",
      "\tTraining batch 57 Loss: 0.000058\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000523\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.062856\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.182657\n",
      "\tTraining batch 72 Loss: 0.020584\n",
      "\tTraining batch 73 Loss: 0.084286\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000117\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "Training set: Average loss: 0.007953\n",
      "Validation set: Average loss: 21.400383, Accuracy: 1361/1959 (69.47%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000332\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.075244\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000044\n",
      "\tTraining batch 15 Loss: 0.000011\n",
      "\tTraining batch 16 Loss: 0.000002\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000078\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.215274\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000001\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000023\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000917\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000445\n",
      "\tTraining batch 57 Loss: 0.000008\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.001397\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000612\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000024\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000006\n",
      "\tTraining batch 68 Loss: 0.020352\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.218750\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.096808\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000016\n",
      "\tTraining batch 77 Loss: 0.000001\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000021\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.481595\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "Training set: Average loss: 0.013082\n",
      "Validation set: Average loss: 20.349624, Accuracy: 1364/1959 (69.63%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000001\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000408\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000007\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000003\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000043\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000184\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000036\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000007\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000001\n",
      "\tTraining batch 77 Loss: 0.000001\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000001\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "Training set: Average loss: 0.000008\n",
      "Validation set: Average loss: 21.883613, Accuracy: 1363/1959 (69.58%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000035\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000320\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000003\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000003\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000042\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000179\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000022\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000007\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000001\n",
      "\tTraining batch 77 Loss: 0.000001\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000001\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "Training set: Average loss: 0.000007\n",
      "Validation set: Average loss: 21.886183, Accuracy: 1363/1959 (69.58%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 47.25%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000025\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000315\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000003\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000002\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000041\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000175\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000015\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000007\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000001\n",
      "\tTraining batch 77 Loss: 0.000001\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000001\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 17.668140\n",
      "Training set: Average loss: 0.205450\n",
      "Validation set: Average loss: 20.436949, Accuracy: 1370/1959 (69.93%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.031854\n",
      "\tTraining batch 5 Loss: 0.497998\n",
      "\tTraining batch 6 Loss: 0.054622\n",
      "\tTraining batch 7 Loss: 0.527184\n",
      "\tTraining batch 8 Loss: 0.012330\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.001181\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.477426\n",
      "\tTraining batch 13 Loss: 0.000010\n",
      "\tTraining batch 14 Loss: 0.545253\n",
      "\tTraining batch 15 Loss: 0.587561\n",
      "\tTraining batch 16 Loss: 0.020002\n",
      "\tTraining batch 17 Loss: 0.007019\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.089231\n",
      "\tTraining batch 20 Loss: 0.000001\n",
      "\tTraining batch 21 Loss: 0.154972\n",
      "\tTraining batch 22 Loss: 0.307691\n",
      "\tTraining batch 23 Loss: 0.003755\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000013\n",
      "\tTraining batch 26 Loss: 0.005150\n",
      "\tTraining batch 27 Loss: 0.180357\n",
      "\tTraining batch 28 Loss: 0.160041\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000153\n",
      "\tTraining batch 34 Loss: 0.001620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 35 Loss: 0.387766\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.003171\n",
      "\tTraining batch 38 Loss: 0.029879\n",
      "\tTraining batch 39 Loss: 0.192603\n",
      "\tTraining batch 40 Loss: 0.684296\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.033022\n",
      "\tTraining batch 44 Loss: 0.019629\n",
      "\tTraining batch 45 Loss: 0.000002\n",
      "\tTraining batch 46 Loss: 0.000013\n",
      "\tTraining batch 47 Loss: 0.494093\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000001\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000956\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.611269\n",
      "\tTraining batch 57 Loss: 0.000270\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.225433\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.727989\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.229486\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.140534\n",
      "\tTraining batch 73 Loss: 0.290003\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.176894\n",
      "\tTraining batch 77 Loss: 0.059486\n",
      "\tTraining batch 78 Loss: 0.187318\n",
      "\tTraining batch 79 Loss: 0.514506\n",
      "\tTraining batch 80 Loss: 0.003291\n",
      "\tTraining batch 81 Loss: 0.009063\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.331099\n",
      "\tTraining batch 86 Loss: 3.899656\n",
      "Training set: Average loss: 0.150199\n",
      "Validation set: Average loss: 23.106557, Accuracy: 1358/1959 (69.32%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.096939\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.421925\n",
      "\tTraining batch 10 Loss: 1.113462\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000001\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.053910\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.736391\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 1.170465\n",
      "\tTraining batch 26 Loss: 0.074546\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000428\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.111189\n",
      "\tTraining batch 43 Loss: 0.000009\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.223372\n",
      "\tTraining batch 47 Loss: 0.000007\n",
      "\tTraining batch 48 Loss: 1.657811\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000021\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000108\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000142\n",
      "\tTraining batch 60 Loss: 0.000243\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.152880\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.605529\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000019\n",
      "\tTraining batch 71 Loss: 0.421186\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000017\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.007611\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000498\n",
      "\tTraining batch 85 Loss: 0.145842\n",
      "\tTraining batch 86 Loss: 1.126216\n",
      "Training set: Average loss: 0.094428\n",
      "Validation set: Average loss: 25.352958, Accuracy: 1380/1959 (70.44%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000059\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.181339\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.549709\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000253\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.027380\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000447\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000005\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.491856\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000864\n",
      "\tTraining batch 54 Loss: 0.000268\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.095192\n",
      "\tTraining batch 57 Loss: 0.000005\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.062877\n",
      "\tTraining batch 60 Loss: 0.003431\n",
      "\tTraining batch 61 Loss: 0.002744\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.199592\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.036308\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.017971\n",
      "\tTraining batch 70 Loss: 1.224629\n",
      "\tTraining batch 71 Loss: 0.017919\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.351053\n",
      "\tTraining batch 75 Loss: 0.000019\n",
      "\tTraining batch 76 Loss: 0.000058\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.020340\n",
      "\tTraining batch 84 Loss: 1.267598\n",
      "\tTraining batch 85 Loss: 0.173550\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "Training set: Average loss: 0.054947\n",
      "Validation set: Average loss: 27.416809, Accuracy: 1351/1959 (68.96%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.021752\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 9 Loss: 0.000689\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000250\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.003254\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.112646\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000003\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000003\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000008\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000175\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000054\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.372174\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000053\n",
      "\tTraining batch 62 Loss: 0.051074\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.018158\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000065\n",
      "\tTraining batch 75 Loss: 0.000057\n",
      "\tTraining batch 76 Loss: 0.000003\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.201888\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "Training set: Average loss: 0.009097\n",
      "Validation set: Average loss: 28.020703, Accuracy: 1354/1959 (69.12%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000060\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000005\n",
      "\tTraining batch 26 Loss: 0.087977\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000002\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000046\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000001\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000618\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000073\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000001\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000001\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000014\n",
      "\tTraining batch 77 Loss: 0.312673\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.044469\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000067\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "Training set: Average loss: 0.005186\n",
      "Validation set: Average loss: 26.499016, Accuracy: 1355/1959 (69.17%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.148274\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.445476\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000328\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.002082\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.341641\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.209665\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.107464\n",
      "\tTraining batch 59 Loss: 0.000003\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.021745\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 1.148947\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.750229\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 72 Loss: 0.001672\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000075\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.005127\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000008\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.059035\n",
      "\tTraining batch 82 Loss: 0.000115\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.357883\n",
      "\tTraining batch 85 Loss: 0.335365\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "Training set: Average loss: 0.045757\n",
      "Validation set: Average loss: 27.313880, Accuracy: 1375/1959 (70.19%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.098535\n",
      "\tTraining batch 4 Loss: 0.000118\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.040477\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.023253\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000813\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000002\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000018\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000004\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000978\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000031\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "Training set: Average loss: 0.001910\n",
      "Validation set: Average loss: 29.638200, Accuracy: 1373/1959 (70.09%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000001\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000001\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000008\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000005\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000001\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "Training set: Average loss: 0.000000\n",
      "Validation set: Average loss: 29.579614, Accuracy: 1373/1959 (70.09%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 49.25%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000001\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000001\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000008\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000005\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000001\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 54.726032\n",
      "Training set: Average loss: 0.629035\n",
      "Validation set: Average loss: 25.585316, Accuracy: 1383/1959 (70.60%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000018\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 1.169126\n",
      "\tTraining batch 7 Loss: 0.128109\n",
      "\tTraining batch 8 Loss: 3.197534\n",
      "\tTraining batch 9 Loss: 0.001564\n",
      "\tTraining batch 10 Loss: 0.000117\n",
      "\tTraining batch 11 Loss: 3.430920\n",
      "\tTraining batch 12 Loss: 0.189075\n",
      "\tTraining batch 13 Loss: 0.535378\n",
      "\tTraining batch 14 Loss: 0.103236\n",
      "\tTraining batch 15 Loss: 0.000198\n",
      "\tTraining batch 16 Loss: 0.001439\n",
      "\tTraining batch 17 Loss: 0.000002\n",
      "\tTraining batch 18 Loss: 0.000005\n",
      "\tTraining batch 19 Loss: 0.195598\n",
      "\tTraining batch 20 Loss: 0.002260\n",
      "\tTraining batch 21 Loss: 0.084090\n",
      "\tTraining batch 22 Loss: 1.845372\n",
      "\tTraining batch 23 Loss: 0.000024\n",
      "\tTraining batch 24 Loss: 1.174497\n",
      "\tTraining batch 25 Loss: 0.677252\n",
      "\tTraining batch 26 Loss: 0.849428\n",
      "\tTraining batch 27 Loss: 0.762291\n",
      "\tTraining batch 28 Loss: 0.768586\n",
      "\tTraining batch 29 Loss: 0.009507\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.753257\n",
      "\tTraining batch 33 Loss: 0.000117\n",
      "\tTraining batch 34 Loss: 0.000003\n",
      "\tTraining batch 35 Loss: 0.000009\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.107547\n",
      "\tTraining batch 38 Loss: 0.003947\n",
      "\tTraining batch 39 Loss: 0.188329\n",
      "\tTraining batch 40 Loss: 0.000006\n",
      "\tTraining batch 41 Loss: 0.012734\n",
      "\tTraining batch 42 Loss: 0.046351\n",
      "\tTraining batch 43 Loss: 0.020421\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000353\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000609\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000004\n",
      "\tTraining batch 52 Loss: 0.002025\n",
      "\tTraining batch 53 Loss: 0.105307\n",
      "\tTraining batch 54 Loss: 0.502540\n",
      "\tTraining batch 55 Loss: 0.000064\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.002622\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000002\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.218413\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000001\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000282\n",
      "\tTraining batch 69 Loss: 0.591369\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.013023\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.064785\n",
      "\tTraining batch 75 Loss: 0.000121\n",
      "\tTraining batch 76 Loss: 0.003545\n",
      "\tTraining batch 77 Loss: 0.162598\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.134421\n",
      "\tTraining batch 80 Loss: 0.508473\n",
      "\tTraining batch 81 Loss: 0.209624\n",
      "\tTraining batch 82 Loss: 0.000302\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.158170\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.014567\n",
      "\tTraining batch 87 Loss: 3.425086\n",
      "Training set: Average loss: 0.257203\n",
      "Validation set: Average loss: 19.946489, Accuracy: 1352/1959 (69.01%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.735614\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.101490\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 1.214240\n",
      "\tTraining batch 7 Loss: 0.382639\n",
      "\tTraining batch 8 Loss: 0.156868\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000002\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001130\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.035691\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.098728\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.206416\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.280609\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000139\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000607\n",
      "\tTraining batch 46 Loss: 0.236623\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000007\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000032\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000002\n",
      "\tTraining batch 54 Loss: 0.002339\n",
      "\tTraining batch 55 Loss: 0.000011\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.006236\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000163\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.579873\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.041655\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000066\n",
      "\tTraining batch 74 Loss: 0.200329\n",
      "\tTraining batch 75 Loss: 0.000050\n",
      "\tTraining batch 76 Loss: 0.000055\n",
      "\tTraining batch 77 Loss: 0.000309\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.432038\n",
      "\tTraining batch 80 Loss: 0.032105\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.240571\n",
      "\tTraining batch 83 Loss: 0.428704\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000251\n",
      "\tTraining batch 87 Loss: 1.987069\n",
      "Training set: Average loss: 0.085088\n",
      "Validation set: Average loss: 17.749654, Accuracy: 1375/1959 (70.19%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.238020\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.149207\n",
      "\tTraining batch 14 Loss: 0.586446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000001\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000840\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.089101\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000061\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000011\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000002\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.431953\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000644\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.002869\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.194051\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000005\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000082\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.214279\n",
      "\tTraining batch 76 Loss: 0.000031\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000001\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 1.069983\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.103296\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "Training set: Average loss: 0.035412\n",
      "Validation set: Average loss: 21.824530, Accuracy: 1386/1959 (70.75%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.205363\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.002248\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.024950\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.230354\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000040\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.005880\n",
      "\tTraining batch 44 Loss: 0.000178\n",
      "\tTraining batch 45 Loss: 0.000002\n",
      "\tTraining batch 46 Loss: 0.053659\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.001270\n",
      "\tTraining batch 55 Loss: 0.000007\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.002407\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000251\n",
      "\tTraining batch 60 Loss: 0.225264\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000176\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.258668\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000495\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.019773\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000493\n",
      "\tTraining batch 87 Loss: 0.823951\n",
      "Training set: Average loss: 0.021327\n",
      "Validation set: Average loss: 24.307692, Accuracy: 1382/1959 (70.55%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.033685\n",
      "\tTraining batch 6 Loss: 0.030887\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.002990\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001241\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.122835\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.247675\n",
      "\tTraining batch 26 Loss: 0.075657\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.004599\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000002\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.095441\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000759\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.067672\n",
      "\tTraining batch 43 Loss: 0.000002\n",
      "\tTraining batch 44 Loss: 0.000321\n",
      "\tTraining batch 45 Loss: 0.000007\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000113\n",
      "\tTraining batch 54 Loss: 0.000283\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.004610\n",
      "\tTraining batch 58 Loss: 0.000005\n",
      "\tTraining batch 59 Loss: 0.000002\n",
      "\tTraining batch 60 Loss: 0.000144\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 1.376700\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000001\n",
      "\tTraining batch 74 Loss: 0.008109\n",
      "\tTraining batch 75 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 76 Loss: 0.000045\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000747\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.093094\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.284944\n",
      "Training set: Average loss: 0.028190\n",
      "Validation set: Average loss: 23.470759, Accuracy: 1381/1959 (70.50%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000034\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000001\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000001\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000034\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000011\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000018\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "Training set: Average loss: 0.000001\n",
      "Validation set: Average loss: 25.651115, Accuracy: 1370/1959 (69.93%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000021\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000001\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000001\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000034\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000008\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000013\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "Training set: Average loss: 0.000001\n",
      "Validation set: Average loss: 25.652090, Accuracy: 1370/1959 (69.93%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 50.0%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000021\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 45 Loss: 0.000001\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000001\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000033\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000006\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000010\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 33.305996\n",
      "Training set: Average loss: 0.378478\n",
      "Validation set: Average loss: 23.972655, Accuracy: 1392/1959 (71.06%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000006\n",
      "\tTraining batch 5 Loss: 0.010219\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.005034\n",
      "\tTraining batch 10 Loss: 0.000056\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.002114\n",
      "\tTraining batch 14 Loss: 0.588183\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000001\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.030340\n",
      "\tTraining batch 20 Loss: 0.841829\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000004\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.541220\n",
      "\tTraining batch 25 Loss: 0.119406\n",
      "\tTraining batch 26 Loss: 0.036805\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.068903\n",
      "\tTraining batch 29 Loss: 0.000011\n",
      "\tTraining batch 30 Loss: 0.003777\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.029157\n",
      "\tTraining batch 33 Loss: 0.001969\n",
      "\tTraining batch 34 Loss: 0.000002\n",
      "\tTraining batch 35 Loss: 0.111571\n",
      "\tTraining batch 36 Loss: 0.074509\n",
      "\tTraining batch 37 Loss: 0.000001\n",
      "\tTraining batch 38 Loss: 0.000043\n",
      "\tTraining batch 39 Loss: 0.078528\n",
      "\tTraining batch 40 Loss: 0.000021\n",
      "\tTraining batch 41 Loss: 0.002554\n",
      "\tTraining batch 42 Loss: 0.157793\n",
      "\tTraining batch 43 Loss: 0.709480\n",
      "\tTraining batch 44 Loss: 0.800288\n",
      "\tTraining batch 45 Loss: 0.186797\n",
      "\tTraining batch 46 Loss: 0.127677\n",
      "\tTraining batch 47 Loss: 0.000001\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.008231\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.001183\n",
      "\tTraining batch 54 Loss: 0.211519\n",
      "\tTraining batch 55 Loss: 0.003124\n",
      "\tTraining batch 56 Loss: 0.000004\n",
      "\tTraining batch 57 Loss: 0.011105\n",
      "\tTraining batch 58 Loss: 0.015585\n",
      "\tTraining batch 59 Loss: 0.164725\n",
      "\tTraining batch 60 Loss: 0.295100\n",
      "\tTraining batch 61 Loss: 0.000002\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.103045\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.001148\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.001590\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000063\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000002\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000022\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.001505\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.221525\n",
      "\tTraining batch 87 Loss: 0.540466\n",
      "\tTraining batch 88 Loss: 0.386624\n",
      "Training set: Average loss: 0.073805\n",
      "Validation set: Average loss: 19.242842, Accuracy: 1394/1959 (71.16%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000001\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000105\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.056890\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000002\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.002305\n",
      "\tTraining batch 20 Loss: 0.000543\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.044813\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000006\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.052574\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000002\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.003741\n",
      "\tTraining batch 44 Loss: 0.000065\n",
      "\tTraining batch 45 Loss: 0.000420\n",
      "\tTraining batch 46 Loss: 0.000024\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000001\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000005\n",
      "\tTraining batch 54 Loss: 0.007870\n",
      "\tTraining batch 55 Loss: 0.000011\n",
      "\tTraining batch 56 Loss: 0.806458\n",
      "\tTraining batch 57 Loss: 0.003750\n",
      "\tTraining batch 58 Loss: 0.002645\n",
      "\tTraining batch 59 Loss: 0.000494\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.027359\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.051481\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.004699\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.097090\n",
      "\tTraining batch 74 Loss: 0.000221\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.054584\n",
      "\tTraining batch 77 Loss: 0.000037\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.079612\n",
      "\tTraining batch 80 Loss: 0.000002\n",
      "\tTraining batch 81 Loss: 0.216890\n",
      "\tTraining batch 82 Loss: 0.000003\n",
      "\tTraining batch 83 Loss: 0.032765\n",
      "\tTraining batch 84 Loss: 0.243484\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000077\n",
      "\tTraining batch 87 Loss: 0.000009\n",
      "\tTraining batch 88 Loss: 0.001410\n",
      "Training set: Average loss: 0.020369\n",
      "Validation set: Average loss: 19.549112, Accuracy: 1407/1959 (71.82%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000061\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.295786\n",
      "\tTraining batch 4 Loss: 0.000082\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.068055\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.069104\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001234\n",
      "\tTraining batch 20 Loss: 0.000093\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.021799\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.026833\n",
      "\tTraining batch 44 Loss: 0.008835\n",
      "\tTraining batch 45 Loss: 0.000004\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.001630\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000485\n",
      "\tTraining batch 58 Loss: 0.000024\n",
      "\tTraining batch 59 Loss: 0.000313\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.130416\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.001274\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.071945\n",
      "\tTraining batch 74 Loss: 0.002923\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000024\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000012\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.047598\n",
      "\tTraining batch 83 Loss: 0.011508\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000001\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.674809\n",
      "Training set: Average loss: 0.016305\n",
      "Validation set: Average loss: 22.343709, Accuracy: 1391/1959 (71.01%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000111\n",
      "\tTraining batch 2 Loss: 0.000001\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.167074\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001812\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000002\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000050\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000064\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000415\n",
      "\tTraining batch 46 Loss: 0.000007\n",
      "\tTraining batch 47 Loss: 0.005170\n",
      "\tTraining batch 48 Loss: 0.000734\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.005817\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.002603\n",
      "\tTraining batch 58 Loss: 0.417795\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.194974\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.024944\n",
      "\tTraining batch 76 Loss: 0.030473\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000001\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 1.125484\n",
      "Training set: Average loss: 0.022472\n",
      "Validation set: Average loss: 23.565517, Accuracy: 1388/1959 (70.85%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000221\n",
      "\tTraining batch 20 Loss: 0.000201\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000002\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000038\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.269055\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000056\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000050\n",
      "\tTraining batch 58 Loss: 0.000001\n",
      "\tTraining batch 59 Loss: 0.000035\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000001\n",
      "\tTraining batch 77 Loss: 0.000012\n",
      "\tTraining batch 78 Loss: 0.002713\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000104\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.001117\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.630466\n",
      "Training set: Average loss: 0.010274\n",
      "Validation set: Average loss: 21.435578, Accuracy: 1400/1959 (71.47%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000528\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 1.470316\n",
      "\tTraining batch 10 Loss: 0.323117\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000085\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.018693\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.003586\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.118501\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000001\n",
      "\tTraining batch 42 Loss: 0.000002\n",
      "\tTraining batch 43 Loss: 0.000410\n",
      "\tTraining batch 44 Loss: 0.000009\n",
      "\tTraining batch 45 Loss: 0.000004\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.036019\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.018378\n",
      "\tTraining batch 57 Loss: 0.002928\n",
      "\tTraining batch 58 Loss: 0.948872\n",
      "\tTraining batch 59 Loss: 0.000237\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.145088\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000013\n",
      "\tTraining batch 77 Loss: 0.000008\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000033\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "Training set: Average loss: 0.035078\n",
      "Validation set: Average loss: 22.355820, Accuracy: 1396/1959 (71.26%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.093023\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000056\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000002\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000512\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000004\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000029\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000001\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000018\n",
      "\tTraining batch 86 Loss: 0.000065\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "Training set: Average loss: 0.001065\n",
      "Validation set: Average loss: 24.264793, Accuracy: 1395/1959 (71.21%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000050\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000001\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000442\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000004\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000029\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000001\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000003\n",
      "\tTraining batch 86 Loss: 0.000024\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "Training set: Average loss: 0.000006\n",
      "Validation set: Average loss: 24.263240, Accuracy: 1395/1959 (71.21%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 46.75%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000050\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000001\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000394\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000004\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000028\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000001\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000002\n",
      "\tTraining batch 86 Loss: 0.000016\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 33.776043\n",
      "Training set: Average loss: 0.379512\n",
      "Validation set: Average loss: 23.267669, Accuracy: 1403/1959 (71.62%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000467\n",
      "\tTraining batch 4 Loss: 0.065344\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.003921\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.092586\n",
      "\tTraining batch 12 Loss: 0.076562\n",
      "\tTraining batch 13 Loss: 0.004057\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.005051\n",
      "\tTraining batch 16 Loss: 0.022401\n",
      "\tTraining batch 17 Loss: 1.892095\n",
      "\tTraining batch 18 Loss: 0.003071\n",
      "\tTraining batch 19 Loss: 0.005923\n",
      "\tTraining batch 20 Loss: 0.000002\n",
      "\tTraining batch 21 Loss: 0.258711\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.001782\n",
      "\tTraining batch 26 Loss: 0.075545\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.145966\n",
      "\tTraining batch 29 Loss: 0.000001\n",
      "\tTraining batch 30 Loss: 0.002066\n",
      "\tTraining batch 31 Loss: 0.000017\n",
      "\tTraining batch 32 Loss: 0.092238\n",
      "\tTraining batch 33 Loss: 0.360060\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000001\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000023\n",
      "\tTraining batch 39 Loss: 0.000161\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.001697\n",
      "\tTraining batch 43 Loss: 0.000582\n",
      "\tTraining batch 44 Loss: 0.027693\n",
      "\tTraining batch 45 Loss: 0.000026\n",
      "\tTraining batch 46 Loss: 0.000770\n",
      "\tTraining batch 47 Loss: 0.500588\n",
      "\tTraining batch 48 Loss: 0.009459\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.067237\n",
      "\tTraining batch 52 Loss: 0.328614\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.037647\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.404832\n",
      "\tTraining batch 58 Loss: 0.210058\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.008092\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000001\n",
      "\tTraining batch 66 Loss: 0.099941\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000046\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.222119\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000053\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 1.322141\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.021658\n",
      "\tTraining batch 77 Loss: 0.000027\n",
      "\tTraining batch 78 Loss: 0.000010\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000112\n",
      "\tTraining batch 83 Loss: 0.187436\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000050\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.128250\n",
      "\tTraining batch 88 Loss: 0.482148\n",
      "\tTraining batch 89 Loss: 1.716204\n",
      "Training set: Average loss: 0.099838\n",
      "Validation set: Average loss: 19.640410, Accuracy: 1408/1959 (71.87%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000009\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.234552\n",
      "\tTraining batch 10 Loss: 0.001817\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 1.467519\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000001\n",
      "\tTraining batch 21 Loss: 0.522120\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.001701\n",
      "\tTraining batch 27 Loss: 0.000218\n",
      "\tTraining batch 28 Loss: 0.000006\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000002\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.343368\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.001007\n",
      "\tTraining batch 44 Loss: 0.000001\n",
      "\tTraining batch 45 Loss: 0.000211\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000028\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.175367\n",
      "\tTraining batch 54 Loss: 0.000068\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000001\n",
      "\tTraining batch 57 Loss: 0.000191\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000001\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000981\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000006\n",
      "\tTraining batch 77 Loss: 0.000002\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.002616\n",
      "\tTraining batch 80 Loss: 0.000040\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.070422\n",
      "\tTraining batch 88 Loss: 0.027421\n",
      "\tTraining batch 89 Loss: 0.628503\n",
      "Training set: Average loss: 0.039081\n",
      "Validation set: Average loss: 21.064071, Accuracy: 1381/1959 (70.50%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.603311\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 1.007297\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.182911\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000268\n",
      "\tTraining batch 20 Loss: 0.000004\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.185180\n",
      "\tTraining batch 27 Loss: 0.084312\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000003\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000091\n",
      "\tTraining batch 35 Loss: 0.048865\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000036\n",
      "\tTraining batch 44 Loss: 0.000001\n",
      "\tTraining batch 45 Loss: 0.018204\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000993\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.001050\n",
      "\tTraining batch 54 Loss: 0.230728\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.169209\n",
      "\tTraining batch 58 Loss: 0.000068\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.076329\n",
      "\tTraining batch 83 Loss: 0.000004\n",
      "\tTraining batch 84 Loss: 0.000001\n",
      "\tTraining batch 85 Loss: 0.034963\n",
      "\tTraining batch 86 Loss: 0.002721\n",
      "\tTraining batch 87 Loss: 0.000138\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.365871\n",
      "Training set: Average loss: 0.033849\n",
      "Validation set: Average loss: 20.348147, Accuracy: 1399/1959 (71.41%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000006\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.011895\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000729\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.093241\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000057\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.001056\n",
      "\tTraining batch 44 Loss: 0.000002\n",
      "\tTraining batch 45 Loss: 0.000011\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000652\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.050052\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.006434\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000226\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000012\n",
      "\tTraining batch 60 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000001\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.005931\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000044\n",
      "\tTraining batch 77 Loss: 0.000093\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.001492\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000001\n",
      "\tTraining batch 88 Loss: 0.000003\n",
      "\tTraining batch 89 Loss: 0.272684\n",
      "Training set: Average loss: 0.004996\n",
      "Validation set: Average loss: 19.719131, Accuracy: 1396/1959 (71.26%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000001\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000004\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000372\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.093360\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.001982\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000067\n",
      "\tTraining batch 55 Loss: 0.000903\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.004251\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000024\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000009\n",
      "\tTraining batch 77 Loss: 0.000002\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000059\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000025\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "Training set: Average loss: 0.001135\n",
      "Validation set: Average loss: 20.171334, Accuracy: 1400/1959 (71.47%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000351\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.046626\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.001602\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000069\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000038\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000017\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000008\n",
      "\tTraining batch 77 Loss: 0.000002\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000055\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000006\n",
      "\tTraining batch 88 Loss: 0.000001\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "Training set: Average loss: 0.000548\n",
      "Validation set: Average loss: 20.171927, Accuracy: 1401/1959 (71.52%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000300\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 26 Loss: 0.016685\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.001534\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000059\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000037\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000016\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000008\n",
      "\tTraining batch 77 Loss: 0.000001\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000047\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000004\n",
      "\tTraining batch 88 Loss: 0.000001\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "Training set: Average loss: 0.000210\n",
      "Validation set: Average loss: 20.192178, Accuracy: 1402/1959 (71.57%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000255\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.007574\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.001452\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000053\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000037\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000015\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000008\n",
      "\tTraining batch 77 Loss: 0.000001\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000041\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000004\n",
      "\tTraining batch 88 Loss: 0.000001\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "Training set: Average loss: 0.000106\n",
      "Validation set: Average loss: 20.213666, Accuracy: 1402/1959 (71.57%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 47.25%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000227\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.004670\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.001368\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000049\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000036\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000015\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000008\n",
      "\tTraining batch 77 Loss: 0.000001\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000035\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000004\n",
      "\tTraining batch 88 Loss: 0.000001\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 40.412434\n",
      "Training set: Average loss: 0.449098\n",
      "Validation set: Average loss: 18.276325, Accuracy: 1406/1959 (71.77%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000086\n",
      "\tTraining batch 2 Loss: 0.001996\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.223824\n",
      "\tTraining batch 8 Loss: 0.287442\n",
      "\tTraining batch 9 Loss: 0.975578\n",
      "\tTraining batch 10 Loss: 0.197456\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.144029\n",
      "\tTraining batch 14 Loss: 0.550785\n",
      "\tTraining batch 15 Loss: 0.235627\n",
      "\tTraining batch 16 Loss: 0.752811\n",
      "\tTraining batch 17 Loss: 0.000164\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.121754\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.215326\n",
      "\tTraining batch 22 Loss: 0.000507\n",
      "\tTraining batch 23 Loss: 0.000327\n",
      "\tTraining batch 24 Loss: 0.001334\n",
      "\tTraining batch 25 Loss: 0.000281\n",
      "\tTraining batch 26 Loss: 0.774046\n",
      "\tTraining batch 27 Loss: 0.891474\n",
      "\tTraining batch 28 Loss: 0.000576\n",
      "\tTraining batch 29 Loss: 0.000007\n",
      "\tTraining batch 30 Loss: 0.000032\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000001\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000389\n",
      "\tTraining batch 35 Loss: 0.000002\n",
      "\tTraining batch 36 Loss: 0.205066\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000193\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.064634\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.255871\n",
      "\tTraining batch 44 Loss: 0.531528\n",
      "\tTraining batch 45 Loss: 1.067631\n",
      "\tTraining batch 46 Loss: 0.294204\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.027614\n",
      "\tTraining batch 50 Loss: 0.005809\n",
      "\tTraining batch 51 Loss: 0.118461\n",
      "\tTraining batch 52 Loss: 0.991333\n",
      "\tTraining batch 53 Loss: 0.007921\n",
      "\tTraining batch 54 Loss: 0.011108\n",
      "\tTraining batch 55 Loss: 0.121564\n",
      "\tTraining batch 56 Loss: 0.000036\n",
      "\tTraining batch 57 Loss: 0.014097\n",
      "\tTraining batch 58 Loss: 0.060582\n",
      "\tTraining batch 59 Loss: 0.000030\n",
      "\tTraining batch 60 Loss: 0.003437\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000006\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.116689\n",
      "\tTraining batch 75 Loss: 0.579616\n",
      "\tTraining batch 76 Loss: 0.071636\n",
      "\tTraining batch 77 Loss: 0.092201\n",
      "\tTraining batch 78 Loss: 2.198327\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.148226\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000002\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.952289\n",
      "\tTraining batch 87 Loss: 0.000139\n",
      "\tTraining batch 88 Loss: 0.057049\n",
      "\tTraining batch 89 Loss: 0.004282\n",
      "\tTraining batch 90 Loss: 6.702698\n",
      "Training set: Average loss: 0.223113\n",
      "Validation set: Average loss: 14.918698, Accuracy: 1401/1959 (71.52%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.004208\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.234269\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.027461\n",
      "\tTraining batch 13 Loss: 0.000002\n",
      "\tTraining batch 14 Loss: 0.582432\n",
      "\tTraining batch 15 Loss: 0.002879\n",
      "\tTraining batch 16 Loss: 0.022237\n",
      "\tTraining batch 17 Loss: 0.085672\n",
      "\tTraining batch 18 Loss: 0.388223\n",
      "\tTraining batch 19 Loss: 0.259017\n",
      "\tTraining batch 20 Loss: 0.067676\n",
      "\tTraining batch 21 Loss: 0.000001\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.023576\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000604\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.037883\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000003\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000324\n",
      "\tTraining batch 46 Loss: 0.000001\n",
      "\tTraining batch 47 Loss: 0.364297\n",
      "\tTraining batch 48 Loss: 0.001484\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000002\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000593\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000004\n",
      "\tTraining batch 57 Loss: 0.001857\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000001\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000005\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000001\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000003\n",
      "\tTraining batch 66 Loss: 0.000013\n",
      "\tTraining batch 67 Loss: 0.000139\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000002\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000452\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.003113\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000009\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000305\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.808948\n",
      "\tTraining batch 88 Loss: 0.004816\n",
      "\tTraining batch 89 Loss: 0.007304\n",
      "\tTraining batch 90 Loss: 0.905343\n",
      "Training set: Average loss: 0.042613\n",
      "Validation set: Average loss: 15.387780, Accuracy: 1397/1959 (71.31%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000022\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000028\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.063748\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000010\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.004244\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000001\n",
      "\tTraining batch 26 Loss: 0.030317\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.016849\n",
      "\tTraining batch 29 Loss: 0.000004\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000001\n",
      "\tTraining batch 34 Loss: 0.000022\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000265\n",
      "\tTraining batch 37 Loss: 0.000149\n",
      "\tTraining batch 38 Loss: 0.353151\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 43 Loss: 0.001759\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000528\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.105579\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000981\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000001\n",
      "\tTraining batch 57 Loss: 0.004625\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000252\n",
      "\tTraining batch 60 Loss: 0.004414\n",
      "\tTraining batch 61 Loss: 0.103066\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000002\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000002\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000020\n",
      "\tTraining batch 73 Loss: 0.055842\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000125\n",
      "\tTraining batch 76 Loss: 0.000026\n",
      "\tTraining batch 77 Loss: 0.907476\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000051\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.145495\n",
      "\tTraining batch 83 Loss: 0.761987\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000091\n",
      "\tTraining batch 86 Loss: 0.004791\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.080500\n",
      "\tTraining batch 90 Loss: 0.410392\n",
      "Training set: Average loss: 0.033965\n",
      "Validation set: Average loss: 16.655308, Accuracy: 1380/1959 (70.44%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.033580\n",
      "\tTraining batch 3 Loss: 0.000015\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000176\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000096\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000061\n",
      "\tTraining batch 17 Loss: 0.001174\n",
      "\tTraining batch 18 Loss: 0.000019\n",
      "\tTraining batch 19 Loss: 0.003252\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000001\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000004\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000003\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000008\n",
      "\tTraining batch 44 Loss: 0.127532\n",
      "\tTraining batch 45 Loss: 0.292733\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.166301\n",
      "\tTraining batch 55 Loss: 0.000003\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.003449\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000001\n",
      "\tTraining batch 65 Loss: 0.003673\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.005939\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000005\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000017\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000022\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.059267\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.791723\n",
      "\tTraining batch 89 Loss: 0.002152\n",
      "\tTraining batch 90 Loss: 0.466572\n",
      "Training set: Average loss: 0.021753\n",
      "Validation set: Average loss: 19.812579, Accuracy: 1399/1959 (71.41%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000416\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 2.807552\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.174071\n",
      "\tTraining batch 10 Loss: 0.000787\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.093501\n",
      "\tTraining batch 13 Loss: 0.002377\n",
      "\tTraining batch 14 Loss: 0.062914\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000257\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.025273\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000485\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000029\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.295332\n",
      "\tTraining batch 35 Loss: 0.000056\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.640984\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.244183\n",
      "\tTraining batch 54 Loss: 0.001206\n",
      "\tTraining batch 55 Loss: 0.000010\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.001172\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.002313\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000254\n",
      "\tTraining batch 69 Loss: 0.097527\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.020773\n",
      "\tTraining batch 72 Loss: 0.027977\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000392\n",
      "\tTraining batch 76 Loss: 1.631219\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000127\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.001050\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000152\n",
      "Training set: Average loss: 0.068138\n",
      "Validation set: Average loss: 18.805969, Accuracy: 1375/1959 (70.19%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000036\n",
      "\tTraining batch 8 Loss: 0.002158\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000059\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000014\n",
      "\tTraining batch 17 Loss: 0.000004\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.006741\n",
      "\tTraining batch 20 Loss: 0.730278\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000002\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.001553\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.316978\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.011877\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000004\n",
      "\tTraining batch 57 Loss: 0.116207\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000045\n",
      "\tTraining batch 60 Loss: 0.094809\n",
      "\tTraining batch 61 Loss: 0.000019\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000002\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.084061\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.183870\n",
      "\tTraining batch 71 Loss: 0.904183\n",
      "\tTraining batch 72 Loss: 0.109470\n",
      "\tTraining batch 73 Loss: 0.405319\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000004\n",
      "\tTraining batch 76 Loss: 0.002039\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.273889\n",
      "\tTraining batch 82 Loss: 0.237486\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000005\n",
      "\tTraining batch 85 Loss: 1.213352\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.702348\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000043\n",
      "\tTraining batch 90 Loss: 0.475213\n",
      "Training set: Average loss: 0.065245\n",
      "Validation set: Average loss: 20.760823, Accuracy: 1379/1959 (70.39%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.001629\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.114427\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000003\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.005180\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000113\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000127\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.122536\n",
      "\tTraining batch 42 Loss: 0.000001\n",
      "\tTraining batch 43 Loss: 0.000002\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000169\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000074\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.002003\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.190285\n",
      "\tTraining batch 57 Loss: 0.003339\n",
      "\tTraining batch 58 Loss: 0.000002\n",
      "\tTraining batch 59 Loss: 0.000102\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.562858\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.002045\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000002\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000006\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000004\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000702\n",
      "\tTraining batch 90 Loss: 0.000722\n",
      "Training set: Average loss: 0.011181\n",
      "Validation set: Average loss: 22.547455, Accuracy: 1384/1959 (70.65%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000078\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000004\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000012\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.008290\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.001007\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000590\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000007\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000005\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "Training set: Average loss: 0.000111\n",
      "Validation set: Average loss: 22.077662, Accuracy: 1393/1959 (71.11%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000085\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000003\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000004\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000759\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000283\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000036\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000006\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000004\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "Training set: Average loss: 0.000013\n",
      "Validation set: Average loss: 22.073726, Accuracy: 1392/1959 (71.06%)\n",
      "\n",
      "Epoch: 11\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000083\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000003\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000004\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000720\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000273\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000036\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000006\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000004\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "Training set: Average loss: 0.000013\n",
      "Validation set: Average loss: 22.076412, Accuracy: 1392/1959 (71.06%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 50.0%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000003\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000004\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000683\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000263\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000036\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000006\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000003\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 26.089142\n",
      "Training set: Average loss: 0.286706\n",
      "Validation set: Average loss: 21.539125, Accuracy: 1369/1959 (69.88%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000032\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000005\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 1.156131\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000044\n",
      "\tTraining batch 22 Loss: 0.042577\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000015\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.056375\n",
      "\tTraining batch 27 Loss: 0.836786\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.006882\n",
      "\tTraining batch 31 Loss: 0.667407\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.018111\n",
      "\tTraining batch 34 Loss: 0.003142\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000001\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.072156\n",
      "\tTraining batch 41 Loss: 0.027499\n",
      "\tTraining batch 42 Loss: 0.000012\n",
      "\tTraining batch 43 Loss: 0.000003\n",
      "\tTraining batch 44 Loss: 0.059567\n",
      "\tTraining batch 45 Loss: 0.000224\n",
      "\tTraining batch 46 Loss: 0.005012\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000039\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000139\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.002274\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.001810\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.822251\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.006554\n",
      "\tTraining batch 63 Loss: 0.020889\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000043\n",
      "\tTraining batch 67 Loss: 0.000001\n",
      "\tTraining batch 68 Loss: 0.540586\n",
      "\tTraining batch 69 Loss: 0.000004\n",
      "\tTraining batch 70 Loss: 0.078491\n",
      "\tTraining batch 71 Loss: 0.000006\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.182157\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.001205\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000967\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000012\n",
      "\tTraining batch 83 Loss: 0.144133\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000001\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.101418\n",
      "\tTraining batch 89 Loss: 0.518500\n",
      "\tTraining batch 90 Loss: 0.425456\n",
      "\tTraining batch 91 Loss: 1.098949\n",
      "Training set: Average loss: 0.075801\n",
      "Validation set: Average loss: 16.044454, Accuracy: 1372/1959 (70.04%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.718363\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000677\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.134308\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.003814\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.001168\n",
      "\tTraining batch 24 Loss: 0.000005\n",
      "\tTraining batch 25 Loss: 0.184798\n",
      "\tTraining batch 26 Loss: 0.026503\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000071\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000402\n",
      "\tTraining batch 34 Loss: 0.000506\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.056788\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000003\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.203156\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000003\n",
      "\tTraining batch 43 Loss: 0.000158\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000101\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000002\n",
      "\tTraining batch 49 Loss: 0.000320\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.001268\n",
      "\tTraining batch 54 Loss: 0.002090\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.041207\n",
      "\tTraining batch 57 Loss: 0.000207\n",
      "\tTraining batch 58 Loss: 0.000038\n",
      "\tTraining batch 59 Loss: 0.000754\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000097\n",
      "\tTraining batch 67 Loss: 0.932287\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.261873\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000010\n",
      "\tTraining batch 72 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.001276\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.038856\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.678835\n",
      "\tTraining batch 89 Loss: 0.000143\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.010590\n",
      "Training set: Average loss: 0.036271\n",
      "Validation set: Average loss: 18.222551, Accuracy: 1366/1959 (69.73%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.009192\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.006778\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000003\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000023\n",
      "\tTraining batch 19 Loss: 0.000144\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.345486\n",
      "\tTraining batch 25 Loss: 0.327758\n",
      "\tTraining batch 26 Loss: 0.000060\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000376\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.041908\n",
      "\tTraining batch 35 Loss: 0.000792\n",
      "\tTraining batch 36 Loss: 0.221187\n",
      "\tTraining batch 37 Loss: 0.545054\n",
      "\tTraining batch 38 Loss: 0.126152\n",
      "\tTraining batch 39 Loss: 0.000260\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000001\n",
      "\tTraining batch 44 Loss: 0.003785\n",
      "\tTraining batch 45 Loss: 0.000013\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000061\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000049\n",
      "\tTraining batch 58 Loss: 0.024865\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.079979\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000058\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000001\n",
      "\tTraining batch 68 Loss: 0.000015\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000676\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000022\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000001\n",
      "\tTraining batch 79 Loss: 0.022743\n",
      "\tTraining batch 80 Loss: 0.208761\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.010282\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.003067\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.300566\n",
      "\tTraining batch 89 Loss: 0.371716\n",
      "\tTraining batch 90 Loss: 0.002354\n",
      "\tTraining batch 91 Loss: 0.323802\n",
      "Training set: Average loss: 0.032725\n",
      "Validation set: Average loss: 19.248973, Accuracy: 1383/1959 (70.60%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.028712\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000001\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001448\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.002252\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000512\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.059382\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000227\n",
      "\tTraining batch 45 Loss: 0.000036\n",
      "\tTraining batch 46 Loss: 0.000001\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 1.104753\n",
      "\tTraining batch 54 Loss: 0.000115\n",
      "\tTraining batch 55 Loss: 0.000002\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.001872\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000001\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.282989\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.573198\n",
      "\tTraining batch 72 Loss: 0.003780\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000002\n",
      "\tTraining batch 75 Loss: 0.000001\n",
      "\tTraining batch 76 Loss: 0.000282\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.547408\n",
      "\tTraining batch 83 Loss: 0.125145\n",
      "\tTraining batch 84 Loss: 0.000001\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000010\n",
      "\tTraining batch 88 Loss: 0.000001\n",
      "\tTraining batch 89 Loss: 0.000001\n",
      "\tTraining batch 90 Loss: 0.000006\n",
      "\tTraining batch 91 Loss: 0.354918\n",
      "Training set: Average loss: 0.033924\n",
      "Validation set: Average loss: 20.436033, Accuracy: 1388/1959 (70.85%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000266\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000134\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.005000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000003\n",
      "\tTraining batch 44 Loss: 0.000011\n",
      "\tTraining batch 45 Loss: 0.000002\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000022\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.001032\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.043435\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000001\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.125044\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.205515\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.001196\n",
      "\tTraining batch 91 Loss: 0.503492\n",
      "Training set: Average loss: 0.009727\n",
      "Validation set: Average loss: 20.896681, Accuracy: 1387/1959 (70.80%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000008\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.104073\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.189448\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000082\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000001\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000259\n",
      "\tTraining batch 44 Loss: 0.000652\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000029\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.101078\n",
      "\tTraining batch 58 Loss: 0.490180\n",
      "\tTraining batch 59 Loss: 0.000105\n",
      "\tTraining batch 60 Loss: 0.007362\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.082435\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000003\n",
      "\tTraining batch 68 Loss: 0.600528\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.315016\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.743058\n",
      "\tTraining batch 76 Loss: 0.049639\n",
      "\tTraining batch 77 Loss: 0.026866\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000005\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.256161\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.313714\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.127679\n",
      "\tTraining batch 91 Loss: 0.008444\n",
      "Training set: Average loss: 0.037548\n",
      "Validation set: Average loss: 23.720013, Accuracy: 1373/1959 (70.09%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000476\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000048\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.004622\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.654296\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000195\n",
      "\tTraining batch 43 Loss: 0.000316\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.014296\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000087\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000003\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000398\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.188074\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000001\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000059\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.608428\n",
      "\tTraining batch 75 Loss: 0.379651\n",
      "\tTraining batch 76 Loss: 0.046642\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000001\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000002\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.054936\n",
      "Training set: Average loss: 0.021456\n",
      "Validation set: Average loss: 22.255042, Accuracy: 1391/1959 (71.01%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.211164\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.006343\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.648286\n",
      "\tTraining batch 19 Loss: 0.000302\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.076118\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000451\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000087\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000003\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000027\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.338788\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000410\n",
      "\tTraining batch 77 Loss: 0.000028\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000001\n",
      "\tTraining batch 80 Loss: 0.000002\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000011\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000004\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 1.178471\n",
      "\tTraining batch 90 Loss: 0.200124\n",
      "\tTraining batch 91 Loss: 0.006398\n",
      "Training set: Average loss: 0.029308\n",
      "Validation set: Average loss: 21.210683, Accuracy: 1390/1959 (70.95%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000085\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.558534\n",
      "\tTraining batch 17 Loss: 0.034008\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.037418\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.183040\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000001\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.030943\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.013675\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.366041\n",
      "\tTraining batch 43 Loss: 0.009350\n",
      "\tTraining batch 44 Loss: 0.326814\n",
      "\tTraining batch 45 Loss: 0.000124\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.132287\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.001262\n",
      "\tTraining batch 54 Loss: 0.000006\n",
      "\tTraining batch 55 Loss: 0.000118\n",
      "\tTraining batch 56 Loss: 0.470615\n",
      "\tTraining batch 57 Loss: 0.000001\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.005932\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000003\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.035480\n",
      "\tTraining batch 75 Loss: 0.000003\n",
      "\tTraining batch 76 Loss: 0.004810\n",
      "\tTraining batch 77 Loss: 0.001139\n",
      "\tTraining batch 78 Loss: 0.220080\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000408\n",
      "\tTraining batch 91 Loss: 0.005455\n",
      "Training set: Average loss: 0.026787\n",
      "Validation set: Average loss: 24.758016, Accuracy: 1373/1959 (70.09%)\n",
      "\n",
      "Epoch: 11\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000001\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000110\n",
      "\tTraining batch 16 Loss: 0.000009\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 1.092397\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000001\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000714\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.002586\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000004\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000001\n",
      "\tTraining batch 39 Loss: 0.000002\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.853315\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 44 Loss: 0.000195\n",
      "\tTraining batch 45 Loss: 0.002068\n",
      "\tTraining batch 46 Loss: 0.727922\n",
      "\tTraining batch 47 Loss: 0.000020\n",
      "\tTraining batch 48 Loss: 0.004425\n",
      "\tTraining batch 49 Loss: 0.000053\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.006893\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000615\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.080760\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000080\n",
      "\tTraining batch 65 Loss: 0.350224\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000014\n",
      "\tTraining batch 74 Loss: 0.452460\n",
      "\tTraining batch 75 Loss: 0.000002\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.016654\n",
      "\tTraining batch 78 Loss: 0.006613\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000098\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.846792\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.745607\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000007\n",
      "\tTraining batch 90 Loss: 0.000003\n",
      "\tTraining batch 91 Loss: 0.002479\n",
      "Training set: Average loss: 0.057068\n",
      "Validation set: Average loss: 26.773806, Accuracy: 1381/1959 (70.50%)\n",
      "\n",
      "Epoch: 12\n",
      "\tTraining batch 1 Loss: 0.170475\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000746\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.019629\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000045\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.014542\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.399179\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000001\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000001\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000003\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.559144\n",
      "\tTraining batch 55 Loss: 0.278926\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.039392\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.722858\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.068205\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.005462\n",
      "Training set: Average loss: 0.025040\n",
      "Validation set: Average loss: 25.603150, Accuracy: 1389/1959 (70.90%)\n",
      "\n",
      "Epoch: 13\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000086\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.006824\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000620\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.003273\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.400515\n",
      "\tTraining batch 44 Loss: 0.000757\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000153\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000005\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000022\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000010\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.001222\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000001\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.005181\n",
      "Training set: Average loss: 0.004601\n",
      "Validation set: Average loss: 26.402976, Accuracy: 1383/1959 (70.60%)\n",
      "\n",
      "Epoch: 14\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000473\n",
      "\tTraining batch 14 Loss: 0.000071\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000041\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000021\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.003003\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000003\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000025\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.004895\n",
      "Training set: Average loss: 0.000094\n",
      "Validation set: Average loss: 26.448962, Accuracy: 1385/1959 (70.70%)\n",
      "\n",
      "Epoch: 15\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000002\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000031\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000021\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000011\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000655\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000003\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000027\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.004684\n",
      "Training set: Average loss: 0.000060\n",
      "Validation set: Average loss: 26.468942, Accuracy: 1385/1959 (70.70%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 47.75%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000002\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000020\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000021\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000008\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000519\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 54 Loss: 0.000003\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000027\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.004478\n",
      "\tTraining batch 92 Loss: 38.052860\n",
      "Training set: Average loss: 0.413673\n",
      "Validation set: Average loss: 24.043659, Accuracy: 1378/1959 (70.34%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000091\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 1.382719\n",
      "\tTraining batch 9 Loss: 0.408796\n",
      "\tTraining batch 10 Loss: 1.716258\n",
      "\tTraining batch 11 Loss: 0.659602\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.066552\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.661733\n",
      "\tTraining batch 16 Loss: 0.159074\n",
      "\tTraining batch 17 Loss: 0.000959\n",
      "\tTraining batch 18 Loss: 0.000005\n",
      "\tTraining batch 19 Loss: 0.476564\n",
      "\tTraining batch 20 Loss: 0.383367\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.558900\n",
      "\tTraining batch 23 Loss: 0.251013\n",
      "\tTraining batch 24 Loss: 0.031818\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.068458\n",
      "\tTraining batch 27 Loss: 0.507919\n",
      "\tTraining batch 28 Loss: 0.232857\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.580150\n",
      "\tTraining batch 31 Loss: 0.000332\n",
      "\tTraining batch 32 Loss: 1.322153\n",
      "\tTraining batch 33 Loss: 0.000013\n",
      "\tTraining batch 34 Loss: 0.000363\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000003\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.315354\n",
      "\tTraining batch 43 Loss: 0.002546\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000538\n",
      "\tTraining batch 46 Loss: 0.121393\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000009\n",
      "\tTraining batch 54 Loss: 0.002904\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.002443\n",
      "\tTraining batch 57 Loss: 0.000194\n",
      "\tTraining batch 58 Loss: 0.000003\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000019\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000540\n",
      "\tTraining batch 67 Loss: 0.000009\n",
      "\tTraining batch 68 Loss: 0.000017\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.157438\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.909138\n",
      "\tTraining batch 75 Loss: 0.214413\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000002\n",
      "\tTraining batch 78 Loss: 0.012911\n",
      "\tTraining batch 79 Loss: 0.000008\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000182\n",
      "\tTraining batch 85 Loss: 0.048752\n",
      "\tTraining batch 86 Loss: 0.135338\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000001\n",
      "\tTraining batch 89 Loss: 0.000079\n",
      "\tTraining batch 90 Loss: 2.346700\n",
      "\tTraining batch 91 Loss: 0.566242\n",
      "\tTraining batch 92 Loss: 2.687078\n",
      "Training set: Average loss: 0.184717\n",
      "Validation set: Average loss: 20.213555, Accuracy: 1361/1959 (69.47%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.049244\n",
      "\tTraining batch 4 Loss: 0.017584\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000021\n",
      "\tTraining batch 16 Loss: 0.173956\n",
      "\tTraining batch 17 Loss: 0.000001\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000773\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.016174\n",
      "\tTraining batch 28 Loss: 0.045301\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.318322\n",
      "\tTraining batch 32 Loss: 0.003482\n",
      "\tTraining batch 33 Loss: 0.000006\n",
      "\tTraining batch 34 Loss: 0.752888\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.473363\n",
      "\tTraining batch 40 Loss: 0.000001\n",
      "\tTraining batch 41 Loss: 0.000001\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.002331\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000059\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.491640\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.371509\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.001878\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000007\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.331786\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000010\n",
      "\tTraining batch 67 Loss: 0.002741\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.084284\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.157042\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000011\n",
      "\tTraining batch 77 Loss: 0.000021\n",
      "\tTraining batch 78 Loss: 0.323138\n",
      "\tTraining batch 79 Loss: 0.206249\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000005\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.392101\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000006\n",
      "\tTraining batch 90 Loss: 0.014517\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 3.901138\n",
      "Training set: Average loss: 0.088387\n",
      "Validation set: Average loss: 24.517331, Accuracy: 1377/1959 (70.29%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.002635\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000004\n",
      "\tTraining batch 14 Loss: 0.000091\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000028\n",
      "\tTraining batch 17 Loss: 0.000007\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.002490\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000047\n",
      "\tTraining batch 23 Loss: 0.002256\n",
      "\tTraining batch 24 Loss: 0.539881\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.070945\n",
      "\tTraining batch 27 Loss: 0.000001\n",
      "\tTraining batch 28 Loss: 0.000200\n",
      "\tTraining batch 29 Loss: 0.000005\n",
      "\tTraining batch 30 Loss: 0.000102\n",
      "\tTraining batch 31 Loss: 0.003892\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000163\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000001\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000002\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.004502\n",
      "\tTraining batch 44 Loss: 0.000002\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.908132\n",
      "\tTraining batch 47 Loss: 0.077662\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000001\n",
      "\tTraining batch 51 Loss: 0.544231\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.001799\n",
      "\tTraining batch 55 Loss: 0.000001\n",
      "\tTraining batch 56 Loss: 0.011628\n",
      "\tTraining batch 57 Loss: 0.001949\n",
      "\tTraining batch 58 Loss: 0.000003\n",
      "\tTraining batch 59 Loss: 0.001139\n",
      "\tTraining batch 60 Loss: 0.001072\n",
      "\tTraining batch 61 Loss: 0.056738\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.299304\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000005\n",
      "\tTraining batch 67 Loss: 0.524046\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000004\n",
      "\tTraining batch 70 Loss: 0.008744\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000001\n",
      "\tTraining batch 77 Loss: 0.024632\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 1.357072\n",
      "\tTraining batch 80 Loss: 0.000060\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000044\n",
      "\tTraining batch 84 Loss: 0.000024\n",
      "\tTraining batch 85 Loss: 0.076190\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000444\n",
      "\tTraining batch 91 Loss: 0.004701\n",
      "\tTraining batch 92 Loss: 0.489646\n",
      "Training set: Average loss: 0.054527\n",
      "Validation set: Average loss: 24.106277, Accuracy: 1376/1959 (70.24%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000016\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.160580\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.025017\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000269\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000052\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.003142\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000023\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.003515\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000007\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.002464\n",
      "\tTraining batch 55 Loss: 0.000001\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000010\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000013\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000001\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.041246\n",
      "\tTraining batch 76 Loss: 0.000007\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000028\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000519\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "Training set: Average loss: 0.002575\n",
      "Validation set: Average loss: 22.248785, Accuracy: 1375/1959 (70.19%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000001\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000531\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000001\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000110\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000007\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000002\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.001776\n",
      "\tTraining batch 55 Loss: 0.000075\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000006\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000007\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000005\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000474\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "Training set: Average loss: 0.000033\n",
      "Validation set: Average loss: 22.632712, Accuracy: 1378/1959 (70.34%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000001\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000532\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000001\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000001\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000007\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000002\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.001670\n",
      "\tTraining batch 55 Loss: 0.000004\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000006\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000002\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000005\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000450\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "Training set: Average loss: 0.000029\n",
      "Validation set: Average loss: 22.657884, Accuracy: 1377/1959 (70.29%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000511\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000001\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000001\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000007\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000002\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.001505\n",
      "\tTraining batch 55 Loss: 0.000005\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000006\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000002\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000005\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000427\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "Training set: Average loss: 0.000027\n",
      "Validation set: Average loss: 22.682133, Accuracy: 1377/1959 (70.29%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 50.5%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000492\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000001\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000001\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000007\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000002\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.001369\n",
      "\tTraining batch 55 Loss: 0.000005\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000005\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000001\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000005\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000406\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 24.849586\n",
      "Training set: Average loss: 0.267225\n",
      "Validation set: Average loss: 20.953883, Accuracy: 1376/1959 (70.24%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000041\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000053\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.262430\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 1.016311\n",
      "\tTraining batch 11 Loss: 1.227023\n",
      "\tTraining batch 12 Loss: 2.424437\n",
      "\tTraining batch 13 Loss: 0.000013\n",
      "\tTraining batch 14 Loss: 1.177680\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 1.255338\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.287723\n",
      "\tTraining batch 19 Loss: 0.051860\n",
      "\tTraining batch 20 Loss: 0.450711\n",
      "\tTraining batch 21 Loss: 0.340374\n",
      "\tTraining batch 22 Loss: 0.012747\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 1.125093\n",
      "\tTraining batch 25 Loss: 0.094027\n",
      "\tTraining batch 26 Loss: 1.285169\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.018061\n",
      "\tTraining batch 29 Loss: 0.000353\n",
      "\tTraining batch 30 Loss: 0.465880\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000124\n",
      "\tTraining batch 33 Loss: 0.000115\n",
      "\tTraining batch 34 Loss: 0.000008\n",
      "\tTraining batch 35 Loss: 0.478545\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 1.558075\n",
      "\tTraining batch 38 Loss: 0.579845\n",
      "\tTraining batch 39 Loss: 0.000008\n",
      "\tTraining batch 40 Loss: 0.000086\n",
      "\tTraining batch 41 Loss: 0.000001\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.013424\n",
      "\tTraining batch 44 Loss: 0.895009\n",
      "\tTraining batch 45 Loss: 0.521511\n",
      "\tTraining batch 46 Loss: 0.000007\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.002388\n",
      "\tTraining batch 49 Loss: 0.431897\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.002886\n",
      "\tTraining batch 53 Loss: 0.095830\n",
      "\tTraining batch 54 Loss: 0.005511\n",
      "\tTraining batch 55 Loss: 0.522387\n",
      "\tTraining batch 56 Loss: 0.019549\n",
      "\tTraining batch 57 Loss: 0.105146\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000046\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.004674\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.097289\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000001\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.577785\n",
      "\tTraining batch 74 Loss: 0.102339\n",
      "\tTraining batch 75 Loss: 0.000010\n",
      "\tTraining batch 76 Loss: 0.000006\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.289726\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.106555\n",
      "\tTraining batch 83 Loss: 0.000389\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000704\n",
      "\tTraining batch 92 Loss: 1.291056\n",
      "\tTraining batch 93 Loss: 2.080487\n",
      "Training set: Average loss: 0.228804\n",
      "Validation set: Average loss: 20.839839, Accuracy: 1382/1959 (70.55%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.186428\n",
      "\tTraining batch 4 Loss: 0.000014\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000109\n",
      "\tTraining batch 7 Loss: 0.000055\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000002\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.687318\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000777\n",
      "\tTraining batch 20 Loss: 0.690062\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000071\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000199\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000002\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.085902\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000019\n",
      "\tTraining batch 36 Loss: 0.000001\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.136285\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000101\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000084\n",
      "\tTraining batch 46 Loss: 0.000001\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000005\n",
      "\tTraining batch 54 Loss: 0.003530\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.203527\n",
      "\tTraining batch 57 Loss: 0.092395\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000002\n",
      "\tTraining batch 60 Loss: 0.371935\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000001\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000002\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000051\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000011\n",
      "\tTraining batch 81 Loss: 0.003188\n",
      "\tTraining batch 82 Loss: 0.536428\n",
      "\tTraining batch 83 Loss: 0.003652\n",
      "\tTraining batch 84 Loss: 0.120643\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.002148\n",
      "\tTraining batch 87 Loss: 0.000010\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.003578\n",
      "\tTraining batch 92 Loss: 0.000001\n",
      "\tTraining batch 93 Loss: 0.000244\n",
      "Training set: Average loss: 0.033643\n",
      "Validation set: Average loss: 22.030209, Accuracy: 1384/1959 (70.65%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.001225\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000051\n",
      "\tTraining batch 6 Loss: 0.000001\n",
      "\tTraining batch 7 Loss: 0.002114\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000026\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.003318\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.011844\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000286\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000001\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000008\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.503848\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000005\n",
      "\tTraining batch 54 Loss: 0.002222\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.504917\n",
      "\tTraining batch 57 Loss: 0.000557\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000001\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000029\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000281\n",
      "\tTraining batch 77 Loss: 0.370043\n",
      "\tTraining batch 78 Loss: 0.108875\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000029\n",
      "\tTraining batch 90 Loss: 0.022963\n",
      "\tTraining batch 91 Loss: 0.348883\n",
      "\tTraining batch 92 Loss: 0.000101\n",
      "\tTraining batch 93 Loss: 0.000001\n",
      "Training set: Average loss: 0.020233\n",
      "Validation set: Average loss: 23.301621, Accuracy: 1382/1959 (70.55%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000002\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.001173\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000129\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000073\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.567358\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.298546\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.028502\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000424\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000002\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.126495\n",
      "\tTraining batch 54 Loss: 0.000561\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000002\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.214783\n",
      "\tTraining batch 76 Loss: 0.000017\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000003\n",
      "\tTraining batch 80 Loss: 0.024324\n",
      "\tTraining batch 81 Loss: 0.030559\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.006621\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.079553\n",
      "Training set: Average loss: 0.014829\n",
      "Validation set: Average loss: 24.450738, Accuracy: 1390/1959 (70.95%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 1.095454\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000619\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000009\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000018\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.001336\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000206\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000028\n",
      "\tTraining batch 45 Loss: 0.000003\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.001358\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000001\n",
      "\tTraining batch 58 Loss: 0.000009\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.001892\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000013\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000005\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.111396\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000909\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "Training set: Average loss: 0.013046\n",
      "Validation set: Average loss: 23.266494, Accuracy: 1387/1959 (70.80%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000011\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000258\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000001\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000003\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000206\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000007\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000622\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "Training set: Average loss: 0.000012\n",
      "Validation set: Average loss: 23.557311, Accuracy: 1391/1959 (71.01%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000973\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000269\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000002\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000002\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000206\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000007\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000611\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "Training set: Average loss: 0.000022\n",
      "Validation set: Average loss: 23.585589, Accuracy: 1393/1959 (71.11%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000266\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000001\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000002\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000202\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000007\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000604\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "Training set: Average loss: 0.000012\n",
      "Validation set: Average loss: 23.588579, Accuracy: 1393/1959 (71.11%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 49.75%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000259\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000001\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000002\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000198\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000007\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000595\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 29.355417\n",
      "Training set: Average loss: 0.312303\n",
      "Validation set: Average loss: 23.497593, Accuracy: 1373/1959 (70.09%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000006\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.690141\n",
      "\tTraining batch 7 Loss: 0.419985\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000024\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.029002\n",
      "\tTraining batch 15 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000030\n",
      "\tTraining batch 18 Loss: 0.000002\n",
      "\tTraining batch 19 Loss: 0.143864\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.645674\n",
      "\tTraining batch 22 Loss: 0.000008\n",
      "\tTraining batch 23 Loss: 0.001298\n",
      "\tTraining batch 24 Loss: 0.000397\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.002752\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.121986\n",
      "\tTraining batch 33 Loss: 0.000026\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.050182\n",
      "\tTraining batch 38 Loss: 0.345587\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000002\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.445220\n",
      "\tTraining batch 44 Loss: 2.748677\n",
      "\tTraining batch 45 Loss: 0.011753\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000824\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.310285\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 1.465773\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000063\n",
      "\tTraining batch 55 Loss: 0.000171\n",
      "\tTraining batch 56 Loss: 0.000103\n",
      "\tTraining batch 57 Loss: 0.000001\n",
      "\tTraining batch 58 Loss: 0.145437\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.040376\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000007\n",
      "\tTraining batch 67 Loss: 0.000009\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000005\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000004\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000070\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.007892\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000001\n",
      "\tTraining batch 83 Loss: 0.270636\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000005\n",
      "\tTraining batch 92 Loss: 0.127687\n",
      "\tTraining batch 93 Loss: 0.000190\n",
      "\tTraining batch 94 Loss: 4.274830\n",
      "Training set: Average loss: 0.130862\n",
      "Validation set: Average loss: 22.477991, Accuracy: 1355/1959 (69.17%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.442977\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.005062\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.750295\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000638\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000063\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000001\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000028\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.005485\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.152469\n",
      "\tTraining batch 44 Loss: 0.024178\n",
      "\tTraining batch 45 Loss: 0.000005\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.011038\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000275\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000003\n",
      "\tTraining batch 58 Loss: 0.075016\n",
      "\tTraining batch 59 Loss: 0.000208\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.030087\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.196400\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000011\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.108354\n",
      "\tTraining batch 83 Loss: 0.000001\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000006\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.101833\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000024\n",
      "\tTraining batch 94 Loss: 0.014497\n",
      "Training set: Average loss: 0.020414\n",
      "Validation set: Average loss: 23.606362, Accuracy: 1381/1959 (70.50%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.352385\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000409\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000001\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000014\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.233051\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000256\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000400\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.075001\n",
      "\tTraining batch 50 Loss: 0.000007\n",
      "\tTraining batch 51 Loss: 0.727908\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.003268\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000058\n",
      "\tTraining batch 57 Loss: 0.006862\n",
      "\tTraining batch 58 Loss: 0.000958\n",
      "\tTraining batch 59 Loss: 0.000116\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000001\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000534\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.145780\n",
      "\tTraining batch 75 Loss: 0.002392\n",
      "\tTraining batch 76 Loss: 0.000312\n",
      "\tTraining batch 77 Loss: 0.195482\n",
      "\tTraining batch 78 Loss: 0.000004\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.004216\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000461\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.467175\n",
      "\tTraining batch 91 Loss: 0.002817\n",
      "\tTraining batch 92 Loss: 0.000542\n",
      "\tTraining batch 93 Loss: 1.384753\n",
      "\tTraining batch 94 Loss: 0.200206\n",
      "Training set: Average loss: 0.040483\n",
      "Validation set: Average loss: 22.929159, Accuracy: 1372/1959 (70.04%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000008\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.119819\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000034\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.012871\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000009\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000065\n",
      "\tTraining batch 64 Loss: 0.513562\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.106875\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.022407\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.009128\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000234\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000001\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000004\n",
      "\tTraining batch 94 Loss: 0.000002\n",
      "Training set: Average loss: 0.008351\n",
      "Validation set: Average loss: 25.129682, Accuracy: 1373/1959 (70.09%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 1.841918\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000354\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.274515\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000001\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000154\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.229607\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.175470\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000001\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.105972\n",
      "\tTraining batch 76 Loss: 0.326946\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.118460\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.656778\n",
      "\tTraining batch 90 Loss: 1.786093\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000001\n",
      "\tTraining batch 94 Loss: 0.000050\n",
      "Training set: Average loss: 0.058684\n",
      "Validation set: Average loss: 26.734070, Accuracy: 1360/1959 (69.42%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.002591\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.525861\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.330255\n",
      "\tTraining batch 8 Loss: 0.176391\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000005\n",
      "\tTraining batch 18 Loss: 0.035523\n",
      "\tTraining batch 19 Loss: 0.000096\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.017175\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.004728\n",
      "\tTraining batch 27 Loss: 0.003068\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000001\n",
      "\tTraining batch 30 Loss: 0.000799\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000434\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000032\n",
      "\tTraining batch 45 Loss: 0.126141\n",
      "\tTraining batch 46 Loss: 0.000009\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000034\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000759\n",
      "\tTraining batch 55 Loss: 0.000918\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.015574\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000011\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 1.443540\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000313\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.330115\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000002\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000126\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.430454\n",
      "\tTraining batch 87 Loss: 0.027875\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.124991\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.192651\n",
      "Training set: Average loss: 0.040324\n",
      "Validation set: Average loss: 24.345462, Accuracy: 1374/1959 (70.14%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000082\n",
      "\tTraining batch 19 Loss: 0.137653\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000002\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.013178\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000003\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.108396\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.019883\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000132\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.038606\n",
      "\tTraining batch 64 Loss: 0.038904\n",
      "\tTraining batch 65 Loss: 0.000002\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.288205\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.002253\n",
      "\tTraining batch 79 Loss: 0.403254\n",
      "\tTraining batch 80 Loss: 0.022494\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000002\n",
      "\tTraining batch 86 Loss: 0.005770\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.443327\n",
      "\tTraining batch 92 Loss: 0.000001\n",
      "\tTraining batch 93 Loss: 0.990693\n",
      "\tTraining batch 94 Loss: 1.139715\n",
      "Training set: Average loss: 0.038857\n",
      "Validation set: Average loss: 26.276534, Accuracy: 1352/1959 (69.01%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.103033\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.252842\n",
      "\tTraining batch 11 Loss: 0.470913\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.001122\n",
      "\tTraining batch 14 Loss: 0.003886\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.228702\n",
      "\tTraining batch 18 Loss: 0.031187\n",
      "\tTraining batch 19 Loss: 0.000004\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.375037\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.002454\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.001726\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.008485\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000197\n",
      "\tTraining batch 46 Loss: 0.002718\n",
      "\tTraining batch 47 Loss: 0.000198\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.293361\n",
      "\tTraining batch 54 Loss: 0.006608\n",
      "\tTraining batch 55 Loss: 0.000001\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000084\n",
      "\tTraining batch 58 Loss: 0.003210\n",
      "\tTraining batch 59 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.205175\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000002\n",
      "\tTraining batch 65 Loss: 0.064774\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000065\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.001664\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.004397\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000042\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.297499\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000006\n",
      "\tTraining batch 92 Loss: 0.210091\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.407654\n",
      "Training set: Average loss: 0.031672\n",
      "Validation set: Average loss: 25.777418, Accuracy: 1373/1959 (70.09%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000019\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.077864\n",
      "\tTraining batch 17 Loss: 0.327895\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000568\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000981\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.087836\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000002\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000001\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000001\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000009\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.228409\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000183\n",
      "\tTraining batch 55 Loss: 0.000003\n",
      "\tTraining batch 56 Loss: 0.021829\n",
      "\tTraining batch 57 Loss: 0.000017\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000001\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.051496\n",
      "\tTraining batch 79 Loss: 0.006345\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.016279\n",
      "\tTraining batch 82 Loss: 0.662471\n",
      "\tTraining batch 83 Loss: 0.000026\n",
      "\tTraining batch 84 Loss: 0.000188\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.060594\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000050\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "Training set: Average loss: 0.016416\n",
      "Validation set: Average loss: 26.061536, Accuracy: 1377/1959 (70.29%)\n",
      "\n",
      "Epoch: 11\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000055\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000001\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000664\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000001\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000001\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "Training set: Average loss: 0.000008\n",
      "Validation set: Average loss: 26.994626, Accuracy: 1383/1959 (70.60%)\n",
      "\n",
      "Epoch: 12\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000046\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000001\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000631\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000001\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000001\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "Training set: Average loss: 0.000007\n",
      "Validation set: Average loss: 27.001772, Accuracy: 1383/1959 (70.60%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 47.25%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000045\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000001\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000601\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000001\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000001\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 25.629288\n",
      "Training set: Average loss: 0.269789\n",
      "Validation set: Average loss: 24.820993, Accuracy: 1395/1959 (71.21%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000017\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000038\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.060761\n",
      "\tTraining batch 11 Loss: 0.251824\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.058268\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000001\n",
      "\tTraining batch 17 Loss: 0.000047\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.005329\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.134477\n",
      "\tTraining batch 22 Loss: 1.360184\n",
      "\tTraining batch 23 Loss: 0.339406\n",
      "\tTraining batch 24 Loss: 0.178237\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.032438\n",
      "\tTraining batch 27 Loss: 0.000070\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.010072\n",
      "\tTraining batch 35 Loss: 0.000003\n",
      "\tTraining batch 36 Loss: 0.667734\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.013390\n",
      "\tTraining batch 39 Loss: 0.000002\n",
      "\tTraining batch 40 Loss: 0.000120\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.574014\n",
      "\tTraining batch 43 Loss: 0.004568\n",
      "\tTraining batch 44 Loss: 0.000006\n",
      "\tTraining batch 45 Loss: 0.465165\n",
      "\tTraining batch 46 Loss: 0.000014\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.001166\n",
      "\tTraining batch 49 Loss: 0.000008\n",
      "\tTraining batch 50 Loss: 0.007802\n",
      "\tTraining batch 51 Loss: 0.004086\n",
      "\tTraining batch 52 Loss: 0.154631\n",
      "\tTraining batch 53 Loss: 0.000865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 54 Loss: 0.007599\n",
      "\tTraining batch 55 Loss: 0.735435\n",
      "\tTraining batch 56 Loss: 0.114238\n",
      "\tTraining batch 57 Loss: 0.005495\n",
      "\tTraining batch 58 Loss: 0.020368\n",
      "\tTraining batch 59 Loss: 0.013631\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000006\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 1.389136\n",
      "\tTraining batch 66 Loss: 0.393754\n",
      "\tTraining batch 67 Loss: 0.585405\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.065160\n",
      "\tTraining batch 70 Loss: 0.391384\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000002\n",
      "\tTraining batch 73 Loss: 0.579994\n",
      "\tTraining batch 74 Loss: 0.125439\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000001\n",
      "\tTraining batch 84 Loss: 0.000002\n",
      "\tTraining batch 85 Loss: 0.796696\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.004283\n",
      "\tTraining batch 91 Loss: 0.001837\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000011\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 4.491250\n",
      "Training set: Average loss: 0.147851\n",
      "Validation set: Average loss: 22.465581, Accuracy: 1370/1959 (69.93%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000001\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000008\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000642\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.068553\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000153\n",
      "\tTraining batch 20 Loss: 0.000001\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.012982\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.002656\n",
      "\tTraining batch 26 Loss: 0.419891\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.009968\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.194356\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.120630\n",
      "\tTraining batch 33 Loss: 0.003613\n",
      "\tTraining batch 34 Loss: 0.002497\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000072\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000006\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000024\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000025\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000155\n",
      "\tTraining batch 58 Loss: 0.422614\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.094843\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.057131\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000012\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.285848\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.002638\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.012705\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.349418\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.001179\n",
      "\tTraining batch 92 Loss: 0.034795\n",
      "\tTraining batch 93 Loss: 0.419579\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 1.422436\n",
      "Training set: Average loss: 0.041468\n",
      "Validation set: Average loss: 23.835836, Accuracy: 1373/1959 (70.09%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000002\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.269800\n",
      "\tTraining batch 14 Loss: 0.000001\n",
      "\tTraining batch 15 Loss: 0.000001\n",
      "\tTraining batch 16 Loss: 0.261288\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.202098\n",
      "\tTraining batch 19 Loss: 0.000019\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000001\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000156\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000236\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.300943\n",
      "\tTraining batch 44 Loss: 0.000001\n",
      "\tTraining batch 45 Loss: 1.349306\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.883091\n",
      "\tTraining batch 48 Loss: 0.000122\n",
      "\tTraining batch 49 Loss: 0.000794\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.139727\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.271343\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.013262\n",
      "\tTraining batch 91 Loss: 0.000004\n",
      "\tTraining batch 92 Loss: 0.000003\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.891475\n",
      "Training set: Average loss: 0.048249\n",
      "Validation set: Average loss: 24.886791, Accuracy: 1388/1959 (70.85%)\n",
      "\n",
      "Epoch: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000016\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000001\n",
      "\tTraining batch 15 Loss: 0.337548\n",
      "\tTraining batch 16 Loss: 0.064958\n",
      "\tTraining batch 17 Loss: 0.018020\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000004\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000002\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.027276\n",
      "\tTraining batch 27 Loss: 0.000015\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000305\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000525\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.034862\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 1.576967\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000429\n",
      "\tTraining batch 55 Loss: 0.014214\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.555506\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.166040\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.359740\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000530\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000004\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 1.274425\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000043\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000001\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.295475\n",
      "Training set: Average loss: 0.049757\n",
      "Validation set: Average loss: 26.485006, Accuracy: 1371/1959 (69.98%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.380734\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000197\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000001\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000023\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.013918\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000019\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000001\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000019\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000011\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000021\n",
      "\tTraining batch 92 Loss: 0.000005\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000002\n",
      "\tTraining batch 95 Loss: 0.000006\n",
      "Training set: Average loss: 0.004157\n",
      "Validation set: Average loss: 26.792166, Accuracy: 1389/1959 (70.90%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000012\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000013\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000001\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000001\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000001\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000016\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000009\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000021\n",
      "\tTraining batch 92 Loss: 0.000004\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000001\n",
      "\tTraining batch 95 Loss: 0.000006\n",
      "Training set: Average loss: 0.000001\n",
      "Validation set: Average loss: 26.794772, Accuracy: 1389/1959 (70.90%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 49.25%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000012\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000012\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000001\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000001\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000001\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000011\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000008\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000021\n",
      "\tTraining batch 92 Loss: 0.000004\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000001\n",
      "\tTraining batch 95 Loss: 0.000006\n",
      "\tTraining batch 96 Loss: 56.331066\n",
      "Training set: Average loss: 0.586783\n",
      "Validation set: Average loss: 25.005128, Accuracy: 1377/1959 (70.29%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.002237\n",
      "\tTraining batch 3 Loss: 1.271831\n",
      "\tTraining batch 4 Loss: 0.000011\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.015290\n",
      "\tTraining batch 8 Loss: 0.000045\n",
      "\tTraining batch 9 Loss: 0.000025\n",
      "\tTraining batch 10 Loss: 0.373919\n",
      "\tTraining batch 11 Loss: 0.169165\n",
      "\tTraining batch 12 Loss: 0.167955\n",
      "\tTraining batch 13 Loss: 0.000304\n",
      "\tTraining batch 14 Loss: 0.932869\n",
      "\tTraining batch 15 Loss: 2.258878\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.150950\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.008037\n",
      "\tTraining batch 20 Loss: 0.593462\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.011555\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.171405\n",
      "\tTraining batch 25 Loss: 1.059792\n",
      "\tTraining batch 26 Loss: 0.210336\n",
      "\tTraining batch 27 Loss: 0.038923\n",
      "\tTraining batch 28 Loss: 0.028000\n",
      "\tTraining batch 29 Loss: 0.282760\n",
      "\tTraining batch 30 Loss: 0.980408\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.209531\n",
      "\tTraining batch 33 Loss: 0.194073\n",
      "\tTraining batch 34 Loss: 0.039225\n",
      "\tTraining batch 35 Loss: 0.002634\n",
      "\tTraining batch 36 Loss: 0.000002\n",
      "\tTraining batch 37 Loss: 0.369706\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.091465\n",
      "\tTraining batch 40 Loss: 0.000483\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.032527\n",
      "\tTraining batch 43 Loss: 0.005212\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000024\n",
      "\tTraining batch 46 Loss: 0.000345\n",
      "\tTraining batch 47 Loss: 0.399306\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000129\n",
      "\tTraining batch 50 Loss: 0.001947\n",
      "\tTraining batch 51 Loss: 1.061871\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000774\n",
      "\tTraining batch 54 Loss: 0.000170\n",
      "\tTraining batch 55 Loss: 0.305316\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000036\n",
      "\tTraining batch 58 Loss: 1.015030\n",
      "\tTraining batch 59 Loss: 0.004599\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000567\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000002\n",
      "\tTraining batch 66 Loss: 0.000057\n",
      "\tTraining batch 67 Loss: 0.289096\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.969290\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.065475\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000006\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000001\n",
      "\tTraining batch 82 Loss: 0.000300\n",
      "\tTraining batch 83 Loss: 0.168785\n",
      "\tTraining batch 84 Loss: 0.290683\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 88 Loss: 0.000126\n",
      "\tTraining batch 89 Loss: 0.037008\n",
      "\tTraining batch 90 Loss: 0.000757\n",
      "\tTraining batch 91 Loss: 0.497630\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000328\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.817462\n",
      "\tTraining batch 96 Loss: 7.632317\n",
      "Training set: Average loss: 0.242005\n",
      "Validation set: Average loss: 19.493704, Accuracy: 1397/1959 (71.31%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000435\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.005295\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.455286\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.525390\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000001\n",
      "\tTraining batch 14 Loss: 0.136087\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.407692\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.005182\n",
      "\tTraining batch 19 Loss: 0.000580\n",
      "\tTraining batch 20 Loss: 0.000004\n",
      "\tTraining batch 21 Loss: 0.067016\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.005126\n",
      "\tTraining batch 26 Loss: 0.024249\n",
      "\tTraining batch 27 Loss: 0.000001\n",
      "\tTraining batch 28 Loss: 0.080396\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000001\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000020\n",
      "\tTraining batch 38 Loss: 0.000009\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.010663\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000778\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.011737\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.202136\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.003704\n",
      "\tTraining batch 51 Loss: 0.005675\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000002\n",
      "\tTraining batch 56 Loss: 0.765294\n",
      "\tTraining batch 57 Loss: 0.400852\n",
      "\tTraining batch 58 Loss: 0.000054\n",
      "\tTraining batch 59 Loss: 0.030408\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.008403\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.013367\n",
      "\tTraining batch 71 Loss: 0.001342\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.281196\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000005\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000340\n",
      "\tTraining batch 79 Loss: 0.003781\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000159\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.831149\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000002\n",
      "\tTraining batch 89 Loss: 0.457412\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000480\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.005565\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.751642\n",
      "Training set: Average loss: 0.057280\n",
      "Validation set: Average loss: 24.738107, Accuracy: 1371/1959 (69.98%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.535302\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.146095\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.004816\n",
      "\tTraining batch 25 Loss: 0.160095\n",
      "\tTraining batch 26 Loss: 0.552452\n",
      "\tTraining batch 27 Loss: 0.000561\n",
      "\tTraining batch 28 Loss: 0.001552\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000403\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000002\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000008\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.184235\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000203\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.064565\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.001071\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000002\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000640\n",
      "\tTraining batch 82 Loss: 0.000224\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.003100\n",
      "\tTraining batch 89 Loss: 0.000093\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.107632\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.273197\n",
      "\tTraining batch 95 Loss: 0.001181\n",
      "\tTraining batch 96 Loss: 0.000024\n",
      "Training set: Average loss: 0.021223\n",
      "Validation set: Average loss: 22.520006, Accuracy: 1388/1959 (70.85%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000002\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000006\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000309\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000054\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.203462\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.120725\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 32 Loss: 0.000002\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000151\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000021\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.102624\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000001\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000090\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000240\n",
      "\tTraining batch 70 Loss: 0.000001\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000001\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000003\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000065\n",
      "\tTraining batch 83 Loss: 0.000001\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000026\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.001915\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000020\n",
      "\tTraining batch 96 Loss: 0.085025\n",
      "Training set: Average loss: 0.005362\n",
      "Validation set: Average loss: 21.892078, Accuracy: 1379/1959 (70.39%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.238337\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000009\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001268\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000280\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000012\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000016\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.729450\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000027\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000003\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000006\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.001388\n",
      "\tTraining batch 54 Loss: 0.002938\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.013001\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000066\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.259350\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.039181\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.538711\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.042843\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000214\n",
      "\tTraining batch 77 Loss: 0.870168\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000414\n",
      "\tTraining batch 83 Loss: 0.000001\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.182643\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000031\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.002219\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.221470\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000003\n",
      "\tTraining batch 96 Loss: 0.000037\n",
      "Training set: Average loss: 0.032751\n",
      "Validation set: Average loss: 22.578521, Accuracy: 1379/1959 (70.39%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 49.25%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000464\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000002\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000302\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000311\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000121\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.107387\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.176473\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000001\n",
      "\tTraining batch 58 Loss: 0.000008\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.230842\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000051\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.003725\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000004\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000004\n",
      "\tTraining batch 85 Loss: 0.000002\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.004050\n",
      "\tTraining batch 90 Loss: 0.139694\n",
      "\tTraining batch 91 Loss: 0.000905\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000062\n",
      "\tTraining batch 94 Loss: 0.000009\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.705674\n",
      "\tTraining batch 97 Loss: 32.379692\n",
      "Training set: Average loss: 0.347936\n",
      "Validation set: Average loss: 20.585595, Accuracy: 1376/1959 (70.24%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.036967\n",
      "\tTraining batch 3 Loss: 0.000003\n",
      "\tTraining batch 4 Loss: 0.000088\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.739941\n",
      "\tTraining batch 8 Loss: 0.631656\n",
      "\tTraining batch 9 Loss: 0.678778\n",
      "\tTraining batch 10 Loss: 0.095385\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.007858\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001742\n",
      "\tTraining batch 20 Loss: 0.006728\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.058794\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000030\n",
      "\tTraining batch 26 Loss: 0.597231\n",
      "\tTraining batch 27 Loss: 0.530366\n",
      "\tTraining batch 28 Loss: 0.000176\n",
      "\tTraining batch 29 Loss: 0.000279\n",
      "\tTraining batch 30 Loss: 0.000351\n",
      "\tTraining batch 31 Loss: 0.763368\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.386401\n",
      "\tTraining batch 34 Loss: 0.005218\n",
      "\tTraining batch 35 Loss: 0.010839\n",
      "\tTraining batch 36 Loss: 0.001435\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.149893\n",
      "\tTraining batch 40 Loss: 0.000316\n",
      "\tTraining batch 41 Loss: 0.057360\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.005177\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.019792\n",
      "\tTraining batch 46 Loss: 0.507118\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000002\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.240126\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000098\n",
      "\tTraining batch 54 Loss: 0.000432\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.214330\n",
      "\tTraining batch 58 Loss: 0.003829\n",
      "\tTraining batch 59 Loss: 0.000181\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000022\n",
      "\tTraining batch 64 Loss: 0.002288\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.092991\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.182865\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000001\n",
      "\tTraining batch 74 Loss: 0.005192\n",
      "\tTraining batch 75 Loss: 0.000228\n",
      "\tTraining batch 76 Loss: 0.001741\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.176886\n",
      "\tTraining batch 80 Loss: 0.000003\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000002\n",
      "\tTraining batch 83 Loss: 0.000084\n",
      "\tTraining batch 84 Loss: 0.131695\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000576\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.007294\n",
      "\tTraining batch 92 Loss: 0.000001\n",
      "\tTraining batch 93 Loss: 0.000873\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.067041\n",
      "\tTraining batch 96 Loss: 0.006389\n",
      "\tTraining batch 97 Loss: 3.665632\n",
      "Training set: Average loss: 0.104063\n",
      "Validation set: Average loss: 19.455286, Accuracy: 1380/1959 (70.44%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000003\n",
      "\tTraining batch 4 Loss: 0.009648\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 1.351699\n",
      "\tTraining batch 7 Loss: 0.000017\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000001\n",
      "\tTraining batch 11 Loss: 0.000002\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000836\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000273\n",
      "\tTraining batch 25 Loss: 0.310944\n",
      "\tTraining batch 26 Loss: 0.000003\n",
      "\tTraining batch 27 Loss: 0.111168\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000001\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000030\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000061\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000076\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.003437\n",
      "\tTraining batch 54 Loss: 0.000005\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000268\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000008\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000982\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.001153\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000002\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000007\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.009907\n",
      "\tTraining batch 74 Loss: 0.000001\n",
      "\tTraining batch 75 Loss: 0.000010\n",
      "\tTraining batch 76 Loss: 0.000278\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.001869\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000017\n",
      "\tTraining batch 83 Loss: 0.000032\n",
      "\tTraining batch 84 Loss: 0.000286\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.097293\n",
      "\tTraining batch 88 Loss: 0.000239\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.008153\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.024053\n",
      "\tTraining batch 96 Loss: 0.012506\n",
      "\tTraining batch 97 Loss: 0.032762\n",
      "Training set: Average loss: 0.020392\n",
      "Validation set: Average loss: 19.014148, Accuracy: 1381/1959 (70.50%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000080\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.086961\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000046\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.386096\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000001\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000009\n",
      "\tTraining batch 44 Loss: 0.000754\n",
      "\tTraining batch 45 Loss: 0.000001\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000001\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000025\n",
      "\tTraining batch 54 Loss: 0.000156\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000006\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000004\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000015\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.010559\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000489\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000002\n",
      "\tTraining batch 76 Loss: 0.000003\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000005\n",
      "\tTraining batch 91 Loss: 0.008379\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000020\n",
      "\tTraining batch 95 Loss: 0.466945\n",
      "\tTraining batch 96 Loss: 0.007629\n",
      "\tTraining batch 97 Loss: 0.000001\n",
      "Training set: Average loss: 0.009981\n",
      "Validation set: Average loss: 20.253680, Accuracy: 1378/1959 (70.34%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000770\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000004\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000091\n",
      "\tTraining batch 44 Loss: 0.000029\n",
      "\tTraining batch 45 Loss: 0.000002\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000009\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.038265\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000026\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000030\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000045\n",
      "\tTraining batch 84 Loss: 0.053023\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000057\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.007998\n",
      "\tTraining batch 92 Loss: 1.438997\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.074348\n",
      "\tTraining batch 96 Loss: 0.001312\n",
      "\tTraining batch 97 Loss: 0.441349\n",
      "Training set: Average loss: 0.021200\n",
      "Validation set: Average loss: 21.186800, Accuracy: 1382/1959 (70.55%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000003\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.288911\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001114\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000006\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.006633\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000001\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000003\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000004\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000017\n",
      "\tTraining batch 54 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.146658\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000002\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000002\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.605650\n",
      "\tTraining batch 83 Loss: 0.124723\n",
      "\tTraining batch 84 Loss: 0.000283\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000069\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.079983\n",
      "\tTraining batch 91 Loss: 0.131588\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000024\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.755496\n",
      "Training set: Average loss: 0.022074\n",
      "Validation set: Average loss: 23.105355, Accuracy: 1392/1959 (71.06%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000036\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.001281\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000004\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.005690\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.330519\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.030786\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.293845\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000106\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000003\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.337454\n",
      "\tTraining batch 55 Loss: 0.000583\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000004\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000002\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000016\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.309531\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000018\n",
      "\tTraining batch 84 Loss: 0.000001\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.007787\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.001340\n",
      "\tTraining batch 92 Loss: 0.004039\n",
      "\tTraining batch 93 Loss: 0.000020\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000006\n",
      "\tTraining batch 97 Loss: 0.021474\n",
      "Training set: Average loss: 0.013861\n",
      "Validation set: Average loss: 22.709270, Accuracy: 1381/1959 (70.50%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001121\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000006\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000014\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000001\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000002\n",
      "\tTraining batch 55 Loss: 0.000001\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000004\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000004\n",
      "\tTraining batch 60 Loss: 0.044027\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000004\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.001694\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000045\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000001\n",
      "\tTraining batch 79 Loss: 0.000002\n",
      "\tTraining batch 80 Loss: 0.000001\n",
      "\tTraining batch 81 Loss: 0.000515\n",
      "\tTraining batch 82 Loss: 0.000004\n",
      "\tTraining batch 83 Loss: 0.124548\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.027977\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000001\n",
      "\tTraining batch 90 Loss: 0.000425\n",
      "\tTraining batch 91 Loss: 0.000482\n",
      "\tTraining batch 92 Loss: 0.000063\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.017626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 96 Loss: 0.030064\n",
      "\tTraining batch 97 Loss: 0.234731\n",
      "Training set: Average loss: 0.004983\n",
      "Validation set: Average loss: 21.344897, Accuracy: 1402/1959 (71.57%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000676\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000162\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.106751\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000007\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000001\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.171857\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000001\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.057033\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.103292\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000002\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000001\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.008047\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.714552\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000002\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000273\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.033277\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.619415\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "Training set: Average loss: 0.018715\n",
      "Validation set: Average loss: 25.251038, Accuracy: 1393/1959 (71.11%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.678295\n",
      "\tTraining batch 5 Loss: 0.000317\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000013\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000011\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000001\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000004\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.005615\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000684\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "Training set: Average loss: 0.007061\n",
      "Validation set: Average loss: 25.512957, Accuracy: 1397/1959 (71.31%)\n",
      "\n",
      "Epoch: 11\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000017\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000001\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000003\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.004294\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "Training set: Average loss: 0.000044\n",
      "Validation set: Average loss: 25.525338, Accuracy: 1399/1959 (71.41%)\n",
      "\n",
      "Epoch: 12\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000016\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000001\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000003\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.003619\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "Training set: Average loss: 0.000038\n",
      "Validation set: Average loss: 25.530721, Accuracy: 1398/1959 (71.36%)\n",
      "\n",
      "Epoch: 13\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000015\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000001\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000002\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.003109\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "Training set: Average loss: 0.000032\n",
      "Validation set: Average loss: 25.535959, Accuracy: 1397/1959 (71.31%)\n",
      "\n",
      "Epoch: 14\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000014\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000001\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000002\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.002709\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "Training set: Average loss: 0.000028\n",
      "Validation set: Average loss: 25.540606, Accuracy: 1397/1959 (71.31%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 48.75%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000014\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000001\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000002\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.002387\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 52.477406\n",
      "Training set: Average loss: 0.535508\n",
      "Validation set: Average loss: 22.476174, Accuracy: 1377/1959 (70.29%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.398578\n",
      "\tTraining batch 5 Loss: 0.179156\n",
      "\tTraining batch 6 Loss: 0.038814\n",
      "\tTraining batch 7 Loss: 0.120845\n",
      "\tTraining batch 8 Loss: 0.206828\n",
      "\tTraining batch 9 Loss: 1.036580\n",
      "\tTraining batch 10 Loss: 0.000957\n",
      "\tTraining batch 11 Loss: 0.377471\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000057\n",
      "\tTraining batch 14 Loss: 0.005293\n",
      "\tTraining batch 15 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 16 Loss: 0.000003\n",
      "\tTraining batch 17 Loss: 0.004567\n",
      "\tTraining batch 18 Loss: 0.701973\n",
      "\tTraining batch 19 Loss: 0.082764\n",
      "\tTraining batch 20 Loss: 0.154520\n",
      "\tTraining batch 21 Loss: 0.235617\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000003\n",
      "\tTraining batch 24 Loss: 0.528971\n",
      "\tTraining batch 25 Loss: 0.015943\n",
      "\tTraining batch 26 Loss: 0.463374\n",
      "\tTraining batch 27 Loss: 0.005498\n",
      "\tTraining batch 28 Loss: 0.317018\n",
      "\tTraining batch 29 Loss: 0.426286\n",
      "\tTraining batch 30 Loss: 0.196423\n",
      "\tTraining batch 31 Loss: 0.052716\n",
      "\tTraining batch 32 Loss: 0.898842\n",
      "\tTraining batch 33 Loss: 0.002151\n",
      "\tTraining batch 34 Loss: 0.002158\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.369802\n",
      "\tTraining batch 37 Loss: 0.007406\n",
      "\tTraining batch 38 Loss: 0.025236\n",
      "\tTraining batch 39 Loss: 0.000021\n",
      "\tTraining batch 40 Loss: 0.005285\n",
      "\tTraining batch 41 Loss: 0.000032\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.402426\n",
      "\tTraining batch 44 Loss: 0.002988\n",
      "\tTraining batch 45 Loss: 0.140597\n",
      "\tTraining batch 46 Loss: 0.776560\n",
      "\tTraining batch 47 Loss: 0.000607\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.334024\n",
      "\tTraining batch 50 Loss: 0.032481\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.359853\n",
      "\tTraining batch 53 Loss: 0.116665\n",
      "\tTraining batch 54 Loss: 0.168381\n",
      "\tTraining batch 55 Loss: 0.003062\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000289\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000022\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000056\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.012025\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.968075\n",
      "\tTraining batch 67 Loss: 0.378419\n",
      "\tTraining batch 68 Loss: 0.020741\n",
      "\tTraining batch 69 Loss: 0.123991\n",
      "\tTraining batch 70 Loss: 0.031465\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.236185\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.027575\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 1.547868\n",
      "\tTraining batch 80 Loss: 0.000004\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.020015\n",
      "\tTraining batch 83 Loss: 0.000013\n",
      "\tTraining batch 84 Loss: 0.000106\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.005850\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.330378\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.176536\n",
      "\tTraining batch 94 Loss: 0.375564\n",
      "\tTraining batch 95 Loss: 0.221588\n",
      "\tTraining batch 96 Loss: 0.876076\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 5.060687\n",
      "Training set: Average loss: 0.200126\n",
      "Validation set: Average loss: 20.445159, Accuracy: 1372/1959 (70.04%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.231117\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.939273\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.078586\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.281338\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000534\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.292497\n",
      "\tTraining batch 22 Loss: 0.008539\n",
      "\tTraining batch 23 Loss: 0.145505\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.002762\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.500512\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.709020\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.218723\n",
      "\tTraining batch 44 Loss: 0.000001\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.001242\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000513\n",
      "\tTraining batch 60 Loss: 0.046670\n",
      "\tTraining batch 61 Loss: 0.342550\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000484\n",
      "\tTraining batch 71 Loss: 0.007256\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000003\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000395\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000430\n",
      "\tTraining batch 81 Loss: 0.000001\n",
      "\tTraining batch 82 Loss: 0.000006\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000049\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.057530\n",
      "\tTraining batch 90 Loss: 0.000001\n",
      "\tTraining batch 91 Loss: 0.006461\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000553\n",
      "\tTraining batch 95 Loss: 0.000264\n",
      "\tTraining batch 96 Loss: 0.090135\n",
      "\tTraining batch 97 Loss: 0.295031\n",
      "\tTraining batch 98 Loss: 0.175882\n",
      "Training set: Average loss: 0.045244\n",
      "Validation set: Average loss: 20.816099, Accuracy: 1366/1959 (69.73%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000276\n",
      "\tTraining batch 6 Loss: 0.000042\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.259304\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000026\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.002451\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000004\n",
      "\tTraining batch 38 Loss: 0.023800\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000350\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000076\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000130\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000005\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000001\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000001\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.003927\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.036626\n",
      "\tTraining batch 81 Loss: 0.009046\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.664417\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.529332\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.006587\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.083130\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000001\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.672034\n",
      "Training set: Average loss: 0.023392\n",
      "Validation set: Average loss: 22.000072, Accuracy: 1383/1959 (70.60%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000001\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000013\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.789083\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000005\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000340\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000001\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000011\n",
      "\tTraining batch 44 Loss: 0.177094\n",
      "\tTraining batch 45 Loss: 0.821072\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.464545\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000030\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000020\n",
      "\tTraining batch 75 Loss: 0.073378\n",
      "\tTraining batch 76 Loss: 0.000003\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.009267\n",
      "\tTraining batch 82 Loss: 0.379661\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.002319\n",
      "\tTraining batch 85 Loss: 0.183916\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000012\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.005896\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.015447\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.749230\n",
      "\tTraining batch 97 Loss: 0.000241\n",
      "\tTraining batch 98 Loss: 0.260345\n",
      "Training set: Average loss: 0.040122\n",
      "Validation set: Average loss: 26.182976, Accuracy: 1373/1959 (70.09%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000002\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000018\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000012\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000001\n",
      "\tTraining batch 30 Loss: 0.007685\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.073622\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000001\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000001\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000001\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000002\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000003\n",
      "\tTraining batch 58 Loss: 0.000001\n",
      "\tTraining batch 59 Loss: 0.000007\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000001\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000058\n",
      "\tTraining batch 71 Loss: 0.232671\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000069\n",
      "\tTraining batch 75 Loss: 0.000031\n",
      "\tTraining batch 76 Loss: 0.000242\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000431\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.352964\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.003114\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000165\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.200698\n",
      "Training set: Average loss: 0.008896\n",
      "Validation set: Average loss: 25.205501, Accuracy: 1372/1959 (70.04%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000034\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.104202\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000019\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.001425\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000010\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000010\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000001\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000001\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.158288\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.005828\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000155\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000425\n",
      "\tTraining batch 92 Loss: 0.467384\n",
      "\tTraining batch 93 Loss: 0.003562\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 1.257463\n",
      "\tTraining batch 96 Loss: 0.023249\n",
      "\tTraining batch 97 Loss: 0.277768\n",
      "\tTraining batch 98 Loss: 0.799189\n",
      "Training set: Average loss: 0.031623\n",
      "Validation set: Average loss: 24.183132, Accuracy: 1396/1959 (71.26%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.191039\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000649\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.135286\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.030405\n",
      "\tTraining batch 20 Loss: 0.045175\n",
      "\tTraining batch 21 Loss: 0.000015\n",
      "\tTraining batch 22 Loss: 0.000033\n",
      "\tTraining batch 23 Loss: 0.321507\n",
      "\tTraining batch 24 Loss: 0.321007\n",
      "\tTraining batch 25 Loss: 0.000003\n",
      "\tTraining batch 26 Loss: 0.000027\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000001\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 1.031660\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000113\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.531356\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.153550\n",
      "\tTraining batch 41 Loss: 0.013280\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000002\n",
      "\tTraining batch 44 Loss: 0.010506\n",
      "\tTraining batch 45 Loss: 0.000067\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.204298\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.320812\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.382479\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000001\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000037\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.441334\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.010161\n",
      "\tTraining batch 74 Loss: 0.000007\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000078\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.273586\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.105321\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.013039\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000005\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.017286\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000001\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "Training set: Average loss: 0.046471\n",
      "Validation set: Average loss: 30.238289, Accuracy: 1358/1959 (69.32%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000006\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000011\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.011379\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.816300\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.216266\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 32 Loss: 0.220330\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000002\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000471\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 1.223263\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.142320\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000031\n",
      "\tTraining batch 60 Loss: 0.000032\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.017887\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000002\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000011\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000001\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.072307\n",
      "\tTraining batch 91 Loss: 0.003433\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000032\n",
      "\tTraining batch 96 Loss: 0.681486\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.001087\n",
      "Training set: Average loss: 0.034762\n",
      "Validation set: Average loss: 25.546280, Accuracy: 1379/1959 (70.39%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000005\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000017\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000032\n",
      "\tTraining batch 33 Loss: 0.000006\n",
      "\tTraining batch 34 Loss: 0.000088\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.271561\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.169114\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.004388\n",
      "\tTraining batch 63 Loss: 0.092412\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000001\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000042\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 1.551697\n",
      "\tTraining batch 75 Loss: 0.002570\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000024\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000011\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.003912\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000818\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.024393\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "Training set: Average loss: 0.021644\n",
      "Validation set: Average loss: 26.100490, Accuracy: 1377/1959 (70.29%)\n",
      "\n",
      "Epoch: 11\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000001\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000042\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000104\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000006\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000003\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000438\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000103\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000001\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000058\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000004\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000005\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.003954\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000002\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "Training set: Average loss: 0.000048\n",
      "Validation set: Average loss: 26.112904, Accuracy: 1381/1959 (70.50%)\n",
      "\n",
      "Epoch: 12\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000003\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000043\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000068\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000006\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000003\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000030\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000099\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000001\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000055\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000004\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000001\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.002482\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000001\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "Training set: Average loss: 0.000029\n",
      "Validation set: Average loss: 26.120846, Accuracy: 1381/1959 (70.50%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 49.25%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000002\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000043\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000052\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000006\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000003\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000022\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000096\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000001\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000055\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000005\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.001645\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 26.435043\n",
      "Training set: Average loss: 0.267040\n",
      "Validation set: Average loss: 26.037179, Accuracy: 1362/1959 (69.53%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000009\n",
      "\tTraining batch 5 Loss: 0.084885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 6 Loss: 0.001187\n",
      "\tTraining batch 7 Loss: 0.203892\n",
      "\tTraining batch 8 Loss: 0.672796\n",
      "\tTraining batch 9 Loss: 0.492457\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.874170\n",
      "\tTraining batch 12 Loss: 0.325497\n",
      "\tTraining batch 13 Loss: 0.647814\n",
      "\tTraining batch 14 Loss: 0.000005\n",
      "\tTraining batch 15 Loss: 0.599328\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.179135\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000686\n",
      "\tTraining batch 20 Loss: 0.383101\n",
      "\tTraining batch 21 Loss: 0.104141\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 1.478020\n",
      "\tTraining batch 26 Loss: 0.430273\n",
      "\tTraining batch 27 Loss: 0.979649\n",
      "\tTraining batch 28 Loss: 0.000081\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.118843\n",
      "\tTraining batch 32 Loss: 0.000020\n",
      "\tTraining batch 33 Loss: 0.838438\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.009054\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.398249\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.503062\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.007230\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.002215\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000285\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.013321\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000511\n",
      "\tTraining batch 54 Loss: 0.001598\n",
      "\tTraining batch 55 Loss: 0.346453\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.406414\n",
      "\tTraining batch 58 Loss: 1.406416\n",
      "\tTraining batch 59 Loss: 0.012511\n",
      "\tTraining batch 60 Loss: 0.000755\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000004\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.002458\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000251\n",
      "\tTraining batch 77 Loss: 0.000002\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000003\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000263\n",
      "\tTraining batch 92 Loss: 0.560533\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000091\n",
      "\tTraining batch 95 Loss: 0.025879\n",
      "\tTraining batch 96 Loss: 0.000141\n",
      "\tTraining batch 97 Loss: 0.357988\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 3.170257\n",
      "Training set: Average loss: 0.157984\n",
      "Validation set: Average loss: 25.513798, Accuracy: 1383/1959 (70.60%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000010\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.001425\n",
      "\tTraining batch 20 Loss: 0.291807\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.001668\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000233\n",
      "\tTraining batch 26 Loss: 0.000355\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000002\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.572900\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.687626\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000001\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000062\n",
      "\tTraining batch 46 Loss: 0.232768\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000002\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.001854\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000126\n",
      "\tTraining batch 58 Loss: 0.005503\n",
      "\tTraining batch 59 Loss: 0.041893\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.025707\n",
      "\tTraining batch 77 Loss: 0.000001\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.160831\n",
      "\tTraining batch 86 Loss: 0.000007\n",
      "\tTraining batch 87 Loss: 0.218552\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000045\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000762\n",
      "\tTraining batch 92 Loss: 0.670367\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.002281\n",
      "\tTraining batch 95 Loss: 0.071013\n",
      "\tTraining batch 96 Loss: 0.211678\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000104\n",
      "Training set: Average loss: 0.032319\n",
      "Validation set: Average loss: 25.001043, Accuracy: 1385/1959 (70.70%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000509\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.686599\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000152\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000003\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 43 Loss: 0.000485\n",
      "\tTraining batch 44 Loss: 0.000118\n",
      "\tTraining batch 45 Loss: 0.000249\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.002603\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 1.416474\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.004972\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000006\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000016\n",
      "\tTraining batch 71 Loss: 0.315333\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000010\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000010\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000637\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.604421\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000001\n",
      "\tTraining batch 92 Loss: 0.124621\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.042382\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000015\n",
      "Training set: Average loss: 0.032319\n",
      "Validation set: Average loss: 24.492290, Accuracy: 1379/1959 (70.39%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000981\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000006\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000010\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000001\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.023753\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000002\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000002\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.711322\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000033\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000424\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000463\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.164638\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.047716\n",
      "\tTraining batch 98 Loss: 0.000007\n",
      "\tTraining batch 99 Loss: 0.975992\n",
      "Training set: Average loss: 0.019448\n",
      "Validation set: Average loss: 26.477408, Accuracy: 1381/1959 (70.50%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.232572\n",
      "\tTraining batch 13 Loss: 0.621321\n",
      "\tTraining batch 14 Loss: 0.000513\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000003\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.001554\n",
      "\tTraining batch 23 Loss: 0.000174\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000001\n",
      "\tTraining batch 26 Loss: 0.000004\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000080\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.124675\n",
      "\tTraining batch 34 Loss: 0.124715\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000011\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.310612\n",
      "\tTraining batch 43 Loss: 0.230191\n",
      "\tTraining batch 44 Loss: 0.000003\n",
      "\tTraining batch 45 Loss: 0.001002\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.005356\n",
      "\tTraining batch 55 Loss: 0.390394\n",
      "\tTraining batch 56 Loss: 0.018042\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.002437\n",
      "\tTraining batch 60 Loss: 0.507782\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000172\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000253\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.455529\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000163\n",
      "\tTraining batch 94 Loss: 0.000002\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.191815\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.365073\n",
      "Training set: Average loss: 0.036207\n",
      "Validation set: Average loss: 29.853265, Accuracy: 1409/1959 (71.92%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000665\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000016\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000001\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000001\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.006544\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.241127\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000001\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000001\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.009913\n",
      "\tTraining batch 52 Loss: 0.605224\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000098\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.093982\n",
      "\tTraining batch 59 Loss: 0.000004\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.161285\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.003041\n",
      "\tTraining batch 76 Loss: 0.000025\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 1.214175\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.050481\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000010\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.002828\n",
      "\tTraining batch 95 Loss: 0.112971\n",
      "\tTraining batch 96 Loss: 0.000109\n",
      "\tTraining batch 97 Loss: 0.000001\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "Training set: Average loss: 0.025278\n",
      "Validation set: Average loss: 30.348547, Accuracy: 1369/1959 (69.88%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000209\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.035967\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.356997\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000040\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000076\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.154409\n",
      "\tTraining batch 29 Loss: 0.000154\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000056\n",
      "\tTraining batch 45 Loss: 0.000005\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000001\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.001465\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 1.051294\n",
      "\tTraining batch 54 Loss: 0.002043\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000002\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 1.180723\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000005\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.001025\n",
      "\tTraining batch 86 Loss: 0.000755\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.424332\n",
      "\tTraining batch 91 Loss: 0.000081\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.016411\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000011\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 1.456664\n",
      "\tTraining batch 99 Loss: 0.135991\n",
      "Training set: Average loss: 0.048674\n",
      "Validation set: Average loss: 25.017879, Accuracy: 1381/1959 (70.50%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.001318\n",
      "\tTraining batch 4 Loss: 0.000008\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 15 Loss: 0.700050\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000865\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.181376\n",
      "\tTraining batch 25 Loss: 0.169946\n",
      "\tTraining batch 26 Loss: 0.001944\n",
      "\tTraining batch 27 Loss: 0.266224\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.412727\n",
      "\tTraining batch 30 Loss: 0.000467\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000853\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 1.948582\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000452\n",
      "\tTraining batch 47 Loss: 0.015509\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000012\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.004066\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.028340\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000006\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.280527\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000004\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000020\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000024\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000025\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.001625\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000001\n",
      "\tTraining batch 97 Loss: 0.000015\n",
      "\tTraining batch 98 Loss: 0.942663\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "Training set: Average loss: 0.050077\n",
      "Validation set: Average loss: 31.773992, Accuracy: 1387/1959 (70.80%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.001365\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.082471\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.355924\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000001\n",
      "\tTraining batch 54 Loss: 0.000051\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.498494\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.008954\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.003433\n",
      "\tTraining batch 76 Loss: 0.000001\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.493874\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000190\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.451945\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "Training set: Average loss: 0.019159\n",
      "Validation set: Average loss: 31.369026, Accuracy: 1368/1959 (69.83%)\n",
      "\n",
      "Epoch: 11\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.455531\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000001\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000001\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000003\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.029280\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 1.101234\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000007\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.359757\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "Training set: Average loss: 0.019655\n",
      "Validation set: Average loss: 34.751037, Accuracy: 1390/1959 (70.95%)\n",
      "\n",
      "Epoch: 12\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.004987\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000003\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000162\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000006\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.001287\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000014\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.567874\n",
      "\tTraining batch 88 Loss: 0.003359\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000002\n",
      "Training set: Average loss: 0.005835\n",
      "Validation set: Average loss: 33.886403, Accuracy: 1396/1959 (71.26%)\n",
      "\n",
      "Epoch: 13\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.294824\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000003\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.824059\n",
      "Training set: Average loss: 0.011302\n",
      "Validation set: Average loss: 33.033512, Accuracy: 1384/1959 (70.65%)\n",
      "\n",
      "Epoch: 14\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000063\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000001\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.072726\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000010\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000048\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000105\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000169\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000077\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.324627\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000087\n",
      "Training set: Average loss: 0.004019\n",
      "Validation set: Average loss: 33.554886, Accuracy: 1369/1959 (69.88%)\n",
      "\n",
      "Epoch: 15\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.002658\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000006\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000003\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000009\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.062554\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.769115\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.010123\n",
      "\tTraining batch 80 Loss: 0.000011\n",
      "\tTraining batch 81 Loss: 0.415175\n",
      "\tTraining batch 82 Loss: 0.004414\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.459529\n",
      "\tTraining batch 97 Loss: 0.000015\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "Training set: Average loss: 0.017410\n",
      "Validation set: Average loss: 38.179996, Accuracy: 1388/1959 (70.85%)\n",
      "\n",
      "Epoch: 16\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.498422\n",
      "\tTraining batch 22 Loss: 0.665033\n",
      "\tTraining batch 23 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.689662\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.001362\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000029\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000001\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.010176\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000001\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000067\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.054996\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "Training set: Average loss: 0.019391\n",
      "Validation set: Average loss: 35.149706, Accuracy: 1373/1959 (70.09%)\n",
      "\n",
      "Epoch: 17\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000084\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.002857\n",
      "\tTraining batch 7 Loss: 0.009894\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.006776\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.013716\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000001\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.482199\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000008\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.525209\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000032\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.001218\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.085462\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000172\n",
      "\tTraining batch 96 Loss: 0.304513\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "Training set: Average loss: 0.014466\n",
      "Validation set: Average loss: 35.329844, Accuracy: 1367/1959 (69.78%)\n",
      "\n",
      "Epoch: 18\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000171\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.060325\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000001\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.162670\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000167\n",
      "\tTraining batch 60 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.430734\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.416027\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000002\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000005\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 1.129839\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000001\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "Training set: Average loss: 0.022222\n",
      "Validation set: Average loss: 37.765101, Accuracy: 1360/1959 (69.42%)\n",
      "\n",
      "Epoch: 19\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.003464\n",
      "\tTraining batch 3 Loss: 0.182219\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.839123\n",
      "\tTraining batch 14 Loss: 0.000331\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.547085\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.401616\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000071\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.319790\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000353\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.943544\n",
      "\tTraining batch 86 Loss: 1.058875\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000887\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.049329\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 3.316766\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.040830\n",
      "Training set: Average loss: 0.077821\n",
      "Validation set: Average loss: 40.421521, Accuracy: 1376/1959 (70.24%)\n",
      "\n",
      "Epoch: 20\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000052\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.011920\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.002724\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.009991\n",
      "\tTraining batch 27 Loss: 0.081455\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.002823\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000049\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.228303\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000005\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 1.868196\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000870\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000713\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 3.343357\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 1.126062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.214035\n",
      "Training set: Average loss: 0.069602\n",
      "Validation set: Average loss: 35.763637, Accuracy: 1382/1959 (70.55%)\n",
      "\n",
      "Epoch: 21\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.006056\n",
      "\tTraining batch 3 Loss: 0.000011\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.118978\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000002\n",
      "\tTraining batch 29 Loss: 0.306153\n",
      "\tTraining batch 30 Loss: 1.505318\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000024\n",
      "\tTraining batch 46 Loss: 0.744882\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000001\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.135580\n",
      "\tTraining batch 54 Loss: 0.862967\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 1.438782\n",
      "\tTraining batch 57 Loss: 0.000004\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.366523\n",
      "\tTraining batch 62 Loss: 0.030948\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.116170\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000028\n",
      "\tTraining batch 76 Loss: 0.000157\n",
      "\tTraining batch 77 Loss: 0.800815\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.216649\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.444489\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.008966\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.636341\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000460\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000008\n",
      "Training set: Average loss: 0.078185\n",
      "Validation set: Average loss: 43.187879, Accuracy: 1368/1959 (69.83%)\n",
      "\n",
      "Epoch: 22\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 3.088277\n",
      "\tTraining batch 3 Loss: 0.152536\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000002\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.040813\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 1.165446\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.778894\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.838864\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.682532\n",
      "\tTraining batch 44 Loss: 0.000125\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000328\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.111777\n",
      "\tTraining batch 59 Loss: 0.000014\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000135\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.133528\n",
      "\tTraining batch 66 Loss: 0.280565\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000047\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.090362\n",
      "\tTraining batch 83 Loss: 0.175422\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000115\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.143730\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000016\n",
      "Training set: Average loss: 0.077611\n",
      "Validation set: Average loss: 41.378331, Accuracy: 1330/1959 (67.89%)\n",
      "\n",
      "Epoch: 23\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 1.211875\n",
      "\tTraining batch 5 Loss: 0.712512\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000521\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.008688\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000001\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000013\n",
      "\tTraining batch 36 Loss: 0.611846\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000108\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.101961\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 1.131268\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000146\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000004\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 2.695231\n",
      "Training set: Average loss: 0.065396\n",
      "Validation set: Average loss: 39.737314, Accuracy: 1372/1959 (70.04%)\n",
      "\n",
      "Epoch: 24\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.186454\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000067\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.001293\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.135761\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.205029\n",
      "\tTraining batch 46 Loss: 0.000020\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000140\n",
      "\tTraining batch 55 Loss: 0.354833\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000039\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000003\n",
      "\tTraining batch 67 Loss: 0.000115\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000010\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.844036\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.146603\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.210213\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 1.067280\n",
      "\tTraining batch 90 Loss: 1.084938\n",
      "\tTraining batch 91 Loss: 0.000303\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.199818\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "Training set: Average loss: 0.044818\n",
      "Validation set: Average loss: 42.016092, Accuracy: 1372/1959 (70.04%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 48.25%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000001\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.487433\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000005\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000216\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.432642\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.520610\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.001980\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000014\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.195395\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000031\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.688983\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000038\n",
      "\tTraining batch 97 Loss: 1.685854\n",
      "\tTraining batch 98 Loss: 0.071534\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 53.727245\n",
      "Training set: Average loss: 0.578120\n",
      "Validation set: Average loss: 38.251497, Accuracy: 1363/1959 (69.58%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 1.308608\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 1.048417\n",
      "\tTraining batch 4 Loss: 0.045948\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000033\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.923052\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.399717\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.007255\n",
      "\tTraining batch 18 Loss: 0.176042\n",
      "\tTraining batch 19 Loss: 0.056767\n",
      "\tTraining batch 20 Loss: 0.002440\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000001\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.019849\n",
      "\tTraining batch 26 Loss: 0.019456\n",
      "\tTraining batch 27 Loss: 0.441622\n",
      "\tTraining batch 28 Loss: 0.000006\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.275561\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.001823\n",
      "\tTraining batch 37 Loss: 0.160693\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000001\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.015465\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.556720\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 2.289792\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000001\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000001\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.003027\n",
      "\tTraining batch 79 Loss: 0.000010\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.011296\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.025700\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.373795\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000002\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.011935\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.008269\n",
      "\tTraining batch 100 Loss: 2.643762\n",
      "Training set: Average loss: 0.108271\n",
      "Validation set: Average loss: 31.918546, Accuracy: 1369/1959 (69.88%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.450879\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.155732\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.125580\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000381\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000593\n",
      "\tTraining batch 26 Loss: 0.045941\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000187\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000086\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000271\n",
      "\tTraining batch 41 Loss: 0.061789\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000006\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000333\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.428897\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000368\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000004\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.132286\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.466356\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.322139\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000005\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.038016\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.060389\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000076\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000006\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000001\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 3.438149\n",
      "Training set: Average loss: 0.057285\n",
      "Validation set: Average loss: 34.598132, Accuracy: 1361/1959 (69.47%)\n",
      "\n",
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000013\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.127052\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.422080\n",
      "\tTraining batch 26 Loss: 0.000122\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000013\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.004890\n",
      "\tTraining batch 70 Loss: 0.646936\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000001\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "Training set: Average loss: 0.012011\n",
      "Validation set: Average loss: 36.454385, Accuracy: 1380/1959 (70.44%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000091\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000001\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000001\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "Training set: Average loss: 0.000001\n",
      "Validation set: Average loss: 36.552026, Accuracy: 1381/1959 (70.50%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000001\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000001\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "Training set: Average loss: 0.000000\n",
      "Validation set: Average loss: 36.551978, Accuracy: 1381/1959 (70.50%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 51.24999999999999%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000001\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000001\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 48.547722\n",
      "Training set: Average loss: 0.480671\n",
      "Validation set: Average loss: 31.801398, Accuracy: 1390/1959 (70.95%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.083407\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.047252\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.373354\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000065\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.353897\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000001\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.705401\n",
      "\tTraining batch 23 Loss: 0.240204\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.072295\n",
      "\tTraining batch 26 Loss: 0.046707\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000789\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.002739\n",
      "\tTraining batch 31 Loss: 0.212212\n",
      "\tTraining batch 32 Loss: 0.543068\n",
      "\tTraining batch 33 Loss: 0.119434\n",
      "\tTraining batch 34 Loss: 0.000036\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.157832\n",
      "\tTraining batch 37 Loss: 0.771151\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000134\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.002493\n",
      "\tTraining batch 44 Loss: 0.299309\n",
      "\tTraining batch 45 Loss: 0.000005\n",
      "\tTraining batch 46 Loss: 0.000009\n",
      "\tTraining batch 47 Loss: 0.000010\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000010\n",
      "\tTraining batch 50 Loss: 0.000008\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.147649\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.781921\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000006\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 1.466486\n",
      "\tTraining batch 65 Loss: 0.094610\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.258300\n",
      "\tTraining batch 70 Loss: 0.245425\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.266684\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000265\n",
      "\tTraining batch 77 Loss: 0.000001\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000042\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.449833\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000002\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000007\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 1.952279\n",
      "\tTraining batch 99 Loss: 0.015128\n",
      "\tTraining batch 100 Loss: 0.220007\n",
      "\tTraining batch 101 Loss: 7.118196\n",
      "Training set: Average loss: 0.168799\n",
      "Validation set: Average loss: 29.547406, Accuracy: 1396/1959 (71.26%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.474608\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.126410\n",
      "\tTraining batch 7 Loss: 0.415471\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.018816\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.446512\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.275937\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000115\n",
      "\tTraining batch 45 Loss: 0.021426\n",
      "\tTraining batch 46 Loss: 0.830544\n",
      "\tTraining batch 47 Loss: 0.000364\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 1.072688\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000043\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.227893\n",
      "\tTraining batch 62 Loss: 1.116021\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000559\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.194136\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000506\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.179405\n",
      "\tTraining batch 87 Loss: 0.000019\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000004\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000035\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 3.880788\n",
      "Training set: Average loss: 0.091904\n",
      "Validation set: Average loss: 28.759240, Accuracy: 1375/1959 (70.19%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000002\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.078837\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000086\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000048\n",
      "\tTraining batch 26 Loss: 0.002118\n",
      "\tTraining batch 27 Loss: 1.058476\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.042965\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.241688\n",
      "\tTraining batch 36 Loss: 0.000002\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000001\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000200\n",
      "\tTraining batch 47 Loss: 0.002359\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.001353\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000001\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.571744\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.067694\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000327\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.003885\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.382421\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.092643\n",
      "\tTraining batch 100 Loss: 1.147223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 101 Loss: 0.894909\n",
      "Training set: Average loss: 0.045435\n",
      "Validation set: Average loss: 34.272414, Accuracy: 1381/1959 (70.50%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000003\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000001\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.436921\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.003352\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.077930\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.036784\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.116332\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.835860\n",
      "Training set: Average loss: 0.014923\n",
      "Validation set: Average loss: 35.428817, Accuracy: 1371/1959 (69.98%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000009\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.551046\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.347176\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000015\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 1.595007\n",
      "\tTraining batch 68 Loss: 0.000046\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.099128\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.552603\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000618\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000022\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.507674\n",
      "\tTraining batch 93 Loss: 0.002868\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.263239\n",
      "\tTraining batch 97 Loss: 0.284973\n",
      "\tTraining batch 98 Loss: 0.683905\n",
      "\tTraining batch 99 Loss: 0.000914\n",
      "\tTraining batch 100 Loss: 1.290869\n",
      "\tTraining batch 101 Loss: 0.130913\n",
      "Training set: Average loss: 0.062485\n",
      "Validation set: Average loss: 37.687450, Accuracy: 1371/1959 (69.98%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 48.5%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.365334\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.767563\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.560311\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.754299\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.797915\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000022\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000025\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 1.004911\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000002\n",
      "\tTraining batch 66 Loss: 0.000039\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000006\n",
      "\tTraining batch 77 Loss: 0.000020\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000001\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.180646\n",
      "\tTraining batch 87 Loss: 0.138769\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.433423\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.440907\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.317960\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.216166\n",
      "\tTraining batch 101 Loss: 0.846461\n",
      "\tTraining batch 102 Loss: 70.793808\n",
      "Training set: Average loss: 0.760967\n",
      "Validation set: Average loss: 31.806849, Accuracy: 1409/1959 (71.92%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.519623\n",
      "\tTraining batch 4 Loss: 0.978651\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.661664\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000210\n",
      "\tTraining batch 19 Loss: 0.008905\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000001\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.002196\n",
      "\tTraining batch 25 Loss: 0.072393\n",
      "\tTraining batch 26 Loss: 0.102333\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.056499\n",
      "\tTraining batch 29 Loss: 0.000003\n",
      "\tTraining batch 30 Loss: 1.328856\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.080451\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.717630\n",
      "\tTraining batch 40 Loss: 0.822579\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000033\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000001\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000501\n",
      "\tTraining batch 49 Loss: 0.342568\n",
      "\tTraining batch 50 Loss: 0.910391\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000181\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000011\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.511219\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.320186\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.377200\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 1.366122\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.006975\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000019\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000003\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.003230\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.137575\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000007\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.324377\n",
      "\tTraining batch 86 Loss: 0.000022\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.176194\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.767346\n",
      "\tTraining batch 91 Loss: 0.000117\n",
      "\tTraining batch 92 Loss: 0.004664\n",
      "\tTraining batch 93 Loss: 0.020995\n",
      "\tTraining batch 94 Loss: 0.000107\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.215466\n",
      "\tTraining batch 97 Loss: 0.000005\n",
      "\tTraining batch 98 Loss: 0.526400\n",
      "\tTraining batch 99 Loss: 0.863388\n",
      "\tTraining batch 100 Loss: 0.047053\n",
      "\tTraining batch 101 Loss: 0.717330\n",
      "\tTraining batch 102 Loss: 4.991866\n",
      "Training set: Average loss: 0.176309\n",
      "Validation set: Average loss: 28.054665, Accuracy: 1389/1959 (70.90%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.111851\n",
      "\tTraining batch 12 Loss: 0.000436\n",
      "\tTraining batch 13 Loss: 0.728095\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.601646\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000009\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.318852\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.044701\n",
      "\tTraining batch 27 Loss: 0.026224\n",
      "\tTraining batch 28 Loss: 0.204209\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000025\n",
      "\tTraining batch 35 Loss: 0.252704\n",
      "\tTraining batch 36 Loss: 0.826160\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 1.475487\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.278425\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000007\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000005\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.005771\n",
      "\tTraining batch 77 Loss: 0.338926\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.004552\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.128074\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.149816\n",
      "\tTraining batch 102 Loss: 0.172787\n",
      "Training set: Average loss: 0.055576\n",
      "Validation set: Average loss: 30.799047, Accuracy: 1414/1959 (72.18%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000002\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000064\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.013466\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000088\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.030274\n",
      "\tTraining batch 102 Loss: 0.961199\n",
      "Training set: Average loss: 0.009854\n",
      "Validation set: Average loss: 31.148188, Accuracy: 1408/1959 (71.87%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000193\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000080\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.044443\n",
      "\tTraining batch 27 Loss: 0.230499\n",
      "\tTraining batch 28 Loss: 0.000001\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 1.186416\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.429690\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000213\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000071\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000026\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.002344\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.149971\n",
      "\tTraining batch 76 Loss: 0.000002\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000001\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.082018\n",
      "\tTraining batch 89 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.003427\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.001955\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000001\n",
      "Training set: Average loss: 0.020896\n",
      "Validation set: Average loss: 29.286035, Accuracy: 1417/1959 (72.33%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000002\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000052\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.198477\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000009\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000002\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000061\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "Training set: Average loss: 0.001947\n",
      "Validation set: Average loss: 30.553532, Accuracy: 1416/1959 (72.28%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000001\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000003\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000001\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000014\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000016\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000067\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "Training set: Average loss: 0.000001\n",
      "Validation set: Average loss: 30.559282, Accuracy: 1416/1959 (72.28%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 48.0%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000001\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000002\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000001\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000014\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000012\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000064\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 37.726059\n",
      "Training set: Average loss: 0.366273\n",
      "Validation set: Average loss: 31.794386, Accuracy: 1386/1959 (70.75%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.788983\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000341\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 2.282720\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.222120\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.002217\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.138690\n",
      "\tTraining batch 25 Loss: 0.178234\n",
      "\tTraining batch 26 Loss: 0.071114\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000001\n",
      "\tTraining batch 29 Loss: 0.122510\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.132767\n",
      "\tTraining batch 32 Loss: 0.732041\n",
      "\tTraining batch 33 Loss: 0.066218\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000007\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.228980\n",
      "\tTraining batch 41 Loss: 0.352981\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.001038\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.546450\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.001503\n",
      "\tTraining batch 49 Loss: 0.895062\n",
      "\tTraining batch 50 Loss: 0.836082\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.209572\n",
      "\tTraining batch 54 Loss: 0.001708\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.001195\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.098287\n",
      "\tTraining batch 60 Loss: 0.000004\n",
      "\tTraining batch 61 Loss: 0.650226\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.001533\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000004\n",
      "\tTraining batch 69 Loss: 0.000230\n",
      "\tTraining batch 70 Loss: 0.000001\n",
      "\tTraining batch 71 Loss: 0.133753\n",
      "\tTraining batch 72 Loss: 2.362794\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.969164\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.221294\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000007\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.003031\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 1.156817\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.034430\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.328090\n",
      "\tTraining batch 93 Loss: 1.005507\n",
      "\tTraining batch 94 Loss: 0.489884\n",
      "\tTraining batch 95 Loss: 0.678201\n",
      "\tTraining batch 96 Loss: 0.011484\n",
      "\tTraining batch 97 Loss: 0.001968\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000037\n",
      "\tTraining batch 100 Loss: 0.156485\n",
      "\tTraining batch 101 Loss: 2.355299\n",
      "\tTraining batch 102 Loss: 3.295316\n",
      "\tTraining batch 103 Loss: 10.917585\n",
      "Training set: Average loss: 0.317320\n",
      "Validation set: Average loss: 35.208442, Accuracy: 1361/1959 (69.47%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.008829\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.003327\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000001\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.622071\n",
      "\tTraining batch 15 Loss: 0.001483\n",
      "\tTraining batch 16 Loss: 0.909746\n",
      "\tTraining batch 17 Loss: 0.310463\n",
      "\tTraining batch 18 Loss: 1.138807\n",
      "\tTraining batch 19 Loss: 0.000039\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000927\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.008360\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 43 Loss: 0.232964\n",
      "\tTraining batch 44 Loss: 3.066270\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.367727\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000059\n",
      "\tTraining batch 55 Loss: 0.022997\n",
      "\tTraining batch 56 Loss: 0.000012\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.061836\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.255327\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.546266\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.377882\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000890\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.070000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.140213\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.501373\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.137638\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000001\n",
      "\tTraining batch 102 Loss: 0.162387\n",
      "\tTraining batch 103 Loss: 0.146562\n",
      "Training set: Average loss: 0.088296\n",
      "Validation set: Average loss: 37.610579, Accuracy: 1361/1959 (69.47%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 47.75%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000126\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.395733\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000346\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.239327\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000001\n",
      "\tTraining batch 40 Loss: 0.000013\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000493\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000206\n",
      "\tTraining batch 60 Loss: 0.806728\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.001056\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000005\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.128658\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000010\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.456895\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.683087\n",
      "\tTraining batch 103 Loss: 0.000296\n",
      "\tTraining batch 104 Loss: 32.870613\n",
      "Training set: Average loss: 0.342150\n",
      "Validation set: Average loss: 36.455658, Accuracy: 1371/1959 (69.98%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000175\n",
      "\tTraining batch 4 Loss: 0.844620\n",
      "\tTraining batch 5 Loss: 0.492113\n",
      "\tTraining batch 6 Loss: 0.121107\n",
      "\tTraining batch 7 Loss: 1.290201\n",
      "\tTraining batch 8 Loss: 5.927879\n",
      "\tTraining batch 9 Loss: 2.036611\n",
      "\tTraining batch 10 Loss: 1.251357\n",
      "\tTraining batch 11 Loss: 1.960092\n",
      "\tTraining batch 12 Loss: 0.206656\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 1.181748\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.002690\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.855852\n",
      "\tTraining batch 22 Loss: 1.670888\n",
      "\tTraining batch 23 Loss: 0.228191\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.176521\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000001\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000264\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000218\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000343\n",
      "\tTraining batch 45 Loss: 0.679885\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.090025\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000013\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.001766\n",
      "\tTraining batch 54 Loss: 0.000021\n",
      "\tTraining batch 55 Loss: 1.637877\n",
      "\tTraining batch 56 Loss: 0.001961\n",
      "\tTraining batch 57 Loss: 1.742413\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000036\n",
      "\tTraining batch 72 Loss: 0.000001\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000012\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000008\n",
      "\tTraining batch 77 Loss: 0.002931\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 1.390425\n",
      "\tTraining batch 82 Loss: 0.000011\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000083\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.451438\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000001\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.663760\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.950340\n",
      "\tTraining batch 103 Loss: 0.605342\n",
      "\tTraining batch 104 Loss: 2.569686\n",
      "Training set: Average loss: 0.279188\n",
      "Validation set: Average loss: 30.462177, Accuracy: 1362/1959 (69.53%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000056\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000014\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.470187\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000122\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000002\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000052\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.115328\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000650\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.088096\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000040\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.063333\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.002722\n",
      "\tTraining batch 104 Loss: 3.143436\n",
      "Training set: Average loss: 0.037347\n",
      "Validation set: Average loss: 32.951189, Accuracy: 1361/1959 (69.47%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.421751\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.184830\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.213411\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.095521\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000001\n",
      "\tTraining batch 19 Loss: 0.000056\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.198291\n",
      "\tTraining batch 22 Loss: 0.110008\n",
      "\tTraining batch 23 Loss: 0.000016\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.043495\n",
      "\tTraining batch 27 Loss: 0.000116\n",
      "\tTraining batch 28 Loss: 0.000001\n",
      "\tTraining batch 29 Loss: 0.000427\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.220364\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.522243\n",
      "\tTraining batch 40 Loss: 0.165566\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.001508\n",
      "\tTraining batch 43 Loss: 0.031717\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.523001\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.539413\n",
      "\tTraining batch 52 Loss: 0.000001\n",
      "\tTraining batch 53 Loss: 0.000226\n",
      "\tTraining batch 54 Loss: 0.022138\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000007\n",
      "\tTraining batch 57 Loss: 0.000001\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000356\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.003369\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.452250\n",
      "\tTraining batch 68 Loss: 1.132839\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000018\n",
      "\tTraining batch 77 Loss: 0.000836\n",
      "\tTraining batch 78 Loss: 0.286821\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000046\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.223111\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 96 Loss: 0.000048\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000117\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "Training set: Average loss: 0.051865\n",
      "Validation set: Average loss: 31.402267, Accuracy: 1380/1959 (70.44%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000011\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000012\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000153\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000034\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.204948\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000001\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.197065\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.033254\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.473924\n",
      "\tTraining batch 104 Loss: 0.357685\n",
      "Training set: Average loss: 0.012184\n",
      "Validation set: Average loss: 33.471356, Accuracy: 1369/1959 (69.88%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.003411\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000001\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000001\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000002\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000394\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000157\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000058\n",
      "\tTraining batch 71 Loss: 0.803173\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000001\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000004\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000292\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000006\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000002\n",
      "\tTraining batch 96 Loss: 0.000028\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000004\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "Training set: Average loss: 0.007765\n",
      "Validation set: Average loss: 32.259730, Accuracy: 1376/1959 (70.24%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000009\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000025\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000024\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.090533\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000007\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000028\n",
      "\tTraining batch 96 Loss: 0.129579\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "Training set: Average loss: 0.002117\n",
      "Validation set: Average loss: 33.196498, Accuracy: 1375/1959 (70.19%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000016\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.206186\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.067225\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000003\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000008\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000012\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000002\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.001414\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.083346\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "Training set: Average loss: 0.003444\n",
      "Validation set: Average loss: 34.711683, Accuracy: 1382/1959 (70.55%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000005\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.415063\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000002\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000018\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000001\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "Training set: Average loss: 0.003991\n",
      "Validation set: Average loss: 35.356580, Accuracy: 1372/1959 (70.04%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000005\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000001\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000001\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000003\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000038\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000001\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "Training set: Average loss: 0.000000\n",
      "Validation set: Average loss: 35.356348, Accuracy: 1372/1959 (70.04%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 49.25%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000004\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000001\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000001\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000003\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000036\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000001\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 97.263672\n",
      "Training set: Average loss: 0.926321\n",
      "Validation set: Average loss: 31.851295, Accuracy: 1389/1959 (70.90%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.106805\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.710096\n",
      "\tTraining batch 9 Loss: 0.101150\n",
      "\tTraining batch 10 Loss: 2.689204\n",
      "\tTraining batch 11 Loss: 0.000046\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.036318\n",
      "\tTraining batch 14 Loss: 0.069119\n",
      "\tTraining batch 15 Loss: 0.774618\n",
      "\tTraining batch 16 Loss: 0.590134\n",
      "\tTraining batch 17 Loss: 0.521546\n",
      "\tTraining batch 18 Loss: 1.082714\n",
      "\tTraining batch 19 Loss: 0.460326\n",
      "\tTraining batch 20 Loss: 0.129027\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 1.080892\n",
      "\tTraining batch 23 Loss: 0.568314\n",
      "\tTraining batch 24 Loss: 0.001880\n",
      "\tTraining batch 25 Loss: 1.888564\n",
      "\tTraining batch 26 Loss: 0.118532\n",
      "\tTraining batch 27 Loss: 0.000877\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.657744\n",
      "\tTraining batch 30 Loss: 0.001213\n",
      "\tTraining batch 31 Loss: 0.000027\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.275389\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000002\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.082585\n",
      "\tTraining batch 38 Loss: 0.000004\n",
      "\tTraining batch 39 Loss: 0.014908\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.248091\n",
      "\tTraining batch 42 Loss: 0.000005\n",
      "\tTraining batch 43 Loss: 0.000050\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.003258\n",
      "\tTraining batch 46 Loss: 0.338423\n",
      "\tTraining batch 47 Loss: 0.602159\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000036\n",
      "\tTraining batch 55 Loss: 0.000080\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000017\n",
      "\tTraining batch 58 Loss: 0.265296\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.143201\n",
      "\tTraining batch 62 Loss: 0.000004\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000001\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 1.261609\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.121076\n",
      "\tTraining batch 75 Loss: 0.102625\n",
      "\tTraining batch 76 Loss: 0.003577\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000065\n",
      "\tTraining batch 80 Loss: 0.374895\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.002070\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.011855\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.098599\n",
      "\tTraining batch 91 Loss: 0.001469\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.490392\n",
      "\tTraining batch 94 Loss: 0.289497\n",
      "\tTraining batch 95 Loss: 0.156682\n",
      "\tTraining batch 96 Loss: 0.464586\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.008862\n",
      "\tTraining batch 101 Loss: 0.202598\n",
      "\tTraining batch 102 Loss: 0.044551\n",
      "\tTraining batch 103 Loss: 0.436884\n",
      "\tTraining batch 104 Loss: 0.970761\n",
      "\tTraining batch 105 Loss: 11.415855\n",
      "Training set: Average loss: 0.285916\n",
      "Validation set: Average loss: 26.867285, Accuracy: 1365/1959 (69.68%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.043467\n",
      "\tTraining batch 4 Loss: 0.056139\n",
      "\tTraining batch 5 Loss: 0.434917\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.098076\n",
      "\tTraining batch 11 Loss: 0.010252\n",
      "\tTraining batch 12 Loss: 0.000336\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000384\n",
      "\tTraining batch 15 Loss: 0.141749\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000900\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.074175\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000007\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000001\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000001\n",
      "\tTraining batch 32 Loss: 0.110780\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.199534\n",
      "\tTraining batch 43 Loss: 0.000128\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000027\n",
      "\tTraining batch 46 Loss: 0.000001\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000012\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000011\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.001331\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.356547\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000077\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.131607\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.017471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.170398\n",
      "\tTraining batch 95 Loss: 0.000059\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.096553\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.101428\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 3.085338\n",
      "Training set: Average loss: 0.048873\n",
      "Validation set: Average loss: 26.758381, Accuracy: 1376/1959 (70.24%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.021637\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.645084\n",
      "\tTraining batch 13 Loss: 0.923504\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000136\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000069\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.073306\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.021939\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000006\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000038\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.101485\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000264\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000011\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000283\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000027\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.003074\n",
      "\tTraining batch 70 Loss: 0.878947\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000001\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000002\n",
      "\tTraining batch 77 Loss: 0.000008\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000001\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.001735\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000001\n",
      "\tTraining batch 92 Loss: 0.121988\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.279360\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.438674\n",
      "\tTraining batch 103 Loss: 0.025512\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.433179\n",
      "Training set: Average loss: 0.037812\n",
      "Validation set: Average loss: 27.473790, Accuracy: 1385/1959 (70.70%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.093626\n",
      "\tTraining batch 5 Loss: 0.000003\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000001\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000197\n",
      "\tTraining batch 20 Loss: 0.000107\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.934869\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000004\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000007\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000004\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000001\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000021\n",
      "\tTraining batch 54 Loss: 0.000148\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000010\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.011079\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.156402\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.117504\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000003\n",
      "\tTraining batch 76 Loss: 0.000007\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000001\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000005\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000003\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000014\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.013699\n",
      "\tTraining batch 95 Loss: 0.000551\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000001\n",
      "\tTraining batch 100 Loss: 0.127018\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.071510\n",
      "Training set: Average loss: 0.014541\n",
      "Validation set: Average loss: 26.575490, Accuracy: 1388/1959 (70.85%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000019\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000008\n",
      "\tTraining batch 20 Loss: 0.000297\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.140897\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000016\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000003\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000006\n",
      "\tTraining batch 56 Loss: 0.108658\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000001\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.001287\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000001\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000005\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.082975\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.070703\n",
      "Training set: Average loss: 0.003856\n",
      "Validation set: Average loss: 29.271170, Accuracy: 1382/1959 (70.55%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.012319\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000001\n",
      "\tTraining batch 20 Loss: 0.000012\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000945\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.009965\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000001\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000002\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.187371\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000014\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.156260\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000001\n",
      "\tTraining batch 90 Loss: 0.000236\n",
      "\tTraining batch 91 Loss: 0.000003\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000013\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000002\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.444838\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.001308\n",
      "\tTraining batch 102 Loss: 0.000010\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000016\n",
      "Training set: Average loss: 0.007746\n",
      "Validation set: Average loss: 30.469318, Accuracy: 1372/1959 (70.04%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000023\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.390771\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000002\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000072\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000149\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000172\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000002\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000008\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000492\n",
      "Training set: Average loss: 0.003730\n",
      "Validation set: Average loss: 31.791512, Accuracy: 1376/1959 (70.24%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000079\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000514\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000014\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000011\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000058\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.121043\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000004\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000002\n",
      "\tTraining batch 77 Loss: 0.202803\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.010498\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.023202\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.001843\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000001\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.182389\n",
      "Training set: Average loss: 0.005166\n",
      "Validation set: Average loss: 32.328930, Accuracy: 1375/1959 (70.19%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000273\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000029\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000035\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.360847\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000001\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000004\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000005\n",
      "\tTraining batch 104 Loss: 0.119983\n",
      "\tTraining batch 105 Loss: 0.010158\n",
      "Training set: Average loss: 0.004679\n",
      "Validation set: Average loss: 31.598359, Accuracy: 1381/1959 (70.50%)\n",
      "\n",
      "Epoch: 11\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000002\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000005\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000007\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.006153\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "Training set: Average loss: 0.000059\n",
      "Validation set: Average loss: 32.259109, Accuracy: 1392/1959 (71.06%)\n",
      "\n",
      "Epoch: 12\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000135\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000004\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000001\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000003\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000001\n",
      "Training set: Average loss: 0.000001\n",
      "Validation set: Average loss: 32.285254, Accuracy: 1392/1959 (71.06%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 49.75%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000027\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000004\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000001\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000001\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000001\n",
      "\tTraining batch 106 Loss: 57.683849\n",
      "Training set: Average loss: 0.544188\n",
      "Validation set: Average loss: 27.741216, Accuracy: 1381/1959 (70.50%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.046011\n",
      "\tTraining batch 4 Loss: 0.002350\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.643715\n",
      "\tTraining batch 10 Loss: 0.114455\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.267223\n",
      "\tTraining batch 13 Loss: 0.656293\n",
      "\tTraining batch 14 Loss: 0.000040\n",
      "\tTraining batch 15 Loss: 0.406576\n",
      "\tTraining batch 16 Loss: 0.938431\n",
      "\tTraining batch 17 Loss: 0.000017\n",
      "\tTraining batch 18 Loss: 0.002441\n",
      "\tTraining batch 19 Loss: 0.012396\n",
      "\tTraining batch 20 Loss: 0.231988\n",
      "\tTraining batch 21 Loss: 0.000244\n",
      "\tTraining batch 22 Loss: 0.141490\n",
      "\tTraining batch 23 Loss: 0.307425\n",
      "\tTraining batch 24 Loss: 0.879650\n",
      "\tTraining batch 25 Loss: 1.571386\n",
      "\tTraining batch 26 Loss: 0.944953\n",
      "\tTraining batch 27 Loss: 0.000002\n",
      "\tTraining batch 28 Loss: 0.264898\n",
      "\tTraining batch 29 Loss: 0.545743\n",
      "\tTraining batch 30 Loss: 0.400001\n",
      "\tTraining batch 31 Loss: 0.195253\n",
      "\tTraining batch 32 Loss: 0.014058\n",
      "\tTraining batch 33 Loss: 0.034097\n",
      "\tTraining batch 34 Loss: 0.012421\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.451960\n",
      "\tTraining batch 37 Loss: 0.000005\n",
      "\tTraining batch 38 Loss: 0.000001\n",
      "\tTraining batch 39 Loss: 0.054090\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.022128\n",
      "\tTraining batch 44 Loss: 0.000025\n",
      "\tTraining batch 45 Loss: 0.000004\n",
      "\tTraining batch 46 Loss: 0.033077\n",
      "\tTraining batch 47 Loss: 0.000124\n",
      "\tTraining batch 48 Loss: 0.015864\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000004\n",
      "\tTraining batch 54 Loss: 0.000023\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.342526\n",
      "\tTraining batch 57 Loss: 0.000007\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.654742\n",
      "\tTraining batch 60 Loss: 0.014622\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.138296\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.241742\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000003\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.111760\n",
      "\tTraining batch 76 Loss: 0.000920\n",
      "\tTraining batch 77 Loss: 0.000379\n",
      "\tTraining batch 78 Loss: 0.967576\n",
      "\tTraining batch 79 Loss: 0.693628\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.001475\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.140539\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000515\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000001\n",
      "\tTraining batch 94 Loss: 0.000001\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.022062\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.002986\n",
      "\tTraining batch 100 Loss: 0.987062\n",
      "\tTraining batch 101 Loss: 0.000677\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.725292\n",
      "\tTraining batch 104 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 105 Loss: 0.005476\n",
      "\tTraining batch 106 Loss: 2.722862\n",
      "Training set: Average loss: 0.160245\n",
      "Validation set: Average loss: 28.870045, Accuracy: 1377/1959 (70.29%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000003\n",
      "\tTraining batch 5 Loss: 0.078179\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.038125\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000115\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.040070\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.501108\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000030\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.001963\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.029243\n",
      "\tTraining batch 38 Loss: 0.165517\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000125\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000018\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.071343\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.040873\n",
      "\tTraining batch 54 Loss: 0.000001\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.513961\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.728665\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.311919\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000002\n",
      "\tTraining batch 97 Loss: 0.469204\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.621257\n",
      "\tTraining batch 103 Loss: 0.036706\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.017366\n",
      "\tTraining batch 106 Loss: 1.486899\n",
      "Training set: Average loss: 0.048610\n",
      "Validation set: Average loss: 29.077688, Accuracy: 1374/1959 (70.14%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.206268\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000003\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.055361\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.036580\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.339717\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.036217\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.136509\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000007\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.001897\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000005\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000572\n",
      "\tTraining batch 63 Loss: 0.000958\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000001\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.310467\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000004\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000426\n",
      "\tTraining batch 88 Loss: 0.923796\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.152602\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.002085\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.017016\n",
      "\tTraining batch 102 Loss: 0.000309\n",
      "\tTraining batch 103 Loss: 0.002483\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.235217\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "Training set: Average loss: 0.023193\n",
      "Validation set: Average loss: 31.562103, Accuracy: 1386/1959 (70.75%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000020\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000116\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000001\n",
      "\tTraining batch 15 Loss: 0.623738\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 1.007428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.039388\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000415\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.001496\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.708764\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000001\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000007\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.501498\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.446375\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.008576\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000971\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.001588\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.006575\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.005009\n",
      "\tTraining batch 101 Loss: 0.210789\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000998\n",
      "\tTraining batch 106 Loss: 0.466377\n",
      "Training set: Average loss: 0.038020\n",
      "Validation set: Average loss: 32.115792, Accuracy: 1379/1959 (70.39%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000233\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.021983\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000207\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000012\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.182430\n",
      "\tTraining batch 48 Loss: 0.193662\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.089313\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000012\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000003\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000028\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000001\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000005\n",
      "\tTraining batch 102 Loss: 0.058012\n",
      "\tTraining batch 103 Loss: 0.062443\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000773\n",
      "\tTraining batch 106 Loss: 0.000004\n",
      "Training set: Average loss: 0.005746\n",
      "Validation set: Average loss: 31.378536, Accuracy: 1376/1959 (70.24%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000002\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.047596\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000414\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 41 Loss: 0.000649\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000291\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000006\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000465\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.984941\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.103785\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000001\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 1.944889\n",
      "Training set: Average loss: 0.029085\n",
      "Validation set: Average loss: 30.258401, Accuracy: 1376/1959 (70.24%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 51.0%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000001\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000282\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.012352\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000001\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.042776\n",
      "\tTraining batch 41 Loss: 0.391191\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.028668\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.591381\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.034572\n",
      "\tTraining batch 54 Loss: 0.000013\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.392315\n",
      "\tTraining batch 58 Loss: 0.011737\n",
      "\tTraining batch 59 Loss: 0.000008\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000024\n",
      "\tTraining batch 76 Loss: 0.077059\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000006\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.993179\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000053\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 59.594982\n",
      "Training set: Average loss: 0.581034\n",
      "Validation set: Average loss: 31.351740, Accuracy: 1386/1959 (70.75%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 1.806160\n",
      "\tTraining batch 3 Loss: 0.000016\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.272044\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.054579\n",
      "\tTraining batch 11 Loss: 0.015920\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.377295\n",
      "\tTraining batch 14 Loss: 0.634752\n",
      "\tTraining batch 15 Loss: 0.114319\n",
      "\tTraining batch 16 Loss: 0.027060\n",
      "\tTraining batch 17 Loss: 0.029224\n",
      "\tTraining batch 18 Loss: 0.001085\n",
      "\tTraining batch 19 Loss: 1.295057\n",
      "\tTraining batch 20 Loss: 1.307279\n",
      "\tTraining batch 21 Loss: 0.598808\n",
      "\tTraining batch 22 Loss: 0.072290\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.873317\n",
      "\tTraining batch 25 Loss: 0.002119\n",
      "\tTraining batch 26 Loss: 0.945607\n",
      "\tTraining batch 27 Loss: 0.010499\n",
      "\tTraining batch 28 Loss: 1.914332\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.715659\n",
      "\tTraining batch 31 Loss: 0.459731\n",
      "\tTraining batch 32 Loss: 0.404423\n",
      "\tTraining batch 33 Loss: 0.000002\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.258555\n",
      "\tTraining batch 36 Loss: 1.090820\n",
      "\tTraining batch 37 Loss: 0.031297\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000001\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.085633\n",
      "\tTraining batch 54 Loss: 2.381127\n",
      "\tTraining batch 55 Loss: 0.769262\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.016800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.007454\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.041791\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000360\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000128\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000601\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000838\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.257089\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 1.658074\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.041409\n",
      "\tTraining batch 107 Loss: 9.700459\n",
      "Training set: Average loss: 0.264236\n",
      "Validation set: Average loss: 31.102116, Accuracy: 1362/1959 (69.53%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000157\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000002\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.020675\n",
      "\tTraining batch 11 Loss: 1.668365\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.461514\n",
      "\tTraining batch 15 Loss: 0.009554\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.618803\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000509\n",
      "\tTraining batch 20 Loss: 0.121757\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.154516\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.022773\n",
      "\tTraining batch 26 Loss: 0.138123\n",
      "\tTraining batch 27 Loss: 0.000004\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 1.280548\n",
      "\tTraining batch 30 Loss: 0.013503\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000139\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000181\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000178\n",
      "\tTraining batch 40 Loss: 0.005311\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000001\n",
      "\tTraining batch 44 Loss: 0.111575\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.418221\n",
      "\tTraining batch 51 Loss: 0.615420\n",
      "\tTraining batch 52 Loss: 1.603934\n",
      "\tTraining batch 53 Loss: 0.098455\n",
      "\tTraining batch 54 Loss: 0.000113\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000507\n",
      "\tTraining batch 60 Loss: 0.000002\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.025559\n",
      "\tTraining batch 65 Loss: 0.005701\n",
      "\tTraining batch 66 Loss: 0.154157\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.008597\n",
      "\tTraining batch 69 Loss: 0.000291\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.685535\n",
      "\tTraining batch 73 Loss: 0.198851\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 1.117288\n",
      "\tTraining batch 86 Loss: 0.000240\n",
      "\tTraining batch 87 Loss: 0.347936\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000007\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.009754\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000002\n",
      "\tTraining batch 103 Loss: 1.040082\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000253\n",
      "\tTraining batch 107 Loss: 1.991482\n",
      "Training set: Average loss: 0.121033\n",
      "Validation set: Average loss: 32.153826, Accuracy: 1369/1959 (69.88%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000003\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000069\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.024719\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000023\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000018\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.053556\n",
      "\tTraining batch 48 Loss: 0.638761\n",
      "\tTraining batch 49 Loss: 0.490864\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000032\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000108\n",
      "\tTraining batch 59 Loss: 0.000002\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.347446\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.190421\n",
      "\tTraining batch 70 Loss: 1.791848\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000006\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 82 Loss: 0.274580\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.136114\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000005\n",
      "\tTraining batch 92 Loss: 0.000002\n",
      "\tTraining batch 93 Loss: 0.037176\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.752890\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000742\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.315688\n",
      "\tTraining batch 103 Loss: 0.800111\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.080440\n",
      "\tTraining batch 106 Loss: 0.664831\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "Training set: Average loss: 0.061687\n",
      "Validation set: Average loss: 31.443434, Accuracy: 1363/1959 (69.58%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.369330\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000037\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000486\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000559\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.109887\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000070\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000025\n",
      "\tTraining batch 59 Loss: 0.000002\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.030971\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000014\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.584910\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000001\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000176\n",
      "\tTraining batch 96 Loss: 1.597131\n",
      "\tTraining batch 97 Loss: 0.338177\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 1.070163\n",
      "\tTraining batch 106 Loss: 0.000082\n",
      "\tTraining batch 107 Loss: 1.341925\n",
      "Training set: Average loss: 0.050878\n",
      "Validation set: Average loss: 29.154746, Accuracy: 1362/1959 (69.53%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000152\n",
      "\tTraining batch 15 Loss: 0.764245\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000011\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000024\n",
      "\tTraining batch 27 Loss: 0.185224\n",
      "\tTraining batch 28 Loss: 0.343366\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.005597\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000252\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000083\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000113\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000001\n",
      "\tTraining batch 76 Loss: 0.000001\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.003287\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000004\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.001073\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.044637\n",
      "\tTraining batch 102 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.410394\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000007\n",
      "Training set: Average loss: 0.016434\n",
      "Validation set: Average loss: 31.624478, Accuracy: 1376/1959 (70.24%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000004\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.012935\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000446\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000107\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000005\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.158744\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000002\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000082\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.229892\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000006\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.150710\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000001\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "Training set: Average loss: 0.005168\n",
      "Validation set: Average loss: 32.730709, Accuracy: 1352/1959 (69.01%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.177894\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000002\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.490798\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000001\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000038\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000247\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000001\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000005\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.010893\n",
      "\tTraining batch 93 Loss: 0.000196\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000037\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000006\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.090789\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "Training set: Average loss: 0.007205\n",
      "Validation set: Average loss: 36.406547, Accuracy: 1360/1959 (69.42%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000003\n",
      "\tTraining batch 12 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000004\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000028\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000002\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000125\n",
      "\tTraining batch 93 Loss: 0.000002\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.126142\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "Training set: Average loss: 0.001180\n",
      "Validation set: Average loss: 37.495648, Accuracy: 1360/1959 (69.42%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 50.5%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000003\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000016\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000008\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000012\n",
      "\tTraining batch 92 Loss: 0.000133\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 35.939407\n",
      "Training set: Average loss: 0.332774\n",
      "Validation set: Average loss: 33.681413, Accuracy: 1400/1959 (71.47%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.163076\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000001\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.031375\n",
      "\tTraining batch 10 Loss: 0.000434\n",
      "\tTraining batch 11 Loss: 0.235525\n",
      "\tTraining batch 12 Loss: 0.000017\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000008\n",
      "\tTraining batch 16 Loss: 0.304825\n",
      "\tTraining batch 17 Loss: 0.003513\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.280985\n",
      "\tTraining batch 20 Loss: 1.258902\n",
      "\tTraining batch 21 Loss: 0.025703\n",
      "\tTraining batch 22 Loss: 1.670330\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.021885\n",
      "\tTraining batch 27 Loss: 0.030257\n",
      "\tTraining batch 28 Loss: 0.464999\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.720956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 31 Loss: 0.206268\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.260083\n",
      "\tTraining batch 34 Loss: 0.264919\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.001510\n",
      "\tTraining batch 38 Loss: 0.000002\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.239678\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.001425\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000012\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000004\n",
      "\tTraining batch 50 Loss: 0.256623\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000001\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.666233\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.848423\n",
      "\tTraining batch 72 Loss: 0.794826\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 2.089696\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.159876\n",
      "\tTraining batch 80 Loss: 0.737100\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000001\n",
      "\tTraining batch 83 Loss: 0.848698\n",
      "\tTraining batch 84 Loss: 0.929272\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 2.874769\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 1.482912\n",
      "\tTraining batch 90 Loss: 2.252874\n",
      "\tTraining batch 91 Loss: 0.000160\n",
      "\tTraining batch 92 Loss: 0.215139\n",
      "\tTraining batch 93 Loss: 0.003180\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000002\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.168529\n",
      "\tTraining batch 100 Loss: 1.034489\n",
      "\tTraining batch 101 Loss: 0.057628\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.103685\n",
      "\tTraining batch 104 Loss: 0.949756\n",
      "\tTraining batch 105 Loss: 0.704690\n",
      "\tTraining batch 106 Loss: 0.092437\n",
      "\tTraining batch 107 Loss: 2.066252\n",
      "\tTraining batch 108 Loss: 3.921000\n",
      "Training set: Average loss: 0.272638\n",
      "Validation set: Average loss: 29.799933, Accuracy: 1416/1959 (72.28%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.382685\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.768758\n",
      "\tTraining batch 8 Loss: 0.407664\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.011989\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000231\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.030649\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.210775\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.003131\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 1.329207\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000006\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.009721\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000001\n",
      "\tTraining batch 59 Loss: 0.000139\n",
      "\tTraining batch 60 Loss: 0.028429\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.656672\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000004\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000494\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.212439\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000389\n",
      "\tTraining batch 95 Loss: 0.002054\n",
      "\tTraining batch 96 Loss: 0.098687\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.135151\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.396923\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.206545\n",
      "\tTraining batch 105 Loss: 0.000009\n",
      "\tTraining batch 106 Loss: 0.005953\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000001\n",
      "Training set: Average loss: 0.045358\n",
      "Validation set: Average loss: 38.244060, Accuracy: 1400/1959 (71.47%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.002617\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000003\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.015110\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000001\n",
      "\tTraining batch 43 Loss: 0.000008\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 50 Loss: 0.868449\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000102\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000007\n",
      "\tTraining batch 55 Loss: 0.000419\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.846012\n",
      "\tTraining batch 59 Loss: 0.000003\n",
      "\tTraining batch 60 Loss: 0.129318\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000191\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.209642\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 1.430486\n",
      "\tTraining batch 95 Loss: 0.001782\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.035726\n",
      "\tTraining batch 98 Loss: 0.710129\n",
      "\tTraining batch 99 Loss: 0.000001\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.256728\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000312\n",
      "Training set: Average loss: 0.041732\n",
      "Validation set: Average loss: 32.104253, Accuracy: 1414/1959 (72.18%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.014858\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000001\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.001618\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000003\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.324013\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000028\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "Training set: Average loss: 0.003153\n",
      "Validation set: Average loss: 32.404464, Accuracy: 1400/1959 (71.47%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000505\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000024\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "Training set: Average loss: 0.000005\n",
      "Validation set: Average loss: 32.396916, Accuracy: 1401/1959 (71.52%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000020\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "Training set: Average loss: 0.000000\n",
      "Validation set: Average loss: 32.396832, Accuracy: 1401/1959 (71.52%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 49.5%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000017\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 41.962593\n",
      "Training set: Average loss: 0.384978\n",
      "Validation set: Average loss: 33.041230, Accuracy: 1367/1959 (69.78%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.624969\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.703484\n",
      "\tTraining batch 16 Loss: 0.001734\n",
      "\tTraining batch 17 Loss: 0.000002\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000069\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.060486\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.849788\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 1.035012\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.033210\n",
      "\tTraining batch 31 Loss: 1.851387\n",
      "\tTraining batch 32 Loss: 0.000001\n",
      "\tTraining batch 33 Loss: 0.000351\n",
      "\tTraining batch 34 Loss: 0.681469\n",
      "\tTraining batch 35 Loss: 0.000014\n",
      "\tTraining batch 36 Loss: 0.000659\n",
      "\tTraining batch 37 Loss: 1.583019\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.002285\n",
      "\tTraining batch 41 Loss: 0.863593\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000493\n",
      "\tTraining batch 44 Loss: 0.000001\n",
      "\tTraining batch 45 Loss: 0.011967\n",
      "\tTraining batch 46 Loss: 0.170297\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.226391\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.018445\n",
      "\tTraining batch 54 Loss: 0.000348\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.223471\n",
      "\tTraining batch 57 Loss: 1.099882\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.364597\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000012\n",
      "\tTraining batch 70 Loss: 0.337129\n",
      "\tTraining batch 71 Loss: 0.016490\n",
      "\tTraining batch 72 Loss: 0.692161\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.001325\n",
      "\tTraining batch 77 Loss: 0.772548\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.325559\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.034303\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000010\n",
      "\tTraining batch 93 Loss: 0.000041\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000927\n",
      "\tTraining batch 96 Loss: 0.174802\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000014\n",
      "\tTraining batch 101 Loss: 0.204495\n",
      "\tTraining batch 102 Loss: 0.256723\n",
      "\tTraining batch 103 Loss: 0.000074\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000637\n",
      "\tTraining batch 106 Loss: 0.000004\n",
      "\tTraining batch 107 Loss: 0.003752\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 2.561004\n",
      "Training set: Average loss: 0.144857\n",
      "Validation set: Average loss: 32.073026, Accuracy: 1373/1959 (70.09%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.011370\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000001\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000001\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000002\n",
      "\tTraining batch 28 Loss: 0.000210\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000002\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000017\n",
      "\tTraining batch 55 Loss: 0.000001\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000001\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.282549\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.126841\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000919\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.003707\n",
      "\tTraining batch 104 Loss: 0.000140\n",
      "\tTraining batch 105 Loss: 0.000466\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.748437\n",
      "\tTraining batch 109 Loss: 0.203295\n",
      "Training set: Average loss: 0.012642\n",
      "Validation set: Average loss: 30.464205, Accuracy: 1387/1959 (70.80%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000113\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.067174\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000001\n",
      "\tTraining batch 20 Loss: 0.231707\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000040\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.134424\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.080155\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000002\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000001\n",
      "\tTraining batch 46 Loss: 0.000012\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.001842\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000074\n",
      "\tTraining batch 58 Loss: 0.709229\n",
      "\tTraining batch 59 Loss: 0.000015\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000229\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.283715\n",
      "\tTraining batch 72 Loss: 0.010593\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000002\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.171225\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.039340\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.145048\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000040\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.002668\n",
      "\tTraining batch 106 Loss: 0.000038\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 2.537239\n",
      "Training set: Average loss: 0.040504\n",
      "Validation set: Average loss: 35.413521, Accuracy: 1370/1959 (69.93%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.139360\n",
      "\tTraining batch 8 Loss: 0.000001\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000001\n",
      "\tTraining batch 15 Loss: 0.000489\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.451532\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.006390\n",
      "\tTraining batch 27 Loss: 0.000074\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.344927\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000022\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.358146\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000007\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.305522\n",
      "\tTraining batch 46 Loss: 1.425944\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.201546\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.266615\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.005253\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000002\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.003776\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.001182\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000560\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.258754\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000002\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000004\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.713084\n",
      "\tTraining batch 109 Loss: 0.327916\n",
      "Training set: Average loss: 0.044139\n",
      "Validation set: Average loss: 31.910592, Accuracy: 1399/1959 (71.41%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.342410\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000058\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.808716\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000429\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000009\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000001\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000057\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000544\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.047621\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000037\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.144441\n",
      "\tTraining batch 98 Loss: 0.000404\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.296610\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000008\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.000006\n",
      "Training set: Average loss: 0.015058\n",
      "Validation set: Average loss: 34.022934, Accuracy: 1390/1959 (70.95%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.007697\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000058\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.373900\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000003\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.072995\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.098270\n",
      "\tTraining batch 109 Loss: 0.000006\n",
      "Training set: Average loss: 0.005073\n",
      "Validation set: Average loss: 36.367792, Accuracy: 1401/1959 (71.52%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.001184\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.005331\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.001182\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000029\n",
      "\tTraining batch 44 Loss: 0.057179\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000083\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.162133\n",
      "\tTraining batch 54 Loss: 0.000003\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000185\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000038\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000147\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000020\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.037858\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000065\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000004\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000010\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000001\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000004\n",
      "\tTraining batch 108 Loss: 0.116824\n",
      "\tTraining batch 109 Loss: 0.000003\n",
      "Training set: Average loss: 0.003507\n",
      "Validation set: Average loss: 33.089823, Accuracy: 1387/1959 (70.80%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.074755\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000053\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000008\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000002\n",
      "\tTraining batch 65 Loss: 0.258361\n",
      "\tTraining batch 66 Loss: 0.000002\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.664442\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.101058\n",
      "\tTraining batch 76 Loss: 0.000066\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000006\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000001\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000044\n",
      "\tTraining batch 99 Loss: 0.024746\n",
      "\tTraining batch 100 Loss: 0.000005\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.317690\n",
      "\tTraining batch 104 Loss: 0.041871\n",
      "\tTraining batch 105 Loss: 0.002779\n",
      "\tTraining batch 106 Loss: 0.000022\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.006144\n",
      "Training set: Average loss: 0.013689\n",
      "Validation set: Average loss: 36.049051, Accuracy: 1390/1959 (70.95%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000002\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000001\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000895\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000001\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000002\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000180\n",
      "\tTraining batch 99 Loss: 0.708098\n",
      "\tTraining batch 100 Loss: 0.000233\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.000000\n",
      "Training set: Average loss: 0.006508\n",
      "Validation set: Average loss: 34.443796, Accuracy: 1381/1959 (70.50%)\n",
      "\n",
      "Epoch: 11\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000001\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.181766\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.177200\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000003\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000754\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000001\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000001\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000010\n",
      "\tTraining batch 109 Loss: 0.002252\n",
      "Training set: Average loss: 0.003321\n",
      "Validation set: Average loss: 34.078250, Accuracy: 1385/1959 (70.70%)\n",
      "\n",
      "Epoch: 12\n",
      "\tTraining batch 1 Loss: 0.061647\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.128819\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000001\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.049060\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000004\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 1.400687\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000207\n",
      "\tTraining batch 104 Loss: 0.334531\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.000008\n",
      "Training set: Average loss: 0.018119\n",
      "Validation set: Average loss: 36.197347, Accuracy: 1396/1959 (71.26%)\n",
      "\n",
      "Epoch: 13\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000768\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000001\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.476406\n",
      "\tTraining batch 31 Loss: 0.000002\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.569361\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000001\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.143973\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.205105\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.403972\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000109\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000008\n",
      "\tTraining batch 95 Loss: 0.010451\n",
      "\tTraining batch 96 Loss: 1.168571\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.004378\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000666\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 1.338281\n",
      "\tTraining batch 109 Loss: 0.689461\n",
      "Training set: Average loss: 0.045977\n",
      "Validation set: Average loss: 36.031095, Accuracy: 1372/1959 (70.04%)\n",
      "\n",
      "Epoch: 14\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 1.289944\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.593807\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 1.215442\n",
      "\tTraining batch 19 Loss: 0.278754\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000976\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 2.787909\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000005\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.702235\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.001400\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.269140\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.910823\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000028\n",
      "\tTraining batch 69 Loss: 0.960531\n",
      "\tTraining batch 70 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000001\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.220713\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.104831\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 2.544334\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000158\n",
      "\tTraining batch 101 Loss: 2.136498\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000161\n",
      "\tTraining batch 104 Loss: 1.267748\n",
      "\tTraining batch 105 Loss: 2.024916\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.000000\n",
      "Training set: Average loss: 0.158811\n",
      "Validation set: Average loss: 41.605840, Accuracy: 1382/1959 (70.55%)\n",
      "\n",
      "Epoch: 15\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 1.298119\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000264\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 1.002292\n",
      "\tTraining batch 13 Loss: 1.441768\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000002\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.001947\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.078530\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000040\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.522413\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.003594\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000001\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.020608\n",
      "Training set: Average loss: 0.040088\n",
      "Validation set: Average loss: 39.776746, Accuracy: 1383/1959 (70.60%)\n",
      "\n",
      "Epoch: 16\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.228294\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 1.045297\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.760573\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.014755\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000002\n",
      "\tTraining batch 44 Loss: 0.000001\n",
      "\tTraining batch 45 Loss: 0.123061\n",
      "\tTraining batch 46 Loss: 0.000034\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000092\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.010175\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.016628\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.676731\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000002\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.007306\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.457520\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.000000\n",
      "Training set: Average loss: 0.030647\n",
      "Validation set: Average loss: 40.485627, Accuracy: 1372/1959 (70.04%)\n",
      "\n",
      "Epoch: 17\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000030\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.144361\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.337493\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.236861\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.067068\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.107299\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000001\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000001\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000028\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.257978\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.000000\n",
      "Training set: Average loss: 0.010561\n",
      "Validation set: Average loss: 43.200289, Accuracy: 1377/1959 (70.29%)\n",
      "\n",
      "Epoch: 18\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000001\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000003\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000185\n",
      "\tTraining batch 109 Loss: 0.000000\n",
      "Training set: Average loss: 0.000002\n",
      "Validation set: Average loss: 42.934142, Accuracy: 1378/1959 (70.34%)\n",
      "\n",
      "Epoch: 19\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000001\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000002\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.000000\n",
      "Training set: Average loss: 0.000000\n",
      "Validation set: Average loss: 42.923798, Accuracy: 1378/1959 (70.34%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 49.75%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000001\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000002\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.000000\n",
      "\tTraining batch 110 Loss: 52.304901\n",
      "Training set: Average loss: 0.475499\n",
      "Validation set: Average loss: 38.511198, Accuracy: 1389/1959 (70.90%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.179969\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.175940\n",
      "\tTraining batch 11 Loss: 0.005095\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.635425\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.311330\n",
      "\tTraining batch 17 Loss: 0.788311\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000021\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.242031\n",
      "\tTraining batch 22 Loss: 0.794990\n",
      "\tTraining batch 23 Loss: 0.164021\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.089016\n",
      "\tTraining batch 26 Loss: 0.080882\n",
      "\tTraining batch 27 Loss: 0.265710\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.134201\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 1.195059\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.390403\n",
      "\tTraining batch 35 Loss: 0.516283\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.286594\n",
      "\tTraining batch 40 Loss: 0.058288\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000289\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.488624\n",
      "\tTraining batch 54 Loss: 0.000096\n",
      "\tTraining batch 55 Loss: 0.000010\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000054\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.017098\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.504521\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000011\n",
      "\tTraining batch 67 Loss: 0.580545\n",
      "\tTraining batch 68 Loss: 0.675660\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.909856\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000455\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.035825\n",
      "\tTraining batch 79 Loss: 0.518755\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.603197\n",
      "\tTraining batch 88 Loss: 0.011972\n",
      "\tTraining batch 89 Loss: 0.002260\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.567428\n",
      "\tTraining batch 92 Loss: 0.001149\n",
      "\tTraining batch 93 Loss: 0.111422\n",
      "\tTraining batch 94 Loss: 0.000006\n",
      "\tTraining batch 95 Loss: 4.818988\n",
      "\tTraining batch 96 Loss: 0.655471\n",
      "\tTraining batch 97 Loss: 0.798463\n",
      "\tTraining batch 98 Loss: 0.344759\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000008\n",
      "\tTraining batch 101 Loss: 0.000007\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.238538\n",
      "\tTraining batch 106 Loss: 0.000001\n",
      "\tTraining batch 107 Loss: 0.000080\n",
      "\tTraining batch 108 Loss: 0.170177\n",
      "\tTraining batch 109 Loss: 0.907279\n",
      "\tTraining batch 110 Loss: 0.000000\n",
      "Training set: Average loss: 0.175242\n",
      "Validation set: Average loss: 34.939211, Accuracy: 1368/1959 (69.83%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.329976\n",
      "\tTraining batch 2 Loss: 0.971251\n",
      "\tTraining batch 3 Loss: 0.922618\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.230454\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000001\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000007\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.176766\n",
      "\tTraining batch 24 Loss: 0.000029\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.064230\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000116\n",
      "\tTraining batch 32 Loss: 0.013728\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.040482\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000350\n",
      "\tTraining batch 44 Loss: 0.032840\n",
      "\tTraining batch 45 Loss: 0.000001\n",
      "\tTraining batch 46 Loss: 0.146564\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.001700\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 1.983265\n",
      "\tTraining batch 53 Loss: 0.000100\n",
      "\tTraining batch 54 Loss: 0.005212\n",
      "\tTraining batch 55 Loss: 0.000042\n",
      "\tTraining batch 56 Loss: 0.000002\n",
      "\tTraining batch 57 Loss: 0.001261\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 1.537919\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000071\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000009\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000117\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.045530\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.000000\n",
      "\tTraining batch 110 Loss: 1.714639\n",
      "Training set: Average loss: 0.074721\n",
      "Validation set: Average loss: 42.356217, Accuracy: 1396/1959 (71.26%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.820481\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 1.502138\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 19 Loss: 0.000002\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.066596\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000002\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000342\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000181\n",
      "\tTraining batch 55 Loss: 0.342032\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000112\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.004486\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.203098\n",
      "\tTraining batch 65 Loss: 0.816640\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000014\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000001\n",
      "\tTraining batch 77 Loss: 0.001433\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 2.007428\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.411892\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.842896\n",
      "\tTraining batch 88 Loss: 0.000002\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.146072\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.018480\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000017\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.003889\n",
      "\tTraining batch 109 Loss: 0.000081\n",
      "\tTraining batch 110 Loss: 2.684496\n",
      "Training set: Average loss: 0.089753\n",
      "Validation set: Average loss: 41.879588, Accuracy: 1390/1959 (70.95%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000002\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000009\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.054278\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000001\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000107\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 3.675475\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.666982\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000002\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000692\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000002\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000002\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 1.800205\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.000000\n",
      "\tTraining batch 110 Loss: 0.000134\n",
      "Training set: Average loss: 0.056344\n",
      "Validation set: Average loss: 41.037288, Accuracy: 1386/1959 (70.75%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000026\n",
      "\tTraining batch 4 Loss: 0.143540\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000001\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000018\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.023941\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.030970\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000002\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.003438\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000287\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.621536\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.092094\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 1.453214\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000035\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.916837\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.000000\n",
      "\tTraining batch 110 Loss: 0.081705\n",
      "Training set: Average loss: 0.030615\n",
      "Validation set: Average loss: 38.949970, Accuracy: 1374/1959 (70.14%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000002\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.013502\n",
      "\tTraining batch 27 Loss: 0.080415\n",
      "\tTraining batch 28 Loss: 0.000699\n",
      "\tTraining batch 29 Loss: 0.000008\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000001\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000001\n",
      "\tTraining batch 55 Loss: 0.000011\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000003\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 2.310877\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.539537\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.489513\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.437279\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.587301\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.282813\n",
      "\tTraining batch 95 Loss: 0.134338\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.284512\n",
      "\tTraining batch 101 Loss: 0.000178\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.071332\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000002\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.000000\n",
      "\tTraining batch 110 Loss: 0.549789\n",
      "Training set: Average loss: 0.052565\n",
      "Validation set: Average loss: 40.677682, Accuracy: 1392/1959 (71.06%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000002\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000109\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000050\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 1.743999\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000054\n",
      "\tTraining batch 46 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000001\n",
      "\tTraining batch 54 Loss: 0.254839\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000040\n",
      "\tTraining batch 58 Loss: 0.000013\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.650091\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.175531\n",
      "\tTraining batch 76 Loss: 1.390337\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.661681\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.001504\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.002421\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 2.040027\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.118336\n",
      "\tTraining batch 109 Loss: 0.014301\n",
      "\tTraining batch 110 Loss: 0.000217\n",
      "Training set: Average loss: 0.064123\n",
      "Validation set: Average loss: 42.113545, Accuracy: 1399/1959 (71.41%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.054759\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.125476\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.345908\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.088212\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.002988\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000039\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.916048\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000845\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000362\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.656301\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.421391\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.008635\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000175\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.000000\n",
      "\tTraining batch 110 Loss: 0.017425\n",
      "Training set: Average loss: 0.023987\n",
      "Validation set: Average loss: 40.877639, Accuracy: 1399/1959 (71.41%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 49.25%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.409036\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.391355\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.949468\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000001\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000092\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.394601\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000039\n",
      "\tTraining batch 100 Loss: 0.327188\n",
      "\tTraining batch 101 Loss: 0.267731\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000005\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.292578\n",
      "\tTraining batch 106 Loss: 0.139024\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.097064\n",
      "\tTraining batch 110 Loss: 0.813728\n",
      "\tTraining batch 111 Loss: 60.683281\n",
      "Training set: Average loss: 0.583470\n",
      "Validation set: Average loss: 35.869903, Accuracy: 1412/1959 (72.08%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.214784\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 1.278200\n",
      "\tTraining batch 8 Loss: 0.064391\n",
      "\tTraining batch 9 Loss: 0.063051\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.635626\n",
      "\tTraining batch 14 Loss: 0.424186\n",
      "\tTraining batch 15 Loss: 0.183383\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.310329\n",
      "\tTraining batch 19 Loss: 0.000031\n",
      "\tTraining batch 20 Loss: 0.459022\n",
      "\tTraining batch 21 Loss: 0.939709\n",
      "\tTraining batch 22 Loss: 1.084075\n",
      "\tTraining batch 23 Loss: 0.000073\n",
      "\tTraining batch 24 Loss: 0.161569\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 1.564561\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000280\n",
      "\tTraining batch 29 Loss: 0.001104\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000001\n",
      "\tTraining batch 32 Loss: 0.553584\n",
      "\tTraining batch 33 Loss: 1.797958\n",
      "\tTraining batch 34 Loss: 1.330549\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 1.592272\n",
      "\tTraining batch 37 Loss: 0.081771\n",
      "\tTraining batch 38 Loss: 0.170042\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000026\n",
      "\tTraining batch 44 Loss: 0.255260\n",
      "\tTraining batch 45 Loss: 0.000009\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 1.073025\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.142638\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.004488\n",
      "\tTraining batch 55 Loss: 0.242301\n",
      "\tTraining batch 56 Loss: 0.014901\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.693830\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.375729\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.372871\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000902\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000008\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.095246\n",
      "\tTraining batch 86 Loss: 0.290365\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.185979\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000342\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000026\n",
      "\tTraining batch 96 Loss: 0.000032\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.689422\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000007\n",
      "\tTraining batch 103 Loss: 3.443954\n",
      "\tTraining batch 104 Loss: 0.032889\n",
      "\tTraining batch 105 Loss: 0.000006\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000211\n",
      "\tTraining batch 109 Loss: 0.501555\n",
      "\tTraining batch 110 Loss: 1.143501\n",
      "\tTraining batch 111 Loss: 4.964751\n",
      "Training set: Average loss: 0.247161\n",
      "Validation set: Average loss: 35.921383, Accuracy: 1415/1959 (72.23%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.008867\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.090154\n",
      "\tTraining batch 11 Loss: 0.423317\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000021\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.445349\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000001\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000004\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000010\n",
      "\tTraining batch 55 Loss: 0.000002\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000001\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.003989\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 2.026159\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.009649\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.004003\n",
      "\tTraining batch 109 Loss: 0.000000\n",
      "\tTraining batch 110 Loss: 0.000000\n",
      "\tTraining batch 111 Loss: 1.663174\n",
      "Training set: Average loss: 0.042114\n",
      "Validation set: Average loss: 34.031777, Accuracy: 1408/1959 (71.87%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.034346\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000035\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.046681\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000003\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000005\n",
      "\tTraining batch 102 Loss: 0.038517\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.000000\n",
      "\tTraining batch 110 Loss: 0.000000\n",
      "\tTraining batch 111 Loss: 0.236078\n",
      "Training set: Average loss: 0.003204\n",
      "Validation set: Average loss: 32.148845, Accuracy: 1403/1959 (71.62%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.006021\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000006\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000020\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000086\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000191\n",
      "\tTraining batch 96 Loss: 0.000021\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.001172\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000001\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.000000\n",
      "\tTraining batch 110 Loss: 0.000000\n",
      "\tTraining batch 111 Loss: 0.004563\n",
      "Training set: Average loss: 0.000109\n",
      "Validation set: Average loss: 32.326348, Accuracy: 1411/1959 (72.03%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.003597\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000005\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000019\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000001\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.000000\n",
      "\tTraining batch 110 Loss: 0.000000\n",
      "\tTraining batch 111 Loss: 0.001412\n",
      "Training set: Average loss: 0.000045\n",
      "Validation set: Average loss: 32.454790, Accuracy: 1412/1959 (72.08%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.002530\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000003\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000019\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000001\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.000000\n",
      "\tTraining batch 110 Loss: 0.000000\n",
      "\tTraining batch 111 Loss: 0.000969\n",
      "Training set: Average loss: 0.000032\n",
      "Validation set: Average loss: 32.477487, Accuracy: 1412/1959 (72.08%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 52.25%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.001929\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000002\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000019\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.000000\n",
      "\tTraining batch 110 Loss: 0.000000\n",
      "\tTraining batch 111 Loss: 0.000732\n",
      "\tTraining batch 112 Loss: 34.980309\n",
      "Training set: Average loss: 0.312348\n",
      "Validation set: Average loss: 32.508641, Accuracy: 1367/1959 (69.78%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000106\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000002\n",
      "\tTraining batch 11 Loss: 1.197275\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.717805\n",
      "\tTraining batch 15 Loss: 0.934036\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000001\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.620218\n",
      "\tTraining batch 26 Loss: 0.000030\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.125550\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000425\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000003\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.243980\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.002775\n",
      "\tTraining batch 40 Loss: 0.094177\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000174\n",
      "\tTraining batch 44 Loss: 0.002352\n",
      "\tTraining batch 45 Loss: 0.000012\n",
      "\tTraining batch 46 Loss: 0.090817\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000134\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.011588\n",
      "\tTraining batch 51 Loss: 0.554856\n",
      "\tTraining batch 52 Loss: 0.275227\n",
      "\tTraining batch 53 Loss: 0.412523\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000001\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.400546\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000007\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.541653\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.496408\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.005779\n",
      "\tTraining batch 81 Loss: 0.000131\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.102833\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.708220\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.616972\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.833278\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.058006\n",
      "\tTraining batch 109 Loss: 0.000000\n",
      "\tTraining batch 110 Loss: 0.000002\n",
      "\tTraining batch 111 Loss: 2.198243\n",
      "\tTraining batch 112 Loss: 0.843505\n",
      "Training set: Average loss: 0.107943\n",
      "Validation set: Average loss: 34.424662, Accuracy: 1379/1959 (70.39%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000002\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.019608\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000241\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000806\n",
      "\tTraining batch 27 Loss: 0.923992\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.003153\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.463940\n",
      "\tTraining batch 33 Loss: 0.000454\n",
      "\tTraining batch 34 Loss: 0.156661\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000001\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 1.526358\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.683322\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.072733\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.178447\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.018744\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000027\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000001\n",
      "\tTraining batch 93 Loss: 0.000031\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000219\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.617668\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000123\n",
      "\tTraining batch 106 Loss: 0.193210\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.000000\n",
      "\tTraining batch 110 Loss: 0.000000\n",
      "\tTraining batch 111 Loss: 0.000000\n",
      "\tTraining batch 112 Loss: 0.184634\n",
      "Training set: Average loss: 0.045039\n",
      "Validation set: Average loss: 38.687421, Accuracy: 1374/1959 (70.14%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 1.194615\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.124992\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000003\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000008\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.401348\n",
      "\tTraining batch 81 Loss: 0.000120\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000093\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000003\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000016\n",
      "\tTraining batch 98 Loss: 0.029113\n",
      "\tTraining batch 99 Loss: 0.035241\n",
      "\tTraining batch 100 Loss: 0.764202\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000194\n",
      "\tTraining batch 106 Loss: 0.001893\n",
      "\tTraining batch 107 Loss: 0.360722\n",
      "\tTraining batch 108 Loss: 0.580613\n",
      "\tTraining batch 109 Loss: 0.000000\n",
      "\tTraining batch 110 Loss: 0.188731\n",
      "\tTraining batch 111 Loss: 0.000000\n",
      "\tTraining batch 112 Loss: 0.466953\n",
      "Training set: Average loss: 0.037043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set: Average loss: 34.231536, Accuracy: 1371/1959 (69.98%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000009\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.710779\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000021\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 1.376621\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000037\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000116\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 2.318459\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.472682\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.146953\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.010601\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000004\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.016710\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.000000\n",
      "\tTraining batch 110 Loss: 0.000000\n",
      "\tTraining batch 111 Loss: 0.374321\n",
      "\tTraining batch 112 Loss: 0.000052\n",
      "Training set: Average loss: 0.048459\n",
      "Validation set: Average loss: 40.086395, Accuracy: 1377/1959 (70.29%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.258678\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000008\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000123\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000010\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000002\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000231\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.675993\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.000000\n",
      "\tTraining batch 110 Loss: 0.005782\n",
      "\tTraining batch 111 Loss: 0.000008\n",
      "\tTraining batch 112 Loss: 0.000000\n",
      "Training set: Average loss: 0.008400\n",
      "Validation set: Average loss: 37.145729, Accuracy: 1373/1959 (70.09%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.322381\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.443040\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000017\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.437811\n",
      "\tTraining batch 103 Loss: 0.102423\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.000000\n",
      "\tTraining batch 110 Loss: 0.000000\n",
      "\tTraining batch 111 Loss: 0.000013\n",
      "\tTraining batch 112 Loss: 0.059239\n",
      "Training set: Average loss: 0.012187\n",
      "Validation set: Average loss: 39.287407, Accuracy: 1381/1959 (70.50%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.547837\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000008\n",
      "\tTraining batch 27 Loss: 0.000003\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.384440\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.256438\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000001\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000220\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.328827\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000001\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 2.113632\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000009\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.584375\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000004\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000036\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 1.415777\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000007\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.636894\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.770020\n",
      "\tTraining batch 110 Loss: 0.000000\n",
      "\tTraining batch 111 Loss: 0.000000\n",
      "\tTraining batch 112 Loss: 0.000000\n",
      "Training set: Average loss: 0.062844\n",
      "Validation set: Average loss: 44.975841, Accuracy: 1376/1959 (70.24%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000028\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.301890\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.628215\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000140\n",
      "\tTraining batch 55 Loss: 0.039987\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000438\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000003\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000002\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000010\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.277137\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.000000\n",
      "\tTraining batch 110 Loss: 0.000000\n",
      "\tTraining batch 111 Loss: 1.122954\n",
      "\tTraining batch 112 Loss: 0.230025\n",
      "Training set: Average loss: 0.023222\n",
      "Validation set: Average loss: 43.340860, Accuracy: 1378/1959 (70.34%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000002\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.373764\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.378570\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.040306\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.029223\n",
      "\tTraining batch 29 Loss: 0.936935\n",
      "\tTraining batch 30 Loss: 0.177898\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.431429\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 1.238031\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.003092\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.390441\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.220201\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.079875\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.550961\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000145\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.704378\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.000000\n",
      "\tTraining batch 110 Loss: 0.000000\n",
      "\tTraining batch 111 Loss: 1.259647\n",
      "\tTraining batch 112 Loss: 0.000000\n",
      "Training set: Average loss: 0.060847\n",
      "Validation set: Average loss: 43.786246, Accuracy: 1372/1959 (70.04%)\n",
      "\n",
      "Epoch: 11\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.073829\n",
      "\tTraining batch 38 Loss: 0.000523\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.128188\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000019\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.925863\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000001\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000025\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000001\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.067030\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.152406\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.008370\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.000000\n",
      "\tTraining batch 110 Loss: 0.000000\n",
      "\tTraining batch 111 Loss: 0.317034\n",
      "\tTraining batch 112 Loss: 0.000000\n",
      "Training set: Average loss: 0.014940\n",
      "Validation set: Average loss: 41.641285, Accuracy: 1376/1959 (70.24%)\n",
      "\n",
      "Epoch: 12\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.065428\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.000000\n",
      "\tTraining batch 110 Loss: 0.000000\n",
      "\tTraining batch 111 Loss: 0.000019\n",
      "\tTraining batch 112 Loss: 0.000000\n",
      "Training set: Average loss: 0.000584\n",
      "Validation set: Average loss: 41.153617, Accuracy: 1378/1959 (70.34%)\n",
      "\n",
      "Epoch: 13\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000001\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000005\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.000000\n",
      "\tTraining batch 110 Loss: 0.000000\n",
      "\tTraining batch 111 Loss: 0.000057\n",
      "\tTraining batch 112 Loss: 0.000000\n",
      "Training set: Average loss: 0.000001\n",
      "Validation set: Average loss: 40.883371, Accuracy: 1377/1959 (70.29%)\n",
      "\n",
      "Epoch: 14\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.000000\n",
      "\tTraining batch 110 Loss: 0.000000\n",
      "\tTraining batch 111 Loss: 0.000051\n",
      "\tTraining batch 112 Loss: 0.000000\n",
      "Training set: Average loss: 0.000000\n",
      "Validation set: Average loss: 40.884953, Accuracy: 1377/1959 (70.29%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 50.24999999999999%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000000\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.000000\n",
      "\tTraining batch 110 Loss: 0.000000\n",
      "\tTraining batch 111 Loss: 0.000046\n",
      "\tTraining batch 112 Loss: 0.000000\n",
      "\tTraining batch 113 Loss: 110.161865\n",
      "Training set: Average loss: 0.974884\n",
      "Validation set: Average loss: 36.262457, Accuracy: 1405/1959 (71.72%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.359007\n",
      "\tTraining batch 4 Loss: 0.778967\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.468982\n",
      "\tTraining batch 7 Loss: 0.543597\n",
      "\tTraining batch 8 Loss: 0.000001\n",
      "\tTraining batch 9 Loss: 0.837951\n",
      "\tTraining batch 10 Loss: 0.749603\n",
      "\tTraining batch 11 Loss: 1.002051\n",
      "\tTraining batch 12 Loss: 0.001313\n",
      "\tTraining batch 13 Loss: 0.873749\n",
      "\tTraining batch 14 Loss: 0.104707\n",
      "\tTraining batch 15 Loss: 0.330458\n",
      "\tTraining batch 16 Loss: 0.002995\n",
      "\tTraining batch 17 Loss: 0.563817\n",
      "\tTraining batch 18 Loss: 0.001176\n",
      "\tTraining batch 19 Loss: 0.057466\n",
      "\tTraining batch 20 Loss: 0.005774\n",
      "\tTraining batch 21 Loss: 0.229751\n",
      "\tTraining batch 22 Loss: 3.835265\n",
      "\tTraining batch 23 Loss: 0.000125\n",
      "\tTraining batch 24 Loss: 0.084804\n",
      "\tTraining batch 25 Loss: 0.084597\n",
      "\tTraining batch 26 Loss: 0.668344\n",
      "\tTraining batch 27 Loss: 0.322464\n",
      "\tTraining batch 28 Loss: 0.001514\n",
      "\tTraining batch 29 Loss: 0.000588\n",
      "\tTraining batch 30 Loss: 0.315236\n",
      "\tTraining batch 31 Loss: 0.667463\n",
      "\tTraining batch 32 Loss: 0.196327\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.017711\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.026623\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.104126\n",
      "\tTraining batch 40 Loss: 0.159300\n",
      "\tTraining batch 41 Loss: 0.088081\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.005427\n",
      "\tTraining batch 44 Loss: 0.000043\n",
      "\tTraining batch 45 Loss: 0.000375\n",
      "\tTraining batch 46 Loss: 0.204898\n",
      "\tTraining batch 47 Loss: 0.056716\n",
      "\tTraining batch 48 Loss: 0.000081\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000004\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000738\n",
      "\tTraining batch 55 Loss: 0.000004\n",
      "\tTraining batch 56 Loss: 0.395492\n",
      "\tTraining batch 57 Loss: 0.477948\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.008674\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.538963\n",
      "\tTraining batch 62 Loss: 0.195531\n",
      "\tTraining batch 63 Loss: 0.861353\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000092\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 1.897481\n",
      "\tTraining batch 72 Loss: 0.000051\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.001330\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.541457\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.176512\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 1.479121\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.006861\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.347235\n",
      "\tTraining batch 97 Loss: 0.001362\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.508649\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.095438\n",
      "\tTraining batch 108 Loss: 0.000763\n",
      "\tTraining batch 109 Loss: 0.525702\n",
      "\tTraining batch 110 Loss: 0.000000\n",
      "\tTraining batch 111 Loss: 0.004324\n",
      "\tTraining batch 112 Loss: 0.000000\n",
      "\tTraining batch 113 Loss: 23.353481\n",
      "Training set: Average loss: 0.399735\n",
      "Validation set: Average loss: 31.589885, Accuracy: 1404/1959 (71.67%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.608473\n",
      "\tTraining batch 3 Loss: 0.680706\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000079\n",
      "\tTraining batch 7 Loss: 0.000007\n",
      "\tTraining batch 8 Loss: 0.624442\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.334226\n",
      "\tTraining batch 12 Loss: 0.000026\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 1.035521\n",
      "\tTraining batch 19 Loss: 0.000001\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000008\n",
      "\tTraining batch 26 Loss: 0.134539\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.151126\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.002272\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.056035\n",
      "\tTraining batch 45 Loss: 0.113153\n",
      "\tTraining batch 46 Loss: 0.000019\n",
      "\tTraining batch 47 Loss: 0.178256\n",
      "\tTraining batch 48 Loss: 0.515327\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000006\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 55 Loss: 0.082403\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.871268\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000002\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000007\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000219\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000001\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000055\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000044\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 2.016709\n",
      "\tTraining batch 91 Loss: 1.632361\n",
      "\tTraining batch 92 Loss: 2.399290\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000050\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.363290\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.305633\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 1.186980\n",
      "\tTraining batch 110 Loss: 1.015301\n",
      "\tTraining batch 111 Loss: 0.005471\n",
      "\tTraining batch 112 Loss: 0.135658\n",
      "\tTraining batch 113 Loss: 1.100950\n",
      "Training set: Average loss: 0.137610\n",
      "Validation set: Average loss: 27.280143, Accuracy: 1408/1959 (71.87%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.204084\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.003465\n",
      "\tTraining batch 6 Loss: 0.302838\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.042441\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.321428\n",
      "\tTraining batch 26 Loss: 0.876281\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000013\n",
      "\tTraining batch 29 Loss: 0.000009\n",
      "\tTraining batch 30 Loss: 0.162620\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.323469\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000004\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.187925\n",
      "\tTraining batch 54 Loss: 0.001280\n",
      "\tTraining batch 55 Loss: 0.000002\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.093130\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.007640\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000557\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.003793\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.968078\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.000000\n",
      "\tTraining batch 110 Loss: 0.000000\n",
      "\tTraining batch 111 Loss: 0.000000\n",
      "\tTraining batch 112 Loss: 0.000000\n",
      "\tTraining batch 113 Loss: 0.507334\n",
      "Training set: Average loss: 0.035455\n",
      "Validation set: Average loss: 30.470141, Accuracy: 1407/1959 (71.82%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000001\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000001\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.089622\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000301\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000003\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000006\n",
      "\tTraining batch 55 Loss: 0.002110\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.049607\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000164\n",
      "\tTraining batch 76 Loss: 0.000003\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.003697\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.118351\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000229\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.194868\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000009\n",
      "\tTraining batch 96 Loss: 0.000021\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.965567\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.000001\n",
      "\tTraining batch 110 Loss: 0.000000\n",
      "\tTraining batch 111 Loss: 0.001064\n",
      "\tTraining batch 112 Loss: 0.000000\n",
      "\tTraining batch 113 Loss: 0.023824\n",
      "Training set: Average loss: 0.012827\n",
      "Validation set: Average loss: 29.223641, Accuracy: 1405/1959 (71.72%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000001\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.023637\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000045\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000036\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.713948\n",
      "\tTraining batch 29 Loss: 0.001115\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000001\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000033\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.320695\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 1.045875\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000008\n",
      "\tTraining batch 55 Loss: 0.385449\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000084\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.409234\n",
      "\tTraining batch 63 Loss: 0.171674\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000003\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.023867\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000021\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000004\n",
      "\tTraining batch 84 Loss: 0.000064\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.105260\n",
      "\tTraining batch 90 Loss: 0.000226\n",
      "\tTraining batch 91 Loss: 0.006746\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.181030\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000026\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.018632\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.015916\n",
      "\tTraining batch 108 Loss: 0.000455\n",
      "\tTraining batch 109 Loss: 0.000000\n",
      "\tTraining batch 110 Loss: 0.421532\n",
      "\tTraining batch 111 Loss: 0.000000\n",
      "\tTraining batch 112 Loss: 0.055776\n",
      "\tTraining batch 113 Loss: 0.000000\n",
      "Training set: Average loss: 0.034526\n",
      "Validation set: Average loss: 35.837423, Accuracy: 1408/1959 (71.87%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.513731\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000001\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.209426\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000009\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.001741\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.272448\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.016195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000015\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000003\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000094\n",
      "\tTraining batch 87 Loss: 0.023980\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000141\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.003301\n",
      "\tTraining batch 110 Loss: 0.000000\n",
      "\tTraining batch 111 Loss: 0.000070\n",
      "\tTraining batch 112 Loss: 0.000000\n",
      "\tTraining batch 113 Loss: 0.000000\n",
      "Training set: Average loss: 0.009214\n",
      "Validation set: Average loss: 36.392825, Accuracy: 1403/1959 (71.62%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000004\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000002\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000061\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000008\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000152\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000418\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.000125\n",
      "\tTraining batch 110 Loss: 0.000000\n",
      "\tTraining batch 111 Loss: 0.000016\n",
      "\tTraining batch 112 Loss: 0.000000\n",
      "\tTraining batch 113 Loss: 0.000000\n",
      "Training set: Average loss: 0.000007\n",
      "Validation set: Average loss: 36.592511, Accuracy: 1400/1959 (71.47%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000003\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000002\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000052\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000151\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.000107\n",
      "\tTraining batch 110 Loss: 0.000000\n",
      "\tTraining batch 111 Loss: 0.000016\n",
      "\tTraining batch 112 Loss: 0.000000\n",
      "\tTraining batch 113 Loss: 0.000000\n",
      "Training set: Average loss: 0.000003\n",
      "Validation set: Average loss: 36.590354, Accuracy: 1400/1959 (71.47%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 49.75%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000003\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000002\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000050\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000001\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000150\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.000094\n",
      "\tTraining batch 110 Loss: 0.000000\n",
      "\tTraining batch 111 Loss: 0.000015\n",
      "\tTraining batch 112 Loss: 0.000000\n",
      "\tTraining batch 113 Loss: 0.000000\n",
      "\tTraining batch 114 Loss: 47.623444\n",
      "Training set: Average loss: 0.417752\n",
      "Validation set: Average loss: 34.939689, Accuracy: 1382/1959 (70.55%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.028185\n",
      "\tTraining batch 5 Loss: 0.130534\n",
      "\tTraining batch 6 Loss: 0.074292\n",
      "\tTraining batch 7 Loss: 2.644593\n",
      "\tTraining batch 8 Loss: 0.000003\n",
      "\tTraining batch 9 Loss: 0.307425\n",
      "\tTraining batch 10 Loss: 0.224845\n",
      "\tTraining batch 11 Loss: 0.972931\n",
      "\tTraining batch 12 Loss: 1.174302\n",
      "\tTraining batch 13 Loss: 0.000047\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.137722\n",
      "\tTraining batch 18 Loss: 0.286198\n",
      "\tTraining batch 19 Loss: 0.033833\n",
      "\tTraining batch 20 Loss: 0.000005\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.404457\n",
      "\tTraining batch 23 Loss: 1.851891\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.900575\n",
      "\tTraining batch 27 Loss: 0.515669\n",
      "\tTraining batch 28 Loss: 1.170557\n",
      "\tTraining batch 29 Loss: 2.037238\n",
      "\tTraining batch 30 Loss: 1.439059\n",
      "\tTraining batch 31 Loss: 1.164734\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.004460\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000194\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.376770\n",
      "\tTraining batch 45 Loss: 0.000006\n",
      "\tTraining batch 46 Loss: 0.014122\n",
      "\tTraining batch 47 Loss: 0.330715\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.224117\n",
      "\tTraining batch 51 Loss: 0.740476\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 1.226135\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.018980\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000002\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.008058\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000494\n",
      "\tTraining batch 68 Loss: 0.212708\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.007327\n",
      "\tTraining batch 74 Loss: 0.158466\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000013\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000001\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000114\n",
      "\tTraining batch 83 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 84 Loss: 1.736040\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000015\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.007356\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.455349\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.024767\n",
      "\tTraining batch 96 Loss: 0.031314\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.168225\n",
      "\tTraining batch 100 Loss: 0.439724\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.004725\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.413131\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.000001\n",
      "\tTraining batch 110 Loss: 0.000000\n",
      "\tTraining batch 111 Loss: 0.000474\n",
      "\tTraining batch 112 Loss: 0.007399\n",
      "\tTraining batch 113 Loss: 0.000000\n",
      "\tTraining batch 114 Loss: 5.639787\n",
      "Training set: Average loss: 0.243426\n",
      "Validation set: Average loss: 30.625098, Accuracy: 1390/1959 (70.95%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.087834\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000559\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.466064\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.270234\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.002562\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.709910\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000304\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000001\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000001\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.413552\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.035974\n",
      "\tTraining batch 66 Loss: 0.000002\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.002334\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000001\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000125\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.864583\n",
      "\tTraining batch 83 Loss: 0.674158\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000008\n",
      "\tTraining batch 91 Loss: 0.000115\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.099676\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.592973\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.437294\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.003849\n",
      "\tTraining batch 108 Loss: 0.332052\n",
      "\tTraining batch 109 Loss: 0.000239\n",
      "\tTraining batch 110 Loss: 0.000000\n",
      "\tTraining batch 111 Loss: 1.068115\n",
      "\tTraining batch 112 Loss: 0.000000\n",
      "\tTraining batch 113 Loss: 0.000000\n",
      "\tTraining batch 114 Loss: 0.868138\n",
      "Training set: Average loss: 0.060795\n",
      "Validation set: Average loss: 39.253709, Accuracy: 1362/1959 (69.53%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.013349\n",
      "\tTraining batch 5 Loss: 0.000068\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000042\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.595820\n",
      "\tTraining batch 16 Loss: 0.000014\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000107\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.061542\n",
      "\tTraining batch 25 Loss: 0.304779\n",
      "\tTraining batch 26 Loss: 0.051351\n",
      "\tTraining batch 27 Loss: 0.000493\n",
      "\tTraining batch 28 Loss: 0.182959\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000045\n",
      "\tTraining batch 31 Loss: 0.923500\n",
      "\tTraining batch 32 Loss: 1.130499\n",
      "\tTraining batch 33 Loss: 0.000139\n",
      "\tTraining batch 34 Loss: 0.115141\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000002\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000016\n",
      "\tTraining batch 43 Loss: 0.000471\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.691784\n",
      "\tTraining batch 47 Loss: 0.000001\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000001\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000001\n",
      "\tTraining batch 55 Loss: 0.065354\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000008\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 1.515897\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000001\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000320\n",
      "\tTraining batch 76 Loss: 0.001759\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.071051\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.400298\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000250\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.354680\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.304634\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000699\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 1.084549\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000415\n",
      "\tTraining batch 109 Loss: 0.000000\n",
      "\tTraining batch 110 Loss: 1.702820\n",
      "\tTraining batch 111 Loss: 0.486751\n",
      "\tTraining batch 112 Loss: 0.000000\n",
      "\tTraining batch 113 Loss: 0.465054\n",
      "\tTraining batch 114 Loss: 1.670265\n",
      "Training set: Average loss: 0.106991\n",
      "Validation set: Average loss: 36.094210, Accuracy: 1400/1959 (71.47%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000076\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.252420\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.002420\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 1.018706\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000001\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000534\n",
      "\tTraining batch 44 Loss: 0.141056\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.260656\n",
      "\tTraining batch 49 Loss: 0.751865\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.136305\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000002\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000005\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000114\n",
      "\tTraining batch 77 Loss: 0.000001\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.890433\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.042222\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000140\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000011\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.585140\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.132158\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000708\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.000000\n",
      "\tTraining batch 110 Loss: 0.000000\n",
      "\tTraining batch 111 Loss: 0.145551\n",
      "\tTraining batch 112 Loss: 0.000000\n",
      "\tTraining batch 113 Loss: 0.701609\n",
      "\tTraining batch 114 Loss: 10.540306\n",
      "Training set: Average loss: 0.136863\n",
      "Validation set: Average loss: 38.286054, Accuracy: 1392/1959 (71.06%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.807957\n",
      "\tTraining batch 11 Loss: 0.632645\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 3.091721\n",
      "\tTraining batch 14 Loss: 1.173092\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.451834\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000291\n",
      "\tTraining batch 20 Loss: 0.000195\n",
      "\tTraining batch 21 Loss: 0.253048\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 1.611155\n",
      "\tTraining batch 26 Loss: 0.142712\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.638294\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000020\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000042\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.010195\n",
      "\tTraining batch 46 Loss: 0.000017\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.352552\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000003\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.174194\n",
      "\tTraining batch 57 Loss: 0.000004\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000111\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.071199\n",
      "\tTraining batch 63 Loss: 1.791779\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 1.031732\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.115236\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000010\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.058577\n",
      "\tTraining batch 78 Loss: 0.281463\n",
      "\tTraining batch 79 Loss: 4.458323\n",
      "\tTraining batch 80 Loss: 0.000174\n",
      "\tTraining batch 81 Loss: 0.408669\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000019\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.003057\n",
      "\tTraining batch 101 Loss: 0.002541\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000008\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 1.448380\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000001\n",
      "\tTraining batch 108 Loss: 0.157641\n",
      "\tTraining batch 109 Loss: 0.000013\n",
      "\tTraining batch 110 Loss: 0.000000\n",
      "\tTraining batch 111 Loss: 0.000719\n",
      "\tTraining batch 112 Loss: 0.229960\n",
      "\tTraining batch 113 Loss: 0.864636\n",
      "\tTraining batch 114 Loss: 0.000000\n",
      "Training set: Average loss: 0.177756\n",
      "Validation set: Average loss: 42.054659, Accuracy: 1370/1959 (69.93%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.002626\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000708\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 1.808803\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000055\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.000058\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000024\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000001\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.703300\n",
      "\tTraining batch 109 Loss: 0.000000\n",
      "\tTraining batch 110 Loss: 0.000000\n",
      "\tTraining batch 111 Loss: 0.000010\n",
      "\tTraining batch 112 Loss: 0.000000\n",
      "\tTraining batch 113 Loss: 0.000000\n",
      "\tTraining batch 114 Loss: 0.187803\n",
      "Training set: Average loss: 0.023714\n",
      "Validation set: Average loss: 44.451447, Accuracy: 1398/1959 (71.36%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000417\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 1.311796\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000001\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000507\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000058\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000004\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.006021\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.000000\n",
      "\tTraining batch 110 Loss: 0.000000\n",
      "\tTraining batch 111 Loss: 0.000828\n",
      "\tTraining batch 112 Loss: 0.000000\n",
      "\tTraining batch 113 Loss: 0.000000\n",
      "\tTraining batch 114 Loss: 0.226842\n",
      "Training set: Average loss: 0.013566\n",
      "Validation set: Average loss: 44.068444, Accuracy: 1397/1959 (71.31%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000003\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.003389\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000001\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000041\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.000000\n",
      "\tTraining batch 110 Loss: 0.000000\n",
      "\tTraining batch 111 Loss: 0.000000\n",
      "\tTraining batch 112 Loss: 0.000000\n",
      "\tTraining batch 113 Loss: 0.000000\n",
      "\tTraining batch 114 Loss: 0.000000\n",
      "Training set: Average loss: 0.000030\n",
      "Validation set: Average loss: 45.295731, Accuracy: 1398/1959 (71.36%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000004\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.002028\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.000000\n",
      "\tTraining batch 110 Loss: 0.000000\n",
      "\tTraining batch 111 Loss: 0.000000\n",
      "\tTraining batch 112 Loss: 0.000000\n",
      "\tTraining batch 113 Loss: 0.000000\n",
      "\tTraining batch 114 Loss: 0.000000\n",
      "Training set: Average loss: 0.000018\n",
      "Validation set: Average loss: 45.289767, Accuracy: 1398/1959 (71.36%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 49.5%\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000004\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000000\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.001509\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.000000\n",
      "\tTraining batch 110 Loss: 0.000000\n",
      "\tTraining batch 111 Loss: 0.000000\n",
      "\tTraining batch 112 Loss: 0.000000\n",
      "\tTraining batch 113 Loss: 0.000000\n",
      "\tTraining batch 114 Loss: 0.000000\n",
      "\tTraining batch 115 Loss: 116.685562\n",
      "Training set: Average loss: 1.014670\n",
      "Validation set: Average loss: 42.984401, Accuracy: 1396/1959 (71.26%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 1.143375\n",
      "\tTraining batch 7 Loss: 0.000007\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.678137\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.844909\n",
      "\tTraining batch 12 Loss: 0.921477\n",
      "\tTraining batch 13 Loss: 2.088305\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.356673\n",
      "\tTraining batch 16 Loss: 0.087494\n",
      "\tTraining batch 17 Loss: 0.000018\n",
      "\tTraining batch 18 Loss: 0.769267\n",
      "\tTraining batch 19 Loss: 0.016999\n",
      "\tTraining batch 20 Loss: 0.000013\n",
      "\tTraining batch 21 Loss: 0.380517\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.475109\n",
      "\tTraining batch 24 Loss: 0.633816\n",
      "\tTraining batch 25 Loss: 3.005098\n",
      "\tTraining batch 26 Loss: 0.680971\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.532904\n",
      "\tTraining batch 29 Loss: 0.000021\n",
      "\tTraining batch 30 Loss: 0.388199\n",
      "\tTraining batch 31 Loss: 0.001290\n",
      "\tTraining batch 32 Loss: 1.209039\n",
      "\tTraining batch 33 Loss: 0.593221\n",
      "\tTraining batch 34 Loss: 3.000134\n",
      "\tTraining batch 35 Loss: 0.000801\n",
      "\tTraining batch 36 Loss: 0.215013\n",
      "\tTraining batch 37 Loss: 0.747655\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.222231\n",
      "\tTraining batch 40 Loss: 0.356759\n",
      "\tTraining batch 41 Loss: 0.110259\n",
      "\tTraining batch 42 Loss: 0.452731\n",
      "\tTraining batch 43 Loss: 0.051751\n",
      "\tTraining batch 44 Loss: 0.087497\n",
      "\tTraining batch 45 Loss: 0.023651\n",
      "\tTraining batch 46 Loss: 0.166934\n",
      "\tTraining batch 47 Loss: 0.042768\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.098972\n",
      "\tTraining batch 50 Loss: 0.484108\n",
      "\tTraining batch 51 Loss: 0.127979\n",
      "\tTraining batch 52 Loss: 0.200939\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.071582\n",
      "\tTraining batch 56 Loss: 0.003148\n",
      "\tTraining batch 57 Loss: 0.000014\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.929423\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.008575\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000002\n",
      "\tTraining batch 73 Loss: 0.595355\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000003\n",
      "\tTraining batch 76 Loss: 0.000349\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000004\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000013\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.575962\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.657663\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.246301\n",
      "\tTraining batch 91 Loss: 0.000120\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.114915\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000001\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.076443\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.848189\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.073121\n",
      "\tTraining batch 108 Loss: 0.663168\n",
      "\tTraining batch 109 Loss: 0.000070\n",
      "\tTraining batch 110 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 111 Loss: 0.252416\n",
      "\tTraining batch 112 Loss: 0.000000\n",
      "\tTraining batch 113 Loss: 0.439899\n",
      "\tTraining batch 114 Loss: 0.000000\n",
      "\tTraining batch 115 Loss: 23.903269\n",
      "Training set: Average loss: 0.440496\n",
      "Validation set: Average loss: 40.023343, Accuracy: 1408/1959 (71.87%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.000001\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 2.081702\n",
      "\tTraining batch 5 Loss: 2.156118\n",
      "\tTraining batch 6 Loss: 0.983042\n",
      "\tTraining batch 7 Loss: 1.851671\n",
      "\tTraining batch 8 Loss: 0.000042\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000001\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000005\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.747345\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000002\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.941830\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.379222\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.387197\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000705\n",
      "\tTraining batch 53 Loss: 0.682436\n",
      "\tTraining batch 54 Loss: 1.833884\n",
      "\tTraining batch 55 Loss: 1.553878\n",
      "\tTraining batch 56 Loss: 0.242797\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.614191\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000001\n",
      "\tTraining batch 76 Loss: 0.000024\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.639044\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 1.222361\n",
      "\tTraining batch 91 Loss: 0.587899\n",
      "\tTraining batch 92 Loss: 0.369644\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.382343\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000035\n",
      "\tTraining batch 102 Loss: 2.386650\n",
      "\tTraining batch 103 Loss: 1.362854\n",
      "\tTraining batch 104 Loss: 0.815502\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 1.282160\n",
      "\tTraining batch 110 Loss: 0.000002\n",
      "\tTraining batch 111 Loss: 4.876163\n",
      "\tTraining batch 112 Loss: 0.834414\n",
      "\tTraining batch 113 Loss: 1.403989\n",
      "\tTraining batch 114 Loss: 0.422053\n",
      "\tTraining batch 115 Loss: 0.000000\n",
      "Training set: Average loss: 0.269924\n",
      "Validation set: Average loss: 39.476224, Accuracy: 1390/1959 (70.95%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.474594\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000001\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.002289\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000940\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000008\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.866925\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.226656\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 3.250034\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.603621\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 1.030872\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000017\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000003\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000113\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000003\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.526942\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.926047\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000010\n",
      "\tTraining batch 91 Loss: 0.003442\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000000\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000013\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.556716\n",
      "\tTraining batch 110 Loss: 0.000000\n",
      "\tTraining batch 111 Loss: 0.000001\n",
      "\tTraining batch 112 Loss: 0.322161\n",
      "\tTraining batch 113 Loss: 0.000000\n",
      "\tTraining batch 114 Loss: 0.390322\n",
      "\tTraining batch 115 Loss: 0.000000\n",
      "Training set: Average loss: 0.079841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set: Average loss: 37.132314, Accuracy: 1396/1959 (71.26%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000006\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000003\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000056\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.029547\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.018946\n",
      "\tTraining batch 71 Loss: 0.000000\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000003\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.002861\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.003400\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.260991\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000005\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.000000\n",
      "\tTraining batch 110 Loss: 0.000000\n",
      "\tTraining batch 111 Loss: 0.000002\n",
      "\tTraining batch 112 Loss: 0.000000\n",
      "\tTraining batch 113 Loss: 0.000000\n",
      "\tTraining batch 114 Loss: 0.000000\n",
      "\tTraining batch 115 Loss: 0.000000\n",
      "Training set: Average loss: 0.002746\n",
      "Validation set: Average loss: 36.919693, Accuracy: 1404/1959 (71.67%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.004200\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000000\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000001\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000004\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000024\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000028\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000001\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000746\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.211952\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.003562\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000098\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000000\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.000000\n",
      "\tTraining batch 110 Loss: 0.000000\n",
      "\tTraining batch 111 Loss: 0.003779\n",
      "\tTraining batch 112 Loss: 0.000000\n",
      "\tTraining batch 113 Loss: 0.000000\n",
      "\tTraining batch 114 Loss: 0.000000\n",
      "\tTraining batch 115 Loss: 0.000000\n",
      "Training set: Average loss: 0.001951\n",
      "Validation set: Average loss: 36.235438, Accuracy: 1394/1959 (71.16%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000009\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000002\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000032\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000184\n",
      "\tTraining batch 77 Loss: 0.000907\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.003581\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000044\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000001\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.000000\n",
      "\tTraining batch 110 Loss: 0.000000\n",
      "\tTraining batch 111 Loss: 0.000027\n",
      "\tTraining batch 112 Loss: 0.000000\n",
      "\tTraining batch 113 Loss: 0.000000\n",
      "\tTraining batch 114 Loss: 0.000000\n",
      "\tTraining batch 115 Loss: 0.000000\n",
      "Training set: Average loss: 0.000042\n",
      "Validation set: Average loss: 36.202507, Accuracy: 1393/1959 (71.11%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.000000\n",
      "\tTraining batch 2 Loss: 0.000000\n",
      "\tTraining batch 3 Loss: 0.000000\n",
      "\tTraining batch 4 Loss: 0.000000\n",
      "\tTraining batch 5 Loss: 0.000000\n",
      "\tTraining batch 6 Loss: 0.000000\n",
      "\tTraining batch 7 Loss: 0.000000\n",
      "\tTraining batch 8 Loss: 0.000000\n",
      "\tTraining batch 9 Loss: 0.000000\n",
      "\tTraining batch 10 Loss: 0.000000\n",
      "\tTraining batch 11 Loss: 0.000000\n",
      "\tTraining batch 12 Loss: 0.000000\n",
      "\tTraining batch 13 Loss: 0.000000\n",
      "\tTraining batch 14 Loss: 0.000000\n",
      "\tTraining batch 15 Loss: 0.000000\n",
      "\tTraining batch 16 Loss: 0.000000\n",
      "\tTraining batch 17 Loss: 0.000000\n",
      "\tTraining batch 18 Loss: 0.000000\n",
      "\tTraining batch 19 Loss: 0.000000\n",
      "\tTraining batch 20 Loss: 0.000000\n",
      "\tTraining batch 21 Loss: 0.000000\n",
      "\tTraining batch 22 Loss: 0.000000\n",
      "\tTraining batch 23 Loss: 0.000000\n",
      "\tTraining batch 24 Loss: 0.000000\n",
      "\tTraining batch 25 Loss: 0.000000\n",
      "\tTraining batch 26 Loss: 0.000000\n",
      "\tTraining batch 27 Loss: 0.000000\n",
      "\tTraining batch 28 Loss: 0.000000\n",
      "\tTraining batch 29 Loss: 0.000000\n",
      "\tTraining batch 30 Loss: 0.000000\n",
      "\tTraining batch 31 Loss: 0.000000\n",
      "\tTraining batch 32 Loss: 0.000000\n",
      "\tTraining batch 33 Loss: 0.000000\n",
      "\tTraining batch 34 Loss: 0.000000\n",
      "\tTraining batch 35 Loss: 0.000000\n",
      "\tTraining batch 36 Loss: 0.000000\n",
      "\tTraining batch 37 Loss: 0.000000\n",
      "\tTraining batch 38 Loss: 0.000000\n",
      "\tTraining batch 39 Loss: 0.000000\n",
      "\tTraining batch 40 Loss: 0.000000\n",
      "\tTraining batch 41 Loss: 0.000000\n",
      "\tTraining batch 42 Loss: 0.000000\n",
      "\tTraining batch 43 Loss: 0.000000\n",
      "\tTraining batch 44 Loss: 0.000000\n",
      "\tTraining batch 45 Loss: 0.000000\n",
      "\tTraining batch 46 Loss: 0.000000\n",
      "\tTraining batch 47 Loss: 0.000000\n",
      "\tTraining batch 48 Loss: 0.000000\n",
      "\tTraining batch 49 Loss: 0.000000\n",
      "\tTraining batch 50 Loss: 0.000000\n",
      "\tTraining batch 51 Loss: 0.000000\n",
      "\tTraining batch 52 Loss: 0.000000\n",
      "\tTraining batch 53 Loss: 0.000000\n",
      "\tTraining batch 54 Loss: 0.000000\n",
      "\tTraining batch 55 Loss: 0.000000\n",
      "\tTraining batch 56 Loss: 0.000000\n",
      "\tTraining batch 57 Loss: 0.000008\n",
      "\tTraining batch 58 Loss: 0.000000\n",
      "\tTraining batch 59 Loss: 0.000000\n",
      "\tTraining batch 60 Loss: 0.000000\n",
      "\tTraining batch 61 Loss: 0.000000\n",
      "\tTraining batch 62 Loss: 0.000000\n",
      "\tTraining batch 63 Loss: 0.000000\n",
      "\tTraining batch 64 Loss: 0.000000\n",
      "\tTraining batch 65 Loss: 0.000000\n",
      "\tTraining batch 66 Loss: 0.000000\n",
      "\tTraining batch 67 Loss: 0.000000\n",
      "\tTraining batch 68 Loss: 0.000000\n",
      "\tTraining batch 69 Loss: 0.000000\n",
      "\tTraining batch 70 Loss: 0.000000\n",
      "\tTraining batch 71 Loss: 0.000009\n",
      "\tTraining batch 72 Loss: 0.000000\n",
      "\tTraining batch 73 Loss: 0.000000\n",
      "\tTraining batch 74 Loss: 0.000000\n",
      "\tTraining batch 75 Loss: 0.000000\n",
      "\tTraining batch 76 Loss: 0.000178\n",
      "\tTraining batch 77 Loss: 0.000000\n",
      "\tTraining batch 78 Loss: 0.000000\n",
      "\tTraining batch 79 Loss: 0.000000\n",
      "\tTraining batch 80 Loss: 0.000000\n",
      "\tTraining batch 81 Loss: 0.000000\n",
      "\tTraining batch 82 Loss: 0.000000\n",
      "\tTraining batch 83 Loss: 0.000000\n",
      "\tTraining batch 84 Loss: 0.000000\n",
      "\tTraining batch 85 Loss: 0.000000\n",
      "\tTraining batch 86 Loss: 0.000000\n",
      "\tTraining batch 87 Loss: 0.000000\n",
      "\tTraining batch 88 Loss: 0.000000\n",
      "\tTraining batch 89 Loss: 0.000000\n",
      "\tTraining batch 90 Loss: 0.000000\n",
      "\tTraining batch 91 Loss: 0.003544\n",
      "\tTraining batch 92 Loss: 0.000000\n",
      "\tTraining batch 93 Loss: 0.000000\n",
      "\tTraining batch 94 Loss: 0.000000\n",
      "\tTraining batch 95 Loss: 0.000000\n",
      "\tTraining batch 96 Loss: 0.000000\n",
      "\tTraining batch 97 Loss: 0.000000\n",
      "\tTraining batch 98 Loss: 0.000000\n",
      "\tTraining batch 99 Loss: 0.000000\n",
      "\tTraining batch 100 Loss: 0.000000\n",
      "\tTraining batch 101 Loss: 0.000029\n",
      "\tTraining batch 102 Loss: 0.000000\n",
      "\tTraining batch 103 Loss: 0.000000\n",
      "\tTraining batch 104 Loss: 0.000000\n",
      "\tTraining batch 105 Loss: 0.000000\n",
      "\tTraining batch 106 Loss: 0.000001\n",
      "\tTraining batch 107 Loss: 0.000000\n",
      "\tTraining batch 108 Loss: 0.000000\n",
      "\tTraining batch 109 Loss: 0.000000\n",
      "\tTraining batch 110 Loss: 0.000000\n",
      "\tTraining batch 111 Loss: 0.000027\n",
      "\tTraining batch 112 Loss: 0.000000\n",
      "\tTraining batch 113 Loss: 0.000000\n",
      "\tTraining batch 114 Loss: 0.000000\n",
      "\tTraining batch 115 Loss: 0.000000\n",
      "Training set: Average loss: 0.000033\n",
      "Validation set: Average loss: 36.197356, Accuracy: 1393/1959 (71.11%)\n",
      "\n",
      "Early stopping\n",
      "Getting predictions from test set\n",
      "Accuracy: 49.5%\n"
     ]
    }
   ],
   "source": [
    "# Use an \"Adam\" optimizer to adjust weights\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Specify the loss criteria\n",
    "loss_criteria = nn.CrossEntropyLoss()\n",
    "\n",
    "# Track metrics in these arrays\n",
    "epoch_nums = []\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "accuracy = []\n",
    "\n",
    "# Train over 50 epochs\n",
    "epochs = 50\n",
    "early_loss = 0.0\n",
    "for batch_no in range(1,116):\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss = train(model, train_loader, optimizer, epoch, batch_no)\n",
    "        test_loss, validation_predict = validation(model, validation_loader)\n",
    "        epoch_nums.append(epoch)\n",
    "        training_loss.append(train_loss)\n",
    "        validation_loss.append(test_loss)\n",
    "        if round(early_loss,2) == round(validation_predict,2):\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        early_loss = validation_predict \n",
    "        \n",
    "    truelabels = []\n",
    "    predictions = []\n",
    "    model.eval()\n",
    "    print(\"Getting predictions from test set\")\n",
    "    for data, target in test_loader:\n",
    "        for label in target.data.numpy():\n",
    "            truelabels.append(label)\n",
    "        for prediction in model(data):\n",
    "            predictions.append(torch.argmax(prediction).item())\n",
    "    cm = confusion_matrix(truelabels, predictions)\n",
    "    accuracy_ = (cm[0][0] + cm[1][1] + cm[2][2] + cm[3][3]) / cm.sum() * 100\n",
    "    print('Accuracy: {}%'.format(accuracy_))\n",
    "    accuracy.append(accuracy_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0abdf690",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = list(range(0,4600))\n",
    "batches_ = list()\n",
    "for i in new_list:\n",
    "    if i % 40 == 0:\n",
    "        batches_.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "20d682ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.transforms as transforms\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "fig, ax=plt.subplots()\n",
    "ax.plot(batches_['Batch'], batches_['Accuracy'])\n",
    "ax.axhline(y=52.25,linestyle='--',color='red',xmax=0.92)\n",
    "ax.axhline(y=51.0,linestyle='--',color='red',xmax=0.27)\n",
    "#ax.axvline(x=1120,linestyle='--',color='red',ymax=0.81)\n",
    "#ax.axvline(x=4400,linestyle='--',color='red',ymax=0.93)\n",
    "plt.title('Plot of accuracy against batches')\n",
    "fig.set_size_inches(18, 12)\n",
    "trans = transforms.blended_transform_factory(\n",
    "    ax.get_yticklabels()[0].get_transform(), ax.transData)\n",
    "ax.text(0,52.25, \"{:.2f}{}\".format(52.25, '%'), color=\"red\", transform=trans, \n",
    "        ha=\"right\", va=\"center\")\n",
    "ax.text(0,51.0, \"{:.2f}{}\".format(51.0, '%'), color=\"red\", transform=trans, \n",
    "        ha=\"right\", va=\"center\")\n",
    "ax.set_xlabel(\"Number of training samples\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b40ceed",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "16d32884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x1008 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_cm = pd.DataFrame(cm, index = classes, columns = classes)\n",
    "plt.figure(figsize = (18,14))\n",
    "sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues, fmt='g')\n",
    "plt.xlabel(\"Predicted Shape\", fontsize = 15)\n",
    "plt.ylabel(\"True Shape\", fontsize = 15)\n",
    "plt.show()\n",
    "\n",
    "batches_ = pd.DataFrame(\n",
    "    {'Batch': batches_,\n",
    "     'Accuracy': accuracy\n",
    "    })\n",
    "\n",
    "batches_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
