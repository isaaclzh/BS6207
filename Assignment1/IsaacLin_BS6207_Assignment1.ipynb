{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "commercial-lindsay",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pytorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-secretariat",
   "metadata": {},
   "source": [
    "## 1. Build the model\n",
    "- Input (x1,x2): 2 nodes\n",
    "- First hidden layer: 10 nodes, with weights (w) and bias (b), sigmoid activation function\n",
    "- Second hidden layer: 10 nodes, with weights (w) and bias (b), sigmoid activation function\n",
    "- Output (predict): 1 node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "regional-counter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\n",
      " Sequential(\n",
      "  (0): Linear(in_features=2, out_features=10, bias=True)\n",
      "  (1): Sigmoid()\n",
      "  (2): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (3): Sigmoid()\n",
      "  (4): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Building Neural Network using nn.Sequential\n",
    "# Hyperparameters for our network\n",
    "input_size = 2\n",
    "hidden_sizes = [10,10]\n",
    "output_size = 1\n",
    "\n",
    "model = nn.Sequential(\n",
    "    # Input with 2 nodes to first hidden layer with 10 nodes\n",
    "    nn.Linear(input_size, hidden_sizes[0]), \n",
    "    # Pass through Sigmoid activation function\n",
    "    nn.Sigmoid(),\n",
    "    # First hidden layer with 10 nodes to second hidden layer with 10 nodes\n",
    "    nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "    # Pass through Sigmoid activation function\n",
    "    nn.Sigmoid(),\n",
    "    # Second hidden layer with 10 nodes to output layer with 1 node\n",
    "    nn.Linear(hidden_sizes[1], output_size),\n",
    ")\n",
    "\n",
    "print('Model:\\n',model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-pasta",
   "metadata": {},
   "source": [
    "## 2. Generate the random number x1, x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "appropriate-mason",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input(x1,x2):\n",
      "  tensor([[0.9670, 0.5472]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(4)\n",
    "x1 = np.random.uniform()\n",
    "x2 = np.random.uniform()\n",
    "x = torch.tensor([[x1,x2]], requires_grad=True)\n",
    "print(\"input(x1,x2):\\n \",x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-cache",
   "metadata": {},
   "source": [
    "## 3. Generate the label y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cordless-publication",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of y_true is:\n",
      " tensor([[0.6173]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "y_true = (x1*x1+x2*x2)/2\n",
    "y_true = torch.tensor([[y_true]], requires_grad=True)\n",
    "print('The value of y_true is:\\n', y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-ancient",
   "metadata": {},
   "source": [
    "## 4. Build a loss function L = (y_predict - y_true)^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "thousand-induction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(y_true, y_pred):\n",
    "    return torch.sum((y_pred - y_true) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-certificate",
   "metadata": {},
   "source": [
    "## 5. Forward / Backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "rolled-colors",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1 | Loss=0.411943\n",
      "Epoch = 2 | Loss=0.048174\n",
      "Epoch = 3 | Loss=0.005531\n",
      "Epoch = 4 | Loss=0.000627\n",
      "Epoch = 5 | Loss=0.000071\n",
      "Epoch = 6 | Loss=0.000008\n",
      "Epoch = 7 | Loss=0.000001\n",
      "Epoch = 8 | Loss=0.000000\n",
      "Epoch = 9 | Loss=0.000000\n",
      "Epoch = 10 | Loss=0.000000\n"
     ]
    }
   ],
   "source": [
    "# 10 epochs\n",
    "# learning rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for i in range(10):\n",
    "    # Zero out all the gradients at every epoch\n",
    "    optimizer.zero_grad()\n",
    "    # Forward propogation\n",
    "    y_pred = model(x)\n",
    "    # Show loss at each epoch\n",
    "    loss = loss_fn(y_pred, y_true)\n",
    "    print(f\"Epoch = {i + 1} | Loss=%f\" % (loss.item())) \n",
    "    # Backward propagation and update the gradients\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ruled-transition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6173]], grad_fn=<AddmmBackward>)\n",
      "tensor([[0.6173]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# y_pred is the same as y_true\n",
    "print(y_pred)\n",
    "print(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-arrow",
   "metadata": {},
   "source": [
    "## 6a. Calculate the gradients of the loss wrt weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "unknown-tuesday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Structure of first hidden layer:\n",
      " Linear(in_features=2, out_features=10, bias=True)\n",
      "Weight gradient of network:\n",
      " tensor([[ 1.1669e-09,  6.6035e-10],\n",
      "        [ 7.4194e-08,  4.1985e-08],\n",
      "        [ 1.7838e-07,  1.0095e-07],\n",
      "        [-1.9635e-07, -1.1111e-07],\n",
      "        [ 7.8348e-08,  4.4337e-08],\n",
      "        [-5.3009e-07, -2.9997e-07],\n",
      "        [ 5.3999e-07,  3.0558e-07],\n",
      "        [-2.6930e-07, -1.5240e-07],\n",
      "        [-1.5825e-07, -8.9551e-08],\n",
      "        [-5.2903e-08, -2.9937e-08]])\n",
      "Bias gradient of network:\n",
      " tensor([ 1.2067e-09,  7.6723e-08,  1.8446e-07, -2.0304e-07,  8.1020e-08,\n",
      "        -5.4816e-07,  5.5840e-07, -2.7848e-07, -1.6364e-07, -5.4706e-08])\n"
     ]
    }
   ],
   "source": [
    "# Check the weights and biases of first hidden layer\n",
    "print('Network Structure of first hidden layer:\\n',model[0])\n",
    "print('Weight gradient of network:\\n',model[0].weight.grad)\n",
    "print('Bias gradient of network:\\n',model[0].bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "periodic-single",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Structure of second hidden layer:\n",
      " Linear(in_features=10, out_features=10, bias=True)\n",
      "Weight gradient of network:\n",
      " tensor([[ 2.5782e-07,  2.6286e-07,  4.1569e-07,  3.1589e-07,  2.2706e-07,\n",
      "          3.0214e-07,  1.7527e-07,  1.9254e-07,  1.6258e-07,  1.5492e-07],\n",
      "        [-1.3693e-06, -1.3961e-06, -2.2078e-06, -1.6777e-06, -1.2059e-06,\n",
      "         -1.6047e-06, -9.3089e-07, -1.0226e-06, -8.6347e-07, -8.2282e-07],\n",
      "        [-1.8169e-06, -1.8524e-06, -2.9294e-06, -2.2261e-06, -1.6001e-06,\n",
      "         -2.1292e-06, -1.2352e-06, -1.3568e-06, -1.1457e-06, -1.0918e-06],\n",
      "        [-2.4050e-06, -2.4521e-06, -3.8778e-06, -2.9468e-06, -2.1181e-06,\n",
      "         -2.8185e-06, -1.6350e-06, -1.7961e-06, -1.5166e-06, -1.4452e-06],\n",
      "        [-1.6985e-06, -1.7318e-06, -2.7386e-06, -2.0811e-06, -1.4959e-06,\n",
      "         -1.9905e-06, -1.1547e-06, -1.2684e-06, -1.0711e-06, -1.0207e-06],\n",
      "        [-1.5941e-06, -1.6252e-06, -2.5702e-06, -1.9531e-06, -1.4039e-06,\n",
      "         -1.8681e-06, -1.0837e-06, -1.1904e-06, -1.0052e-06, -9.5788e-07],\n",
      "        [-9.7934e-07, -9.9850e-07, -1.5790e-06, -1.1999e-06, -8.6250e-07,\n",
      "         -1.1477e-06, -6.6578e-07, -7.3136e-07, -6.1756e-07, -5.8849e-07],\n",
      "        [ 1.5813e-06,  1.6123e-06,  2.5497e-06,  1.9375e-06,  1.3927e-06,\n",
      "          1.8532e-06,  1.0750e-06,  1.1809e-06,  9.9717e-07,  9.5023e-07],\n",
      "        [-1.2811e-06, -1.3062e-06, -2.0656e-06, -1.5697e-06, -1.1283e-06,\n",
      "         -1.5014e-06, -8.7094e-07, -9.5672e-07, -8.0786e-07, -7.6982e-07],\n",
      "        [ 1.1127e-06,  1.1345e-06,  1.7941e-06,  1.3634e-06,  9.7999e-07,\n",
      "          1.3040e-06,  7.5647e-07,  8.3098e-07,  7.0168e-07,  6.6865e-07]])\n",
      "Bias gradient of network:\n",
      " tensor([ 6.2213e-07, -3.3042e-06, -4.3842e-06, -5.8036e-06, -4.0987e-06,\n",
      "        -3.8466e-06, -2.3632e-06,  3.8159e-06, -3.0914e-06,  2.6851e-06])\n"
     ]
    }
   ],
   "source": [
    "# Check the weights and biases of second hidden layer\n",
    "print('Network Structure of second hidden layer:\\n',model[2])\n",
    "print('Weight gradient of network:\\n',model[2].weight.grad)\n",
    "print('Bias gradient of network:\\n',model[2].bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "olympic-seafood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Structure of output layer:\n",
      " Linear(in_features=10, out_features=1, bias=True)\n",
      "Weight gradient of network:\n",
      " tensor([[-3.0957e-05, -3.4134e-05, -3.2646e-05, -3.6027e-05, -3.5243e-05,\n",
      "         -4.4999e-05, -2.6554e-05, -2.8694e-05, -2.5792e-05, -3.9136e-05]])\n",
      "Bias gradient of network:\n",
      " tensor([-7.1406e-05])\n"
     ]
    }
   ],
   "source": [
    "# Check the weights and biases of output layer\n",
    "print('Network Structure of output layer:\\n',model[4])\n",
    "print('Weight gradient of network:\\n',model[4].weight.grad)\n",
    "print('Bias gradient of network:\\n',model[4].bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-notion",
   "metadata": {},
   "source": [
    "## 6b. Write to torch_autograd.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-philippines",
   "metadata": {},
   "source": [
    "## 7a. Implement forward propagation and backpropagation algorithm from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-journalism",
   "metadata": {},
   "source": [
    "### Define loss function, sigmoid function, sigmoid derivative function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "charitable-lesson",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Loss function\n",
    "def loss(y_true, y_pred):\n",
    "    return np.power(y_pred - y_true, 2)\n",
    "\n",
    "# Sigmoid Activation function for feed-forward\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Sigmoid Derivative function for back-propagation\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-lafayette",
   "metadata": {},
   "source": [
    "### x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "equivalent-employee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input(x1,x2):\n",
      " [[0.96702984 0.54723225]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(4)\n",
    "x1 = np.random.uniform()\n",
    "x2 = np.random.uniform()\n",
    "x_train = np.array([[x1,x2]])\n",
    "print(\"input(x1,x2):\\n\", x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "married-bookmark",
   "metadata": {},
   "source": [
    "### y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "marine-colombia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.61730492]]\n"
     ]
    }
   ],
   "source": [
    "y_train = np.array([[(x_train[0][0]*x_train[0][0] + x_train[0][1]*x_train[0][1])/2]])\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standing-bedroom",
   "metadata": {},
   "source": [
    "### Define forward propagation and backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "divine-default",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 epochs\n",
    "# learning rate = 0.1\n",
    "\n",
    "class FeedForward:\n",
    "    def __init__(self, input_layer, hidden_layer_1, hidden_layer_2, output_layer):\n",
    "        # Number of nodes for each layer\n",
    "        self.input_layer = input_layer\n",
    "        self.hidden_layer_1 = hidden_layer_1\n",
    "        self.hidden_layer_2 = hidden_layer_2\n",
    "        self.output_layer = output_layer\n",
    "\n",
    "        # Since there are 3 layers (excluding the input layer), there are 6 unknown parameters for weights and biases\n",
    "        # Initialization of parameters w1, w2, w3, b1, b2, b3\n",
    "        self.w1 = np.random.rand(input_layer, hidden_layer_1)\n",
    "        self.w2 = np.random.rand(hidden_layer_1, hidden_layer_2)\n",
    "        self.w3 = np.random.rand(hidden_layer_2, output_layer)\n",
    "        self.b1 = np.random.rand(hidden_layer_1)\n",
    "        self.b2 = np.random.rand(hidden_layer_2)\n",
    "        self.b3 = np.random.rand(output_layer)\n",
    "        \n",
    "    # Method to train model\n",
    "    # Change the number of epochs \n",
    "    def fit(self, x_training, y_training, learning_rate=0.1, epochs=10): \n",
    "        # Epoch loop\n",
    "        for epoch in range(epochs):\n",
    "            error = 0\n",
    "            # Training set loop\n",
    "            # Training data is passed through the Sigmoid function to add non-linearity to the model\n",
    "            for x, y_true in zip(x_training, y_training):\n",
    "                # Forward propagation\n",
    "                h1_ = np.dot(x, self.w1) + self.b1\n",
    "                h1 = sigmoid(h1_)\n",
    "             \n",
    "                h2_ = np.dot(h1, self.w2) + self.b2\n",
    "                h2 = sigmoid(h2_)\n",
    "              \n",
    "                y = np.dot(h2, self.w3) + self.b3\n",
    "\n",
    "                # Error computation against the true value of y using sum of squared errors method\n",
    "                error += loss(y_true, y)\n",
    "      \n",
    "                # Backward propagation\n",
    "                derivative_w1 = np.zeros((self.input_layer, self.hidden_layer_1))\n",
    "                derivative_w2 = np.zeros((self.hidden_layer_1, self.hidden_layer_2))\n",
    "                derivative_w3 = np.zeros((self.hidden_layer_2, self.output_layer))\n",
    "                derivative_b1 = np.zeros(self.hidden_layer_1)\n",
    "                derivative_b2 = np.zeros(self.hidden_layer_2)\n",
    "                derivative_b3 = np.zeros(self.output_layer)\n",
    "            \n",
    "                for i in range(self.output_layer):\n",
    "                    derivative_b3[i] = (y[i] - y_true[i])\n",
    "            \n",
    "                for i in range(self.hidden_layer_2):\n",
    "                    for j in range(self.output_layer):\n",
    "                        derivative_w3[i][j] = derivative_b3[j] * h2[i]\n",
    "                        \n",
    "                for i in range(self.hidden_layer_2):\n",
    "                    derivative_b2[i] = sum([derivative_b3[j] * self.w3[i][j] * sigmoid_derivative(h2_[i]) for j in range(self.output_layer)])\n",
    "                \n",
    "                for i in range(self.hidden_layer_1):\n",
    "                    for j in range(self.hidden_layer_2):\n",
    "                        derivative_w2[i][j] = derivative_b2[j] * h1[i]\n",
    "                        \n",
    "                for i in range(self.hidden_layer_1):\n",
    "                    derivative_b1[i] = sum([derivative_b2[j] * self.w2[i][j] * sigmoid_derivative(h1_[i]) for j in range(self.hidden_layer_1)])\n",
    "                    \n",
    "                for i in range(self.input_layer):\n",
    "                    for j in range(self.hidden_layer_1):\n",
    "                        derivative_w1[i][j] = derivative_b1[j] * x[i]\n",
    "                                            \n",
    "                # Gradient Descent\n",
    "                self.b1 -= learning_rate * derivative_b1\n",
    "                self.w1 -= learning_rate * derivative_w1\n",
    "                self.b2 -= learning_rate * derivative_b2\n",
    "                self.w2 -= learning_rate * derivative_w2\n",
    "                self.b3 -= learning_rate * derivative_b3\n",
    "                self.w3 -= learning_rate * derivative_w3\n",
    "                           \n",
    "            # Show loss at each epoch\n",
    "            error /= x_training.shape[0]\n",
    "            print(f\"Epoch = {epoch + 1} | Loss = {error}\")\n",
    "        \n",
    "        print()\n",
    "        print('Weight gradient of first hidden layer:\\n', derivative_w1)\n",
    "        print('Bias gradient of first hidden layer:\\n', derivative_b1)\n",
    "        print()\n",
    "        print('Weight gradient of second hidden layer:\\n', derivative_w2)\n",
    "        print('Bias gradient of second hidden layer:\\n', derivative_b2)\n",
    "        print()\n",
    "        print('Weight gradient of output layer:\\n', derivative_w3)\n",
    "        print('Bias gradient of output layer:\\n', derivative_b3)\n",
    "            \n",
    "    # Method to do a single prediction based on x1, x2\n",
    "    def predict(self, x_test):\n",
    "        # Forward propagation\n",
    "        # Input to first hidden layer\n",
    "        h1_ = np.dot(x_test, self.w1) + self.b1\n",
    "        # Sigmoid activation function\n",
    "        h1 = sigmoid(h1_)\n",
    "        # First hidden layer to second hidden layer\n",
    "        h2_ = np.dot(h1, self.w2) + self.b2\n",
    "        # Sigmoid activation function\n",
    "        h2 = sigmoid(h2_)\n",
    "        # Output layer\n",
    "        y = np.dot(h2, self.w3) + self.b3\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-mayor",
   "metadata": {},
   "source": [
    "### Build and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "central-characteristic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1 | Loss = [27.70769141]\n",
      "Epoch = 2 | Loss = [0.16445874]\n",
      "Epoch = 3 | Loss = [0.00095962]\n",
      "Epoch = 4 | Loss = [5.59923213e-06]\n",
      "Epoch = 5 | Loss = [3.2670576e-08]\n",
      "Epoch = 6 | Loss = [1.90627393e-10]\n",
      "Epoch = 7 | Loss = [1.11227919e-12]\n",
      "Epoch = 8 | Loss = [6.48996442e-15]\n",
      "Epoch = 9 | Loss = [3.78678678e-17]\n",
      "Epoch = 10 | Loss = [2.20952638e-19]\n",
      "\n",
      "Weight gradient of first hidden layer:\n",
      " [[-2.10219322e-13  1.27505382e-13  1.28203309e-13  1.97357924e-13\n",
      "  -1.65282859e-13 -1.01060617e-13  3.30899500e-15 -2.17395276e-13\n",
      "  -4.80810282e-13 -2.26756252e-13]\n",
      " [-1.18960954e-13  7.21539853e-14  7.25489350e-14  1.11682821e-13\n",
      "  -9.35318714e-14 -5.71891645e-14  1.87252627e-15 -1.23021753e-13\n",
      "  -2.72085598e-13 -1.28319033e-13]]\n",
      "Bias gradient of first hidden layer:\n",
      " [-2.17386593e-13  1.31852582e-13  1.32574305e-13  2.04086695e-13\n",
      " -1.70918055e-13 -1.04506203e-13  3.42181272e-15 -2.24807205e-13\n",
      " -4.97203150e-13 -2.34487337e-13]\n",
      "\n",
      "Weight gradient of second hidden layer:\n",
      " [[-9.53685898e-13 -1.89116603e-12 -2.40351047e-12  2.05107085e-12\n",
      "   3.21381186e-12 -3.44344201e-13  1.21263590e-12 -2.15076203e-13\n",
      "  -3.22225324e-13 -1.58318598e-12]\n",
      " [-9.01968149e-13 -1.78860936e-12 -2.27316970e-12  1.93984265e-12\n",
      "   3.03952899e-12 -3.25670645e-13  1.14687546e-12 -2.03412765e-13\n",
      "  -3.04751260e-13 -1.49733086e-12]\n",
      " [-9.34657548e-13 -1.85343268e-12 -2.35555460e-12  2.01014701e-12\n",
      "   3.14968851e-12 -3.37473698e-13  1.18844087e-12 -2.10784911e-13\n",
      "  -3.15796146e-13 -1.55159757e-12]\n",
      " [-8.59495586e-13 -1.70438596e-12 -2.16612896e-12  1.84849786e-12\n",
      "   2.89640134e-12 -3.10335218e-13  1.09287052e-12 -1.93834310e-13\n",
      "  -2.90400901e-13 -1.42682340e-12]\n",
      " [-7.95190012e-13 -1.57686755e-12 -2.00406394e-12  1.71019731e-12\n",
      "   2.67969894e-12 -2.87116618e-13  1.01110434e-12 -1.79332052e-13\n",
      "  -2.68673743e-13 -1.32007161e-12]\n",
      " [-6.37203122e-13 -1.26357840e-12 -1.60590020e-12  1.37041845e-12\n",
      "   2.14730128e-12 -2.30072816e-13  8.10219990e-13 -1.43702689e-13\n",
      "  -2.15294137e-13 -1.05780221e-12]\n",
      " [-7.83081997e-13 -1.55285726e-12 -1.97354892e-12  1.68415687e-12\n",
      "   2.63889632e-12 -2.82744817e-13  9.95708692e-13 -1.76601440e-13\n",
      "  -2.64582763e-13 -1.29997145e-12]\n",
      " [-8.53438608e-13 -1.69237493e-12 -2.15086396e-12  1.83547125e-12\n",
      "   2.87599001e-12 -3.08148244e-13  1.08516891e-12 -1.92468334e-13\n",
      "  -2.88354407e-13 -1.41676839e-12]\n",
      " [-8.84307894e-13 -1.75358895e-12 -2.22866175e-12  1.90186113e-12\n",
      "   2.98001596e-12 -3.19294115e-13  1.12442000e-12 -1.99430006e-13\n",
      "  -2.98784326e-13 -1.46801359e-12]\n",
      " [-8.99971388e-13 -1.78464977e-12 -2.26813740e-12  1.93554826e-12\n",
      "   3.03280014e-12 -3.24949681e-13  1.14433653e-12 -2.02962453e-13\n",
      "  -3.04076607e-13 -1.49401610e-12]]\n",
      "Bias gradient of second hidden layer:\n",
      " [-1.10369828e-12 -2.18864166e-12 -2.78157658e-12  2.37369910e-12\n",
      "  3.71933633e-12 -3.98508672e-13  1.40338045e-12 -2.48907146e-13\n",
      " -3.72910552e-13 -1.83221712e-12]\n",
      "\n",
      "Weight gradient of output layer:\n",
      " [[-4.65905682e-10]\n",
      " [-4.65609780e-10]\n",
      " [-4.64312155e-10]\n",
      " [-4.64635634e-10]\n",
      " [-4.61904508e-10]\n",
      " [-4.63453747e-10]\n",
      " [-4.66927596e-10]\n",
      " [-4.68589682e-10]\n",
      " [-4.62955017e-10]\n",
      " [-4.60081100e-10]]\n",
      "Bias gradient of output layer:\n",
      " [-4.70055994e-10]\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "feed_forward = FeedForward(2, 10, 10, 1)\n",
    "# Fit the model\n",
    "feed_forward.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "oriental-agency",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train is: [[0.61730492]]\n",
      "y_predict is: [[0.61730492]]\n"
     ]
    }
   ],
   "source": [
    "# y_pred is the same as y_train\n",
    "print('y_train is:', y_train)\n",
    "print('y_predict is:', feed_forward.predict(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-techno",
   "metadata": {},
   "source": [
    "## 7b. Write to my_autograd.dat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
