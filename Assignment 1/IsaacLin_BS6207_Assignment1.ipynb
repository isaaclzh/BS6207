{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "powerful-external",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "selective-alabama",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pytorch\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-treat",
   "metadata": {},
   "source": [
    "## 1. Build the model\n",
    "- Input (x1,x2): 2 nodes\n",
    "- First hidden layer: 10 nodes, with weights (w) and bias (b), sigmoid activation function\n",
    "- Second hidden layer: 10 nodes, with weights (w) and bias (b), sigmoid activation function\n",
    "- Output (predict): 1 node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "decimal-outside",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\n",
      " Sequential(\n",
      "  (0): Linear(in_features=2, out_features=10, bias=True)\n",
      "  (1): Sigmoid()\n",
      "  (2): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (3): Sigmoid()\n",
      "  (4): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Building Neural Network using nn.Sequential\n",
    "# Hyperparameters for our network\n",
    "input_size = 2\n",
    "hidden_sizes = [10,10]\n",
    "output_size = 1\n",
    "\n",
    "model = nn.Sequential(\n",
    "    # Input with 2 nodes to first hidden layer with 10 nodes\n",
    "    nn.Linear(input_size, hidden_sizes[0]), \n",
    "    # Pass through Sigmoid activation function\n",
    "    nn.Sigmoid(),\n",
    "    # First hidden layer with 10 nodes to second hidden layer with 10 nodes\n",
    "    nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "    # Pass through Sigmoid activation function\n",
    "    nn.Sigmoid(),\n",
    "    # Second hidden layer with 10 nodes to output layer with 1 node\n",
    "    nn.Linear(hidden_sizes[1], output_size),\n",
    ")\n",
    "\n",
    "print('Model:\\n',model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-shoot",
   "metadata": {},
   "source": [
    "## 2. Generate the random number x1, x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "complete-privilege",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input(x1,x2):\n",
      "  tensor([[0.1117, 0.8158]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(100)\n",
    "\n",
    "# Batch size of 1\n",
    "x = torch.rand(1,2)\n",
    "print(\"input(x1,x2):\\n \",x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-terrorist",
   "metadata": {},
   "source": [
    "## 3. Generate the label y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "realistic-landscape",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of y_true is:\n",
      " tensor([[0.3390]])\n"
     ]
    }
   ],
   "source": [
    "# Generate y-labels\n",
    "y_true = torch.empty(1,1)\n",
    "y_true[0,0] = (x[0][0]**2 + x[0][1]**2)/2\n",
    "print('The value of y_true is:\\n', y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saving-marks",
   "metadata": {},
   "source": [
    "## 4. Build a loss function L = (y_predict - y_true)^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "desperate-university",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(y_true, y_pred):\n",
    "    return torch.sum((y_pred - y_true) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-investment",
   "metadata": {},
   "source": [
    "## 5. Forward / Backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "rotary-italian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1 | Loss=0.905356\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    # Forward propogation\n",
    "    y_pred = model(x)\n",
    "    # Show loss at each epoch\n",
    "    loss = loss_fn(y_pred, y_true)\n",
    "    print(f\"Epoch = {i + 1} | Loss=%f\" % (loss.item())) \n",
    "    # Backward propagation and update the gradients\n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "tender-contributor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6125]], grad_fn=<AddmmBackward>)\n",
      "tensor([[0.3390]])\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)\n",
    "print(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-spectacular",
   "metadata": {},
   "source": [
    "## 6a. Calculate the gradients of the loss wrt weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "relative-phoenix",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights gradient of first hidden layer:\n",
      " tensor([[-0.0001,  0.0012, -0.0006,  0.0019,  0.0014,  0.0005,  0.0007,  0.0010,\n",
      "         -0.0016,  0.0005],\n",
      "        [-0.0011,  0.0090, -0.0043,  0.0139,  0.0103,  0.0035,  0.0050,  0.0069,\n",
      "         -0.0113,  0.0039]])\n",
      "Bias gradient of first hidden layer:\n",
      " tensor([-0.0013,  0.0110, -0.0052,  0.0170,  0.0126,  0.0043,  0.0062,  0.0085,\n",
      "        -0.0139,  0.0047])\n"
     ]
    }
   ],
   "source": [
    "# Check the weights and biases of first hidden layer\n",
    "print('Weights gradient of first hidden layer:\\n',model[0].weight.grad.t())\n",
    "print('Bias gradient of first hidden layer:\\n',model[0].bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dedicated-croatia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights gradient of second hidden layer:\n",
      " tensor([[ 0.0584,  0.0033,  0.0085,  0.0147,  0.0071, -0.0085, -0.0718,  0.0740,\n",
      "          0.0567,  0.0014],\n",
      "        [ 0.0844,  0.0048,  0.0123,  0.0212,  0.0102, -0.0122, -0.1037,  0.1069,\n",
      "          0.0819,  0.0020],\n",
      "        [ 0.0398,  0.0022,  0.0058,  0.0100,  0.0048, -0.0058, -0.0489,  0.0504,\n",
      "          0.0386,  0.0009],\n",
      "        [ 0.0620,  0.0035,  0.0091,  0.0155,  0.0075, -0.0090, -0.0762,  0.0785,\n",
      "          0.0601,  0.0015],\n",
      "        [ 0.0735,  0.0042,  0.0108,  0.0184,  0.0089, -0.0106, -0.0903,  0.0931,\n",
      "          0.0713,  0.0017],\n",
      "        [ 0.0678,  0.0038,  0.0099,  0.0170,  0.0082, -0.0098, -0.0833,  0.0858,\n",
      "          0.0658,  0.0016],\n",
      "        [ 0.0603,  0.0034,  0.0088,  0.0151,  0.0073, -0.0087, -0.0740,  0.0763,\n",
      "          0.0585,  0.0014],\n",
      "        [ 0.0761,  0.0043,  0.0111,  0.0191,  0.0092, -0.0110, -0.0935,  0.0964,\n",
      "          0.0738,  0.0018],\n",
      "        [ 0.0511,  0.0029,  0.0075,  0.0128,  0.0062, -0.0074, -0.0628,  0.0647,\n",
      "          0.0496,  0.0012],\n",
      "        [ 0.0568,  0.0032,  0.0083,  0.0142,  0.0069, -0.0082, -0.0698,  0.0719,\n",
      "          0.0551,  0.0013]])\n",
      "Bias gradient of second hidden layer:\n",
      " tensor([ 0.1156,  0.0065,  0.0169,  0.0290,  0.0140, -0.0167, -0.1420,  0.1463,\n",
      "         0.1121,  0.0027])\n"
     ]
    }
   ],
   "source": [
    "# Check the weights and biases of second hidden layer\n",
    "print('Weights gradient of second hidden layer:\\n',model[2].weight.grad.t())\n",
    "print('Bias gradient of second hidden layer:\\n',model[2].bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cardiovascular-growing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights gradient of output layer:\n",
      " tensor([[-0.8713],\n",
      "        [-0.9655],\n",
      "        [-1.0034],\n",
      "        [-1.3373],\n",
      "        [-1.1203],\n",
      "        [-0.8077],\n",
      "        [-0.7448],\n",
      "        [-1.1001],\n",
      "        [-1.2675],\n",
      "        [-1.0747]])\n",
      "Bias gradient of output layer:\n",
      " tensor([-1.9030])\n"
     ]
    }
   ],
   "source": [
    "# Check the weights and biases of output layer\n",
    "print('Weights gradient of output layer:\\n',model[4].weight.grad.t())\n",
    "print('Bias gradient of output layer:\\n',model[4].bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-polish",
   "metadata": {},
   "source": [
    "## 6b. Write to torch_autograd.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-sheffield",
   "metadata": {},
   "source": [
    "## 7a. Implement forward propagation and backpropagation algorithm from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-marker",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "pending-geology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, D_in, H1,H2, D_out):\n",
    "        \n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H1)\n",
    "        self.linear2 = torch.nn.Linear(H1, H2)\n",
    "        self.linear3 = torch.nn.Linear(H2, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        sigmoid = torch.nn.Sigmoid()        \n",
    "        h1_sigmoid = sigmoid(self.linear1(x))       \n",
    "        h2_sigmoid = sigmoid(self.linear2(h1_sigmoid))  \n",
    "        y_pred = self.linear3(h2_sigmoid)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-cheat",
   "metadata": {},
   "source": [
    "### Construct the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "indian-prerequisite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct our model by instantiating the class defined above\n",
    "model = TwoLayerNet(2, 10, 10, 1)\n",
    "\n",
    "# Save the gradients of the weights and biases\n",
    "b3 = model.linear3.bias\n",
    "w3 = model.linear3.weight.t()\n",
    "b2 = model.linear2.bias\n",
    "w2 = model.linear2.weight.t()\n",
    "b1 = model.linear1.bias\n",
    "w1 = model.linear1.weight.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "another-clothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "        return 1.0 / (1 + np.exp(-x))\n",
    "    \n",
    "def sigmoid_derivationx(y):\n",
    "    return y * (1 - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "political-earthquake",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1 | Loss=0.905356\n",
      "First hidden layer weights gradient: \n",
      " tensor([[-0.0001,  0.0012, -0.0006,  0.0019,  0.0014,  0.0005,  0.0007,  0.0010,\n",
      "         -0.0016,  0.0005],\n",
      "        [-0.0011,  0.0090, -0.0043,  0.0139,  0.0103,  0.0035,  0.0050,  0.0069,\n",
      "         -0.0113,  0.0039]], grad_fn=<MmBackward>)\n",
      "First hidden layer bias gradient: \n",
      " tensor([[-0.0013,  0.0110, -0.0052,  0.0170,  0.0126,  0.0043,  0.0062,  0.0085,\n",
      "         -0.0139,  0.0047]], grad_fn=<MulBackward0>)\n",
      "Second hidden layer weight gradient:\n",
      " tensor([[ 0.0584,  0.0033,  0.0085,  0.0147,  0.0071, -0.0085, -0.0718,  0.0740,\n",
      "          0.0567,  0.0014],\n",
      "        [ 0.0844,  0.0048,  0.0123,  0.0212,  0.0102, -0.0122, -0.1037,  0.1069,\n",
      "          0.0819,  0.0020],\n",
      "        [ 0.0398,  0.0022,  0.0058,  0.0100,  0.0048, -0.0058, -0.0489,  0.0504,\n",
      "          0.0386,  0.0009],\n",
      "        [ 0.0620,  0.0035,  0.0091,  0.0155,  0.0075, -0.0090, -0.0762,  0.0785,\n",
      "          0.0601,  0.0015],\n",
      "        [ 0.0735,  0.0042,  0.0108,  0.0184,  0.0089, -0.0106, -0.0903,  0.0931,\n",
      "          0.0713,  0.0017],\n",
      "        [ 0.0678,  0.0038,  0.0099,  0.0170,  0.0082, -0.0098, -0.0833,  0.0858,\n",
      "          0.0658,  0.0016],\n",
      "        [ 0.0603,  0.0034,  0.0088,  0.0151,  0.0073, -0.0087, -0.0740,  0.0763,\n",
      "          0.0585,  0.0014],\n",
      "        [ 0.0761,  0.0043,  0.0111,  0.0191,  0.0092, -0.0110, -0.0935,  0.0964,\n",
      "          0.0738,  0.0018],\n",
      "        [ 0.0511,  0.0029,  0.0075,  0.0128,  0.0062, -0.0074, -0.0628,  0.0647,\n",
      "          0.0496,  0.0012],\n",
      "        [ 0.0568,  0.0032,  0.0083,  0.0142,  0.0069, -0.0082, -0.0698,  0.0719,\n",
      "          0.0551,  0.0013]], grad_fn=<MmBackward>)\n",
      "Second hidden layer bias gradient: \n",
      " tensor([[ 0.1156,  0.0065,  0.0169,  0.0290,  0.0140, -0.0167, -0.1420,  0.1463,\n",
      "          0.1121,  0.0027]], grad_fn=<MulBackward0>)\n",
      "Output layer bias gradient: \n",
      " tensor([[-1.9030]], grad_fn=<MulBackward0>)\n",
      "Output layer weight gradient: \n",
      " tensor([[-0.8713],\n",
      "        [-0.9655],\n",
      "        [-1.0034],\n",
      "        [-1.3373],\n",
      "        [-1.1203],\n",
      "        [-0.8077],\n",
      "        [-0.7448],\n",
      "        [-1.1001],\n",
      "        [-1.2675],\n",
      "        [-1.0747]], grad_fn=<MmBackward>)\n",
      "tensor([[-0.6125]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "for t in range(1):\n",
    "    # Forward Propagation \n",
    "    h1 = x.mm(w1) + b1  \n",
    "    h1_sigmoid = torch.sigmoid(h1)   \n",
    "    h2 = h1_sigmoid.mm(w2) + b2   \n",
    "    h2_sigmoid = torch.sigmoid(h2)  \n",
    "    y_pred  = h2_sigmoid.mm(w3) + b3 \n",
    "         \n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y_true).pow(2).sum().item()\n",
    "    print(f\"Epoch = {i + 1} | Loss=%f\" % (loss))\n",
    "    \n",
    "    # Backward Propagation\n",
    "    grad_y_pred = -2.0 *(y_true - y_pred)    \n",
    "    delta = grad_y_pred#N*D_out                           \n",
    "    grad_w3 = h2_sigmoid.T.mm(delta)                   \n",
    "    grad_b3 = delta  \n",
    "    \n",
    "    delta = delta.mm(w3.T)*sigmoid_derivationx(h2_sigmoid)\n",
    "    grad_w2 = h1_sigmoid.T.mm(delta)\n",
    "    grad_b2 = delta  \n",
    "    \n",
    "    delta = delta.mm(w2.T)*sigmoid_derivationx(h1_sigmoid)\n",
    "    grad_w1 = x.T.mm(delta)\n",
    "    grad_b1 = delta \n",
    "     \n",
    "    print(\"First hidden layer weights gradient: \\n\",grad_w1)\n",
    "    print(\"First hidden layer bias gradient: \\n\", grad_b1)\n",
    "    print(\"Second hidden layer weight gradient:\\n\", grad_w2)\n",
    "    print(\"Second hidden layer bias gradient: \\n\",grad_b2)\n",
    "    print(\"Output layer bias gradient: \\n\",grad_b3)   \n",
    "    print(\"Output layer weight gradient: \\n\",grad_w3)  \n",
    "    \n",
    "    # Update weights\n",
    "    w3 = w3 - learning_rate * grad_w3  \n",
    "    w2 = w2 - learning_rate * grad_w2\n",
    "    w1 = w1 - learning_rate * grad_w1\n",
    "    \n",
    "    b3 = b3 - learning_rate * grad_b3\n",
    "    b2 = b2 - learning_rate * grad_b2\n",
    "    b1 = b1 - learning_rate * grad_b1  \n",
    "    \n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-paragraph",
   "metadata": {},
   "source": [
    "\n",
    "## 7b. Write to my_autograd.dat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
